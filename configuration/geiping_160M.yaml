model_config:
  execution_mode: geiping_std
  d_model: 768
  d_ffn: 3072
  n_repeats: 1
  n_heads: 12
  d_head: 64
  n_layers: 4
  vocab_size: 50368
  max_seq_len: 1024
  n_expert_shared_attn: 0
  n_expert_shared_ffn: 0
  collect_reg_loss: False
  enable_early_exit: False
  norm_structure: pre
  rescaling_method: none


# class NormStructure(Enum):
#     Peri = 0
#     Pre = 1
#     Post = 2
#     moeut = 3
# class RescaleMethod(Enum):
#     none = 0
#     cum_avg_prot_emb = 1
#     cum_avg_no_prot_emb = 2
#     sqrt_prot_emb = 3
#     sqrt_no_prot_emb = 4
    
train:
  max_duration: "3171ba"
  warmup: "634ba"
  cooldown: "634ba"
  device_train_batch_size: 1024

callbacks:
  lr_monitor: {}
  clean_metrics:
    interval: 100
  speed_monitor:
    window_size: 10
    gpu_flops_available:
        fp64: 67e12,
        fp32: 67e12,
        tf32: 989e12 / 2,
        fp16: 1.979e15 / 2,
        amp_fp16: 1.979e15 / 2,
        bf16: 1.979e15 / 2,
        amp_bf16: 1.979e15 / 2,
        fp8: 3.958e15 / 2,
        amp_fp8: 3.958e15 / 2,
        int8: 3.958e15 / 2,
  memory_monitor: {}
  runtime_estimator: {}
  optimizer_monitor: {}
  entropy_callback:
    log_interval: 100
  # layer_usage_monitor:
  #   log_interval: 20
  # # # activation_monitor_c: {}

data_config:
  path: "/nfs-share/pa511/code_bases/dyna_project/dyna/configuration/streams"
  dataset:
    max_seq_len: ${model_config.max_seq_len}
  num_workers: 24


trainer_config:
  run_name: "geiping"
  max_duration: ${train.max_duration}
  train_dataloader_label: "train"
  device_train_microbatch_size: auto
  precision: "amp_bf16"
  device: "gpu"
  save_folder: "s3://loop-llm/dyna/"
  save_interval: "100ba"

scheduler_config:
  name: "wsld"
  t_warmup: ${train.warmup}
  t_max: ${train.max_duration}
  t_cooldown: ${train.cooldown}
  alpha_f: 1.0