defaults:
  - _self_
  - scheduler_config: wsld
  - callbacks: default_100
  - data_config: smoll
  - optimizer_config: adamw

model_config:
  execution_mode: transformer
  d_model: 128
  d_ffn: 512
  n_repeats: 1
  n_heads: 4
  d_head: 32
  n_layers: 6
  vocab_size: 50368
  max_seq_len: 1024
  n_expert_shared_attn: 0
  n_expert_shared_ffn: 0
  collect_reg_loss: False
  enable_early_exit: False
  norm_structure: pre
  rescaling_method: none

train:
  max_duration: "3171ba"
  warmup: "634ba"
  cooldown: "634ba"
  device_train_batch_size: 1024

trainer_config:
  run_name: "base"
  max_duration: ${train.max_duration}
  train_dataloader_label: "train"
  device_train_microbatch_size: auto
  precision: "amp_bf16"
  device: "gpu"
  save_folder: "s3://loop-llm/dyna/"
  save_interval: "100ba"
  parallelism_config:
    pipeline_parallel_size: null
    tensor_parallel_size: null


