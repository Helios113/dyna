model_config:
  d_model: 412
  n_layers: 16
  n_heads: 4
  n_experts_ffn: 155
  n_experts_attn: 8
  d_head: 84
  k_ffn: 12
  k_attn: 2
  d_expert_ffn: 128
  n_group: 2
  vocab_size: 49152
  max_seq_len: 1024
  scale_add: False
  prot_emb: True
  n_expert_shared_attn: 0
  n_expert_shared_ffn: 0
  collect_reg_loss: False
  enable_early_exit: False
  use_simple_residual: True
train:
  max_duration: "1495ba"
  warmup: "299ba"
  cooldown: "299ba"
  device_train_batch_size: 1024

callbacks:
  clean_metrics: {}
  speed_monitor:
    window_size: 10
  lr_monitor: {}
  memory_monitor: {}
  runtime_estimator: {}
  optimizer_monitor: {}
  expert_selection_callback:
    attn_experts: ${model_config.n_experts_attn}
    ffn_experts: ${model_config.n_experts_ffn}
  entropy_callback: {}
  # activation_monitor_c: {}


  

data_config:
  path: "/nfs-share/pa511/code_bases/dyna_project/dyna/configuration/streams"
  dataset:
    max_seq_len: ${model_config.max_seq_len}

trainer_config:
  max_duration: ${train.max_duration}
  eval_interval: 100ba
  eval_subset_num_batches: -1
  train_dataloader_label: "train"
  device_train_microbatch_size: 32
  precision: "amp_bf16"
  device: "gpu"
  parallelism_config:
    pipeline_parallel_size: null
    tensor_parallel_size: null

scheduler_config:
  name: "wsld"
  t_warmup: ${train.warmup}
  t_max: ${train.max_duration}
  t_cooldown: ${train.cooldown}
  alpha_f: 1.0
