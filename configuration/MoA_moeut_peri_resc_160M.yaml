defaults:
  - _self_
  - scheduler_config: wsld
  - callbacks: default_100
  - data_config: smoll
  - optimizer_config: adamw

model_config:
  execution_mode: moe
  d_model: 768
  n_repeats: 6
  n_heads: 3
  n_experts_ffn: 192
  n_experts_attn: 10
  d_head: 128
  k_ffn: 12
  k_attn: 2
  d_expert_ffn: 128
  n_layers: 2
  vocab_size: 50368
  max_seq_len: 1024
  n_expert_shared_attn: 0
  n_expert_shared_ffn: 0
  collect_reg_loss: False
  enable_early_exit: False
  norm_structure: "peri"
  rescaling_method: "cum_avg_prot_emb"

train:
  max_duration: "3171ba"
  warmup: "634ba"
  cooldown: "634ba"
  device_train_batch_size: 1024

trainer_config:
  run_name: "base"
  max_duration: ${train.max_duration}
  train_dataloader_label: "train"
  device_train_microbatch_size: auto
  precision: "amp_bf16"
  device: "gpu"
  save_folder: "s3://loop-llm/dyna/"
  load_path: "s3://loop-llm/dyna/1_base_03sep25_pz0bmm4a_dim~768_n_l~2_n_r~6_n_h~3_d_hd~128_n_e_ffn~192_n_e_attn~10_ee~False_mode~moe_norm~peri_rescale~cum_avg_prot_emb-ba100.pt"
  save_interval: "100ba"
  parallelism_config:
    pipeline_parallel_size: null
    tensor_parallel_size: null


