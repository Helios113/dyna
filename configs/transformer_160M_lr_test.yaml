defaults:
    - _self_
    - execute_config/scheduler_config: wsld
    - execute_config/callbacks: test_lr
    - execute_config/data_config: smoll_fast
    - execute_config/optimizer_config: adamw
    - execute_config/fsdp_config: null
execute_config:
    model_config:
        execution_mode: transformer
        d_model: 768
        d_ffn: 3072
        n_repeats: 1
        n_heads: 12
        d_head: 64
        n_layers: 12
        vocab_size: 50368
        max_seq_len: 1024
        n_expert_shared_attn: 0
        n_expert_shared_ffn: 0
        collect_reg_loss: False
        enable_early_exit: False
        norm_structure: pre
        rescaling_method: complete_p
        loop_normalization: False
        sample_iterations: False
        loop_rope_theta_rebase: False
        transformer_type: dyna
        layer_type: simple
        # norms:
        #     norm_type: dynamic_tanh
        #     attn_eps: 3.0
        #     ffn_eps: 3.0
        base_depth: 12
        current_depth: 12
        base_width: 768
        current_width: 768
        cp_alpha: 1
    train:
        max_duration: "150ba"
        warmup: "634ba"
        cooldown: "634ba"
        device_train_batch_size: 1024
    trainer_config:
        run_name: "LrTest"
        max_duration: ${train.max_duration}
        train_dataloader_label: "train"
        device_train_microbatch_size: 32
        precision: "amp_bf16"
        device: "gpu"
        # save_folder: "s3://loop-llm/dyna/"
        save_folder: null
        save_interval: "100ba"
