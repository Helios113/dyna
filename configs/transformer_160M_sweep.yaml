defaults:
    - _self_
    - execute_config/scheduler_config: wsld
    - execute_config/callbacks: default_100
    - execute_config/data_config: smoll
    - execute_config/optimizer_config: adamw
    - execute_config/fsdp_config: null

sweep_config:
    sweeps:
        param1:
            name: "optimizer_config.lr"
            min_val: 1e-3
            max_val: 1e-2
            step_size: 1e-3
            type: "continious"
    run_steps: 10
execute_config:
    model_config:
        execution_mode: transformer
        d_model: 768
        d_ffn: 3072
        n_repeats: 1
        n_heads: 12
        d_head: 64
        n_layers: 12
        vocab_size: 50368
        max_seq_len: 1024
        n_expert_shared_attn: 0
        n_expert_shared_ffn: 0
        collect_reg_loss: False
        enable_early_exit: False
        norm_structure: pre
        rescaling_method: none
        loop_normalization: False
        sample_iterations: False
        loop_rope_theta_rebase: False
        transformer_type: dyna
        layer_type: simple
        # norms:
        #     norm_type: dynamic_tanh
        #     attn_eps: 3.0
        #     ffn_eps: 3.0

    train:
        max_duration: "500ba"
        warmup: "634ba"
        cooldown: "634ba"
        device_train_batch_size: 1024
    trainer_config:
        run_name: "sweepWandb"
        max_duration: ${train.max_duration}
        train_dataloader_label: "train"
        device_train_microbatch_size: 32
        precision: "amp_bf16"
        device: "gpu"
        # save_folder: "s3://loop-llm/dyna/"
        save_folder: null
        save_interval: "100ba"
