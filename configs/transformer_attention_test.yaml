defaults:
    - _self_
    - execute_config/scheduler_config: wsld
    - execute_config/callbacks: default_100
    - execute_config/data_config: smoll_tune
    - execute_config/optimizer_config: adamw
    - execute_config/fsdp_config: null

execute_config:
    model_config:
        execution_mode: transformer
        d_model: 256
        d_ffn: 1024
        n_repeats: 1
        n_heads: 4
        d_head: 64
        n_layers: 2
        vocab_size: 50368
        max_seq_len: 1024
        n_expert_shared_attn: 0
        n_expert_shared_ffn: 0
        collect_reg_loss: False
        enable_early_exit: False
        norm_structure: pre
        rescaling_method: complete_p
        loop_normalization: False
        sample_iterations: False
        loop_rope_theta_rebase: False
        transformer_type: dyna
        layer_type: simple
        base_depth: 2
        current_depth: 2
        base_width: 256
        current_width: 256
        cp_alpha: 1
    train:
        max_duration: "521ba"
        warmup: "100ba"
        cooldown: "100ba"
        device_train_batch_size: 1024
    trainer_config:
        run_name: "attentionTest"
        max_duration: ${train.max_duration}
        train_dataloader_label: "train"
        device_train_microbatch_size: 32
        precision: "amp_bf16"
        device: "gpu"
        # save_folder: "s3://loop-llm/dyna/"
        save_folder: null
        save_interval: "100ba"
