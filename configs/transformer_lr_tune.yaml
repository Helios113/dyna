defaults:
    - _self_
    - execute_config/scheduler_config: wsld
    - execute_config/callbacks: default_100
    - execute_config/data_config: smoll
    - execute_config/optimizer_config: adamw
    - execute_config/fsdp_config: null
sweep_config:
    method: bayes
    metric:
        name: loss/train/total
        goal: minimize

    parameters:
        optimizer_config.lr:
            distribution: log_uniform_values
            min: 1e-4
            max: 1e-1
        model_config.init_sigma:
            distribution: uniform
            min: 0.01
            max: 0.1

    early_terminate:
        type: hyperband
        s: 2
        eta: 3
        max_iter: 500
        min_iter: 100


execute_config:
    model_config:
        execution_mode: transformer
        d_model: 256
        d_ffn: 1024
        n_repeats: 1
        n_heads: 4
        d_head: 64
        n_layers: 2
        vocab_size: 50368
        max_seq_len: 1024
        n_expert_shared_attn: 0
        n_expert_shared_ffn: 0
        collect_reg_loss: False
        enable_early_exit: False
        norm_structure: pre
        rescaling_method: complete_p
        loop_normalization: False
        sample_iterations: False
        loop_rope_theta_rebase: False
        transformer_type: dyna
        layer_type: simple
        base_depth: 2
        current_depth: 2
        base_width: 256
        current_width: 256
        cp_alpha: 1
        sqrt_attention_scale: False
    train:
        max_duration: "521ba"
        warmup: "100ba"
        cooldown: "100ba"
        device_train_batch_size: 1024
    trainer_config:
        run_name: "sweep_init"
        max_duration: ${train.max_duration}
        train_dataloader_label: "train"
        device_train_microbatch_size: 32
        precision: "amp_bf16"
        device: "gpu"
        # save_folder: "s3://loop-llm/dyna/"
        save_folder: null
        save_interval: "100ba"
