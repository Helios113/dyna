defaults:
    - _self_
    - scheduler_config: wsld
    - callbacks: default_100
    - data_config: smoll
    - optimizer_config: adamw
    - fsdp_config: null

model_config:
    execution_mode: transformer
    d_model: 1024
    d_ffn: 4096
    n_repeats: 1
    n_heads: 8
    d_head: 512
    n_layers: 9
    vocab_size: 50368
    max_seq_len: 1024
    n_expert_shared_attn: 0
    n_expert_shared_ffn: 0
    collect_reg_loss: False
    enable_early_exit: False
    norm_structure: pre
    rescaling_method: none
    loop_normalization: False
    sample_iterations: False
    loop_rope_theta_rebase: False
    transformer_type: dyna

train:
    max_duration: "6240ba"
    warmup: "1248ba"
    cooldown: "1248ba"
    device_train_batch_size: 1024

trainer_config:
    run_name: "loopSmaller"
    max_duration: ${train.max_duration}
    train_dataloader_label: "train"
    device_train_microbatch_size: auto
    precision: "amp_bf16"
    device: "gpu"
    save_folder: "s3://loop-llm/dyna/"
    save_interval: "100ba"
