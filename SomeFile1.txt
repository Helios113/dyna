/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/model/cvmm.py:402: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("mylib::cvmm_triton", cvmm_triton)
[2025-07-29 18:07:21,405][streaming.base.dataset][INFO] - Because `predownload` was not specified, it will default to 8*batch_size if batch_size is not None, otherwise 64.
[2025-07-29 18:07:21,601][composer.utils.reproducibility][INFO] - Setting seed to 1968843327
[2025-07-29 18:07:21,635][composer.trainer.trainer][INFO] - Run name: 1753808841-pastel-stingray
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: preslav-aleksandrov (camlsys). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.7
wandb: Run data is saved locally in /nfs-share/pa511/code_bases/dyna_project/dyna/wandb/run-20250729_180722-9c43iczg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ttkunurg_29jul25_t5dwfglh_scale_add=True_prot_emb=True_d_model=412_n_layers=16_n_heads=4_n_experts_ffn=180_n_experts_attn=10_ff_expert_size=10_dropout=0.0
wandb: ‚≠êÔ∏è View project at https://wandb.ai/camlsys/dyna
wandb: üöÄ View run at https://wandb.ai/camlsys/dyna/runs/9c43iczg
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/callbacks/speed_monitor.py:290: UserWarning: gpu_flop count not found for nvidia h100 nvl with precision=amp_bf16 so MFU cannot be calculated and reported. gpu_flops_available can be manually overridden by setting gpu_flops_available in SpeedMonitor or nvidia h100 nvl can be added to GPU_AVAILABLE_FLOPS in composer/callbacks/speed_monitor.py
  self.gpu_flops_available = get_gpu_flops_available(state)
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/trainer/trainer.py:1556: UserWarning: Specifying `eval_interval=100ba` without an `eval_dataloader` has no effect. If trying to run an evaluator, make sure `eval_dataloader` is specified. Otherwise, set `eval_interval` to 0 or default value 1.
  warnings.warn(
[2025-07-29 18:07:23,114][composer.trainer.trainer][INFO] - Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
[2025-07-29 18:07:23,115][composer.trainer.trainer][INFO] - Setting seed to 1968843327
[2025-07-29 18:07:23,115][composer.utils.reproducibility][INFO] - Setting seed to 1968843327
[2025-07-29 18:07:23,115][composer.trainer.trainer][INFO] - Using precision Precision.AMP_BF16
******************************
Config:
composer_commit_hash: None
composer_version: 0.31.0
node_name: unknown because NODENAME environment variable not set
num_gpus_per_node: 1
num_nodes: 1
rank_zero_seed: 1968843327
time/remaining_estimate_unit: hours

******************************
[2025-07-29 18:07:23,186][streaming.base.dataset][INFO] - Because `num_canonical_nodes` was not specified, and `shuffle_algo` is py1e, it will default to be equal to physical nodes.
[2025-07-29 18:07:23,186][streaming.base.dataset][INFO] - Because `shuffle_block_size` was not specified, it will default to max(4_000_000 // num_canonical_nodes, 1 << 18) if num_canonical_nodes is not None, otherwise 262144.
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6100, 0.6783, 0.4050,  ..., 0.3942, 0.4163, 0.4883],
          [0.5960, 0.5660, 0.6469,  ..., 0.5237, 0.5694, 0.5118],
          [0.3684, 0.4668, 0.3702,  ..., 0.4721, 0.4330, 0.4801],
          [0.5031, 0.4064, 0.5470,  ..., 0.5443, 0.4999, 0.4278]],

         [[0.6100, 0.6783, 0.4050,  ..., 0.3942, 0.4163, 0.4883],
          [0.5960, 0.5660, 0.6469,  ..., 0.5237, 0.5694, 0.5118],
          [0.3684, 0.4668, 0.3702,  ..., 0.4721, 0.4330, 0.4801],
          [0.5031, 0.4064, 0.5470,  ..., 0.5443, 0.4999, 0.4278]],

         [[0.5043, 0.3522, 0.4102,  ..., 0.4268, 0.5189, 0.6478],
          [0.4340, 0.3495, 0.4450,  ..., 0.5110, 0.3961, 0.5583],
          [0.6352, 0.4235, 0.3522,  ..., 0.4857, 0.4980, 0.4340],
          [0.5898, 0.6105, 0.6183,  ..., 0.5443, 0.4325, 0.4813]],

         ...,

         [[0.4545, 0.5171, 0.4827,  ..., 0.4576, 0.3603, 0.4676],
          [0.5879, 0.5475, 0.5407,  ..., 0.3576, 0.5076, 0.6016],
          [0.4278, 0.4369, 0.5060,  ..., 0.5194, 0.5950, 0.5631],
          [0.4518, 0.6118, 0.4244,  ..., 0.4952, 0.5689, 0.6049]],

         [[0.4828, 0.5761, 0.4036,  ..., 0.5770, 0.4659, 0.4149],
          [0.4149, 0.4073, 0.5477,  ..., 0.5187, 0.4671, 0.6025],
          [0.6002, 0.4230, 0.4022,  ..., 0.3789, 0.3684, 0.4744],
          [0.3780, 0.4721, 0.3639,  ..., 0.6361, 0.5988, 0.4414]],

         [[0.4244, 0.5346, 0.5187,  ..., 0.5161, 0.5983, 0.5170],
          [0.4405, 0.5277, 0.5627,  ..., 0.3979, 0.3478, 0.5339],
          [0.4017, 0.5631, 0.4130,  ..., 0.3049, 0.5363, 0.3166],
          [0.4106, 0.4969, 0.4321,  ..., 0.3821, 0.4730, 0.6072]]],


        [[[0.4596, 0.4990, 0.7082,  ..., 0.3877, 0.3803, 0.4927],
          [0.4610, 0.3956, 0.5636,  ..., 0.6859, 0.5631, 0.5221],
          [0.5378, 0.5184, 0.4706,  ..., 0.5307, 0.5530, 0.5607],
          [0.4163, 0.6109, 0.5143,  ..., 0.4976, 0.4564, 0.3998]],

         [[0.4739, 0.5239, 0.5257,  ..., 0.4378, 0.5808, 0.3984],
          [0.4106, 0.5684, 0.4182,  ..., 0.5351, 0.4666, 0.4395],
          [0.4879, 0.4410, 0.5552,  ..., 0.4426, 0.4412, 0.5472],
          [0.5631, 0.3355, 0.5832,  ..., 0.5395, 0.6575, 0.5837]],

         [[0.7813, 0.4799, 0.6151,  ..., 0.3840, 0.4036, 0.5431],
          [0.6397, 0.4287, 0.4905,  ..., 0.5475, 0.7033, 0.5020],
          [0.4941, 0.4230, 0.4647,  ..., 0.5523, 0.5424, 0.5655],
          [0.4746, 0.4045, 0.5063,  ..., 0.5761, 0.4755, 0.5751]],

         ...,

         [[0.3789, 0.5278, 0.5217,  ..., 0.3900, 0.6077, 0.6192],
          [0.5997, 0.4311, 0.3730,  ..., 0.4192, 0.5110, 0.5889],
          [0.3891, 0.4111, 0.5305,  ..., 0.4311, 0.3835, 0.4720],
          [0.4087, 0.6160, 0.4460,  ..., 0.5765, 0.6316, 0.5879]],

         [[0.5595, 0.5134, 0.5494,  ..., 0.5011, 0.3872, 0.3886],
          [0.5751, 0.4773, 0.6406,  ..., 0.4301, 0.4417, 0.4235],
          [0.4630, 0.5097, 0.6540,  ..., 0.5931, 0.6243, 0.5174],
          [0.4177, 0.4622, 0.5655,  ..., 0.5516, 0.6151, 0.5903]],

         [[0.4121, 0.3372, 0.5013,  ..., 0.4292, 0.6049, 0.4869],
          [0.4417, 0.4886, 0.4625,  ..., 0.4364, 0.4426, 0.5516],
          [0.5383, 0.6316, 0.3914,  ..., 0.3785, 0.5595, 0.4676],
          [0.6142, 0.7042, 0.4762,  ..., 0.3558, 0.3522, 0.4955]]]],
       device='cuda:0')
tensor([[[[0.6100, 0.6783, 0.4050,  ..., 0.3942, 0.4163, 0.4883],
          [0.5960, 0.5660, 0.6469,  ..., 0.5237, 0.5694, 0.5118],
          [0.3684, 0.4668, 0.3702,  ..., 0.4721, 0.4330, 0.4801],
          [0.5031, 0.4064, 0.5470,  ..., 0.5443, 0.4999, 0.4278]],

         [[0.6100, 0.6783, 0.4050,  ..., 0.3942, 0.4163, 0.4883],
          [0.5960, 0.5660, 0.6469,  ..., 0.5237, 0.5694, 0.5118],
          [0.3684, 0.4668, 0.3702,  ..., 0.4721, 0.4330, 0.4801],
          [0.5031, 0.4064, 0.5470,  ..., 0.5443, 0.4999, 0.4278]],

         [[0.5043, 0.3522, 0.4102,  ..., 0.4268, 0.5189, 0.6478],
          [0.4340, 0.3495, 0.4450,  ..., 0.5110, 0.3961, 0.5583],
          [0.6352, 0.4235, 0.3522,  ..., 0.4857, 0.4980, 0.4340],
          [0.5898, 0.6105, 0.6183,  ..., 0.5443, 0.4325, 0.4813]],

         ...,

         [[0.4545, 0.5171, 0.4827,  ..., 0.4576, 0.3603, 0.4676],
          [0.5879, 0.5475, 0.5407,  ..., 0.3576, 0.5076, 0.6016],
          [0.4278, 0.4369, 0.5060,  ..., 0.5194, 0.5950, 0.5631],
          [0.4518, 0.6118, 0.4244,  ..., 0.4952, 0.5689, 0.6049]],

         [[0.4828, 0.5761, 0.4036,  ..., 0.5770, 0.4659, 0.4149],
          [0.4149, 0.4073, 0.5477,  ..., 0.5187, 0.4671, 0.6025],
          [0.6002, 0.4230, 0.4022,  ..., 0.3789, 0.3684, 0.4744],
          [0.3780, 0.4721, 0.3639,  ..., 0.6361, 0.5988, 0.4414]],

         [[0.4244, 0.5346, 0.5187,  ..., 0.5161, 0.5983, 0.5170],
          [0.4405, 0.5277, 0.5627,  ..., 0.3979, 0.3478, 0.5339],
          [0.4017, 0.5631, 0.4130,  ..., 0.3049, 0.5363, 0.3166],
          [0.4106, 0.4969, 0.4321,  ..., 0.3821, 0.4730, 0.6072]]],


        [[[0.4596, 0.4990, 0.7082,  ..., 0.3877, 0.3803, 0.4927],
          [0.4610, 0.3956, 0.5636,  ..., 0.6859, 0.5631, 0.5221],
          [0.5378, 0.5184, 0.4706,  ..., 0.5307, 0.5530, 0.5607],
          [0.4163, 0.6109, 0.5143,  ..., 0.4976, 0.4564, 0.3998]],

         [[0.4739, 0.5239, 0.5257,  ..., 0.4378, 0.5808, 0.3984],
          [0.4106, 0.5684, 0.4182,  ..., 0.5351, 0.4666, 0.4395],
          [0.4879, 0.4410, 0.5552,  ..., 0.4426, 0.4412, 0.5472],
          [0.5631, 0.3355, 0.5832,  ..., 0.5395, 0.6575, 0.5837]],

         [[0.7813, 0.4799, 0.6151,  ..., 0.3840, 0.4036, 0.5431],
          [0.6397, 0.4287, 0.4905,  ..., 0.5475, 0.7033, 0.5020],
          [0.4941, 0.4230, 0.4647,  ..., 0.5523, 0.5424, 0.5655],
          [0.4746, 0.4045, 0.5063,  ..., 0.5761, 0.4755, 0.5751]],

         ...,

         [[0.3789, 0.5278, 0.5217,  ..., 0.3900, 0.6077, 0.6192],
          [0.5997, 0.4311, 0.3730,  ..., 0.4192, 0.5110, 0.5889],
          [0.3891, 0.4111, 0.5305,  ..., 0.4311, 0.3835, 0.4720],
          [0.4087, 0.6160, 0.4460,  ..., 0.5765, 0.6316, 0.5879]],

         [[0.5595, 0.5134, 0.5494,  ..., 0.5011, 0.3872, 0.3886],
          [0.5751, 0.4773, 0.6406,  ..., 0.4301, 0.4417, 0.4235],
          [0.4630, 0.5097, 0.6540,  ..., 0.5931, 0.6243, 0.5174],
          [0.4177, 0.4622, 0.5655,  ..., 0.5516, 0.6151, 0.5903]],

         [[0.4121, 0.3372, 0.5013,  ..., 0.4292, 0.6049, 0.4869],
          [0.4417, 0.4886, 0.4625,  ..., 0.4364, 0.4426, 0.5516],
          [0.5383, 0.6316, 0.3914,  ..., 0.3785, 0.5595, 0.4676],
          [0.6142, 0.7042, 0.4762,  ..., 0.3558, 0.3522, 0.4955]]]],
       device='cuda:0', requires_grad=True)
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
selected experts tensor([1894, 1745, 1523, 1527, 1479, 1542, 1828, 1526, 1445, 1875],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5226, 0.4504, 0.4761,  ..., 0.3840, 0.4794, 0.5636],
          [0.6531, 0.3540, 0.5993,  ..., 0.4749, 0.5086, 0.6007],
          [0.6039, 0.5088, 0.5025,  ..., 0.4651, 0.4292, 0.4173],
          [0.3965, 0.6397, 0.6809,  ..., 0.6030, 0.5713, 0.5537]],

         [[0.5226, 0.4504, 0.4761,  ..., 0.3840, 0.4794, 0.5636],
          [0.6531, 0.3540, 0.5993,  ..., 0.4749, 0.5086, 0.6007],
          [0.6039, 0.5088, 0.5025,  ..., 0.4651, 0.4292, 0.4173],
          [0.3965, 0.6397, 0.6809,  ..., 0.6030, 0.5713, 0.5537]],

         [[0.4429, 0.4996, 0.3739,  ..., 0.4441, 0.4661, 0.5832],
          [0.4335, 0.4613, 0.4513,  ..., 0.5533, 0.3748, 0.6487],
          [0.5496, 0.4610, 0.3657,  ..., 0.4535, 0.5115, 0.5404],
          [0.3886, 0.6128, 0.4897,  ..., 0.3285, 0.5718, 0.4576]],

         ...,

         [[0.5574, 0.6775, 0.4158,  ..., 0.6091, 0.4989, 0.5117],
          [0.4847, 0.6442, 0.3785,  ..., 0.6280, 0.5470, 0.5446],
          [0.4984, 0.3567, 0.6146,  ..., 0.4874, 0.6270, 0.3149],
          [0.4321, 0.3016, 0.4839,  ..., 0.4886, 0.5358, 0.5603]],

         [[0.5349, 0.5450, 0.5448,  ..., 0.4083, 0.4012, 0.5402],
          [0.4605, 0.6298, 0.5808,  ..., 0.5022, 0.5727, 0.4740],
          [0.5496, 0.4593, 0.6706,  ..., 0.4393, 0.5518, 0.5898],
          [0.4949, 0.5177, 0.6002,  ..., 0.6406, 0.4458, 0.4833]],

         [[0.5842, 0.4712, 0.4936,  ..., 0.5392, 0.6220, 0.4704],
          [0.5071, 0.4330, 0.4944,  ..., 0.4149, 0.4489, 0.5056],
          [0.5496, 0.4533, 0.6325,  ..., 0.4883, 0.4869, 0.4364],
          [0.5941, 0.5600, 0.5312,  ..., 0.4381, 0.5684, 0.4567]]],


        [[[0.5431, 0.5978, 0.4083,  ..., 0.5043, 0.6968, 0.4852],
          [0.6165, 0.6211, 0.4017,  ..., 0.6352, 0.5660, 0.3914],
          [0.6671, 0.5358, 0.4163,  ..., 0.5612, 0.4216, 0.4704],
          [0.4381, 0.4769, 0.4676,  ..., 0.6689, 0.4467, 0.5076]],

         [[0.4833, 0.5679, 0.6379,  ..., 0.3337, 0.4477, 0.4026],
          [0.5475, 0.7178, 0.5708,  ..., 0.4306, 0.6030, 0.4852],
          [0.4278, 0.5245, 0.4917,  ..., 0.4952, 0.4116, 0.3863],
          [0.4816, 0.5010, 0.4354,  ..., 0.6514, 0.3989, 0.5060]],

         [[0.4239, 0.4596, 0.4325,  ..., 0.3639, 0.4479, 0.5400],
          [0.5894, 0.5195, 0.4496,  ..., 0.4935, 0.4783, 0.5789],
          [0.5171, 0.5913, 0.5794,  ..., 0.5775, 0.5346, 0.4230],
          [0.4192, 0.5419, 0.5679,  ..., 0.4292, 0.4678, 0.4622]],

         ...,

         [[0.5856, 0.5641, 0.4297,  ..., 0.5255, 0.4533, 0.5195],
          [0.5397, 0.6169, 0.5373,  ..., 0.6058, 0.3381, 0.3407],
          [0.4933, 0.4760, 0.4904,  ..., 0.2958, 0.4540, 0.3919],
          [0.6095, 0.3337, 0.5660,  ..., 0.3639, 0.5552, 0.4335]],

         [[0.3639, 0.4907, 0.5470,  ..., 0.5513, 0.5332, 0.5894],
          [0.4429, 0.4501, 0.6114,  ..., 0.4785, 0.6976, 0.4422],
          [0.4743, 0.4596, 0.4407,  ..., 0.4292, 0.5722, 0.3337],
          [0.4249, 0.5390, 0.6379,  ..., 0.4758, 0.5077, 0.5670]],

         [[0.5794, 0.6361, 0.4278,  ..., 0.4537, 0.5641, 0.3504],
          [0.4489, 0.4455, 0.3478,  ..., 0.4050, 0.5426, 0.4550],
          [0.6531, 0.4140, 0.5448,  ..., 0.3877, 0.5143, 0.5475],
          [0.3872, 0.4373, 0.4078,  ..., 0.6077, 0.6325, 0.4928]]]],
       device='cuda:0')/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/activation_monitor.py:211: UserWarning: Value passed to ActivationMonitor is not a torch.Tensor, skipping.
  warnings.warn(

tensor([[[[0.5226, 0.4504, 0.4761,  ..., 0.3840, 0.4794, 0.5636],
          [0.6531, 0.3540, 0.5993,  ..., 0.4749, 0.5086, 0.6007],
          [0.6039, 0.5088, 0.5025,  ..., 0.4651, 0.4292, 0.4173],
          [0.3965, 0.6397, 0.6809,  ..., 0.6030, 0.5713, 0.5537]],

         [[0.5226, 0.4504, 0.4761,  ..., 0.3840, 0.4794, 0.5636],
          [0.6531, 0.3540, 0.5993,  ..., 0.4749, 0.5086, 0.6007],
          [0.6039, 0.5088, 0.5025,  ..., 0.4651, 0.4292, 0.4173],
          [0.3965, 0.6397, 0.6809,  ..., 0.6030, 0.5713, 0.5537]],

         [[0.4429, 0.4996, 0.3739,  ..., 0.4441, 0.4661, 0.5832],
          [0.4335, 0.4613, 0.4513,  ..., 0.5533, 0.3748, 0.6487],
          [0.5496, 0.4610, 0.3657,  ..., 0.4535, 0.5115, 0.5404],
          [0.3886, 0.6128, 0.4897,  ..., 0.3285, 0.5718, 0.4576]],

         ...,

         [[0.5574, 0.6775, 0.4158,  ..., 0.6091, 0.4989, 0.5117],
          [0.4847, 0.6442, 0.3785,  ..., 0.6280, 0.5470, 0.5446],
          [0.4984, 0.3567, 0.6146,  ..., 0.4874, 0.6270, 0.3149],
          [0.4321, 0.3016, 0.4839,  ..., 0.4886, 0.5358, 0.5603]],

         [[0.5349, 0.5450, 0.5448,  ..., 0.4083, 0.4012, 0.5402],
          [0.4605, 0.6298, 0.5808,  ..., 0.5022, 0.5727, 0.4740],
          [0.5496, 0.4593, 0.6706,  ..., 0.4393, 0.5518, 0.5898],
          [0.4949, 0.5177, 0.6002,  ..., 0.6406, 0.4458, 0.4833]],

         [[0.5842, 0.4712, 0.4936,  ..., 0.5392, 0.6220, 0.4704],
          [0.5071, 0.4330, 0.4944,  ..., 0.4149, 0.4489, 0.5056],
          [0.5496, 0.4533, 0.6325,  ..., 0.4883, 0.4869, 0.4364],
          [0.5941, 0.5600, 0.5312,  ..., 0.4381, 0.5684, 0.4567]]],


        [[[0.5431, 0.5978, 0.4083,  ..., 0.5043, 0.6968, 0.4852],
          [0.6165, 0.6211, 0.4017,  ..., 0.6352, 0.5660, 0.3914],
          [0.6671, 0.5358, 0.4163,  ..., 0.5612, 0.4216, 0.4704],
          [0.4381, 0.4769, 0.4676,  ..., 0.6689, 0.4467, 0.5076]],

         [[0.4833, 0.5679, 0.6379,  ..., 0.3337, 0.4477, 0.4026],
          [0.5475, 0.7178, 0.5708,  ..., 0.4306, 0.6030, 0.4852],
          [0.4278, 0.5245, 0.4917,  ..., 0.4952, 0.4116, 0.3863],
          [0.4816, 0.5010, 0.4354,  ..., 0.6514, 0.3989, 0.5060]],

         [[0.4239, 0.4596, 0.4325,  ..., 0.3639, 0.4479, 0.5400],
          [0.5894, 0.5195, 0.4496,  ..., 0.4935, 0.4783, 0.5789],
          [0.5171, 0.5913, 0.5794,  ..., 0.5775, 0.5346, 0.4230],
          [0.4192, 0.5419, 0.5679,  ..., 0.4292, 0.4678, 0.4622]],

         ...,

         [[0.5856, 0.5641, 0.4297,  ..., 0.5255, 0.4533, 0.5195],
          [0.5397, 0.6169, 0.5373,  ..., 0.6058, 0.3381, 0.3407],
          [0.4933, 0.4760, 0.4904,  ..., 0.2958, 0.4540, 0.3919],
          [0.6095, 0.3337, 0.5660,  ..., 0.3639, 0.5552, 0.4335]],

         [[0.3639, 0.4907, 0.5470,  ..., 0.5513, 0.5332, 0.5894],
          [0.4429, 0.4501, 0.6114,  ..., 0.4785, 0.6976, 0.4422],
          [0.4743, 0.4596, 0.4407,  ..., 0.4292, 0.5722, 0.3337],
          [0.4249, 0.5390, 0.6379,  ..., 0.4758, 0.5077, 0.5670]],

         [[0.5794, 0.6361, 0.4278,  ..., 0.4537, 0.5641, 0.3504],
          [0.4489, 0.4455, 0.3478,  ..., 0.4050, 0.5426, 0.4550],
          [0.6531, 0.4140, 0.5448,  ..., 0.3877, 0.5143, 0.5475],
          [0.3872, 0.4373, 0.4078,  ..., 0.6077, 0.6325, 0.4928]]]],
       device='cuda:0', requires_grad=True)
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
selected experts tensor([1575, 1956, 1434, 1768, 1475, 1451, 1888, 1834, 1587, 1416],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4258, 0.5409, 0.5576,  ..., 0.3567, 0.4381, 0.5722],
          [0.6424, 0.5506, 0.5116,  ..., 0.4031, 0.4102, 0.5378],
          [0.4925, 0.5353, 0.5296,  ..., 0.5317, 0.2958, 0.5746],
          [0.3748, 0.3965, 0.4073,  ..., 0.4523, 0.5148, 0.5988]],

         [[0.4282, 0.5443, 0.5586,  ..., 0.3549, 0.4373, 0.5727],
          [0.6433, 0.5484, 0.5112,  ..., 0.4031, 0.4106, 0.5375],
          [0.4910, 0.5363, 0.5300,  ..., 0.5311, 0.2967, 0.5756],
          [0.3748, 0.3970, 0.4073,  ..., 0.4528, 0.5156, 0.5974]],

         [[0.3993, 0.5171, 0.4718,  ..., 0.4750, 0.3844, 0.5346],
          [0.3711, 0.6261, 0.4321,  ..., 0.4841, 0.5018, 0.5130],
          [0.4596, 0.5525, 0.4681,  ..., 0.5138, 0.4816, 0.5118],
          [0.6007, 0.3789, 0.4278,  ..., 0.5023, 0.5581, 0.5571]],

         ...,

         [[0.4696, 0.5846, 0.3513,  ..., 0.3975, 0.4135, 0.4632],
          [0.4642, 0.6584, 0.4460,  ..., 0.4940, 0.3909, 0.3919],
          [0.5694, 0.5096, 0.4730,  ..., 0.4826, 0.3329, 0.4431],
          [0.5960, 0.5689, 0.4752,  ..., 0.7447, 0.5419, 0.6142]],

         [[0.5506, 0.4369, 0.4966,  ..., 0.4518, 0.4583, 0.6179],
          [0.5593, 0.5964, 0.5818,  ..., 0.5946, 0.3840, 0.3346],
          [0.6243, 0.3835, 0.5164,  ..., 0.3872, 0.5135, 0.4839],
          [0.5665, 0.5426, 0.5856,  ..., 0.6715, 0.3826, 0.6876]],

         [[0.6628, 0.3868, 0.5470,  ..., 0.4254, 0.4966, 0.4973],
          [0.5351, 0.6016, 0.4031,  ..., 0.6370, 0.4700, 0.4244],
          [0.5378, 0.3993, 0.5931,  ..., 0.4177, 0.5060, 0.4598],
          [0.4846, 0.3766, 0.6156,  ..., 0.7154, 0.3951, 0.5243]]],


        [[[0.4513, 0.4974, 0.5713,  ..., 0.4983, 0.4385, 0.4516],
          [0.5670, 0.5593, 0.4130,  ..., 0.5304, 0.5523, 0.4729],
          [0.5808, 0.5390, 0.5694,  ..., 0.6234, 0.4045, 0.4321],
          [0.6826, 0.3970, 0.4349,  ..., 0.4875, 0.5756, 0.6575]],

         [[0.4273, 0.5199, 0.4912,  ..., 0.4482, 0.4325, 0.6671],
          [0.5487, 0.5832, 0.5605,  ..., 0.4031, 0.3460, 0.5231],
          [0.4007, 0.3951, 0.6016,  ..., 0.4050, 0.4959, 0.5341],
          [0.5031, 0.3549, 0.5294,  ..., 0.6800, 0.5107, 0.4554]],

         [[0.5153, 0.4932, 0.3711,  ..., 0.3675, 0.3748, 0.5299],
          [0.5746, 0.4482, 0.4926,  ..., 0.4458, 0.3666, 0.5380],
          [0.3675, 0.5684, 0.5794,  ..., 0.4910, 0.3355, 0.4762],
          [0.3398, 0.3132, 0.4851,  ..., 0.5545, 0.5492, 0.5718]],

         ...,

         [[0.3558, 0.4040, 0.4287,  ..., 0.4484, 0.5078, 0.3531],
          [0.3993, 0.5095, 0.4586,  ..., 0.4537, 0.5368, 0.5375],
          [0.4688, 0.6077, 0.5846,  ..., 0.5511, 0.5424, 0.4273],
          [0.4330, 0.4414, 0.3947,  ..., 0.5646, 0.6325, 0.3720]],

         [[0.4630, 0.4007, 0.4996,  ..., 0.3320, 0.5794, 0.5322],
          [0.5504, 0.3961, 0.5039,  ..., 0.3877, 0.4535, 0.4472],
          [0.5703, 0.5285, 0.6343,  ..., 0.4506, 0.4979, 0.3208],
          [0.3844, 0.4713, 0.6316,  ..., 0.4797, 0.5960, 0.4600]],

         [[0.4335, 0.6081, 0.4588,  ..., 0.4196, 0.3320, 0.4823],
          [0.6628, 0.4605, 0.4345,  ..., 0.5390, 0.3965, 0.3895],
          [0.6105, 0.4106, 0.4889,  ..., 0.4671, 0.5010, 0.4335],
          [0.5631, 0.3612, 0.5101,  ..., 0.5025, 0.4957, 0.5941]]]],
       device='cuda:0')
tensor([[[[0.4258, 0.5409, 0.5576,  ..., 0.3567, 0.4381, 0.5722],
          [0.6424, 0.5506, 0.5116,  ..., 0.4031, 0.4102, 0.5378],
          [0.4925, 0.5353, 0.5296,  ..., 0.5317, 0.2958, 0.5746],
          [0.3748, 0.3965, 0.4073,  ..., 0.4523, 0.5148, 0.5988]],

         [[0.4282, 0.5443, 0.5586,  ..., 0.3549, 0.4373, 0.5727],
          [0.6433, 0.5484, 0.5112,  ..., 0.4031, 0.4106, 0.5375],
          [0.4910, 0.5363, 0.5300,  ..., 0.5311, 0.2967, 0.5756],
          [0.3748, 0.3970, 0.4073,  ..., 0.4528, 0.5156, 0.5974]],

         [[0.3993, 0.5171, 0.4718,  ..., 0.4750, 0.3844, 0.5346],
          [0.3711, 0.6261, 0.4321,  ..., 0.4841, 0.5018, 0.5130],
          [0.4596, 0.5525, 0.4681,  ..., 0.5138, 0.4816, 0.5118],
          [0.6007, 0.3789, 0.4278,  ..., 0.5023, 0.5581, 0.5571]],

         ...,

         [[0.4696, 0.5846, 0.3513,  ..., 0.3975, 0.4135, 0.4632],
          [0.4642, 0.6584, 0.4460,  ..., 0.4940, 0.3909, 0.3919],
          [0.5694, 0.5096, 0.4730,  ..., 0.4826, 0.3329, 0.4431],
          [0.5960, 0.5689, 0.4752,  ..., 0.7447, 0.5419, 0.6142]],

         [[0.5506, 0.4369, 0.4966,  ..., 0.4518, 0.4583, 0.6179],
          [0.5593, 0.5964, 0.5818,  ..., 0.5946, 0.3840, 0.3346],
          [0.6243, 0.3835, 0.5164,  ..., 0.3872, 0.5135, 0.4839],
          [0.5665, 0.5426, 0.5856,  ..., 0.6715, 0.3826, 0.6876]],

         [[0.6628, 0.3868, 0.5470,  ..., 0.4254, 0.4966, 0.4973],
          [0.5351, 0.6016, 0.4031,  ..., 0.6370, 0.4700, 0.4244],
          [0.5378, 0.3993, 0.5931,  ..., 0.4177, 0.5060, 0.4598],
          [0.4846, 0.3766, 0.6156,  ..., 0.7154, 0.3951, 0.5243]]],


        [[[0.4513, 0.4974, 0.5713,  ..., 0.4983, 0.4385, 0.4516],
          [0.5670, 0.5593, 0.4130,  ..., 0.5304, 0.5523, 0.4729],
          [0.5808, 0.5390, 0.5694,  ..., 0.6234, 0.4045, 0.4321],
          [0.6826, 0.3970, 0.4349,  ..., 0.4875, 0.5756, 0.6575]],

         [[0.4273, 0.5199, 0.4912,  ..., 0.4482, 0.4325, 0.6671],
          [0.5487, 0.5832, 0.5605,  ..., 0.4031, 0.3460, 0.5231],
          [0.4007, 0.3951, 0.6016,  ..., 0.4050, 0.4959, 0.5341],
          [0.5031, 0.3549, 0.5294,  ..., 0.6800, 0.5107, 0.4554]],

         [[0.5153, 0.4932, 0.3711,  ..., 0.3675, 0.3748, 0.5299],
          [0.5746, 0.4482, 0.4926,  ..., 0.4458, 0.3666, 0.5380],
          [0.3675, 0.5684, 0.5794,  ..., 0.4910, 0.3355, 0.4762],
          [0.3398, 0.3132, 0.4851,  ..., 0.5545, 0.5492, 0.5718]],

         ...,

         [[0.3558, 0.4040, 0.4287,  ..., 0.4484, 0.5078, 0.3531],
          [0.3993, 0.5095, 0.4586,  ..., 0.4537, 0.5368, 0.5375],
          [0.4688, 0.6077, 0.5846,  ..., 0.5511, 0.5424, 0.4273],
          [0.4330, 0.4414, 0.3947,  ..., 0.5646, 0.6325, 0.3720]],

         [[0.4630, 0.4007, 0.4996,  ..., 0.3320, 0.5794, 0.5322],
          [0.5504, 0.3961, 0.5039,  ..., 0.3877, 0.4535, 0.4472],
          [0.5703, 0.5285, 0.6343,  ..., 0.4506, 0.4979, 0.3208],
          [0.3844, 0.4713, 0.6316,  ..., 0.4797, 0.5960, 0.4600]],

         [[0.4335, 0.6081, 0.4588,  ..., 0.4196, 0.3320, 0.4823],
          [0.6628, 0.4605, 0.4345,  ..., 0.5390, 0.3965, 0.3895],
          [0.6105, 0.4106, 0.4889,  ..., 0.4671, 0.5010, 0.4335],
          [0.5631, 0.3612, 0.5101,  ..., 0.5025, 0.4957, 0.5941]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
[batch=1/40]:
	 Train time/epoch: 0
	 Train time/batch: 0
	 Train time/sample: 0
	 Train time/batch_in_epoch: 0
	 Train time/sample_in_epoch: 0
	 Train time/token: 0
	 Train time/token_in_epoch: 0
	 Train memory/current_allocated_mem: 0.6918
	 Train memory/current_active_mem: 0.6918
	 Train memory/current_inactive_mem: 0.3023
	 Train memory/current_reserved_mem: 2.9318
	 Train memory/peak_allocated_mem: 2.0817
	 Train memory/peak_active_mem: 2.0817
	 Train memory/peak_inactive_mem: 0.4754
	 Train memory/peak_reserved_mem: 2.9318
	 Train memory/alloc_retries: 0
	 Train trainer/device_train_microbatch_size: 10
	 Train loss/train/total: 0.0053
	 Train metrics/train/LanguageCrossEntropy: 10.9287
	 Train metrics/train/LanguagePerplexity: 55753.6719
	 Train metrics/train/TokenAccuracy: 0.0000
	 Train time/train: 0.0087
	 Train time/val: 0.0000
	 Train time/total: 0.0087
	 Train lr-DecoupledAdamW/group0: 0.0000
	 Train metrics/shannon_entropy: 10.6357
	 Train metrics/batch_shannon_entropy: <wandb.sdk.data_types.table.Table object at 0x7097b028cfb0>
	 Train metrics/seq_shannon_entropy: CustomChart(table=<wandb.sdk.data_types.table.Table object at 0x70984219ff50>, spec=CustomChartSpec(spec_name='wandb/lineseries/v0', fields={'step': 'step', 'lineKey': 'lineKey', 'lineVal': 'lineVal'}, string_fields={'title': 'Per-Layer Shannon Entropy', 'xname': 'Block Index'}, key='', panel_type='Vega2', split_table=False))
	 Train metrics/exit_entropy: 0.6641
	 Train metrics/batch_exit_entropy: <wandb.sdk.data_types.table.Table object at 0x70984213f1a0>
	 Train metrics/seq_exit_entropy: CustomChart(table=<wandb.sdk.data_types.table.Table object at 0x70984202c4a0>, spec=CustomChartSpec(spec_name='wandb/lineseries/v0', fields={'step': 'step', 'lineKey': 'lineKey', 'lineVal': 'lineVal'}, string_fields={'title': 'Per-Layer Exit Entropy', 'xname': 'Block Index'}, key='', panel_type='Vega2', split_table=False))
	 Train expert_selection/ffn_layer: <wandb.sdk.data_types.image.Image object at 0x7098412bec30>
	 Train expert_selection/attn_o_layer: <wandb.sdk.data_types.image.Image object at 0x709840ddede0>
	 Train expert_selection/attn_v_layer: <wandb.sdk.data_types.image.Image object at 0x709841111220>
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
selected experts tensor([1519, 1774, 1987, 1597, 1926, 1387, 1548, 1767,  650, 2229],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.4783, 0.6151, 0.5306,  ..., 0.3407, 0.5703, 0.4668],
          [0.5165, 0.4465, 0.3416,  ..., 0.5132, 0.6035, 0.5317],
          [0.4097, 0.5025, 0.5641,  ..., 0.3775, 0.5566, 0.4484],
          [0.5317, 0.5295, 0.4448,  ..., 0.4321, 0.5149, 0.4499]],

         [[0.4808, 0.6146, 0.5312,  ..., 0.3398, 0.5708, 0.4659],
          [0.5162, 0.4460, 0.3425,  ..., 0.5142, 0.6044, 0.5299],
          [0.4073, 0.5029, 0.5627,  ..., 0.3789, 0.5562, 0.4475],
          [0.5322, 0.5295, 0.4436,  ..., 0.4325, 0.5118, 0.4477]],

         [[0.3540, 0.6165, 0.6593,  ..., 0.3882, 0.3937, 0.5450],
          [0.4390, 0.4749, 0.5329,  ..., 0.3808, 0.5569, 0.5022],
          [0.5075, 0.3657, 0.5506,  ..., 0.5319, 0.6007, 0.5794],
          [0.6234, 0.5780, 0.5074,  ..., 0.4364, 0.4516, 0.6011]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.4800, 0.6086, 0.6645,  ..., 0.4732, 0.4808, 0.5278],
          [0.4695, 0.4359, 0.4874,  ..., 0.3320, 0.7122, 0.4724],
          [0.4410, 0.4405, 0.5732,  ..., 0.5053, 0.6058, 0.5414],
          [0.6379, 0.5470, 0.6215,  ..., 0.5765, 0.3730, 0.5927]],

         [[0.4528, 0.4767, 0.6671,  ..., 0.4545, 0.7154, 0.5581],
          [0.5134, 0.5277, 0.4946,  ..., 0.4192, 0.5142, 0.5879],
          [0.4723, 0.4591, 0.4829,  ..., 0.4683, 0.6637, 0.5983],
          [0.4782, 0.4087, 0.3730,  ..., 0.5818, 0.4282, 0.5504]],

         [[0.3923, 0.5419, 0.5588,  ..., 0.4429, 0.4671, 0.4287],
          [0.4774, 0.3891, 0.4839,  ..., 0.4235, 0.4282, 0.5775],
          [0.4811, 0.5014, 0.5960,  ..., 0.4487, 0.5421, 0.5603],
          [0.5875, 0.5402, 0.4579,  ..., 0.5552, 0.5050, 0.5799]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0')
tensor([[[[0.4783, 0.6151, 0.5306,  ..., 0.3407, 0.5703, 0.4668],
          [0.5165, 0.4465, 0.3416,  ..., 0.5132, 0.6035, 0.5317],
          [0.4097, 0.5025, 0.5641,  ..., 0.3775, 0.5566, 0.4484],
          [0.5317, 0.5295, 0.4448,  ..., 0.4321, 0.5149, 0.4499]],

         [[0.4808, 0.6146, 0.5312,  ..., 0.3398, 0.5708, 0.4659],
          [0.5162, 0.4460, 0.3425,  ..., 0.5142, 0.6044, 0.5299],
          [0.4073, 0.5029, 0.5627,  ..., 0.3789, 0.5562, 0.4475],
          [0.5322, 0.5295, 0.4436,  ..., 0.4325, 0.5118, 0.4477]],

         [[0.3540, 0.6165, 0.6593,  ..., 0.3882, 0.3937, 0.5450],
          [0.4390, 0.4749, 0.5329,  ..., 0.3808, 0.5569, 0.5022],
          [0.5075, 0.3657, 0.5506,  ..., 0.5319, 0.6007, 0.5794],
          [0.6234, 0.5780, 0.5074,  ..., 0.4364, 0.4516, 0.6011]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.4800, 0.6086, 0.6645,  ..., 0.4732, 0.4808, 0.5278],
          [0.4695, 0.4359, 0.4874,  ..., 0.3320, 0.7122, 0.4724],
          [0.4410, 0.4405, 0.5732,  ..., 0.5053, 0.6058, 0.5414],
          [0.6379, 0.5470, 0.6215,  ..., 0.5765, 0.3730, 0.5927]],

         [[0.4528, 0.4767, 0.6671,  ..., 0.4545, 0.7154, 0.5581],
          [0.5134, 0.5277, 0.4946,  ..., 0.4192, 0.5142, 0.5879],
          [0.4723, 0.4591, 0.4829,  ..., 0.4683, 0.6637, 0.5983],
          [0.4782, 0.4087, 0.3730,  ..., 0.5818, 0.4282, 0.5504]],

         [[0.3923, 0.5419, 0.5588,  ..., 0.4429, 0.4671, 0.4287],
          [0.4774, 0.3891, 0.4839,  ..., 0.4235, 0.4282, 0.5775],
          [0.4811, 0.5014, 0.5960,  ..., 0.4487, 0.5421, 0.5603],
          [0.5875, 0.5402, 0.4579,  ..., 0.5552, 0.5050, 0.5799]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
selected experts tensor([1938, 1945, 1308, 1126, 1075,  893, 1006,  367, 1501, 1129],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.3965, 0.5665, 0.5388,  ..., 0.4586, 0.5163, 0.4716],
          [0.4992, 0.4130, 0.4383,  ..., 0.5339, 0.3640, 0.4901],
          [0.4722, 0.5850, 0.4932,  ..., 0.4688, 0.4596, 0.5218],
          [0.5703, 0.4710, 0.5694,  ..., 0.3640, 0.4661, 0.3530]],

         [[0.4111, 0.2588, 0.5137,  ..., 0.3295, 0.4569, 0.4783],
          [0.4615, 0.4817, 0.2638,  ..., 0.6524, 0.4596, 0.4315],
          [0.5033, 0.4871, 0.5284,  ..., 0.3622, 0.4966, 0.3895],
          [0.4740, 0.4277, 0.4364,  ..., 0.5187, 0.4288, 0.5356]],

         [[0.5031, 0.4707, 0.5083,  ..., 0.5828, 0.6054, 0.4040],
          [0.4397, 0.4007, 0.4173,  ..., 0.4678, 0.4843, 0.4325],
          [0.4637, 0.4893, 0.6559,  ..., 0.5535, 0.5373, 0.3867],
          [0.6396, 0.4593, 0.4661,  ..., 0.5422, 0.4792, 0.4961]],

         ...,

         [[0.6155, 0.4905, 0.5728,  ..., 0.5732, 0.5011, 0.5094],
          [0.6215, 0.3955, 0.4954,  ..., 0.5560, 0.3235, 0.4407],
          [0.4856, 0.5124, 0.6073,  ..., 0.3443, 0.7545, 0.3904],
          [0.3821, 0.3941, 0.5680,  ..., 0.4383, 0.5183, 0.4770]],

         [[0.4163, 0.3867, 0.6585,  ..., 0.3631, 0.4625, 0.5339],
          [0.5363, 0.5094, 0.5526,  ..., 0.4596, 0.4984, 0.5590],
          [0.4889, 0.3974, 0.6699,  ..., 0.4834, 0.5031, 0.4725],
          [0.4590, 0.6360, 0.4683,  ..., 0.5766, 0.5147, 0.6405]],

         [[0.4523, 0.6132, 0.6119,  ..., 0.5363, 0.5528, 0.5223],
          [0.3224, 0.4306, 0.3488,  ..., 0.5183, 0.3919, 0.4905],
          [0.5817, 0.5404, 0.4391,  ..., 0.5473, 0.4829, 0.4919],
          [0.4368, 0.4552, 0.5152,  ..., 0.5637, 0.5066, 0.4508]]],


        [[[0.5370, 0.4739, 0.5264,  ..., 0.5346, 0.3957, 0.4206],
          [0.4869, 0.4808, 0.6207,  ..., 0.4523, 0.4278, 0.5609],
          [0.3468, 0.3784, 0.4060,  ..., 0.4202, 0.4936, 0.4496],
          [0.4935, 0.4296, 0.5069,  ..., 0.5533, 0.4230, 0.4537]],

         [[0.4445, 0.5595, 0.3994,  ..., 0.3971, 0.5400, 0.5665],
          [0.5267, 0.5585, 0.3985,  ..., 0.6105, 0.5641, 0.5879],
          [0.3710, 0.5122, 0.6836,  ..., 0.4436, 0.4429, 0.5024],
          [0.4465, 0.3683, 0.5119,  ..., 0.5842, 0.5064, 0.5736]],

         [[0.6076, 0.4513, 0.3604,  ..., 0.6344, 0.5833, 0.5322],
          [0.6001, 0.5255, 0.5249,  ..., 0.5485, 0.4364, 0.6215],
          [0.6136, 0.5760, 0.3435,  ..., 0.5110, 0.5368, 0.4186],
          [0.5302, 0.3881, 0.4555,  ..., 0.5231, 0.4784, 0.6104]],

         ...,

         [[0.4068, 0.5494, 0.6012,  ..., 0.4538, 0.4572, 0.5841],
          [0.4975, 0.5641, 0.3703,  ..., 0.5960, 0.4485, 0.5525],
          [0.5836, 0.4244, 0.4798,  ..., 0.4814, 0.4902, 0.6020],
          [0.5370, 0.4049, 0.4259,  ..., 0.4598, 0.7132, 0.7293]],

         [[0.5708, 0.5200, 0.5346,  ..., 0.4927, 0.5175, 0.5978],
          [0.5195, 0.5394, 0.3749,  ..., 0.6353, 0.4064, 0.6351],
          [0.5884, 0.4849, 0.5569,  ..., 0.5516, 0.2936, 0.4007],
          [0.5136, 0.5912, 0.4441,  ..., 0.6175, 0.6326, 0.4759]],

         [[0.4775, 0.4790, 0.5429,  ..., 0.5747, 0.5280, 0.5755],
          [0.5621, 0.4390, 0.5272,  ..., 0.5574, 0.5593, 0.3932],
          [0.5973, 0.3566, 0.5298,  ..., 0.5157, 0.4079, 0.6574],
          [0.4349, 0.4311, 0.5747,  ..., 0.5965, 0.5424, 0.7611]]]],
       device='cuda:0')
tensor([[[[0.3975, 0.5675, 0.5378,  ..., 0.4576, 0.5153, 0.4726],
          [0.5002, 0.4140, 0.4373,  ..., 0.5329, 0.3630, 0.4911],
          [0.4732, 0.5860, 0.4922,  ..., 0.4678, 0.4586, 0.5228],
          [0.5713, 0.4720, 0.5684,  ..., 0.3630, 0.4651, 0.3540]],

         [[0.4121, 0.2598, 0.5127,  ..., 0.3285, 0.4559, 0.4793],
          [0.4625, 0.4827, 0.2628,  ..., 0.6514, 0.4586, 0.4325],
          [0.5043, 0.4881, 0.5274,  ..., 0.3612, 0.4956, 0.3905],
          [0.4750, 0.4287, 0.4354,  ..., 0.5177, 0.4278, 0.5366]],

         [[0.5041, 0.4717, 0.5073,  ..., 0.5818, 0.6044, 0.4050],
          [0.4407, 0.4017, 0.4163,  ..., 0.4668, 0.4833, 0.4335],
          [0.4647, 0.4903, 0.6549,  ..., 0.5525, 0.5363, 0.3877],
          [0.6406, 0.4603, 0.4651,  ..., 0.5412, 0.4782, 0.4971]],

         ...,

         [[0.6165, 0.4915, 0.5718,  ..., 0.5722, 0.5001, 0.5104],
          [0.6225, 0.3965, 0.4944,  ..., 0.5550, 0.3225, 0.4417],
          [0.4866, 0.5134, 0.6063,  ..., 0.3433, 0.7535, 0.3914],
          [0.3831, 0.3951, 0.5670,  ..., 0.4373, 0.5173, 0.4780]],

         [[0.4173, 0.3877, 0.6575,  ..., 0.3621, 0.4615, 0.5349],
          [0.5373, 0.5104, 0.5516,  ..., 0.4586, 0.4974, 0.5600],
          [0.4899, 0.3984, 0.6689,  ..., 0.4824, 0.5021, 0.4735],
          [0.4600, 0.6370, 0.4673,  ..., 0.5756, 0.5137, 0.6415]],

         [[0.4533, 0.6142, 0.6109,  ..., 0.5353, 0.5518, 0.5233],
          [0.3234, 0.4316, 0.3478,  ..., 0.5173, 0.3909, 0.4915],
          [0.5827, 0.5414, 0.4381,  ..., 0.5463, 0.4819, 0.4929],
          [0.4378, 0.4562, 0.5142,  ..., 0.5627, 0.5056, 0.4518]]],


        [[[0.5380, 0.4749, 0.5254,  ..., 0.5336, 0.3947, 0.4216],
          [0.4879, 0.4818, 0.6197,  ..., 0.4513, 0.4268, 0.5619],
          [0.3478, 0.3794, 0.4050,  ..., 0.4192, 0.4926, 0.4506],
          [0.4945, 0.4306, 0.5059,  ..., 0.5523, 0.4220, 0.4547]],

         [[0.4455, 0.5605, 0.3984,  ..., 0.3961, 0.5390, 0.5675],
          [0.5277, 0.5595, 0.3975,  ..., 0.6095, 0.5631, 0.5889],
          [0.3720, 0.5132, 0.6826,  ..., 0.4426, 0.4419, 0.5034],
          [0.4475, 0.3693, 0.5109,  ..., 0.5832, 0.5054, 0.5746]],

         [[0.6086, 0.4523, 0.3594,  ..., 0.6334, 0.5823, 0.5332],
          [0.6011, 0.5265, 0.5239,  ..., 0.5475, 0.4354, 0.6225],
          [0.6146, 0.5770, 0.3425,  ..., 0.5100, 0.5358, 0.4196],
          [0.5312, 0.3891, 0.4545,  ..., 0.5221, 0.4774, 0.6114]],

         ...,

         [[0.4078, 0.5504, 0.6002,  ..., 0.4528, 0.4562, 0.5851],
          [0.4985, 0.5651, 0.3693,  ..., 0.5950, 0.4475, 0.5535],
          [0.5846, 0.4254, 0.4788,  ..., 0.4804, 0.4892, 0.6030],
          [0.5380, 0.4059, 0.4249,  ..., 0.4588, 0.7122, 0.7303]],

         [[0.5718, 0.5210, 0.5336,  ..., 0.4917, 0.5165, 0.5988],
          [0.5205, 0.5404, 0.3739,  ..., 0.6343, 0.4054, 0.6361],
          [0.5894, 0.4859, 0.5559,  ..., 0.5506, 0.2926, 0.4017],
          [0.5146, 0.5922, 0.4431,  ..., 0.6165, 0.6316, 0.4769]],

         [[0.4785, 0.4800, 0.5419,  ..., 0.5737, 0.5270, 0.5765],
          [0.5631, 0.4400, 0.5262,  ..., 0.5564, 0.5583, 0.3942],
          [0.5983, 0.3576, 0.5288,  ..., 0.5147, 0.4069, 0.6584],
          [0.4359, 0.4321, 0.5737,  ..., 0.5955, 0.5414, 0.7621]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0010, -0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0010,  0.0010,
         0.0010, -0.0010], device='cuda:0')
selected experts tensor([1944, 1717, 1552, 1536, 1552, 1497, 1746, 1526, 1444, 1870],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5373, 0.5827, 0.4422,  ..., 0.4354, 0.4884, 0.5889],
          [0.5732, 0.6062, 0.4706,  ..., 0.6459, 0.4985, 0.4620],
          [0.5742, 0.4795, 0.5261,  ..., 0.5203, 0.5675, 0.6262],
          [0.4395, 0.4736, 0.6416,  ..., 0.5038, 0.4131, 0.5970]],

         [[0.6515, 0.5161, 0.5255,  ..., 0.4903, 0.5833, 0.5027],
          [0.5327, 0.4972, 0.3901,  ..., 0.3039, 0.4727, 0.5448],
          [0.4591, 0.5339, 0.6707,  ..., 0.4436, 0.5242, 0.5723],
          [0.5460, 0.4615, 0.5463,  ..., 0.6782, 0.4533, 0.5349]],

         [[0.4772, 0.4059, 0.4055,  ..., 0.4746, 0.5363, 0.4533],
          [0.3966, 0.5846, 0.3966,  ..., 0.4320, 0.3905, 0.4782],
          [0.5077, 0.3380, 0.5334,  ..., 0.4030, 0.3740, 0.5988],
          [0.5271, 0.4561, 0.5470,  ..., 0.5650, 0.4748, 0.5790]],

         ...,

         [[0.5984, 0.3353, 0.4564,  ..., 0.3139, 0.4197, 0.6335],
          [0.5145, 0.5402, 0.4509,  ..., 0.6011, 0.5359, 0.4102],
          [0.3443, 0.5194, 0.4150,  ..., 0.4950, 0.4946, 0.4635],
          [0.5203, 0.6609, 0.4921,  ..., 0.5268, 0.5189, 0.6506]],

         [[0.3586, 0.5803, 0.4715,  ..., 0.5162, 0.4545, 0.6175],
          [0.4268, 0.5903, 0.3994,  ..., 0.4961, 0.4574, 0.3818],
          [0.4197, 0.4424, 0.6142,  ..., 0.3593, 0.3313, 0.4623],
          [0.4159, 0.5784, 0.3989,  ..., 0.5660, 0.4584, 0.5974]],

         [[0.5262, 0.4830, 0.6007,  ..., 0.4759, 0.5766, 0.5894],
          [0.4107, 0.6260, 0.4971,  ..., 0.5319, 0.5349, 0.5494],
          [0.4254, 0.4906, 0.4784,  ..., 0.5019, 0.6398, 0.6271],
          [0.4316, 0.4986, 0.5152,  ..., 0.3629, 0.4178, 0.4939]]],


        [[[0.5752, 0.3775, 0.6452,  ..., 0.4201, 0.5842, 0.6452],
          [0.5443, 0.4847, 0.5984,  ..., 0.4905, 0.6198, 0.4960],
          [0.4405, 0.5765, 0.4550,  ..., 0.5453, 0.5463, 0.5098],
          [0.3295, 0.3862, 0.4497,  ..., 0.4851, 0.3417, 0.5709]],

         [[0.6953, 0.4814, 0.6638,  ..., 0.5472, 0.5540, 0.5318],
          [0.4013, 0.5084, 0.3957,  ..., 0.4629, 0.5199, 0.3740],
          [0.3505, 0.3969, 0.4860,  ..., 0.6076, 0.4439, 0.4364],
          [0.4259, 0.4716, 0.5018,  ..., 0.6270, 0.4674, 0.6810]],

         [[0.4728, 0.7184, 0.4307,  ..., 0.6841, 0.4572, 0.5327],
          [0.4083, 0.2957, 0.4069,  ..., 0.6081, 0.5049, 0.4991],
          [0.4975, 0.6224, 0.6110,  ..., 0.4671, 0.5062, 0.4818],
          [0.4703, 0.6164, 0.3505,  ..., 0.5636, 0.5680, 0.3980]],

         ...,

         [[0.4359, 0.4816, 0.5254,  ..., 0.4402, 0.4657, 0.3721],
          [0.5694, 0.4949, 0.5316,  ..., 0.6333, 0.4790, 0.4884],
          [0.4822, 0.6169, 0.5265,  ..., 0.4844, 0.5588, 0.3479],
          [0.3952, 0.6233, 0.5610,  ..., 0.5224, 0.6184, 0.5794]],

         [[0.3989, 0.7088, 0.6105,  ..., 0.3904, 0.5065, 0.4942],
          [0.5482, 0.4938, 0.5183,  ..., 0.5273, 0.4809, 0.6479],
          [0.5094, 0.5440, 0.4302,  ..., 0.5397, 0.6262, 0.5439],
          [0.6550, 0.5281, 0.5334,  ..., 0.3983, 0.4355, 0.3790]],

         [[0.3443, 0.5827, 0.4345,  ..., 0.4716, 0.3685, 0.5182],
          [0.5756, 0.3014, 0.4041,  ..., 0.4206, 0.2920, 0.4644],
          [0.3740, 0.6034, 0.4112,  ..., 0.6748, 0.4283, 0.4164],
          [0.4097, 0.4728, 0.6559,  ..., 0.4330, 0.4497, 0.4654]]]],
       device='cuda:0')
tensor([[[[0.5363, 0.5837, 0.4412,  ..., 0.4364, 0.4874, 0.5879],
          [0.5722, 0.6072, 0.4696,  ..., 0.6469, 0.4975, 0.4610],
          [0.5732, 0.4805, 0.5251,  ..., 0.5213, 0.5665, 0.6252],
          [0.4385, 0.4746, 0.6406,  ..., 0.5048, 0.4121, 0.5960]],

         [[0.6505, 0.5171, 0.5245,  ..., 0.4913, 0.5823, 0.5017],
          [0.5317, 0.4982, 0.3891,  ..., 0.3049, 0.4717, 0.5438],
          [0.4581, 0.5349, 0.6697,  ..., 0.4446, 0.5232, 0.5713],
          [0.5450, 0.4625, 0.5453,  ..., 0.6792, 0.4523, 0.5339]],

         [[0.4762, 0.4069, 0.4045,  ..., 0.4756, 0.5353, 0.4523],
          [0.3956, 0.5856, 0.3956,  ..., 0.4330, 0.3895, 0.4772],
          [0.5067, 0.3390, 0.5324,  ..., 0.4040, 0.3730, 0.5978],
          [0.5261, 0.4571, 0.5460,  ..., 0.5660, 0.4738, 0.5780]],

         ...,

         [[0.5974, 0.3363, 0.4554,  ..., 0.3149, 0.4187, 0.6325],
          [0.5135, 0.5412, 0.4499,  ..., 0.6021, 0.5349, 0.4092],
          [0.3433, 0.5204, 0.4140,  ..., 0.4960, 0.4936, 0.4625],
          [0.5193, 0.6619, 0.4911,  ..., 0.5278, 0.5179, 0.6496]],

         [[0.3576, 0.5813, 0.4705,  ..., 0.5172, 0.4535, 0.6165],
          [0.4258, 0.5913, 0.3984,  ..., 0.4971, 0.4564, 0.3808],
          [0.4187, 0.4434, 0.6132,  ..., 0.3603, 0.3303, 0.4613],
          [0.4149, 0.5794, 0.3979,  ..., 0.5670, 0.4574, 0.5964]],

         [[0.5252, 0.4840, 0.5997,  ..., 0.4769, 0.5756, 0.5884],
          [0.4097, 0.6270, 0.4961,  ..., 0.5329, 0.5339, 0.5484],
          [0.4244, 0.4916, 0.4774,  ..., 0.5029, 0.6388, 0.6261],
          [0.4306, 0.4996, 0.5142,  ..., 0.3639, 0.4168, 0.4929]]],


        [[[0.5742, 0.3785, 0.6442,  ..., 0.4211, 0.5832, 0.6442],
          [0.5433, 0.4857, 0.5974,  ..., 0.4915, 0.6188, 0.4950],
          [0.4395, 0.5775, 0.4540,  ..., 0.5463, 0.5453, 0.5088],
          [0.3285, 0.3872, 0.4487,  ..., 0.4861, 0.3407, 0.5699]],

         [[0.6943, 0.4824, 0.6628,  ..., 0.5482, 0.5530, 0.5308],
          [0.4003, 0.5094, 0.3947,  ..., 0.4639, 0.5189, 0.3730],
          [0.3495, 0.3979, 0.4850,  ..., 0.6086, 0.4429, 0.4354],
          [0.4249, 0.4726, 0.5008,  ..., 0.6280, 0.4664, 0.6800]],

         [[0.4718, 0.7194, 0.4297,  ..., 0.6851, 0.4562, 0.5317],
          [0.4073, 0.2967, 0.4059,  ..., 0.6091, 0.5039, 0.4981],
          [0.4965, 0.6234, 0.6100,  ..., 0.4681, 0.5052, 0.4808],
          [0.4693, 0.6174, 0.3495,  ..., 0.5646, 0.5670, 0.3970]],

         ...,

         [[0.4349, 0.4826, 0.5244,  ..., 0.4412, 0.4647, 0.3711],
          [0.5684, 0.4959, 0.5306,  ..., 0.6343, 0.4780, 0.4874],
          [0.4812, 0.6179, 0.5255,  ..., 0.4854, 0.5578, 0.3469],
          [0.3942, 0.6243, 0.5600,  ..., 0.5234, 0.6174, 0.5784]],

         [[0.3979, 0.7098, 0.6095,  ..., 0.3914, 0.5055, 0.4932],
          [0.5472, 0.4948, 0.5173,  ..., 0.5283, 0.4799, 0.6469],
          [0.5084, 0.5450, 0.4292,  ..., 0.5407, 0.6252, 0.5429],
          [0.6540, 0.5291, 0.5324,  ..., 0.3993, 0.4345, 0.3780]],

         [[0.3433, 0.5837, 0.4335,  ..., 0.4726, 0.3675, 0.5172],
          [0.5746, 0.3024, 0.4031,  ..., 0.4216, 0.2910, 0.4634],
          [0.3730, 0.6044, 0.4102,  ..., 0.6758, 0.4273, 0.4154],
          [0.4087, 0.4738, 0.6549,  ..., 0.4340, 0.4487, 0.4644]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0010, -0.0010,  0.0010, -0.0010,  0.0010,  0.0010, -0.0010, -0.0010,
         0.0010,  0.0010], device='cuda:0')
selected experts tensor([1494, 1905, 1536, 1667, 1590, 1422, 1812, 1814, 1587, 1557],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.3896, 0.3397, 0.5240,  ..., 0.4467, 0.3975, 0.5382],
          [0.4908, 0.4484, 0.4937,  ..., 0.4578, 0.6119, 0.2664],
          [0.3740, 0.6644, 0.6297,  ..., 0.5212, 0.4999, 0.4040],
          [0.4810, 0.4125, 0.3793,  ..., 0.6916, 0.4036, 0.5722]],

         [[0.4283, 0.5992, 0.5156,  ..., 0.6233, 0.3304, 0.5200],
          [0.4934, 0.7032, 0.5964,  ..., 0.4710, 0.4268, 0.6029],
          [0.5373, 0.5172, 0.4875,  ..., 0.6306, 0.5129, 0.5010],
          [0.6353, 0.4225, 0.4349,  ..., 0.4805, 0.5465, 0.4210]],

         [[0.4121, 0.4496, 0.5394,  ..., 0.4830, 0.3373, 0.5094],
          [0.5885, 0.5385, 0.4144,  ..., 0.4277, 0.5427, 0.4239],
          [0.3804, 0.5225, 0.5203,  ..., 0.4335, 0.4456, 0.4935],
          [0.5704, 0.3423, 0.4263,  ..., 0.5133, 0.4545, 0.5626]],

         ...,

         [[0.5189, 0.3747, 0.5127,  ..., 0.5155, 0.4278, 0.5822],
          [0.4164, 0.4845, 0.5898,  ..., 0.6270, 0.4772, 0.4244],
          [0.4654, 0.3258, 0.4694,  ..., 0.5552, 0.6235, 0.4106],
          [0.5325, 0.5143, 0.6378,  ..., 0.4390, 0.4924, 0.5109]],

         [[0.4632, 0.4106, 0.4654,  ..., 0.4923, 0.3910, 0.4096],
          [0.4311, 0.4139, 0.4557,  ..., 0.5331, 0.3721, 0.4711],
          [0.5632, 0.4428, 0.5273,  ..., 0.4450, 0.5576, 0.5322],
          [0.4640, 0.4766, 0.6468,  ..., 0.5043, 0.3749, 0.5722]],

         [[0.5732, 0.4914, 0.4874,  ..., 0.5898, 0.4311, 0.6224],
          [0.5155, 0.7223, 0.4708,  ..., 0.3476, 0.5419, 0.5336],
          [0.5937, 0.4191, 0.6609,  ..., 0.5869, 0.4511, 0.5527],
          [0.4974, 0.3858, 0.4920,  ..., 0.6512, 0.4391, 0.3164]]],


        [[[0.3703, 0.4248, 0.4380,  ..., 0.4838, 0.3435, 0.4984],
          [0.4837, 0.4206, 0.5469,  ..., 0.4654, 0.4424, 0.4750],
          [0.5166, 0.5190, 0.4339,  ..., 0.3983, 0.4292, 0.4400],
          [0.3947, 0.4371, 0.5693,  ..., 0.6756, 0.5117, 0.4518]],

         [[0.3269, 0.5242, 0.4460,  ..., 0.5722, 0.4331, 0.6807],
          [0.5307, 0.5736, 0.6155,  ..., 0.4943, 0.4331, 0.6883],
          [0.5205, 0.3620, 0.4818,  ..., 0.4229, 0.5089, 0.4661],
          [0.3523, 0.4885, 0.5307,  ..., 0.5508, 0.5275, 0.4685]],

         [[0.5127, 0.5152, 0.5931,  ..., 0.4111, 0.3933, 0.4590],
          [0.5560, 0.4951, 0.4876,  ..., 0.5183, 0.4420, 0.3336],
          [0.5545, 0.5580, 0.6423,  ..., 0.5703, 0.4321, 0.4277],
          [0.5292, 0.3566, 0.4220,  ..., 0.6108, 0.4278, 0.6495]],

         ...,

         [[0.4050, 0.4287, 0.3459,  ..., 0.4790, 0.4518, 0.4234],
          [0.4814, 0.6113, 0.5094,  ..., 0.4972, 0.4226, 0.5636],
          [0.5847, 0.5501, 0.6260,  ..., 0.4711, 0.4555, 0.3336],
          [0.4562, 0.4201, 0.4021,  ..., 0.5684, 0.6073, 0.4825]],

         [[0.4569, 0.4739, 0.5893,  ..., 0.5712, 0.3622, 0.3701],
          [0.3999, 0.6270, 0.4753,  ..., 0.6477, 0.3244, 0.4894],
          [0.5256, 0.3683, 0.6396,  ..., 0.6173, 0.4221, 0.4044],
          [0.5685, 0.4030, 0.5621,  ..., 0.5494, 0.6054, 0.5339]],

         [[0.5288, 0.4144, 0.5076,  ..., 0.4813, 0.3541, 0.4467],
          [0.6759, 0.5206, 0.5269,  ..., 0.5266, 0.2375, 0.5025],
          [0.5818, 0.4853, 0.5846,  ..., 0.4932, 0.4767, 0.5477],
          [0.5262, 0.4404, 0.6182,  ..., 0.7200, 0.2977, 0.5794]]]],
       device='cuda:0')
tensor([[[[0.3886, 0.3407, 0.5250,  ..., 0.4477, 0.3965, 0.5392],
          [0.4898, 0.4494, 0.4947,  ..., 0.4588, 0.6109, 0.2674],
          [0.3730, 0.6654, 0.6307,  ..., 0.5222, 0.4989, 0.4050],
          [0.4800, 0.4135, 0.3803,  ..., 0.6926, 0.4026, 0.5732]],

         [[0.4273, 0.6002, 0.5166,  ..., 0.6243, 0.3294, 0.5210],
          [0.4924, 0.7042, 0.5974,  ..., 0.4720, 0.4258, 0.6039],
          [0.5363, 0.5182, 0.4885,  ..., 0.6316, 0.5119, 0.5020],
          [0.6343, 0.4235, 0.4359,  ..., 0.4815, 0.5455, 0.4220]],

         [[0.4111, 0.4506, 0.5404,  ..., 0.4840, 0.3363, 0.5104],
          [0.5875, 0.5395, 0.4154,  ..., 0.4287, 0.5417, 0.4249],
          [0.3794, 0.5235, 0.5213,  ..., 0.4345, 0.4446, 0.4945],
          [0.5694, 0.3433, 0.4273,  ..., 0.5143, 0.4535, 0.5636]],

         ...,

         [[0.5179, 0.3757, 0.5137,  ..., 0.5165, 0.4268, 0.5832],
          [0.4154, 0.4855, 0.5908,  ..., 0.6280, 0.4762, 0.4254],
          [0.4644, 0.3268, 0.4704,  ..., 0.5562, 0.6225, 0.4116],
          [0.5315, 0.5153, 0.6388,  ..., 0.4400, 0.4914, 0.5119]],

         [[0.4622, 0.4116, 0.4664,  ..., 0.4933, 0.3900, 0.4106],
          [0.4301, 0.4149, 0.4567,  ..., 0.5341, 0.3711, 0.4721],
          [0.5622, 0.4438, 0.5283,  ..., 0.4460, 0.5566, 0.5332],
          [0.4630, 0.4776, 0.6478,  ..., 0.5053, 0.3739, 0.5732]],

         [[0.5722, 0.4924, 0.4884,  ..., 0.5908, 0.4301, 0.6234],
          [0.5145, 0.7233, 0.4718,  ..., 0.3486, 0.5409, 0.5346],
          [0.5927, 0.4201, 0.6619,  ..., 0.5879, 0.4501, 0.5537],
          [0.4964, 0.3868, 0.4930,  ..., 0.6522, 0.4381, 0.3174]]],


        [[[0.3693, 0.4258, 0.4390,  ..., 0.4848, 0.3425, 0.4994],
          [0.4827, 0.4216, 0.5479,  ..., 0.4664, 0.4414, 0.4760],
          [0.5156, 0.5200, 0.4349,  ..., 0.3993, 0.4282, 0.4410],
          [0.3937, 0.4381, 0.5703,  ..., 0.6766, 0.5107, 0.4528]],

         [[0.3259, 0.5252, 0.4470,  ..., 0.5732, 0.4321, 0.6817],
          [0.5297, 0.5746, 0.6165,  ..., 0.4953, 0.4321, 0.6893],
          [0.5195, 0.3630, 0.4828,  ..., 0.4239, 0.5079, 0.4671],
          [0.3513, 0.4895, 0.5317,  ..., 0.5518, 0.5265, 0.4695]],

         [[0.5117, 0.5162, 0.5941,  ..., 0.4121, 0.3923, 0.4600],
          [0.5550, 0.4961, 0.4886,  ..., 0.5193, 0.4410, 0.3346],
          [0.5535, 0.5590, 0.6433,  ..., 0.5713, 0.4311, 0.4287],
          [0.5282, 0.3576, 0.4230,  ..., 0.6118, 0.4268, 0.6505]],

         ...,

         [[0.4040, 0.4297, 0.3469,  ..., 0.4800, 0.4508, 0.4244],
          [0.4804, 0.6123, 0.5104,  ..., 0.4982, 0.4216, 0.5646],
          [0.5837, 0.5511, 0.6270,  ..., 0.4721, 0.4545, 0.3346],
          [0.4552, 0.4211, 0.4031,  ..., 0.5694, 0.6063, 0.4835]],

         [[0.4559, 0.4749, 0.5903,  ..., 0.5722, 0.3612, 0.3711],
          [0.3989, 0.6280, 0.4763,  ..., 0.6487, 0.3234, 0.4904],
          [0.5246, 0.3693, 0.6406,  ..., 0.6183, 0.4211, 0.4054],
          [0.5675, 0.4040, 0.5631,  ..., 0.5504, 0.6044, 0.5349]],

         [[0.5278, 0.4154, 0.5086,  ..., 0.4823, 0.3531, 0.4477],
          [0.6749, 0.5216, 0.5279,  ..., 0.5276, 0.2365, 0.5035],
          [0.5808, 0.4863, 0.5856,  ..., 0.4942, 0.4757, 0.5487],
          [0.5252, 0.4414, 0.6192,  ..., 0.7210, 0.2967, 0.5804]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0010, -0.0010, -0.0010,  0.0010, -0.0010,  0.0010,  0.0010, -0.0010,
         0.0010, -0.0010], device='cuda:0')
selected experts tensor([1836, 1770, 2041, 1391, 1520, 1689, 1785, 1736,  622, 1994],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 512, 4, 10])
tensor([[[[0.4663, 0.4897, 0.4869,  ..., 0.4226, 0.5250, 0.4168],
          [0.5426, 0.5789, 0.3310,  ..., 0.4211, 0.7104, 0.4331],
          [0.5136, 0.5221, 0.5157,  ..., 0.5221, 0.6670, 0.6603],
          [0.4823, 0.4915, 0.4840,  ..., 0.5337, 0.5950, 0.5514]],

         [[0.5789, 0.4581, 0.5817,  ..., 0.5368, 0.4409, 0.3845],
          [0.4450, 0.5358, 0.4734,  ..., 0.3339, 0.4438, 0.4302],
          [0.6034, 0.5561, 0.6178,  ..., 0.4676, 0.7407, 0.4831],
          [0.5968, 0.5474, 0.3114,  ..., 0.3795, 0.4844, 0.5613]],

         [[0.4330, 0.5542, 0.5641,  ..., 0.4798, 0.4919, 0.6068],
          [0.4182, 0.4325, 0.6025,  ..., 0.3776, 0.6315, 0.4857],
          [0.3867, 0.3997, 0.4930,  ..., 0.6271, 0.6670, 0.4767],
          [0.5641, 0.5343, 0.4525,  ..., 0.5533, 0.4007, 0.5790]],

         ...,

         [[0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010],
          [0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010],
          [0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010],
          [0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010]],

         [[0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010],
          [0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010],
          [0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010],
          [0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010]],

         [[0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010],
          [0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010],
          [0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010],
          [0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010]]],


        [[[0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010],
          [0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010],
          [0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010],
          [0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010]],

         [[0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010],
          [0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010],
          [0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010],
          [0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010]],

         [[0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010],
          [0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010],
          [0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010],
          [0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010]],

         ...,

         [[0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010],
          [0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010],
          [0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010],
          [0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010]],

         [[0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010],
          [0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010],
          [0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010],
          [0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010]],

         [[0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010],
          [0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010],
          [0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010],
          [0.4990, 0.4990, 0.4990,  ..., 0.5010, 0.4990, 0.5010]]]],
       device='cuda:0')/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([[[[0.4673, 0.4907, 0.4879,  ..., 0.4216, 0.5260, 0.4158],
          [0.5436, 0.5799, 0.3320,  ..., 0.4201, 0.7114, 0.4321],
          [0.5146, 0.5231, 0.5167,  ..., 0.5211, 0.6680, 0.6593],
          [0.4833, 0.4925, 0.4850,  ..., 0.5327, 0.5960, 0.5504]],

         [[0.5799, 0.4591, 0.5827,  ..., 0.5358, 0.4419, 0.3835],
          [0.4460, 0.5368, 0.4744,  ..., 0.3329, 0.4448, 0.4292],
          [0.6044, 0.5571, 0.6188,  ..., 0.4666, 0.7417, 0.4821],
          [0.5978, 0.5484, 0.3124,  ..., 0.3785, 0.4854, 0.5603]],

         [[0.4340, 0.5552, 0.5651,  ..., 0.4788, 0.4929, 0.6058],
          [0.4192, 0.4335, 0.6035,  ..., 0.3766, 0.6325, 0.4847],
          [0.3877, 0.4007, 0.4940,  ..., 0.6261, 0.6680, 0.4757],
          [0.5651, 0.5353, 0.4535,  ..., 0.5523, 0.4017, 0.5780]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0010, -0.0010, -0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,
        -0.0010,  0.0010], device='cuda:0')
selected experts tensor([ 280,  337,  512, 2556, 2568,  309,  371,  209,  669,  381],
       device='cuda:0')
total tokens tensor(8192, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4513, 0.5131, 0.6444,  ..., 0.5342, 0.4070, 0.5556],
          [0.6149, 0.5101, 0.6073,  ..., 0.5866, 0.5359, 0.4679],
          [0.5532, 0.5764, 0.4089,  ..., 0.3777, 0.5918, 0.4414],
          [0.5240, 0.7739, 0.4531,  ..., 0.4918, 0.4131, 0.5812]],

         [[0.5094, 0.6140, 0.4495,  ..., 0.3777, 0.4889, 0.6529],
          [0.4890, 0.5038, 0.3480,  ..., 0.5352, 0.3976, 0.5107],
          [0.3317, 0.3903, 0.5316,  ..., 0.4765, 0.5049, 0.4272],
          [0.5033, 0.4757, 0.5330,  ..., 0.6059, 0.4490, 0.4414]],

         [[0.4162, 0.3343, 0.4009,  ..., 0.3392, 0.3401, 0.6122],
          [0.3964, 0.6005, 0.6254,  ..., 0.5352, 0.4719, 0.4753],
          [0.4101, 0.5329, 0.4671,  ..., 0.4791, 0.3632, 0.4011],
          [0.5010, 0.5214, 0.5557,  ..., 0.5574, 0.4732, 0.4925]],

         ...,

         [[0.5542, 0.5602, 0.5956,  ..., 0.5292, 0.3357, 0.5204],
          [0.6024, 0.5888, 0.6354,  ..., 0.5033, 0.3489, 0.5731],
          [0.4859, 0.5855, 0.4831,  ..., 0.5446, 0.4747, 0.4947],
          [0.5095, 0.5621, 0.6444,  ..., 0.3892, 0.4866, 0.4464]],

         [[0.4353, 0.5114, 0.4302,  ..., 0.5032, 0.3194, 0.5153],
          [0.7070, 0.4394, 0.3915,  ..., 0.5054, 0.6462, 0.4414],
          [0.4641, 0.3778, 0.4307,  ..., 0.4321, 0.4117, 0.6573],
          [0.4776, 0.4969, 0.3713,  ..., 0.5560, 0.5063, 0.5755]],

         [[0.4583, 0.3012, 0.4642,  ..., 0.3925, 0.4207, 0.3710],
          [0.5621, 0.5926, 0.4331,  ..., 0.6083, 0.4341, 0.4547],
          [0.6981, 0.5304, 0.4046,  ..., 0.5113, 0.4735, 0.5774],
          [0.6278, 0.2578, 0.4487,  ..., 0.5168, 0.3418, 0.4902]]],


        [[[0.4890, 0.5845, 0.4886,  ..., 0.4922, 0.4869, 0.3969],
          [0.5864, 0.4901, 0.6138,  ..., 0.4916, 0.5577, 0.5392],
          [0.5274, 0.5326, 0.4719,  ..., 0.5184, 0.5738, 0.6043],
          [0.4902, 0.4888, 0.4567,  ..., 0.5123, 0.6630, 0.4148]],

         [[0.6538, 0.4020, 0.4572,  ..., 0.4729, 0.4875, 0.3637],
          [0.4305, 0.6010, 0.5618,  ..., 0.5352, 0.3809, 0.4191],
          [0.5469, 0.5418, 0.4927,  ..., 0.6031, 0.6120, 0.5578],
          [0.5242, 0.5198, 0.4164,  ..., 0.4360, 0.4079, 0.6168]],

         [[0.5440, 0.3997, 0.5613,  ..., 0.4516, 0.6921, 0.5299],
          [0.4044, 0.5195, 0.6381,  ..., 0.6157, 0.4365, 0.4636],
          [0.4181, 0.5045, 0.4750,  ..., 0.5495, 0.5499, 0.6066],
          [0.4563, 0.6223, 0.3750,  ..., 0.5159, 0.4221, 0.3317]],

         ...,

         [[0.4402, 0.4542, 0.4841,  ..., 0.5555, 0.5280, 0.4210],
          [0.4377, 0.5372, 0.5933,  ..., 0.4952, 0.5171, 0.4291],
          [0.4993, 0.5597, 0.5087,  ..., 0.6354, 0.4859, 0.5295],
          [0.3871, 0.6269, 0.3560,  ..., 0.5226, 0.6622, 0.4843]],

         [[0.4832, 0.4334, 0.6444,  ..., 0.5109, 0.5478, 0.6005],
          [0.4229, 0.5498, 0.5061,  ..., 0.4957, 0.4864, 0.4503],
          [0.4840, 0.3838, 0.4013,  ..., 0.5998, 0.4536, 0.4738],
          [0.5650, 0.5968, 0.5088,  ..., 0.4485, 0.4698, 0.3936]],

         [[0.6956, 0.3964, 0.5302,  ..., 0.4824, 0.5776, 0.5292],
          [0.4834, 0.4191, 0.5344,  ..., 0.5053, 0.5885, 0.5105],
          [0.3405, 0.4578, 0.6281,  ..., 0.4667, 0.6300, 0.2802],
          [0.4143, 0.4893, 0.3551,  ..., 0.5800, 0.4245, 0.4510]]]],
       device='cuda:0')
tensor([[[[0.4533, 0.5151, 0.6424,  ..., 0.5322, 0.4050, 0.5576],
          [0.6169, 0.5121, 0.6053,  ..., 0.5846, 0.5339, 0.4699],
          [0.5552, 0.5784, 0.4069,  ..., 0.3757, 0.5898, 0.4434],
          [0.5260, 0.7759, 0.4511,  ..., 0.4898, 0.4111, 0.5832]],

         [[0.5114, 0.6160, 0.4475,  ..., 0.3757, 0.4869, 0.6549],
          [0.4910, 0.5058, 0.3460,  ..., 0.5332, 0.3956, 0.5127],
          [0.3337, 0.3923, 0.5296,  ..., 0.4745, 0.5029, 0.4292],
          [0.5053, 0.4777, 0.5310,  ..., 0.6039, 0.4470, 0.4434]],

         [[0.4182, 0.3363, 0.3989,  ..., 0.3372, 0.3381, 0.6142],
          [0.3984, 0.6025, 0.6234,  ..., 0.5332, 0.4699, 0.4773],
          [0.4121, 0.5349, 0.4651,  ..., 0.4771, 0.3612, 0.4031],
          [0.5030, 0.5234, 0.5537,  ..., 0.5554, 0.4712, 0.4945]],

         ...,

         [[0.5562, 0.5622, 0.5936,  ..., 0.5272, 0.3337, 0.5224],
          [0.6044, 0.5908, 0.6334,  ..., 0.5013, 0.3469, 0.5751],
          [0.4879, 0.5875, 0.4811,  ..., 0.5426, 0.4727, 0.4967],
          [0.5115, 0.5641, 0.6424,  ..., 0.3872, 0.4846, 0.4484]],

         [[0.4373, 0.5134, 0.4282,  ..., 0.5012, 0.3174, 0.5173],
          [0.7090, 0.4414, 0.3895,  ..., 0.5034, 0.6442, 0.4434],
          [0.4661, 0.3798, 0.4287,  ..., 0.4301, 0.4097, 0.6593],
          [0.4796, 0.4989, 0.3693,  ..., 0.5540, 0.5043, 0.5775]],

         [[0.4603, 0.3032, 0.4622,  ..., 0.3905, 0.4187, 0.3730],
          [0.5641, 0.5946, 0.4311,  ..., 0.6063, 0.4321, 0.4567],
          [0.7001, 0.5324, 0.4026,  ..., 0.5093, 0.4715, 0.5794],
          [0.6298, 0.2598, 0.4467,  ..., 0.5148, 0.3398, 0.4922]]],


        [[[0.4910, 0.5865, 0.4866,  ..., 0.4902, 0.4849, 0.3989],
          [0.5884, 0.4921, 0.6118,  ..., 0.4896, 0.5557, 0.5412],
          [0.5294, 0.5346, 0.4699,  ..., 0.5164, 0.5718, 0.6063],
          [0.4922, 0.4908, 0.4547,  ..., 0.5103, 0.6610, 0.4168]],

         [[0.6558, 0.4040, 0.4552,  ..., 0.4709, 0.4855, 0.3657],
          [0.4325, 0.6030, 0.5598,  ..., 0.5332, 0.3789, 0.4211],
          [0.5489, 0.5438, 0.4907,  ..., 0.6011, 0.6100, 0.5598],
          [0.5262, 0.5218, 0.4144,  ..., 0.4340, 0.4059, 0.6188]],

         [[0.5460, 0.4017, 0.5593,  ..., 0.4496, 0.6901, 0.5319],
          [0.4064, 0.5215, 0.6361,  ..., 0.6137, 0.4345, 0.4656],
          [0.4201, 0.5065, 0.4730,  ..., 0.5475, 0.5479, 0.6086],
          [0.4583, 0.6243, 0.3730,  ..., 0.5139, 0.4201, 0.3337]],

         ...,

         [[0.4422, 0.4562, 0.4821,  ..., 0.5535, 0.5260, 0.4230],
          [0.4397, 0.5392, 0.5913,  ..., 0.4932, 0.5151, 0.4311],
          [0.5013, 0.5617, 0.5067,  ..., 0.6334, 0.4839, 0.5315],
          [0.3891, 0.6289, 0.3540,  ..., 0.5206, 0.6602, 0.4863]],

         [[0.4852, 0.4354, 0.6424,  ..., 0.5089, 0.5458, 0.6025],
          [0.4249, 0.5518, 0.5041,  ..., 0.4937, 0.4844, 0.4523],
          [0.4860, 0.3858, 0.3993,  ..., 0.5978, 0.4516, 0.4758],
          [0.5670, 0.5988, 0.5068,  ..., 0.4465, 0.4678, 0.3956]],

         [[0.6976, 0.3984, 0.5282,  ..., 0.4804, 0.5756, 0.5312],
          [0.4854, 0.4211, 0.5324,  ..., 0.5033, 0.5865, 0.5125],
          [0.3425, 0.4598, 0.6261,  ..., 0.4647, 0.6280, 0.2822],
          [0.4163, 0.4913, 0.3531,  ..., 0.5780, 0.4225, 0.4530]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0020, -0.0020,  0.0020,  0.0020,  0.0020,  0.0020, -0.0020,  0.0020,
         0.0020, -0.0020], device='cuda:0')
selected experts tensor([1781, 1632, 1650, 1536, 1602, 1613, 1770, 1562, 1517, 1721],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5159, 0.5968, 0.5036,  ..., 0.5590, 0.4521, 0.4891],
          [0.5240, 0.5319, 0.4326,  ..., 0.4025, 0.5298, 0.5174],
          [0.5468, 0.4998, 0.5928,  ..., 0.5132, 0.4516, 0.6226],
          [0.4723, 0.5426, 0.5319,  ..., 0.4566, 0.5885, 0.4953]],

         [[0.4681, 0.5702, 0.5271,  ..., 0.3493, 0.5610, 0.5620],
          [0.6327, 0.5907, 0.4018,  ..., 0.4779, 0.3632, 0.4802],
          [0.4317, 0.4708, 0.4122,  ..., 0.5996, 0.4769, 0.5497],
          [0.4842, 0.4824, 0.5497,  ..., 0.6108, 0.4630, 0.5461]],

         [[0.4803, 0.4262, 0.5591,  ..., 0.5831, 0.5216, 0.6235],
          [0.4574, 0.4522, 0.3436,  ..., 0.6898, 0.4009, 0.5029],
          [0.4659, 0.5530, 0.4892,  ..., 0.3248, 0.4136, 0.4929],
          [0.5134, 0.2548, 0.4504,  ..., 0.3171, 0.5002, 0.5097]],

         ...,

         [[0.5175, 0.5292, 0.5800,  ..., 0.3239, 0.5015, 0.4611],
          [0.4183, 0.5561, 0.6399,  ..., 0.6865, 0.3874, 0.4838],
          [0.3722, 0.4172, 0.5349,  ..., 0.4334, 0.6700, 0.4966],
          [0.3759, 0.4248, 0.6203,  ..., 0.5307, 0.4868, 0.5704]],

         [[0.4560, 0.5265, 0.4832,  ..., 0.5285, 0.5502, 0.5733],
          [0.5138, 0.5223, 0.6138,  ..., 0.4515, 0.6444, 0.4727],
          [0.4871, 0.5450, 0.4705,  ..., 0.4706, 0.4771, 0.3722],
          [0.6017, 0.4911, 0.4046,  ..., 0.6005, 0.4861, 0.5410]],

         [[0.5733, 0.5041, 0.4691,  ..., 0.4757, 0.3211, 0.4589],
          [0.6453, 0.3655, 0.5838,  ..., 0.5056, 0.5497, 0.5492],
          [0.4495, 0.6422, 0.4589,  ..., 0.4522, 0.5994, 0.4623],
          [0.5398, 0.4696, 0.4255,  ..., 0.3815, 0.4032, 0.6996]]],


        [[[0.5083, 0.5117, 0.7021,  ..., 0.5741, 0.4221, 0.5184],
          [0.5565, 0.4698, 0.5332,  ..., 0.5760, 0.4809, 0.4155],
          [0.3943, 0.4433, 0.5766,  ..., 0.5341, 0.6208, 0.3587],
          [0.5666, 0.4291, 0.4828,  ..., 0.4590, 0.5661, 0.5714]],

         [[0.5562, 0.6449, 0.5468,  ..., 0.6287, 0.6363, 0.4398],
          [0.4112, 0.4708, 0.5151,  ..., 0.4978, 0.4996, 0.5458],
          [0.4965, 0.5029, 0.4574,  ..., 0.4960, 0.6390, 0.6309],
          [0.5695, 0.4903, 0.6390,  ..., 0.5358, 0.5615, 0.4601]],

         [[0.5134, 0.5517, 0.5209,  ..., 0.6296, 0.6560, 0.4216],
          [0.3786, 0.6024, 0.3641,  ..., 0.5585, 0.4738, 0.5994],
          [0.4664, 0.6061, 0.4981,  ..., 0.4375, 0.5148, 0.4948],
          [0.4961, 0.5104, 0.5757,  ..., 0.5585, 0.3925, 0.3686]],

         ...,

         [[0.3480, 0.4859, 0.4155,  ..., 0.3765, 0.4811, 0.3777],
          [0.3999, 0.5764, 0.5642,  ..., 0.5350, 0.4594, 0.6203],
          [0.6106, 0.4702, 0.5618,  ..., 0.4044, 0.6996, 0.5483],
          [0.4686, 0.5491, 0.4093,  ..., 0.4619, 0.4369, 0.6569]],

         [[0.5937, 0.4784, 0.5182,  ..., 0.4091, 0.4742, 0.4611],
          [0.5824, 0.5491, 0.5766,  ..., 0.5430, 0.3596, 0.4274],
          [0.5989, 0.4172, 0.4207,  ..., 0.5493, 0.4968, 0.4827],
          [0.3731, 0.5014, 0.5171,  ..., 0.5944, 0.4065, 0.3967]],

         [[0.4468, 0.4537, 0.4579,  ..., 0.4938, 0.6542, 0.5181],
          [0.4430, 0.5413, 0.4548,  ..., 0.5505, 0.4750, 0.5016],
          [0.5639, 0.4305, 0.5176,  ..., 0.4774, 0.4817, 0.4193],
          [0.5742, 0.5510, 0.6017,  ..., 0.4101, 0.4714, 0.5405]]]],
       device='cuda:0')
tensor([[[[0.5139, 0.5988, 0.5016,  ..., 0.5610, 0.4501, 0.4871],
          [0.5220, 0.5339, 0.4306,  ..., 0.4045, 0.5278, 0.5154],
          [0.5448, 0.5018, 0.5908,  ..., 0.5152, 0.4496, 0.6206],
          [0.4703, 0.5446, 0.5299,  ..., 0.4586, 0.5865, 0.4933]],

         [[0.4661, 0.5722, 0.5251,  ..., 0.3513, 0.5590, 0.5600],
          [0.6307, 0.5927, 0.3998,  ..., 0.4799, 0.3612, 0.4782],
          [0.4297, 0.4728, 0.4102,  ..., 0.6016, 0.4749, 0.5477],
          [0.4822, 0.4844, 0.5477,  ..., 0.6128, 0.4610, 0.5441]],

         [[0.4783, 0.4282, 0.5571,  ..., 0.5851, 0.5196, 0.6215],
          [0.4554, 0.4542, 0.3416,  ..., 0.6918, 0.3989, 0.5009],
          [0.4639, 0.5550, 0.4872,  ..., 0.3268, 0.4116, 0.4909],
          [0.5114, 0.2568, 0.4484,  ..., 0.3191, 0.4982, 0.5077]],

         ...,

         [[0.5155, 0.5312, 0.5780,  ..., 0.3259, 0.4995, 0.4591],
          [0.4163, 0.5581, 0.6379,  ..., 0.6885, 0.3854, 0.4818],
          [0.3702, 0.4192, 0.5329,  ..., 0.4354, 0.6680, 0.4946],
          [0.3739, 0.4268, 0.6183,  ..., 0.5327, 0.4848, 0.5684]],

         [[0.4540, 0.5285, 0.4812,  ..., 0.5305, 0.5482, 0.5713],
          [0.5118, 0.5243, 0.6118,  ..., 0.4535, 0.6424, 0.4707],
          [0.4851, 0.5470, 0.4685,  ..., 0.4726, 0.4751, 0.3702],
          [0.5997, 0.4931, 0.4026,  ..., 0.6025, 0.4841, 0.5390]],

         [[0.5713, 0.5061, 0.4671,  ..., 0.4777, 0.3191, 0.4569],
          [0.6433, 0.3675, 0.5818,  ..., 0.5076, 0.5477, 0.5472],
          [0.4475, 0.6442, 0.4569,  ..., 0.4542, 0.5974, 0.4603],
          [0.5378, 0.4716, 0.4235,  ..., 0.3835, 0.4012, 0.6976]]],


        [[[0.5063, 0.5137, 0.7001,  ..., 0.5761, 0.4201, 0.5164],
          [0.5545, 0.4718, 0.5312,  ..., 0.5780, 0.4789, 0.4135],
          [0.3923, 0.4453, 0.5746,  ..., 0.5361, 0.6188, 0.3567],
          [0.5646, 0.4311, 0.4808,  ..., 0.4610, 0.5641, 0.5694]],

         [[0.5542, 0.6469, 0.5448,  ..., 0.6307, 0.6343, 0.4378],
          [0.4092, 0.4728, 0.5131,  ..., 0.4998, 0.4976, 0.5438],
          [0.4945, 0.5049, 0.4554,  ..., 0.4980, 0.6370, 0.6289],
          [0.5675, 0.4923, 0.6370,  ..., 0.5378, 0.5595, 0.4581]],

         [[0.5114, 0.5537, 0.5189,  ..., 0.6316, 0.6540, 0.4196],
          [0.3766, 0.6044, 0.3621,  ..., 0.5605, 0.4718, 0.5974],
          [0.4644, 0.6081, 0.4961,  ..., 0.4395, 0.5128, 0.4928],
          [0.4941, 0.5124, 0.5737,  ..., 0.5605, 0.3905, 0.3666]],

         ...,

         [[0.3460, 0.4879, 0.4135,  ..., 0.3785, 0.4791, 0.3757],
          [0.3979, 0.5784, 0.5622,  ..., 0.5370, 0.4574, 0.6183],
          [0.6086, 0.4722, 0.5598,  ..., 0.4064, 0.6976, 0.5463],
          [0.4666, 0.5511, 0.4073,  ..., 0.4639, 0.4349, 0.6549]],

         [[0.5917, 0.4804, 0.5162,  ..., 0.4111, 0.4722, 0.4591],
          [0.5804, 0.5511, 0.5746,  ..., 0.5450, 0.3576, 0.4254],
          [0.5969, 0.4192, 0.4187,  ..., 0.5513, 0.4948, 0.4807],
          [0.3711, 0.5034, 0.5151,  ..., 0.5964, 0.4045, 0.3947]],

         [[0.4448, 0.4557, 0.4559,  ..., 0.4958, 0.6522, 0.5161],
          [0.4410, 0.5433, 0.4528,  ..., 0.5525, 0.4730, 0.4996],
          [0.5619, 0.4325, 0.5156,  ..., 0.4794, 0.4797, 0.4173],
          [0.5722, 0.5530, 0.5997,  ..., 0.4121, 0.4694, 0.5385]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0020, -0.0020,  0.0020, -0.0020,  0.0020,  0.0020, -0.0020, -0.0020,
         0.0020,  0.0020], device='cuda:0')
selected experts tensor([1555, 1778, 1610, 1758, 1671, 1469, 1718, 1774, 1564, 1487],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.3657, 0.4481, 0.6617,  ..., 0.5137, 0.3948, 0.3978],
          [0.4106, 0.5116, 0.3945,  ..., 0.6136, 0.3934, 0.2373],
          [0.4410, 0.6136, 0.6721,  ..., 0.5784, 0.5092, 0.4334],
          [0.5699, 0.3475, 0.4105,  ..., 0.5973, 0.4780, 0.6241]],

         [[0.5903, 0.5826, 0.6529,  ..., 0.5626, 0.4718, 0.4622],
          [0.5708, 0.6839, 0.4277,  ..., 0.5807, 0.4155, 0.5503],
          [0.4852, 0.4703, 0.5268,  ..., 0.5370, 0.3971, 0.3788],
          [0.6540, 0.2708, 0.4277,  ..., 0.4717, 0.4226, 0.5498]],

         [[0.5694, 0.5380, 0.3343,  ..., 0.3619, 0.4060, 0.4787],
          [0.4823, 0.4851, 0.4399,  ..., 0.3848, 0.4696, 0.4205],
          [0.5188, 0.5309, 0.5817,  ..., 0.4702, 0.3489, 0.4795],
          [0.4278, 0.3983, 0.4110,  ..., 0.5430, 0.4174, 0.6368]],

         ...,

         [[0.5789, 0.5190, 0.5578,  ..., 0.5459, 0.4259, 0.3931],
          [0.4036, 0.6431, 0.4491,  ..., 0.6047, 0.4895, 0.4238],
          [0.5034, 0.4661, 0.5788,  ..., 0.5106, 0.5072, 0.4258],
          [0.4681, 0.4806, 0.4286,  ..., 0.4607, 0.4555, 0.4167]],

         [[0.4858, 0.5065, 0.5537,  ..., 0.4472, 0.4341, 0.4186],
          [0.5742, 0.5469, 0.6057,  ..., 0.4753, 0.5344, 0.4607],
          [0.4888, 0.6296, 0.5779,  ..., 0.3913, 0.3962, 0.4370],
          [0.5540, 0.3783, 0.5000,  ..., 0.5626, 0.4056, 0.5764]],

         [[0.6030, 0.4124, 0.5501,  ..., 0.3889, 0.3177, 0.4760],
          [0.5132, 0.5493, 0.4910,  ..., 0.4768, 0.5369, 0.5496],
          [0.5708, 0.5803, 0.4730,  ..., 0.3936, 0.4733, 0.4443],
          [0.5827, 0.3646, 0.5314,  ..., 0.5268, 0.4742, 0.5688]]],


        [[[0.4576, 0.5826, 0.4917,  ..., 0.5525, 0.4492, 0.5307],
          [0.5329, 0.4607, 0.4044,  ..., 0.5418, 0.5361, 0.4583],
          [0.6081, 0.4086, 0.5659,  ..., 0.3538, 0.4764, 0.4387],
          [0.4991, 0.5640, 0.3619,  ..., 0.5836, 0.4202, 0.4138]],

         [[0.4726, 0.5779, 0.6205,  ..., 0.4752, 0.5620, 0.5513],
          [0.5670, 0.5435, 0.4759,  ..., 0.7336, 0.3445, 0.4971],
          [0.5993, 0.4176, 0.5745,  ..., 0.5252, 0.4437, 0.4072],
          [0.4997, 0.3029, 0.4561,  ..., 0.4899, 0.5410, 0.5578]],

         [[0.5266, 0.6287, 0.4830,  ..., 0.4016, 0.3357, 0.3941],
          [0.5137, 0.4513, 0.5435,  ..., 0.5253, 0.4633, 0.5185],
          [0.5894, 0.6332, 0.5563,  ..., 0.6449, 0.3650, 0.5284],
          [0.6442, 0.4205, 0.3936,  ..., 0.5397, 0.5272, 0.6305]],

         ...,

         [[0.6433, 0.6200, 0.5522,  ..., 0.4939, 0.4543, 0.4990],
          [0.5525, 0.5764, 0.4110,  ..., 0.6071, 0.4708, 0.5174],
          [0.6352, 0.3917, 0.4976,  ..., 0.6140, 0.5470, 0.3556],
          [0.5146, 0.3719, 0.4715,  ..., 0.4775, 0.4483, 0.5640]],

         [[0.4201, 0.4291, 0.4329,  ..., 0.5037, 0.4103, 0.6223],
          [0.5936, 0.5544, 0.5195,  ..., 0.4706, 0.5332, 0.4020],
          [0.6397, 0.4648, 0.4760,  ..., 0.5235, 0.4103, 0.4692],
          [0.5265, 0.4243, 0.4110,  ..., 0.5698, 0.4027, 0.6431]],

         [[0.4617, 0.3214, 0.6159,  ..., 0.4049, 0.3851, 0.5443],
          [0.3979, 0.5888, 0.3619,  ..., 0.5165, 0.4449, 0.3788],
          [0.5765, 0.4527, 0.5341,  ..., 0.4286, 0.4514, 0.4762],
          [0.5470, 0.3983, 0.5779,  ..., 0.5246, 0.4601, 0.6529]]]],
       device='cuda:0')
tensor([[[[0.3657, 0.4501, 0.6637,  ..., 0.5157, 0.3928, 0.3998],
          [0.4106, 0.5136, 0.3965,  ..., 0.6156, 0.3914, 0.2393],
          [0.4410, 0.6156, 0.6741,  ..., 0.5804, 0.5072, 0.4354],
          [0.5699, 0.3495, 0.4125,  ..., 0.5993, 0.4760, 0.6261]],

         [[0.5903, 0.5846, 0.6549,  ..., 0.5646, 0.4698, 0.4642],
          [0.5708, 0.6859, 0.4297,  ..., 0.5827, 0.4135, 0.5523],
          [0.4852, 0.4723, 0.5288,  ..., 0.5390, 0.3951, 0.3808],
          [0.6540, 0.2728, 0.4297,  ..., 0.4737, 0.4206, 0.5518]],

         [[0.5694, 0.5400, 0.3363,  ..., 0.3639, 0.4040, 0.4807],
          [0.4823, 0.4871, 0.4419,  ..., 0.3868, 0.4676, 0.4225],
          [0.5188, 0.5329, 0.5837,  ..., 0.4722, 0.3469, 0.4815],
          [0.4278, 0.4003, 0.4130,  ..., 0.5450, 0.4154, 0.6388]],

         ...,

         [[0.5789, 0.5210, 0.5598,  ..., 0.5479, 0.4239, 0.3951],
          [0.4036, 0.6451, 0.4511,  ..., 0.6067, 0.4875, 0.4258],
          [0.5034, 0.4681, 0.5808,  ..., 0.5126, 0.5052, 0.4278],
          [0.4681, 0.4826, 0.4306,  ..., 0.4627, 0.4535, 0.4187]],

         [[0.4858, 0.5085, 0.5557,  ..., 0.4492, 0.4321, 0.4206],
          [0.5742, 0.5489, 0.6077,  ..., 0.4773, 0.5324, 0.4627],
          [0.4888, 0.6316, 0.5799,  ..., 0.3933, 0.3942, 0.4390],
          [0.5540, 0.3803, 0.5020,  ..., 0.5646, 0.4036, 0.5784]],

         [[0.6030, 0.4144, 0.5521,  ..., 0.3909, 0.3157, 0.4780],
          [0.5132, 0.5513, 0.4930,  ..., 0.4788, 0.5349, 0.5516],
          [0.5708, 0.5823, 0.4750,  ..., 0.3956, 0.4713, 0.4463],
          [0.5827, 0.3666, 0.5334,  ..., 0.5288, 0.4722, 0.5708]]],


        [[[0.4576, 0.5846, 0.4937,  ..., 0.5545, 0.4472, 0.5327],
          [0.5329, 0.4627, 0.4064,  ..., 0.5438, 0.5341, 0.4603],
          [0.6081, 0.4106, 0.5679,  ..., 0.3558, 0.4744, 0.4407],
          [0.4991, 0.5660, 0.3639,  ..., 0.5856, 0.4182, 0.4158]],

         [[0.4726, 0.5799, 0.6225,  ..., 0.4772, 0.5600, 0.5533],
          [0.5670, 0.5455, 0.4779,  ..., 0.7356, 0.3425, 0.4991],
          [0.5993, 0.4196, 0.5765,  ..., 0.5272, 0.4417, 0.4092],
          [0.4997, 0.3049, 0.4581,  ..., 0.4919, 0.5390, 0.5598]],

         [[0.5266, 0.6307, 0.4850,  ..., 0.4036, 0.3337, 0.3961],
          [0.5137, 0.4533, 0.5455,  ..., 0.5273, 0.4613, 0.5205],
          [0.5894, 0.6352, 0.5583,  ..., 0.6469, 0.3630, 0.5304],
          [0.6442, 0.4225, 0.3956,  ..., 0.5417, 0.5252, 0.6325]],

         ...,

         [[0.6433, 0.6220, 0.5542,  ..., 0.4959, 0.4523, 0.5010],
          [0.5525, 0.5784, 0.4130,  ..., 0.6091, 0.4688, 0.5194],
          [0.6352, 0.3937, 0.4996,  ..., 0.6160, 0.5450, 0.3576],
          [0.5146, 0.3739, 0.4735,  ..., 0.4795, 0.4463, 0.5660]],

         [[0.4201, 0.4311, 0.4349,  ..., 0.5057, 0.4083, 0.6243],
          [0.5936, 0.5564, 0.5215,  ..., 0.4726, 0.5312, 0.4040],
          [0.6397, 0.4668, 0.4780,  ..., 0.5255, 0.4083, 0.4712],
          [0.5265, 0.4263, 0.4130,  ..., 0.5718, 0.4007, 0.6451]],

         [[0.4617, 0.3234, 0.6179,  ..., 0.4069, 0.3831, 0.5463],
          [0.3979, 0.5908, 0.3639,  ..., 0.5185, 0.4429, 0.3808],
          [0.5765, 0.4547, 0.5361,  ..., 0.4306, 0.4494, 0.4782],
          [0.5470, 0.4003, 0.5799,  ..., 0.5266, 0.4581, 0.6549]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0000, -0.0020, -0.0020,  0.0020,  0.0000,  0.0000,  0.0000, -0.0020,
         0.0020, -0.0020], device='cuda:0')
selected experts tensor([1822, 1878, 1935, 1163, 2018, 1621, 1525, 1787,  648, 1987],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.5051, 0.6192, 0.5780,  ..., 0.4594, 0.7009, 0.4881],
          [0.5222, 0.5636, 0.5317,  ..., 0.4603, 0.5261, 0.4908],
          [0.5689, 0.5279, 0.6478,  ..., 0.4150, 0.6100, 0.5371],
          [0.5392, 0.6715, 0.4757,  ..., 0.4403, 0.5784, 0.5781]],

         [[0.4634, 0.6192, 0.6169,  ..., 0.3578, 0.4930, 0.4550],
          [0.4828, 0.5137, 0.4211,  ..., 0.4117, 0.4838, 0.4705],
          [0.5455, 0.4530, 0.5622,  ..., 0.5833, 0.5217, 0.5244],
          [0.6406, 0.5015, 0.3460,  ..., 0.5499, 0.4608, 0.6551]],

         [[0.5619, 0.5997, 0.5446,  ..., 0.4555, 0.4921, 0.4897],
          [0.5319, 0.5506, 0.4656,  ..., 0.4906, 0.5578, 0.3587],
          [0.6741, 0.4610, 0.5249,  ..., 0.4962, 0.6885, 0.6595],
          [0.5694, 0.4625, 0.3900,  ..., 0.5226, 0.4777, 0.4964]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020]]],


        [[[0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5000, 0.5020]]]],
       device='cuda:0')
tensor([[[[0.5051, 0.6192, 0.5780,  ..., 0.4574, 0.7009, 0.4861],
          [0.5222, 0.5636, 0.5317,  ..., 0.4583, 0.5261, 0.4888],
          [0.5689, 0.5279, 0.6478,  ..., 0.4130, 0.6100, 0.5351],
          [0.5392, 0.6715, 0.4757,  ..., 0.4383, 0.5784, 0.5761]],

         [[0.4634, 0.6192, 0.6169,  ..., 0.3558, 0.4930, 0.4530],
          [0.4828, 0.5137, 0.4211,  ..., 0.4097, 0.4838, 0.4685],
          [0.5455, 0.4530, 0.5622,  ..., 0.5813, 0.5217, 0.5224],
          [0.6406, 0.5015, 0.3460,  ..., 0.5479, 0.4608, 0.6531]],

         [[0.5619, 0.5997, 0.5446,  ..., 0.4535, 0.4921, 0.4877],
          [0.5319, 0.5506, 0.4656,  ..., 0.4886, 0.5578, 0.3567],
          [0.6741, 0.4610, 0.5249,  ..., 0.4942, 0.6885, 0.6575],
          [0.5694, 0.4625, 0.3900,  ..., 0.5206, 0.4777, 0.4944]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0020, 0.0020, 0.0020, 0.0000,
        0.0020], device='cuda:0')
selected experts tensor([ 481,  410,  492,  434,  581, 4318, 4111,  157,  778,  526],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6268, 0.4012, 0.6115,  ..., 0.5938, 0.5391, 0.4893],
          [0.5606, 0.6297, 0.4235,  ..., 0.4684, 0.4582, 0.5014],
          [0.4329, 0.5467, 0.3943,  ..., 0.4265, 0.4222, 0.4505],
          [0.4355, 0.5443, 0.4946,  ..., 0.4084, 0.5215, 0.6139]],

         [[0.4760, 0.6441, 0.4991,  ..., 0.5647, 0.4621, 0.6624],
          [0.4720, 0.5722, 0.5766,  ..., 0.4701, 0.6693, 0.6987],
          [0.4295, 0.4092, 0.5456,  ..., 0.6382, 0.4005, 0.4975],
          [0.3709, 0.5515, 0.5927,  ..., 0.4103, 0.2940, 0.3290]],

         [[0.5887, 0.4957, 0.5151,  ..., 0.5393, 0.5055, 0.4551],
          [0.4310, 0.5513, 0.6077,  ..., 0.5269, 0.3642, 0.6615],
          [0.4710, 0.5903, 0.3443,  ..., 0.3778, 0.4711, 0.4658],
          [0.4039, 0.2529, 0.4904,  ..., 0.3324, 0.3660, 0.5311]],

         ...,

         [[0.4839, 0.4390, 0.5833,  ..., 0.5560, 0.4807, 0.3917],
          [0.4510, 0.4287, 0.5856,  ..., 0.5999, 0.5700, 0.5012],
          [0.5302, 0.4344, 0.4145,  ..., 0.4425, 0.4901, 0.4233],
          [0.3750, 0.4532, 0.4159,  ..., 0.3977, 0.4652, 0.4836]],

         [[0.6728, 0.7293, 0.6280,  ..., 0.4698, 0.6400, 0.5428],
          [0.5377, 0.4932, 0.6110,  ..., 0.5425, 0.4893, 0.6204],
          [0.5520, 0.5846, 0.6353,  ..., 0.4913, 0.4862, 0.4665],
          [0.4416, 0.3923, 0.3479,  ..., 0.5258, 0.5275, 0.5401]],

         [[0.4319, 0.5827, 0.6142,  ..., 0.5237, 0.3967, 0.5625],
          [0.5592, 0.5000, 0.4569,  ..., 0.7047, 0.4880, 0.4500],
          [0.4360, 0.5450, 0.5685,  ..., 0.4870, 0.3750, 0.4442],
          [0.5173, 0.5865, 0.4871,  ..., 0.5328, 0.4493, 0.6065]]],


        [[[0.4568, 0.5893, 0.5016,  ..., 0.6981, 0.5531, 0.5716],
          [0.4653, 0.4513, 0.3882,  ..., 0.4567, 0.5886, 0.4324],
          [0.6014, 0.4196, 0.4793,  ..., 0.5369, 0.5237, 0.3465],
          [0.4935, 0.3319, 0.3649,  ..., 0.4703, 0.6065, 0.5089]],

         [[0.5664, 0.4263, 0.5271,  ..., 0.5695, 0.5083, 0.5188],
          [0.5916, 0.6958, 0.4805,  ..., 0.5290, 0.5666, 0.6222],
          [0.4896, 0.5110, 0.4492,  ..., 0.5039, 0.4751, 0.4520],
          [0.5524, 0.4479, 0.4345,  ..., 0.4893, 0.4584, 0.6667]],

         [[0.4392, 0.4035, 0.4671,  ..., 0.5608, 0.5270, 0.4816],
          [0.5224, 0.4315, 0.4709,  ..., 0.5943, 0.6445, 0.4835],
          [0.2928, 0.4153, 0.4812,  ..., 0.4174, 0.5781, 0.6112],
          [0.6589, 0.5336, 0.4979,  ..., 0.5534, 0.3741, 0.4607]],

         ...,

         [[0.4425, 0.4603, 0.4742,  ..., 0.3907, 0.4427, 0.3921],
          [0.4020, 0.4666, 0.5918,  ..., 0.6779, 0.4108, 0.6403],
          [0.4683, 0.5983, 0.6105,  ..., 0.5086, 0.4108, 0.5664],
          [0.6056, 0.4096, 0.6179,  ..., 0.5276, 0.4236, 0.3255]],

         [[0.4238, 0.6169, 0.4669,  ..., 0.4996, 0.5488, 0.4558],
          [0.4861, 0.6297, 0.5110,  ..., 0.2613, 0.4370, 0.3439],
          [0.6037, 0.5407, 0.6380,  ..., 0.4760, 0.4878, 0.4624],
          [0.4602, 0.5755, 0.4499,  ..., 0.4926, 0.5709, 0.5146]],

         [[0.4339, 0.5547, 0.4895,  ..., 0.6570, 0.6153, 0.5321],
          [0.4024, 0.5869, 0.4302,  ..., 0.3062, 0.4795, 0.4057],
          [0.4892, 0.4919, 0.5429,  ..., 0.6176, 0.5705, 0.5707],
          [0.6728, 0.4822, 0.5443,  ..., 0.4505, 0.5239, 0.3759]]]],
       device='cuda:0')
tensor([[[[0.6298, 0.4022, 0.6105,  ..., 0.5908, 0.5361, 0.4923],
          [0.5636, 0.6307, 0.4225,  ..., 0.4654, 0.4552, 0.5044],
          [0.4359, 0.5477, 0.3933,  ..., 0.4235, 0.4192, 0.4535],
          [0.4385, 0.5453, 0.4936,  ..., 0.4054, 0.5185, 0.6169]],

         [[0.4790, 0.6451, 0.4981,  ..., 0.5617, 0.4591, 0.6654],
          [0.4750, 0.5732, 0.5756,  ..., 0.4671, 0.6663, 0.7017],
          [0.4325, 0.4102, 0.5446,  ..., 0.6352, 0.3975, 0.5005],
          [0.3739, 0.5525, 0.5917,  ..., 0.4073, 0.2910, 0.3320]],

         [[0.5917, 0.4967, 0.5141,  ..., 0.5363, 0.5025, 0.4581],
          [0.4340, 0.5523, 0.6067,  ..., 0.5239, 0.3612, 0.6645],
          [0.4740, 0.5913, 0.3433,  ..., 0.3748, 0.4681, 0.4688],
          [0.4069, 0.2539, 0.4894,  ..., 0.3294, 0.3630, 0.5341]],

         ...,

         [[0.4869, 0.4400, 0.5823,  ..., 0.5530, 0.4777, 0.3947],
          [0.4540, 0.4297, 0.5846,  ..., 0.5969, 0.5670, 0.5042],
          [0.5332, 0.4354, 0.4135,  ..., 0.4395, 0.4871, 0.4263],
          [0.3780, 0.4542, 0.4149,  ..., 0.3947, 0.4622, 0.4866]],

         [[0.6758, 0.7303, 0.6270,  ..., 0.4668, 0.6370, 0.5458],
          [0.5407, 0.4942, 0.6100,  ..., 0.5395, 0.4863, 0.6234],
          [0.5550, 0.5856, 0.6343,  ..., 0.4883, 0.4832, 0.4695],
          [0.4446, 0.3933, 0.3469,  ..., 0.5228, 0.5245, 0.5431]],

         [[0.4349, 0.5837, 0.6132,  ..., 0.5207, 0.3937, 0.5655],
          [0.5622, 0.5010, 0.4559,  ..., 0.7017, 0.4850, 0.4530],
          [0.4390, 0.5460, 0.5675,  ..., 0.4840, 0.3720, 0.4472],
          [0.5203, 0.5875, 0.4861,  ..., 0.5297, 0.4463, 0.6095]]],


        [[[0.4598, 0.5903, 0.5006,  ..., 0.6951, 0.5501, 0.5746],
          [0.4683, 0.4523, 0.3872,  ..., 0.4537, 0.5856, 0.4354],
          [0.6044, 0.4206, 0.4783,  ..., 0.5339, 0.5207, 0.3495],
          [0.4965, 0.3329, 0.3639,  ..., 0.4673, 0.6035, 0.5119]],

         [[0.5694, 0.4273, 0.5261,  ..., 0.5665, 0.5053, 0.5218],
          [0.5946, 0.6968, 0.4795,  ..., 0.5260, 0.5636, 0.6252],
          [0.4926, 0.5120, 0.4482,  ..., 0.5009, 0.4721, 0.4550],
          [0.5554, 0.4489, 0.4335,  ..., 0.4863, 0.4554, 0.6697]],

         [[0.4422, 0.4045, 0.4661,  ..., 0.5578, 0.5240, 0.4846],
          [0.5254, 0.4325, 0.4699,  ..., 0.5913, 0.6415, 0.4865],
          [0.2958, 0.4163, 0.4802,  ..., 0.4144, 0.5751, 0.6142],
          [0.6619, 0.5346, 0.4969,  ..., 0.5504, 0.3711, 0.4637]],

         ...,

         [[0.4455, 0.4613, 0.4732,  ..., 0.3877, 0.4397, 0.3951],
          [0.4050, 0.4676, 0.5908,  ..., 0.6749, 0.4078, 0.6433],
          [0.4713, 0.5993, 0.6095,  ..., 0.5056, 0.4078, 0.5694],
          [0.6086, 0.4106, 0.6169,  ..., 0.5246, 0.4206, 0.3285]],

         [[0.4268, 0.6179, 0.4659,  ..., 0.4966, 0.5458, 0.4588],
          [0.4891, 0.6307, 0.5100,  ..., 0.2583, 0.4340, 0.3469],
          [0.6067, 0.5417, 0.6370,  ..., 0.4730, 0.4848, 0.4654],
          [0.4632, 0.5765, 0.4489,  ..., 0.4896, 0.5679, 0.5176]],

         [[0.4369, 0.5557, 0.4885,  ..., 0.6540, 0.6123, 0.5351],
          [0.4054, 0.5879, 0.4292,  ..., 0.3032, 0.4765, 0.4087],
          [0.4922, 0.4929, 0.5419,  ..., 0.6146, 0.5675, 0.5737],
          [0.6758, 0.4832, 0.5433,  ..., 0.4475, 0.5209, 0.3789]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0030, -0.0010,  0.0010,  0.0030,  0.0030,  0.0030, -0.0030,  0.0030,
         0.0030, -0.0030], device='cuda:0')
selected experts tensor([1892, 1566, 1547, 1525, 1552, 1614, 1740, 1636, 1470, 1842],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5795, 0.4575, 0.5587,  ..., 0.6240, 0.6490, 0.5442],
          [0.3642, 0.4830, 0.4570,  ..., 0.5017, 0.6445, 0.3847],
          [0.5814, 0.4724, 0.5575,  ..., 0.5995, 0.5524, 0.4456],
          [0.5169, 0.4691, 0.6649,  ..., 0.3325, 0.5212, 0.5352]],

         [[0.4117, 0.4495, 0.5149,  ..., 0.6403, 0.4863, 0.4543],
          [0.6667, 0.4505, 0.4293,  ..., 0.5778, 0.4672, 0.4667],
          [0.4760, 0.5635, 0.5178,  ..., 0.5654, 0.4650, 0.5623],
          [0.5231, 0.5104, 0.3420,  ..., 0.3264, 0.5052, 0.6796]],

         [[0.5895, 0.5972, 0.5345,  ..., 0.4744, 0.5661, 0.4643],
          [0.6167, 0.4960, 0.6241,  ..., 0.6821, 0.6088, 0.5243],
          [0.4974, 0.4743, 0.5485,  ..., 0.5154, 0.6328, 0.5601],
          [0.3247, 0.5199, 0.4657,  ..., 0.5420, 0.4274, 0.5608]],

         ...,

         [[0.6125, 0.4765, 0.4770,  ..., 0.4157, 0.6102, 0.5572],
          [0.5178, 0.4466, 0.4921,  ..., 0.3035, 0.6013, 0.4437],
          [0.5439, 0.3229, 0.4919,  ..., 0.5522, 0.5606, 0.4360],
          [0.5705, 0.4491, 0.4136,  ..., 0.4595, 0.3750, 0.4747]],

         [[0.5608, 0.6979, 0.5485,  ..., 0.5257, 0.5136, 0.4179],
          [0.5580, 0.5294, 0.5909,  ..., 0.4271, 0.5197, 0.4623],
          [0.4584, 0.5425, 0.4284,  ..., 0.5474, 0.4531, 0.5290],
          [0.5461, 0.5343, 0.4834,  ..., 0.5222, 0.5536, 0.4864]],

         [[0.6346, 0.6572, 0.3516,  ..., 0.5645, 0.5876, 0.5301],
          [0.6710, 0.4979, 0.4965,  ..., 0.5577, 0.4934, 0.5905],
          [0.4413, 0.4776, 0.4246,  ..., 0.5379, 0.4435, 0.4341],
          [0.4640, 0.5041, 0.6623,  ..., 0.7012, 0.5212, 0.5719]]],


        [[[0.6255, 0.5219, 0.4589,  ..., 0.4960, 0.5862, 0.5117],
          [0.4994, 0.4380, 0.4817,  ..., 0.3856, 0.5189, 0.2367],
          [0.4898, 0.4858, 0.5002,  ..., 0.4209, 0.6018, 0.5705],
          [0.4184, 0.4575, 0.4265,  ..., 0.4546, 0.6684, 0.5514]],

         [[0.5637, 0.4904, 0.6445,  ..., 0.5577, 0.4293, 0.5454],
          [0.5976, 0.4553, 0.4464,  ..., 0.5721, 0.5334, 0.3842],
          [0.4066, 0.4295, 0.3723,  ..., 0.3865, 0.4887, 0.5567],
          [0.3561, 0.3681, 0.4293,  ..., 0.4133, 0.5478, 0.5393]],

         [[0.4531, 0.5565, 0.5425,  ..., 0.5673, 0.5695, 0.5990],
          [0.5524, 0.4392, 0.4580,  ..., 0.4634, 0.4483, 0.4664],
          [0.4708, 0.5750, 0.5451,  ..., 0.4920, 0.3455, 0.4464],
          [0.4946, 0.2553, 0.5016,  ..., 0.5887, 0.4170, 0.3787]],

         ...,

         [[0.5485, 0.3519, 0.4222,  ..., 0.4010, 0.4260, 0.5519],
          [0.4989, 0.5821, 0.6526,  ..., 0.4491, 0.4255, 0.4866],
          [0.3615, 0.5264, 0.4432,  ..., 0.5712, 0.5177, 0.5340],
          [0.6199, 0.4195, 0.4914,  ..., 0.4551, 0.4312, 0.3597]],

         [[0.5652, 0.5115, 0.5582,  ..., 0.4760, 0.5604, 0.5490],
          [0.4601, 0.4152, 0.5776,  ..., 0.4968, 0.5709, 0.5381],
          [0.5743, 0.5532, 0.4519,  ..., 0.4300, 0.3847, 0.5560],
          [0.5473, 0.4858, 0.5124,  ..., 0.6126, 0.5430, 0.5246]],

         [[0.4859, 0.5134, 0.4184,  ..., 0.6185, 0.5099, 0.3805],
          [0.3307, 0.4660, 0.4122,  ..., 0.5399, 0.4635, 0.5374],
          [0.5437, 0.4445, 0.5398,  ..., 0.3299, 0.6588, 0.4813],
          [0.5714, 0.4731, 0.2758,  ..., 0.5357, 0.3385, 0.3921]]]],
       device='cuda:0')
tensor([[[[0.5765, 0.4605, 0.5557,  ..., 0.6270, 0.6460, 0.5412],
          [0.3612, 0.4860, 0.4540,  ..., 0.5047, 0.6415, 0.3817],
          [0.5784, 0.4754, 0.5545,  ..., 0.6025, 0.5494, 0.4426],
          [0.5139, 0.4721, 0.6619,  ..., 0.3355, 0.5182, 0.5322]],

         [[0.4087, 0.4525, 0.5119,  ..., 0.6433, 0.4833, 0.4513],
          [0.6637, 0.4535, 0.4263,  ..., 0.5808, 0.4642, 0.4637],
          [0.4730, 0.5665, 0.5148,  ..., 0.5684, 0.4620, 0.5593],
          [0.5201, 0.5134, 0.3390,  ..., 0.3294, 0.5022, 0.6766]],

         [[0.5865, 0.6002, 0.5315,  ..., 0.4774, 0.5631, 0.4613],
          [0.6137, 0.4990, 0.6211,  ..., 0.6851, 0.6058, 0.5213],
          [0.4944, 0.4773, 0.5455,  ..., 0.5184, 0.6298, 0.5571],
          [0.3217, 0.5229, 0.4627,  ..., 0.5450, 0.4244, 0.5578]],

         ...,

         [[0.6095, 0.4795, 0.4740,  ..., 0.4187, 0.6072, 0.5542],
          [0.5148, 0.4496, 0.4891,  ..., 0.3065, 0.5983, 0.4407],
          [0.5409, 0.3259, 0.4889,  ..., 0.5552, 0.5576, 0.4330],
          [0.5675, 0.4521, 0.4106,  ..., 0.4625, 0.3720, 0.4717]],

         [[0.5578, 0.7009, 0.5455,  ..., 0.5287, 0.5106, 0.4149],
          [0.5550, 0.5324, 0.5879,  ..., 0.4301, 0.5167, 0.4593],
          [0.4554, 0.5455, 0.4254,  ..., 0.5504, 0.4501, 0.5260],
          [0.5431, 0.5373, 0.4804,  ..., 0.5252, 0.5506, 0.4834]],

         [[0.6316, 0.6602, 0.3486,  ..., 0.5675, 0.5846, 0.5271],
          [0.6680, 0.5009, 0.4935,  ..., 0.5607, 0.4904, 0.5875],
          [0.4383, 0.4806, 0.4216,  ..., 0.5409, 0.4405, 0.4311],
          [0.4610, 0.5071, 0.6593,  ..., 0.7042, 0.5182, 0.5689]]],


        [[[0.6225, 0.5249, 0.4559,  ..., 0.4990, 0.5832, 0.5087],
          [0.4964, 0.4410, 0.4787,  ..., 0.3886, 0.5159, 0.2337],
          [0.4868, 0.4888, 0.4972,  ..., 0.4239, 0.5988, 0.5675],
          [0.4154, 0.4605, 0.4235,  ..., 0.4576, 0.6654, 0.5484]],

         [[0.5607, 0.4934, 0.6415,  ..., 0.5607, 0.4263, 0.5424],
          [0.5946, 0.4583, 0.4434,  ..., 0.5751, 0.5304, 0.3812],
          [0.4036, 0.4325, 0.3693,  ..., 0.3895, 0.4857, 0.5537],
          [0.3531, 0.3711, 0.4263,  ..., 0.4163, 0.5448, 0.5363]],

         [[0.4501, 0.5595, 0.5395,  ..., 0.5703, 0.5665, 0.5960],
          [0.5494, 0.4422, 0.4550,  ..., 0.4664, 0.4453, 0.4634],
          [0.4678, 0.5780, 0.5421,  ..., 0.4950, 0.3425, 0.4434],
          [0.4916, 0.2583, 0.4986,  ..., 0.5917, 0.4140, 0.3757]],

         ...,

         [[0.5455, 0.3549, 0.4192,  ..., 0.4040, 0.4230, 0.5489],
          [0.4959, 0.5851, 0.6496,  ..., 0.4521, 0.4225, 0.4836],
          [0.3585, 0.5294, 0.4402,  ..., 0.5742, 0.5147, 0.5310],
          [0.6169, 0.4225, 0.4884,  ..., 0.4581, 0.4282, 0.3567]],

         [[0.5622, 0.5145, 0.5552,  ..., 0.4790, 0.5574, 0.5460],
          [0.4571, 0.4182, 0.5746,  ..., 0.4998, 0.5679, 0.5351],
          [0.5713, 0.5562, 0.4489,  ..., 0.4330, 0.3817, 0.5530],
          [0.5443, 0.4888, 0.5094,  ..., 0.6156, 0.5400, 0.5216]],

         [[0.4829, 0.5164, 0.4154,  ..., 0.6215, 0.5069, 0.3775],
          [0.3277, 0.4690, 0.4092,  ..., 0.5429, 0.4605, 0.5344],
          [0.5407, 0.4475, 0.5368,  ..., 0.3329, 0.6558, 0.4783],
          [0.5684, 0.4761, 0.2728,  ..., 0.5387, 0.3355, 0.3891]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0030, -0.0030,  0.0030, -0.0030,  0.0010,  0.0030, -0.0030, -0.0030,
         0.0030,  0.0030], device='cuda:0')
selected experts tensor([1557, 1818, 1561, 1537, 1602, 1639, 1651, 1740, 1665, 1614],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4735, 0.5413, 0.4205,  ..., 0.4370, 0.3606, 0.3448],
          [0.4975, 0.3801, 0.5430,  ..., 0.5972, 0.5117, 0.4921],
          [0.6858, 0.5413, 0.5788,  ..., 0.5302, 0.4293, 0.4319],
          [0.4748, 0.5510, 0.4854,  ..., 0.5250, 0.4370, 0.6554]],

         [[0.6432, 0.4945, 0.5778,  ..., 0.3884, 0.3819, 0.4563],
          [0.5431, 0.5911, 0.5991,  ..., 0.5507, 0.3991, 0.6195],
          [0.5193, 0.4726, 0.5573,  ..., 0.4034, 0.4365, 0.4602],
          [0.4855, 0.4162, 0.4367,  ..., 0.5041, 0.4490, 0.6615]],

         [[0.4632, 0.5678, 0.4612,  ..., 0.5692, 0.4418, 0.5187],
          [0.5727, 0.5148, 0.5094,  ..., 0.5423, 0.5175, 0.6172],
          [0.4605, 0.6185, 0.5678,  ..., 0.5231, 0.5165, 0.3977],
          [0.4639, 0.3782, 0.3377,  ..., 0.5235, 0.3986, 0.6277]],

         ...,

         [[0.5612, 0.6061, 0.5498,  ..., 0.4692, 0.5652, 0.5654],
          [0.5597, 0.6457, 0.4238,  ..., 0.4425, 0.4080, 0.5102],
          [0.4486, 0.5774, 0.4888,  ..., 0.5565, 0.6463, 0.5246],
          [0.5992, 0.5015, 0.4133,  ..., 0.6231, 0.3935, 0.4190]],

         [[0.4690, 0.5360, 0.6572,  ..., 0.4186, 0.5032, 0.5163],
          [0.5038, 0.5606, 0.5218,  ..., 0.6304, 0.4464, 0.5897],
          [0.4707, 0.4392, 0.4343,  ..., 0.5001, 0.6418, 0.5258],
          [0.6908, 0.4956, 0.3456,  ..., 0.5132, 0.3750, 0.5294]],

         [[0.4934, 0.4930, 0.5630,  ..., 0.4363, 0.4512, 0.5560],
          [0.5031, 0.5635, 0.6376,  ..., 0.4305, 0.3967, 0.5645],
          [0.5147, 0.4481, 0.4138,  ..., 0.4828, 0.4960, 0.4001],
          [0.5266, 0.5204, 0.4897,  ..., 0.4029, 0.3660, 0.5333]]],


        [[[0.6104, 0.5454, 0.6322,  ..., 0.3360, 0.5153, 0.4774],
          [0.5387, 0.6322, 0.3609,  ..., 0.5493, 0.5434, 0.5930],
          [0.4778, 0.4143, 0.4619,  ..., 0.4800, 0.3874, 0.4095],
          [0.5457, 0.3921, 0.5606,  ..., 0.5054, 0.5567, 0.6277]],

         [[0.4354, 0.4425, 0.6439,  ..., 0.4674, 0.4222, 0.6650],
          [0.5736, 0.5797, 0.4147,  ..., 0.4406, 0.4432, 0.6519],
          [0.4044, 0.4324, 0.4973,  ..., 0.4992, 0.5485, 0.3421],
          [0.3816, 0.3170, 0.5389,  ..., 0.4286, 0.5729, 0.4343]],

         [[0.5741, 0.4893, 0.4495,  ..., 0.3856, 0.4928, 0.4759],
          [0.5431, 0.5197, 0.5563,  ..., 0.4602, 0.3769, 0.4143],
          [0.4419, 0.4624, 0.5266,  ..., 0.4968, 0.4217, 0.5225],
          [0.3825, 0.4086, 0.4669,  ..., 0.5047, 0.4862, 0.5697]],

         ...,

         [[0.4607, 0.6421, 0.3709,  ..., 0.5051, 0.3615, 0.3764],
          [0.4818, 0.5948, 0.5802,  ..., 0.4305, 0.4466, 0.4190],
          [0.4923, 0.5032, 0.5683,  ..., 0.4817, 0.5349, 0.5575],
          [0.5631, 0.4491, 0.5030,  ..., 0.3973, 0.4558, 0.6023]],

         [[0.5067, 0.4686, 0.5953,  ..., 0.4360, 0.4303, 0.3403],
          [0.5511, 0.6641, 0.5078,  ..., 0.5250, 0.4033, 0.4969],
          [0.6378, 0.6051, 0.4319,  ..., 0.4466, 0.5225, 0.4267],
          [0.4830, 0.5345, 0.4859,  ..., 0.4818, 0.4719, 0.5350]],

         [[0.5595, 0.5105, 0.5558,  ..., 0.4507, 0.5172, 0.5343],
          [0.5817, 0.4867, 0.3907,  ..., 0.5778, 0.4541, 0.4725],
          [0.5540, 0.4537, 0.5606,  ..., 0.3582, 0.5114, 0.5104],
          [0.4229, 0.4281, 0.4110,  ..., 0.6331, 0.4308, 0.3750]]]],
       device='cuda:0')
tensor([[[[0.4745, 0.5443, 0.4235,  ..., 0.4400, 0.3576, 0.3478],
          [0.4985, 0.3831, 0.5460,  ..., 0.6002, 0.5087, 0.4951],
          [0.6868, 0.5443, 0.5818,  ..., 0.5332, 0.4263, 0.4349],
          [0.4758, 0.5540, 0.4884,  ..., 0.5280, 0.4340, 0.6584]],

         [[0.6442, 0.4975, 0.5808,  ..., 0.3914, 0.3789, 0.4593],
          [0.5441, 0.5941, 0.6021,  ..., 0.5537, 0.3961, 0.6225],
          [0.5203, 0.4756, 0.5603,  ..., 0.4064, 0.4335, 0.4632],
          [0.4865, 0.4192, 0.4397,  ..., 0.5071, 0.4460, 0.6645]],

         [[0.4642, 0.5708, 0.4642,  ..., 0.5722, 0.4388, 0.5217],
          [0.5737, 0.5178, 0.5124,  ..., 0.5453, 0.5145, 0.6202],
          [0.4615, 0.6215, 0.5708,  ..., 0.5261, 0.5135, 0.4007],
          [0.4649, 0.3812, 0.3407,  ..., 0.5265, 0.3956, 0.6307]],

         ...,

         [[0.5622, 0.6091, 0.5528,  ..., 0.4722, 0.5622, 0.5684],
          [0.5607, 0.6487, 0.4268,  ..., 0.4455, 0.4050, 0.5132],
          [0.4496, 0.5804, 0.4918,  ..., 0.5595, 0.6433, 0.5276],
          [0.6002, 0.5045, 0.4163,  ..., 0.6261, 0.3905, 0.4220]],

         [[0.4700, 0.5390, 0.6602,  ..., 0.4216, 0.5002, 0.5193],
          [0.5048, 0.5636, 0.5248,  ..., 0.6334, 0.4434, 0.5927],
          [0.4717, 0.4422, 0.4373,  ..., 0.5031, 0.6388, 0.5288],
          [0.6918, 0.4986, 0.3486,  ..., 0.5162, 0.3720, 0.5324]],

         [[0.4944, 0.4960, 0.5660,  ..., 0.4393, 0.4482, 0.5590],
          [0.5041, 0.5665, 0.6406,  ..., 0.4335, 0.3937, 0.5675],
          [0.5157, 0.4511, 0.4168,  ..., 0.4858, 0.4930, 0.4031],
          [0.5276, 0.5234, 0.4927,  ..., 0.4059, 0.3630, 0.5363]]],


        [[[0.6114, 0.5484, 0.6352,  ..., 0.3390, 0.5123, 0.4804],
          [0.5397, 0.6352, 0.3639,  ..., 0.5523, 0.5404, 0.5960],
          [0.4788, 0.4173, 0.4649,  ..., 0.4830, 0.3844, 0.4125],
          [0.5467, 0.3951, 0.5636,  ..., 0.5084, 0.5537, 0.6307]],

         [[0.4364, 0.4455, 0.6469,  ..., 0.4704, 0.4192, 0.6680],
          [0.5746, 0.5827, 0.4177,  ..., 0.4436, 0.4402, 0.6549],
          [0.4054, 0.4354, 0.5003,  ..., 0.5022, 0.5455, 0.3451],
          [0.3826, 0.3200, 0.5419,  ..., 0.4316, 0.5699, 0.4373]],

         [[0.5751, 0.4923, 0.4525,  ..., 0.3886, 0.4898, 0.4789],
          [0.5441, 0.5227, 0.5593,  ..., 0.4632, 0.3739, 0.4173],
          [0.4429, 0.4654, 0.5296,  ..., 0.4998, 0.4187, 0.5255],
          [0.3835, 0.4116, 0.4699,  ..., 0.5077, 0.4832, 0.5727]],

         ...,

         [[0.4617, 0.6451, 0.3739,  ..., 0.5081, 0.3585, 0.3794],
          [0.4828, 0.5978, 0.5832,  ..., 0.4335, 0.4436, 0.4220],
          [0.4933, 0.5062, 0.5713,  ..., 0.4847, 0.5319, 0.5605],
          [0.5641, 0.4521, 0.5060,  ..., 0.4003, 0.4528, 0.6053]],

         [[0.5077, 0.4716, 0.5983,  ..., 0.4390, 0.4273, 0.3433],
          [0.5521, 0.6671, 0.5108,  ..., 0.5280, 0.4003, 0.4999],
          [0.6388, 0.6081, 0.4349,  ..., 0.4496, 0.5195, 0.4297],
          [0.4840, 0.5375, 0.4889,  ..., 0.4848, 0.4689, 0.5380]],

         [[0.5605, 0.5135, 0.5588,  ..., 0.4537, 0.5142, 0.5373],
          [0.5827, 0.4897, 0.3937,  ..., 0.5808, 0.4511, 0.4755],
          [0.5550, 0.4567, 0.5636,  ..., 0.3612, 0.5084, 0.5134],
          [0.4239, 0.4311, 0.4140,  ..., 0.6361, 0.4278, 0.3780]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([-0.0010, -0.0030, -0.0030,  0.0030, -0.0010,  0.0010,  0.0010, -0.0030,
         0.0030, -0.0030], device='cuda:0')
selected experts tensor([1593, 2052, 1648, 1231, 1994, 1594, 1705, 1950,  617, 2000],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4446, 0.4966, 0.6928,  ..., 0.4580, 0.4789, 0.3787],
          [0.3961, 0.4178, 0.5143,  ..., 0.4284, 0.6244, 0.5207],
          [0.6253, 0.6054, 0.5965,  ..., 0.4430, 0.6647, 0.5824],
          [0.5144, 0.5145, 0.4916,  ..., 0.7039, 0.4868, 0.4495]],

         [[0.4041, 0.4748, 0.6541,  ..., 0.4888, 0.4206, 0.5248],
          [0.4429, 0.5388, 0.5120,  ..., 0.4370, 0.5366, 0.5018],
          [0.5785, 0.4395, 0.5178,  ..., 0.4403, 0.6091, 0.5957],
          [0.5937, 0.5184, 0.4453,  ..., 0.5127, 0.4494, 0.6209]],

         [[0.5552, 0.6541, 0.6202,  ..., 0.4650, 0.4509, 0.5772],
          [0.3795, 0.4064, 0.4074,  ..., 0.4459, 0.6280, 0.5118],
          [0.3859, 0.4364, 0.5269,  ..., 0.4575, 0.5894, 0.4122],
          [0.5538, 0.4316, 0.4886,  ..., 0.4808, 0.3443, 0.6032]],

         ...,

         [[0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030]],

         [[0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030]],

         [[0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030]]],


        [[[0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030]],

         [[0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030]],

         [[0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030]],

         ...,

         [[0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030]],

         [[0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030]],

         [[0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5010, 0.5030]]]],
       device='cuda:0')
tensor([[[[0.4436, 0.4956, 0.6918,  ..., 0.4550, 0.4779, 0.3757],
          [0.3951, 0.4168, 0.5133,  ..., 0.4254, 0.6234, 0.5177],
          [0.6243, 0.6044, 0.5955,  ..., 0.4400, 0.6637, 0.5794],
          [0.5134, 0.5135, 0.4906,  ..., 0.7009, 0.4858, 0.4465]],

         [[0.4031, 0.4738, 0.6531,  ..., 0.4858, 0.4196, 0.5218],
          [0.4419, 0.5378, 0.5110,  ..., 0.4340, 0.5356, 0.4988],
          [0.5775, 0.4385, 0.5168,  ..., 0.4373, 0.6081, 0.5927],
          [0.5927, 0.5174, 0.4443,  ..., 0.5097, 0.4484, 0.6179]],

         [[0.5542, 0.6531, 0.6192,  ..., 0.4620, 0.4499, 0.5742],
          [0.3785, 0.4054, 0.4064,  ..., 0.4429, 0.6270, 0.5088],
          [0.3849, 0.4354, 0.5259,  ..., 0.4545, 0.5884, 0.4092],
          [0.5528, 0.4306, 0.4876,  ..., 0.4778, 0.3433, 0.6002]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0030, 0.0010,
        0.0030], device='cuda:0')
selected experts tensor([ 333,  473,  565,  716,  557,  818,  683, 5505, 1013, 5721],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4415, 0.5603, 0.4004,  ..., 0.4001, 0.5430, 0.5639],
          [0.5237, 0.5595, 0.3995,  ..., 0.6135, 0.5671, 0.5849],
          [0.3680, 0.5132, 0.6846,  ..., 0.4469, 0.4459, 0.4994],
          [0.4437, 0.3693, 0.5128,  ..., 0.5872, 0.5094, 0.5706]],

         [[0.2454, 0.5293, 0.5591,  ..., 0.4778, 0.5719, 0.5938],
          [0.3572, 0.4827, 0.4046,  ..., 0.5153, 0.5705, 0.4415],
          [0.4483, 0.4891, 0.5168,  ..., 0.4493, 0.3697, 0.4204],
          [0.4365, 0.5875, 0.6665,  ..., 0.4356, 0.6028, 0.5768]],

         [[0.5098, 0.5192, 0.5909,  ..., 0.6121, 0.4313, 0.4478],
          [0.5473, 0.4946, 0.4207,  ..., 0.4298, 0.3616, 0.4437],
          [0.4176, 0.4848, 0.5236,  ..., 0.4823, 0.4527, 0.4836],
          [0.5514, 0.4443, 0.5521,  ..., 0.2942, 0.4308, 0.4005]],

         ...,

         [[0.5495, 0.4092, 0.5598,  ..., 0.5251, 0.6908, 0.4920],
          [0.3726, 0.6442, 0.4894,  ..., 0.4800, 0.4942, 0.6605],
          [0.4114, 0.5429, 0.3085,  ..., 0.4184, 0.4953, 0.5172],
          [0.3865, 0.4345, 0.6272,  ..., 0.6624, 0.4830, 0.5570]],

         [[0.6465, 0.5400, 0.4126,  ..., 0.4942, 0.5199, 0.4982],
          [0.4809, 0.3469, 0.4820,  ..., 0.5282, 0.6018, 0.4638],
          [0.4252, 0.4661, 0.6064,  ..., 0.5223, 0.6014, 0.5149],
          [0.5906, 0.5216, 0.3731,  ..., 0.5085, 0.4672, 0.3134]],

         [[0.3376, 0.5082, 0.4074,  ..., 0.4380, 0.5347, 0.5490],
          [0.5070, 0.4494, 0.6129,  ..., 0.5570, 0.5432, 0.4305],
          [0.5173, 0.5617, 0.5495,  ..., 0.3806, 0.5541, 0.6692],
          [0.3902, 0.4983, 0.6390,  ..., 0.5316, 0.7138, 0.6162]]],


        [[[0.5697, 0.3720, 0.5221,  ..., 0.4880, 0.4279, 0.3717],
          [0.4147, 0.5248, 0.4293,  ..., 0.5339, 0.4623, 0.3791],
          [0.5873, 0.6067, 0.5666,  ..., 0.4760, 0.4831, 0.3977],
          [0.3101, 0.3303, 0.5606,  ..., 0.4546, 0.5512, 0.4266]],

         [[0.4616, 0.3775, 0.5485,  ..., 0.5240, 0.5163, 0.4663],
          [0.6447, 0.5008, 0.5928,  ..., 0.5088, 0.4948, 0.3800],
          [0.4403, 0.5117, 0.4321,  ..., 0.5777, 0.5867, 0.5560],
          [0.3341, 0.6379, 0.6700,  ..., 0.4891, 0.2942, 0.3662]],

         [[0.4626, 0.4834, 0.5577,  ..., 0.4876, 0.4385, 0.4324],
          [0.5668, 0.4603, 0.3999,  ..., 0.4180, 0.6392, 0.6102],
          [0.6701, 0.5026, 0.4004,  ..., 0.4279, 0.4337, 0.4747],
          [0.5778, 0.5751, 0.4422,  ..., 0.4773, 0.5252, 0.5915]],

         ...,

         [[0.4792, 0.6234, 0.5699,  ..., 0.4270, 0.4980, 0.3944],
          [0.4952, 0.4116, 0.7190,  ..., 0.5891, 0.4781, 0.5316],
          [0.5706, 0.3998, 0.5354,  ..., 0.4565, 0.4361, 0.5178],
          [0.4166, 0.3835, 0.4923,  ..., 0.6000, 0.5234, 0.4553]],

         [[0.4841, 0.5290, 0.4504,  ..., 0.4869, 0.4442, 0.4195],
          [0.4403, 0.4789, 0.5485,  ..., 0.5408, 0.6205, 0.6051],
          [0.4751, 0.5511, 0.4321,  ..., 0.5089, 0.4010, 0.5287],
          [0.5203, 0.4983, 0.6157,  ..., 0.5054, 0.5036, 0.5439]],

         [[0.4029, 0.5964, 0.4640,  ..., 0.4043, 0.6228, 0.4214],
          [0.4209, 0.6035, 0.5728,  ..., 0.4867, 0.5824, 0.4345],
          [0.3699, 0.5917, 0.4944,  ..., 0.4812, 0.5633, 0.4444],
          [0.4095, 0.5060, 0.3489,  ..., 0.6329, 0.5934, 0.4714]]]],
       device='cuda:0')
tensor([[[[0.4455, 0.5603, 0.3984,  ..., 0.3961, 0.5390, 0.5679],
          [0.5277, 0.5595, 0.3975,  ..., 0.6095, 0.5631, 0.5889],
          [0.3720, 0.5132, 0.6826,  ..., 0.4429, 0.4419, 0.5034],
          [0.4477, 0.3693, 0.5108,  ..., 0.5832, 0.5054, 0.5746]],

         [[0.2494, 0.5293, 0.5571,  ..., 0.4738, 0.5679, 0.5978],
          [0.3612, 0.4827, 0.4026,  ..., 0.5113, 0.5665, 0.4455],
          [0.4523, 0.4891, 0.5148,  ..., 0.4453, 0.3657, 0.4244],
          [0.4405, 0.5875, 0.6645,  ..., 0.4316, 0.5988, 0.5808]],

         [[0.5138, 0.5192, 0.5889,  ..., 0.6081, 0.4273, 0.4518],
          [0.5513, 0.4946, 0.4187,  ..., 0.4258, 0.3576, 0.4477],
          [0.4216, 0.4848, 0.5216,  ..., 0.4783, 0.4487, 0.4876],
          [0.5554, 0.4443, 0.5501,  ..., 0.2902, 0.4268, 0.4045]],

         ...,

         [[0.5535, 0.4092, 0.5578,  ..., 0.5211, 0.6868, 0.4960],
          [0.3766, 0.6442, 0.4874,  ..., 0.4760, 0.4902, 0.6645],
          [0.4154, 0.5429, 0.3065,  ..., 0.4144, 0.4913, 0.5212],
          [0.3905, 0.4345, 0.6252,  ..., 0.6584, 0.4790, 0.5610]],

         [[0.6505, 0.5400, 0.4106,  ..., 0.4902, 0.5159, 0.5022],
          [0.4849, 0.3469, 0.4800,  ..., 0.5242, 0.5978, 0.4678],
          [0.4292, 0.4661, 0.6044,  ..., 0.5183, 0.5974, 0.5189],
          [0.5946, 0.5216, 0.3711,  ..., 0.5045, 0.4632, 0.3174]],

         [[0.3416, 0.5082, 0.4054,  ..., 0.4340, 0.5307, 0.5530],
          [0.5110, 0.4494, 0.6109,  ..., 0.5530, 0.5392, 0.4345],
          [0.5213, 0.5617, 0.5475,  ..., 0.3766, 0.5501, 0.6732],
          [0.3942, 0.4983, 0.6370,  ..., 0.5276, 0.7098, 0.6202]]],


        [[[0.5737, 0.3720, 0.5201,  ..., 0.4840, 0.4239, 0.3757],
          [0.4187, 0.5248, 0.4273,  ..., 0.5299, 0.4583, 0.3831],
          [0.5913, 0.6067, 0.5646,  ..., 0.4720, 0.4791, 0.4017],
          [0.3141, 0.3303, 0.5586,  ..., 0.4506, 0.5472, 0.4306]],

         [[0.4656, 0.3775, 0.5465,  ..., 0.5200, 0.5123, 0.4703],
          [0.6487, 0.5008, 0.5908,  ..., 0.5048, 0.4908, 0.3840],
          [0.4443, 0.5117, 0.4301,  ..., 0.5737, 0.5827, 0.5600],
          [0.3381, 0.6379, 0.6680,  ..., 0.4851, 0.2902, 0.3702]],

         [[0.4666, 0.4834, 0.5557,  ..., 0.4836, 0.4345, 0.4364],
          [0.5708, 0.4603, 0.3979,  ..., 0.4140, 0.6352, 0.6142],
          [0.6741, 0.5026, 0.3984,  ..., 0.4239, 0.4297, 0.4787],
          [0.5818, 0.5751, 0.4402,  ..., 0.4733, 0.5212, 0.5955]],

         ...,

         [[0.4832, 0.6234, 0.5679,  ..., 0.4230, 0.4940, 0.3984],
          [0.4992, 0.4116, 0.7170,  ..., 0.5851, 0.4741, 0.5356],
          [0.5746, 0.3998, 0.5334,  ..., 0.4525, 0.4321, 0.5218],
          [0.4206, 0.3835, 0.4903,  ..., 0.5960, 0.5194, 0.4593]],

         [[0.4881, 0.5290, 0.4484,  ..., 0.4829, 0.4402, 0.4235],
          [0.4443, 0.4789, 0.5465,  ..., 0.5368, 0.6165, 0.6091],
          [0.4791, 0.5511, 0.4301,  ..., 0.5049, 0.3970, 0.5327],
          [0.5243, 0.4983, 0.6137,  ..., 0.5014, 0.4996, 0.5479]],

         [[0.4069, 0.5964, 0.4620,  ..., 0.4003, 0.6188, 0.4254],
          [0.4249, 0.6035, 0.5708,  ..., 0.4827, 0.5784, 0.4385],
          [0.3739, 0.5917, 0.4924,  ..., 0.4772, 0.5593, 0.4484],
          [0.4135, 0.5060, 0.3469,  ..., 0.6289, 0.5894, 0.4754]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0040,  0.0000,  0.0020,  0.0040,  0.0040,  0.0040, -0.0040,  0.0040,
         0.0040, -0.0040], device='cuda:0')
selected experts tensor([1778, 1653, 1665, 1651, 1689, 1587, 1670, 1548, 1514, 1629],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6983, 0.4783, 0.6668,  ..., 0.5439, 0.5553, 0.5350],
          [0.4043, 0.5054, 0.3987,  ..., 0.4599, 0.5209, 0.3770],
          [0.3535, 0.3939, 0.4889,  ..., 0.6046, 0.4449, 0.4394],
          [0.4289, 0.4686, 0.5048,  ..., 0.6240, 0.4684, 0.6840]],

         [[0.5782, 0.5063, 0.4517,  ..., 0.4558, 0.4079, 0.4399],
          [0.5212, 0.6384, 0.4662,  ..., 0.6294, 0.4926, 0.5858],
          [0.5107, 0.4698, 0.3742,  ..., 0.4295, 0.4480, 0.4170],
          [0.6126, 0.6605, 0.4471,  ..., 0.5611, 0.5804, 0.5418]],

         [[0.4175, 0.6023, 0.6009,  ..., 0.5702, 0.5185, 0.7106],
          [0.5577, 0.4541, 0.5007,  ..., 0.3749, 0.4810, 0.5218],
          [0.3898, 0.6249, 0.4806,  ..., 0.5601, 0.5281, 0.4590],
          [0.5398, 0.3563, 0.5013,  ..., 0.4675, 0.4331, 0.5168]],

         ...,

         [[0.5568, 0.5401, 0.5524,  ..., 0.5316, 0.3976, 0.5111],
          [0.5990, 0.4683, 0.6023,  ..., 0.4981, 0.5880, 0.5222],
          [0.5010, 0.4801, 0.5435,  ..., 0.6171, 0.5424, 0.4729],
          [0.6065, 0.5635, 0.5227,  ..., 0.4095, 0.3878, 0.4365]],

         [[0.5331, 0.5522, 0.5342,  ..., 0.4408, 0.6846, 0.4289],
          [0.4466, 0.6527, 0.6607,  ..., 0.3671, 0.3605, 0.4161],
          [0.5758, 0.5031, 0.3770,  ..., 0.6588, 0.5128, 0.4752],
          [0.4071, 0.4807, 0.7396,  ..., 0.5906, 0.5079, 0.5033]],

         [[0.5724, 0.3958, 0.5274,  ..., 0.4638, 0.4906, 0.5815],
          [0.5594, 0.5203, 0.5423,  ..., 0.5716, 0.5366, 0.5976],
          [0.6158, 0.4233, 0.6126,  ..., 0.6640, 0.5190, 0.4161],
          [0.5618, 0.6166, 0.5119,  ..., 0.5183, 0.4255, 0.3894]]],


        [[[0.5179, 0.5906, 0.5249,  ..., 0.3800, 0.3878, 0.5393],
          [0.4246, 0.5237, 0.4213,  ..., 0.4362, 0.5038, 0.4909],
          [0.5905, 0.4029, 0.6089,  ..., 0.4959, 0.5847, 0.3898],
          [0.3945, 0.5740, 0.4602,  ..., 0.5659, 0.3750, 0.4966]],

         [[0.4930, 0.4314, 0.5777,  ..., 0.6004, 0.4475, 0.5217],
          [0.5594, 0.4631, 0.5410,  ..., 0.4348, 0.5019, 0.5111],
          [0.5266, 0.5292, 0.4213,  ..., 0.4357, 0.4726, 0.5038],
          [0.3889, 0.3228, 0.4457,  ..., 0.4271, 0.4993, 0.5410]],

         [[0.4880, 0.6692, 0.5447,  ..., 0.5985, 0.7102, 0.5157],
          [0.5372, 0.5981, 0.6260,  ..., 0.5721, 0.4384, 0.5962],
          [0.5590, 0.4751, 0.3894,  ..., 0.6129, 0.6309, 0.4279],
          [0.4203, 0.4166, 0.4318,  ..., 0.3117, 0.4657, 0.5444]],

         ...,

         [[0.3996, 0.4901, 0.5568,  ..., 0.3289, 0.5675, 0.5715],
          [0.5178, 0.5357, 0.5498,  ..., 0.3581, 0.4652, 0.2886],
          [0.6065, 0.5957, 0.3386,  ..., 0.5083, 0.3587, 0.5026],
          [0.3806, 0.5275, 0.4850,  ..., 0.4391, 0.6203, 0.5782]],

         [[0.5095, 0.5067, 0.5575,  ..., 0.5386, 0.4524, 0.5805],
          [0.5587, 0.3263, 0.5743,  ..., 0.5792, 0.6674, 0.4689],
          [0.3935, 0.5099, 0.5283,  ..., 0.5981, 0.6648, 0.4841],
          [0.4142, 0.6069, 0.6571,  ..., 0.6004, 0.5400, 0.6562]],

         [[0.4577, 0.4909, 0.3935,  ..., 0.5191, 0.4730, 0.4493],
          [0.5196, 0.4000, 0.4829,  ..., 0.7034, 0.3971, 0.6051],
          [0.4132, 0.6171, 0.5500,  ..., 0.5553, 0.5127, 0.7531],
          [0.4952, 0.4128, 0.3697,  ..., 0.3509, 0.5885, 0.3023]]]],
       device='cuda:0')
tensor([[[[0.6943, 0.4823, 0.6628,  ..., 0.5479, 0.5533, 0.5310],
          [0.4003, 0.5094, 0.3947,  ..., 0.4639, 0.5189, 0.3730],
          [0.3495, 0.3979, 0.4849,  ..., 0.6086, 0.4429, 0.4354],
          [0.4249, 0.4726, 0.5008,  ..., 0.6280, 0.4664, 0.6800]],

         [[0.5742, 0.5103, 0.4477,  ..., 0.4598, 0.4059, 0.4359],
          [0.5172, 0.6424, 0.4622,  ..., 0.6334, 0.4906, 0.5818],
          [0.5067, 0.4738, 0.3702,  ..., 0.4335, 0.4460, 0.4130],
          [0.6086, 0.6645, 0.4431,  ..., 0.5651, 0.5784, 0.5378]],

         [[0.4135, 0.6063, 0.5969,  ..., 0.5742, 0.5165, 0.7066],
          [0.5537, 0.4581, 0.4967,  ..., 0.3789, 0.4790, 0.5178],
          [0.3858, 0.6289, 0.4766,  ..., 0.5641, 0.5261, 0.4550],
          [0.5358, 0.3603, 0.4973,  ..., 0.4715, 0.4311, 0.5128]],

         ...,

         [[0.5528, 0.5441, 0.5484,  ..., 0.5356, 0.3956, 0.5071],
          [0.5950, 0.4723, 0.5983,  ..., 0.5021, 0.5860, 0.5182],
          [0.4970, 0.4841, 0.5395,  ..., 0.6211, 0.5404, 0.4689],
          [0.6025, 0.5675, 0.5187,  ..., 0.4135, 0.3858, 0.4325]],

         [[0.5291, 0.5562, 0.5302,  ..., 0.4448, 0.6826, 0.4249],
          [0.4426, 0.6567, 0.6567,  ..., 0.3711, 0.3585, 0.4121],
          [0.5718, 0.5071, 0.3730,  ..., 0.6628, 0.5108, 0.4712],
          [0.4031, 0.4847, 0.7356,  ..., 0.5946, 0.5059, 0.4993]],

         [[0.5684, 0.3998, 0.5234,  ..., 0.4678, 0.4886, 0.5775],
          [0.5554, 0.5243, 0.5383,  ..., 0.5756, 0.5346, 0.5936],
          [0.6118, 0.4273, 0.6086,  ..., 0.6680, 0.5170, 0.4121],
          [0.5578, 0.6206, 0.5079,  ..., 0.5223, 0.4235, 0.3854]]],


        [[[0.5139, 0.5946, 0.5209,  ..., 0.3840, 0.3858, 0.5353],
          [0.4206, 0.5277, 0.4173,  ..., 0.4402, 0.5018, 0.4869],
          [0.5865, 0.4069, 0.6049,  ..., 0.4999, 0.5827, 0.3858],
          [0.3905, 0.5780, 0.4562,  ..., 0.5699, 0.3730, 0.4926]],

         [[0.4890, 0.4354, 0.5737,  ..., 0.6044, 0.4455, 0.5177],
          [0.5554, 0.4671, 0.5370,  ..., 0.4388, 0.4999, 0.5071],
          [0.5226, 0.5332, 0.4173,  ..., 0.4397, 0.4706, 0.4998],
          [0.3849, 0.3268, 0.4417,  ..., 0.4311, 0.4973, 0.5370]],

         [[0.4840, 0.6732, 0.5407,  ..., 0.6025, 0.7082, 0.5117],
          [0.5332, 0.6021, 0.6220,  ..., 0.5761, 0.4364, 0.5922],
          [0.5550, 0.4791, 0.3854,  ..., 0.6169, 0.6289, 0.4239],
          [0.4163, 0.4206, 0.4278,  ..., 0.3157, 0.4637, 0.5404]],

         ...,

         [[0.3956, 0.4941, 0.5528,  ..., 0.3329, 0.5655, 0.5675],
          [0.5138, 0.5397, 0.5458,  ..., 0.3621, 0.4632, 0.2846],
          [0.6025, 0.5997, 0.3346,  ..., 0.5123, 0.3567, 0.4986],
          [0.3766, 0.5315, 0.4810,  ..., 0.4431, 0.6183, 0.5742]],

         [[0.5055, 0.5107, 0.5535,  ..., 0.5426, 0.4504, 0.5765],
          [0.5547, 0.3303, 0.5703,  ..., 0.5832, 0.6654, 0.4649],
          [0.3895, 0.5139, 0.5243,  ..., 0.6021, 0.6628, 0.4801],
          [0.4102, 0.6109, 0.6531,  ..., 0.6044, 0.5380, 0.6522]],

         [[0.4537, 0.4949, 0.3895,  ..., 0.5231, 0.4710, 0.4453],
          [0.5156, 0.4040, 0.4789,  ..., 0.7074, 0.3951, 0.6011],
          [0.4092, 0.6211, 0.5460,  ..., 0.5593, 0.5107, 0.7491],
          [0.4912, 0.4168, 0.3657,  ..., 0.3549, 0.5865, 0.2983]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0040, -0.0040,  0.0040, -0.0020,  0.0020,  0.0020, -0.0040, -0.0040,
         0.0020,  0.0040], device='cuda:0')
selected experts tensor([1682, 1764, 1728, 1754, 1484, 1594, 1764, 1640, 1518, 1456],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.3407, 0.5971, 0.4568,  ..., 0.6027, 0.4701, 0.6579],
          [0.5409, 0.5223, 0.5806,  ..., 0.4233, 0.4217, 0.6321],
          [0.5458, 0.3545, 0.4228,  ..., 0.3930, 0.4751, 0.5145],
          [0.4158, 0.5023, 0.5318,  ..., 0.6535, 0.5719, 0.5256]],

         [[0.4920, 0.4778, 0.6294,  ..., 0.5180, 0.4740, 0.4790],
          [0.5207, 0.5858, 0.4290,  ..., 0.5326, 0.5002, 0.3800],
          [0.5216, 0.4171, 0.5606,  ..., 0.4573, 0.3571, 0.4854],
          [0.4149, 0.3194, 0.4924,  ..., 0.5948, 0.4853, 0.5423]],

         [[0.5511, 0.5635, 0.5296,  ..., 0.3883, 0.4580, 0.4495],
          [0.4220, 0.5659, 0.4585,  ..., 0.4541, 0.4213, 0.4459],
          [0.4206, 0.5345, 0.5490,  ..., 0.3986, 0.4337, 0.5522],
          [0.4627, 0.4000, 0.3772,  ..., 0.5200, 0.4931, 0.4471]],

         ...,

         [[0.3989, 0.4626, 0.5012,  ..., 0.4500, 0.6037, 0.6579],
          [0.3684, 0.4109, 0.4305,  ..., 0.6212, 0.3820, 0.6366],
          [0.6252, 0.6285, 0.6819,  ..., 0.4773, 0.6392, 0.4456],
          [0.4325, 0.3977, 0.4609,  ..., 0.5749, 0.6392, 0.5418]],

         [[0.4768, 0.3323, 0.4052,  ..., 0.5234, 0.4256, 0.5985],
          [0.4787, 0.5215, 0.4319,  ..., 0.5150, 0.4841, 0.5063],
          [0.5251, 0.5478, 0.5318,  ..., 0.4290, 0.5038, 0.4693],
          [0.4816, 0.4309, 0.5097,  ..., 0.4652, 0.4544, 0.5096]],

         [[0.6316, 0.3708, 0.5764,  ..., 0.4811, 0.4327, 0.4577],
          [0.4349, 0.5182, 0.4510,  ..., 0.6375, 0.3580, 0.5183],
          [0.4897, 0.3690, 0.6760,  ..., 0.3536, 0.4580, 0.3699],
          [0.4453, 0.4161, 0.5318,  ..., 0.4128, 0.5534, 0.5844]]],


        [[[0.4659, 0.4800, 0.5321,  ..., 0.3991, 0.4457, 0.6212],
          [0.3998, 0.5957, 0.4309,  ..., 0.5461, 0.4997, 0.3581],
          [0.5210, 0.6294, 0.7098,  ..., 0.4464, 0.5877, 0.5049],
          [0.5380, 0.4266, 0.4071,  ..., 0.4997, 0.5152, 0.6853]],

         [[0.4724, 0.4242, 0.5121,  ..., 0.2951, 0.5005, 0.5009],
          [0.4883, 0.5152, 0.5279,  ..., 0.5125, 0.4778, 0.5350],
          [0.5799, 0.3768, 0.4918,  ..., 0.4580, 0.5223, 0.6553],
          [0.4003, 0.5754, 0.5350,  ..., 0.4066, 0.5355, 0.6570]],

         [[0.4414, 0.4019, 0.3228,  ..., 0.4459, 0.3968, 0.5117],
          [0.5288, 0.4738, 0.6037,  ..., 0.6675, 0.4327, 0.5335],
          [0.4530, 0.5596, 0.5635,  ..., 0.6046, 0.5758, 0.4790],
          [0.4846, 0.4062, 0.3804,  ..., 0.5546, 0.4814, 0.5797]],

         ...,

         [[0.5528, 0.4822, 0.6162,  ..., 0.4575, 0.5500, 0.4907],
          [0.4149, 0.6777, 0.5971,  ..., 0.6683, 0.5810, 0.3518],
          [0.5063, 0.4655, 0.6752,  ..., 0.3795, 0.4486, 0.3680],
          [0.5651, 0.3977, 0.4500,  ..., 0.5145, 0.6374, 0.4857]],

         [[0.5004, 0.5896, 0.5343,  ..., 0.4176, 0.3945, 0.3804],
          [0.6002, 0.5500, 0.6134,  ..., 0.5778, 0.4394, 0.5268],
          [0.6067, 0.4749, 0.4619,  ..., 0.6004, 0.3679, 0.5999],
          [0.5185, 0.4300, 0.5247,  ..., 0.5248, 0.4655, 0.6152]],

         [[0.5627, 0.5418, 0.6579,  ..., 0.5687, 0.4370, 0.6276],
          [0.5290, 0.7637, 0.5202,  ..., 0.6649, 0.4916, 0.5620],
          [0.5419, 0.4394, 0.6640,  ..., 0.4176, 0.4256, 0.3925],
          [0.7705, 0.2750, 0.4970,  ..., 0.4905, 0.5305, 0.4305]]]],
       device='cuda:0')
tensor([[[[0.3407, 0.6011, 0.4608,  ..., 0.6067, 0.4661, 0.6619],
          [0.5409, 0.5263, 0.5846,  ..., 0.4273, 0.4177, 0.6361],
          [0.5458, 0.3585, 0.4268,  ..., 0.3970, 0.4711, 0.5185],
          [0.4158, 0.5063, 0.5358,  ..., 0.6575, 0.5679, 0.5296]],

         [[0.4920, 0.4818, 0.6334,  ..., 0.5220, 0.4700, 0.4830],
          [0.5207, 0.5898, 0.4330,  ..., 0.5366, 0.4962, 0.3840],
          [0.5216, 0.4211, 0.5646,  ..., 0.4613, 0.3531, 0.4894],
          [0.4149, 0.3234, 0.4964,  ..., 0.5988, 0.4813, 0.5463]],

         [[0.5511, 0.5675, 0.5336,  ..., 0.3923, 0.4540, 0.4535],
          [0.4220, 0.5699, 0.4625,  ..., 0.4581, 0.4173, 0.4499],
          [0.4206, 0.5385, 0.5530,  ..., 0.4026, 0.4297, 0.5562],
          [0.4627, 0.4040, 0.3812,  ..., 0.5240, 0.4891, 0.4511]],

         ...,

         [[0.3989, 0.4666, 0.5052,  ..., 0.4540, 0.5997, 0.6619],
          [0.3684, 0.4149, 0.4345,  ..., 0.6252, 0.3780, 0.6406],
          [0.6252, 0.6325, 0.6859,  ..., 0.4813, 0.6352, 0.4496],
          [0.4325, 0.4017, 0.4649,  ..., 0.5789, 0.6352, 0.5458]],

         [[0.4768, 0.3363, 0.4092,  ..., 0.5274, 0.4216, 0.6025],
          [0.4787, 0.5255, 0.4359,  ..., 0.5190, 0.4801, 0.5103],
          [0.5251, 0.5518, 0.5358,  ..., 0.4330, 0.4998, 0.4733],
          [0.4816, 0.4349, 0.5137,  ..., 0.4692, 0.4504, 0.5136]],

         [[0.6316, 0.3748, 0.5804,  ..., 0.4851, 0.4287, 0.4617],
          [0.4349, 0.5222, 0.4550,  ..., 0.6415, 0.3540, 0.5223],
          [0.4897, 0.3730, 0.6800,  ..., 0.3576, 0.4540, 0.3739],
          [0.4453, 0.4201, 0.5358,  ..., 0.4168, 0.5494, 0.5884]]],


        [[[0.4659, 0.4840, 0.5361,  ..., 0.4031, 0.4417, 0.6252],
          [0.3998, 0.5997, 0.4349,  ..., 0.5501, 0.4957, 0.3621],
          [0.5210, 0.6334, 0.7138,  ..., 0.4504, 0.5837, 0.5089],
          [0.5380, 0.4306, 0.4111,  ..., 0.5037, 0.5112, 0.6893]],

         [[0.4724, 0.4282, 0.5161,  ..., 0.2991, 0.4965, 0.5049],
          [0.4883, 0.5192, 0.5319,  ..., 0.5165, 0.4738, 0.5390],
          [0.5799, 0.3808, 0.4958,  ..., 0.4620, 0.5183, 0.6593],
          [0.4003, 0.5794, 0.5390,  ..., 0.4106, 0.5315, 0.6610]],

         [[0.4414, 0.4059, 0.3268,  ..., 0.4499, 0.3928, 0.5157],
          [0.5288, 0.4778, 0.6077,  ..., 0.6715, 0.4287, 0.5375],
          [0.4530, 0.5636, 0.5675,  ..., 0.6086, 0.5718, 0.4830],
          [0.4846, 0.4102, 0.3844,  ..., 0.5586, 0.4774, 0.5837]],

         ...,

         [[0.5528, 0.4862, 0.6202,  ..., 0.4615, 0.5460, 0.4947],
          [0.4149, 0.6817, 0.6011,  ..., 0.6723, 0.5770, 0.3558],
          [0.5063, 0.4695, 0.6792,  ..., 0.3835, 0.4446, 0.3720],
          [0.5651, 0.4017, 0.4540,  ..., 0.5185, 0.6334, 0.4897]],

         [[0.5004, 0.5936, 0.5383,  ..., 0.4216, 0.3905, 0.3844],
          [0.6002, 0.5540, 0.6174,  ..., 0.5818, 0.4354, 0.5308],
          [0.6067, 0.4789, 0.4659,  ..., 0.6044, 0.3639, 0.6039],
          [0.5185, 0.4340, 0.5287,  ..., 0.5288, 0.4615, 0.6192]],

         [[0.5627, 0.5458, 0.6619,  ..., 0.5727, 0.4330, 0.6316],
          [0.5290, 0.7677, 0.5242,  ..., 0.6689, 0.4876, 0.5660],
          [0.5419, 0.4434, 0.6680,  ..., 0.4216, 0.4216, 0.3965],
          [0.7705, 0.2790, 0.5010,  ..., 0.4945, 0.5265, 0.4345]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0000, -0.0040, -0.0040,  0.0040, -0.0020,  0.0020,  0.0000, -0.0040,
         0.0040, -0.0040], device='cuda:0')
selected experts tensor([1659, 1789, 1811, 1354, 1827, 1508, 1766, 1648,  815, 2207],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.5608, 0.5606, 0.5427,  ..., 0.4664, 0.6417, 0.4341],
          [0.6017, 0.5480, 0.4910,  ..., 0.3722, 0.4288, 0.5656],
          [0.4174, 0.5070, 0.6111,  ..., 0.5199, 0.5008, 0.4799],
          [0.5975, 0.5543, 0.3750,  ..., 0.4898, 0.5984, 0.5220]],

         [[0.4250, 0.6059, 0.5439,  ..., 0.5166, 0.4231, 0.5709],
          [0.4112, 0.3677, 0.5747,  ..., 0.3383, 0.5814, 0.5179],
          [0.5255, 0.4425, 0.5021,  ..., 0.5203, 0.5130, 0.5331],
          [0.6471, 0.4872, 0.4245,  ..., 0.6245, 0.4355, 0.6761]],

         [[0.3288, 0.5272, 0.6871,  ..., 0.4787, 0.5400, 0.5809],
          [0.3506, 0.4278, 0.5699,  ..., 0.4846, 0.5695, 0.6829],
          [0.5104, 0.5857, 0.4757,  ..., 0.4442, 0.5637, 0.4620],
          [0.6263, 0.5110, 0.5325,  ..., 0.5429, 0.4240, 0.5610]],

         ...,

         [[0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020]],

         [[0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020]],

         [[0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020]]],


        [[[0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020]],

         [[0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020]],

         [[0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020]],

         ...,

         [[0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020]],

         [[0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020]],

         [[0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5020]]]],
       device='cuda:0')/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([[[[0.5588, 0.5586, 0.5407,  ..., 0.4644, 0.6397, 0.4321],
          [0.5997, 0.5460, 0.4890,  ..., 0.3702, 0.4268, 0.5636],
          [0.4154, 0.5050, 0.6091,  ..., 0.5179, 0.4988, 0.4779],
          [0.5955, 0.5523, 0.3730,  ..., 0.4878, 0.5964, 0.5200]],

         [[0.4230, 0.6039, 0.5419,  ..., 0.5146, 0.4211, 0.5689],
          [0.4092, 0.3657, 0.5727,  ..., 0.3363, 0.5794, 0.5159],
          [0.5235, 0.4405, 0.5001,  ..., 0.5183, 0.5110, 0.5311],
          [0.6451, 0.4852, 0.4225,  ..., 0.6225, 0.4335, 0.6741]],

         [[0.3268, 0.5252, 0.6851,  ..., 0.4767, 0.5380, 0.5789],
          [0.3486, 0.4258, 0.5679,  ..., 0.4826, 0.5675, 0.6809],
          [0.5084, 0.5837, 0.4737,  ..., 0.4422, 0.5617, 0.4600],
          [0.6243, 0.5090, 0.5305,  ..., 0.5409, 0.4220, 0.5590]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,
        0.0020], device='cuda:0')
selected experts tensor([4173, 4136,  546,  412,  453,  424,  384,  319,  698,  743],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5403, 0.4234, 0.3957,  ..., 0.6173, 0.4380, 0.5911],
          [0.4643, 0.5770, 0.4311,  ..., 0.5544, 0.4633, 0.4987],
          [0.4904, 0.5450, 0.3712,  ..., 0.6047, 0.6348, 0.5830],
          [0.5016, 0.4349, 0.4584,  ..., 0.3626, 0.5796, 0.6580]],

         [[0.5550, 0.4092, 0.5124,  ..., 0.5744, 0.4558, 0.4367],
          [0.3313, 0.6279, 0.5576,  ..., 0.4913, 0.4970, 0.5242],
          [0.3939, 0.3974, 0.3994,  ..., 0.6302, 0.5691, 0.6349],
          [0.5711, 0.4751, 0.4996,  ..., 0.4663, 0.2763, 0.5534]],

         [[0.5233, 0.5794, 0.4771,  ..., 0.5948, 0.5041, 0.5697],
          [0.4704, 0.4306, 0.3613,  ..., 0.4751, 0.6019, 0.4820],
          [0.3141, 0.5832, 0.5499,  ..., 0.4467, 0.6687, 0.4474],
          [0.5500, 0.5346, 0.5656,  ..., 0.3361, 0.4414, 0.5745]],

         ...,

         [[0.6455, 0.5390, 0.4116,  ..., 0.4952, 0.5207, 0.4991],
          [0.4800, 0.3459, 0.4810,  ..., 0.5292, 0.6028, 0.4648],
          [0.4242, 0.4651, 0.6054,  ..., 0.5233, 0.6024, 0.5159],
          [0.5896, 0.5207, 0.3721,  ..., 0.5095, 0.4682, 0.3144]],

         [[0.4827, 0.4856, 0.4620,  ..., 0.5486, 0.5873, 0.6412],
          [0.3383, 0.4540, 0.3426,  ..., 0.4711, 0.4571, 0.4067],
          [0.4507, 0.4234, 0.5385,  ..., 0.4889, 0.4865, 0.5261],
          [0.4710, 0.6315, 0.4451,  ..., 0.4104, 0.5187, 0.4781]],

         [[0.4961, 0.3830, 0.3943,  ..., 0.4280, 0.5686, 0.3935],
          [0.5033, 0.4424, 0.5685,  ..., 0.3396, 0.3997, 0.5659],
          [0.6003, 0.5698, 0.5349,  ..., 0.6136, 0.5313, 0.7060],
          [0.5044, 0.4282, 0.5238,  ..., 0.4718, 0.5571, 0.5247]]],


        [[[0.6455, 0.5390, 0.4116,  ..., 0.4952, 0.5207, 0.4991],
          [0.4800, 0.3459, 0.4810,  ..., 0.5292, 0.6028, 0.4648],
          [0.4242, 0.4651, 0.6054,  ..., 0.5233, 0.6024, 0.5159],
          [0.5896, 0.5207, 0.3721,  ..., 0.5095, 0.4682, 0.3144]],

         [[0.4934, 0.4629, 0.5007,  ..., 0.4957, 0.4748, 0.4804],
          [0.5572, 0.4455, 0.5998,  ..., 0.5454, 0.5483, 0.4959],
          [0.4621, 0.4371, 0.4550,  ..., 0.5725, 0.5237, 0.5401],
          [0.4376, 0.3738, 0.4429,  ..., 0.4299, 0.4748, 0.5864]],

         [[0.4228, 0.4349, 0.4901,  ..., 0.3798, 0.3528, 0.4176],
          [0.4754, 0.4148, 0.6035,  ..., 0.5665, 0.7100, 0.4934],
          [0.4425, 0.3476, 0.3134,  ..., 0.2888, 0.6339, 0.6176],
          [0.3526, 0.5503, 0.4647,  ..., 0.5454, 0.5004, 0.4435]],

         ...,

         [[0.4328, 0.5091, 0.5870,  ..., 0.3770, 0.4419, 0.4447],
          [0.5478, 0.5537, 0.5737,  ..., 0.3309, 0.4728, 0.4300],
          [0.5649, 0.5554, 0.4800,  ..., 0.4721, 0.3671, 0.3212],
          [0.4335, 0.6423, 0.5480,  ..., 0.4757, 0.5379, 0.4780]],

         [[0.5536, 0.4082, 0.2968,  ..., 0.5036, 0.5396, 0.3229],
          [0.3850, 0.4831, 0.5856,  ..., 0.6038, 0.4474, 0.3787],
          [0.5181, 0.4400, 0.5775,  ..., 0.5171, 0.4575, 0.6439],
          [0.4938, 0.4707, 0.4506,  ..., 0.4706, 0.5459, 0.4962]],

         [[0.4500, 0.5160, 0.4836,  ..., 0.4626, 0.3653, 0.4648],
          [0.5829, 0.5467, 0.5417,  ..., 0.3626, 0.5127, 0.5991],
          [0.4228, 0.4359, 0.5069,  ..., 0.5244, 0.6000, 0.5601],
          [0.4468, 0.6108, 0.4249,  ..., 0.5002, 0.5739, 0.6019]]]],
       device='cuda:0')
tensor([[[[0.5453, 0.4244, 0.3947,  ..., 0.6123, 0.4330, 0.5941],
          [0.4693, 0.5780, 0.4301,  ..., 0.5494, 0.4583, 0.5017],
          [0.4954, 0.5460, 0.3702,  ..., 0.5997, 0.6298, 0.5860],
          [0.5066, 0.4359, 0.4574,  ..., 0.3576, 0.5746, 0.6610]],

         [[0.5600, 0.4102, 0.5114,  ..., 0.5694, 0.4508, 0.4397],
          [0.3363, 0.6289, 0.5566,  ..., 0.4863, 0.4920, 0.5272],
          [0.3989, 0.3984, 0.3984,  ..., 0.6252, 0.5641, 0.6379],
          [0.5761, 0.4761, 0.4986,  ..., 0.4613, 0.2713, 0.5564]],

         [[0.5283, 0.5804, 0.4761,  ..., 0.5898, 0.4991, 0.5727],
          [0.4754, 0.4316, 0.3603,  ..., 0.4701, 0.5969, 0.4850],
          [0.3191, 0.5842, 0.5489,  ..., 0.4417, 0.6637, 0.4504],
          [0.5550, 0.5356, 0.5646,  ..., 0.3311, 0.4364, 0.5775]],

         ...,

         [[0.6505, 0.5400, 0.4106,  ..., 0.4902, 0.5157, 0.5021],
          [0.4850, 0.3469, 0.4800,  ..., 0.5242, 0.5978, 0.4678],
          [0.4292, 0.4661, 0.6044,  ..., 0.5183, 0.5974, 0.5189],
          [0.5946, 0.5217, 0.3711,  ..., 0.5045, 0.4632, 0.3174]],

         [[0.4877, 0.4866, 0.4610,  ..., 0.5436, 0.5823, 0.6442],
          [0.3433, 0.4550, 0.3416,  ..., 0.4661, 0.4521, 0.4097],
          [0.4557, 0.4244, 0.5375,  ..., 0.4839, 0.4815, 0.5291],
          [0.4760, 0.6325, 0.4441,  ..., 0.4054, 0.5137, 0.4811]],

         [[0.5011, 0.3840, 0.3933,  ..., 0.4230, 0.5636, 0.3965],
          [0.5083, 0.4434, 0.5675,  ..., 0.3346, 0.3947, 0.5689],
          [0.6053, 0.5708, 0.5339,  ..., 0.6086, 0.5263, 0.7090],
          [0.5094, 0.4292, 0.5228,  ..., 0.4668, 0.5521, 0.5277]]],


        [[[0.6505, 0.5400, 0.4106,  ..., 0.4902, 0.5157, 0.5021],
          [0.4850, 0.3469, 0.4800,  ..., 0.5242, 0.5978, 0.4678],
          [0.4292, 0.4661, 0.6044,  ..., 0.5183, 0.5974, 0.5189],
          [0.5946, 0.5217, 0.3711,  ..., 0.5045, 0.4632, 0.3174]],

         [[0.4984, 0.4639, 0.4997,  ..., 0.4907, 0.4698, 0.4834],
          [0.5622, 0.4465, 0.5988,  ..., 0.5404, 0.5433, 0.4989],
          [0.4671, 0.4381, 0.4540,  ..., 0.5675, 0.5187, 0.5431],
          [0.4426, 0.3748, 0.4419,  ..., 0.4249, 0.4698, 0.5894]],

         [[0.4278, 0.4359, 0.4891,  ..., 0.3748, 0.3478, 0.4206],
          [0.4804, 0.4158, 0.6025,  ..., 0.5615, 0.7050, 0.4964],
          [0.4475, 0.3486, 0.3124,  ..., 0.2838, 0.6289, 0.6206],
          [0.3576, 0.5513, 0.4637,  ..., 0.5404, 0.4954, 0.4465]],

         ...,

         [[0.4378, 0.5101, 0.5860,  ..., 0.3720, 0.4369, 0.4477],
          [0.5528, 0.5547, 0.5727,  ..., 0.3259, 0.4678, 0.4330],
          [0.5699, 0.5564, 0.4790,  ..., 0.4671, 0.3621, 0.3242],
          [0.4385, 0.6433, 0.5470,  ..., 0.4707, 0.5329, 0.4810]],

         [[0.5586, 0.4092, 0.2958,  ..., 0.4986, 0.5346, 0.3259],
          [0.3900, 0.4841, 0.5846,  ..., 0.5988, 0.4424, 0.3817],
          [0.5231, 0.4410, 0.5765,  ..., 0.5121, 0.4525, 0.6469],
          [0.4988, 0.4717, 0.4496,  ..., 0.4656, 0.5409, 0.4992]],

         [[0.4550, 0.5170, 0.4826,  ..., 0.4576, 0.3603, 0.4678],
          [0.5879, 0.5477, 0.5407,  ..., 0.3576, 0.5077, 0.6021],
          [0.4278, 0.4369, 0.5059,  ..., 0.5194, 0.5950, 0.5631],
          [0.4518, 0.6118, 0.4239,  ..., 0.4952, 0.5689, 0.6049]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0050, -0.0010,  0.0010,  0.0030,  0.0030,  0.0050, -0.0050,  0.0050,
         0.0050, -0.0030], device='cuda:0')
selected experts tensor([1730, 1647, 1564, 1604, 1586, 1608, 1609, 1649, 1575, 1812],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4635, 0.6801, 0.5243,  ..., 0.6156, 0.3633, 0.5462],
          [0.5497, 0.4376, 0.2312,  ..., 0.6115, 0.5848, 0.5384],
          [0.6008, 0.6142, 0.7477,  ..., 0.3436, 0.4440, 0.4156],
          [0.5724, 0.5485, 0.3870,  ..., 0.4085, 0.5410, 0.5245]],

         [[0.5184, 0.5027, 0.4893,  ..., 0.5853, 0.5461, 0.5753],
          [0.4592, 0.6699, 0.6027,  ..., 0.6428, 0.5173, 0.5811],
          [0.3516, 0.5863, 0.4633,  ..., 0.3652, 0.4413, 0.5602],
          [0.6046, 0.6534, 0.4674,  ..., 0.4271, 0.4327, 0.6660]],

         [[0.5263, 0.3562, 0.4331,  ..., 0.6392, 0.4108, 0.5844],
          [0.4056, 0.4606, 0.5795,  ..., 0.5408, 0.4413, 0.4232],
          [0.4775, 0.4360, 0.5235,  ..., 0.6045, 0.5690, 0.3932],
          [0.5729, 0.5572, 0.2758,  ..., 0.3589, 0.4904, 0.6219]],

         ...,

         [[0.5319, 0.5514, 0.5331,  ..., 0.4400, 0.6856, 0.4299],
          [0.4456, 0.6517, 0.6597,  ..., 0.3661, 0.3615, 0.4171],
          [0.5748, 0.5021, 0.3760,  ..., 0.6578, 0.5138, 0.4762],
          [0.4061, 0.4796, 0.7386,  ..., 0.5896, 0.5089, 0.5043]],

         [[0.6526, 0.4446, 0.5425,  ..., 0.6983, 0.5625, 0.4921],
          [0.3255, 0.6036, 0.3543,  ..., 0.6499, 0.5729, 0.4912],
          [0.4874, 0.4066, 0.5705,  ..., 0.4275, 0.5623, 0.5633],
          [0.5345, 0.3689, 0.3723,  ..., 0.3287, 0.4274, 0.5705]],

         [[0.3778, 0.5867, 0.5628,  ..., 0.5555, 0.4548, 0.4237],
          [0.5008, 0.5383, 0.6675,  ..., 0.5223, 0.4447, 0.6033],
          [0.3606, 0.6329, 0.4203,  ..., 0.4805, 0.4895, 0.4779],
          [0.3819, 0.4948, 0.4974,  ..., 0.4912, 0.3171, 0.5529]]],


        [[[0.5319, 0.5514, 0.5331,  ..., 0.4400, 0.6856, 0.4299],
          [0.4456, 0.6517, 0.6597,  ..., 0.3661, 0.3615, 0.4171],
          [0.5748, 0.5021, 0.3760,  ..., 0.6578, 0.5138, 0.4762],
          [0.4061, 0.4796, 0.7386,  ..., 0.5896, 0.5089, 0.5043]],

         [[0.5560, 0.3776, 0.5795,  ..., 0.4570, 0.4769, 0.5948],
          [0.5178, 0.5957, 0.3912,  ..., 0.5458, 0.3847, 0.4479],
          [0.4184, 0.4166, 0.3723,  ..., 0.6410, 0.4241, 0.7452],
          [0.4806, 0.5240, 0.5134,  ..., 0.5853, 0.5829, 0.4812]],

         [[0.5582, 0.6284, 0.4480,  ..., 0.6517, 0.4480, 0.3894],
          [0.5867, 0.5829, 0.4174,  ..., 0.6647, 0.4667, 0.3798],
          [0.3842, 0.4844, 0.5240,  ..., 0.5213, 0.4293, 0.5097],
          [0.4500, 0.5696, 0.5011,  ..., 0.5388, 0.5328, 0.5189]],

         ...,

         [[0.6130, 0.6110, 0.4876,  ..., 0.4204, 0.5194, 0.5171],
          [0.5834, 0.5782, 0.5914,  ..., 0.4529, 0.5192, 0.5260],
          [0.4198, 0.4529, 0.5690,  ..., 0.5156, 0.5337, 0.4034],
          [0.5064, 0.5400, 0.4331,  ..., 0.5024, 0.6004, 0.6687]],

         [[0.4739, 0.3158, 0.4889,  ..., 0.4132, 0.4389, 0.5051],
          [0.5194, 0.4166, 0.3967,  ..., 0.5715, 0.5413, 0.6229],
          [0.4674, 0.6329, 0.4871,  ..., 0.4706, 0.5685, 0.5344],
          [0.5374, 0.4882, 0.5341,  ..., 0.5265, 0.5166, 0.5887]],

         [[0.5606, 0.6733, 0.4188,  ..., 0.6041, 0.5017, 0.5167],
          [0.4877, 0.6383, 0.3815,  ..., 0.6230, 0.5497, 0.5496],
          [0.5014, 0.3517, 0.6176,  ..., 0.4824, 0.6300, 0.3199],
          [0.4355, 0.2966, 0.4868,  ..., 0.4836, 0.5388, 0.5653]]]],
       device='cuda:0')
tensor([[[[0.4605, 0.6851, 0.5213,  ..., 0.6206, 0.3603, 0.5412],
          [0.5467, 0.4426, 0.2282,  ..., 0.6165, 0.5818, 0.5334],
          [0.5978, 0.6192, 0.7447,  ..., 0.3486, 0.4410, 0.4106],
          [0.5694, 0.5535, 0.3840,  ..., 0.4135, 0.5380, 0.5195]],

         [[0.5154, 0.5077, 0.4863,  ..., 0.5903, 0.5431, 0.5703],
          [0.4562, 0.6749, 0.5997,  ..., 0.6478, 0.5143, 0.5761],
          [0.3486, 0.5913, 0.4603,  ..., 0.3702, 0.4383, 0.5552],
          [0.6016, 0.6584, 0.4644,  ..., 0.4321, 0.4297, 0.6610]],

         [[0.5233, 0.3612, 0.4301,  ..., 0.6442, 0.4078, 0.5794],
          [0.4026, 0.4656, 0.5765,  ..., 0.5458, 0.4383, 0.4182],
          [0.4745, 0.4410, 0.5205,  ..., 0.6095, 0.5660, 0.3882],
          [0.5699, 0.5622, 0.2728,  ..., 0.3639, 0.4874, 0.6169]],

         ...,

         [[0.5289, 0.5564, 0.5301,  ..., 0.4450, 0.6826, 0.4249],
          [0.4426, 0.6567, 0.6567,  ..., 0.3711, 0.3585, 0.4121],
          [0.5718, 0.5071, 0.3730,  ..., 0.6628, 0.5108, 0.4712],
          [0.4031, 0.4846, 0.7356,  ..., 0.5946, 0.5059, 0.4993]],

         [[0.6496, 0.4496, 0.5395,  ..., 0.7033, 0.5595, 0.4871],
          [0.3225, 0.6086, 0.3513,  ..., 0.6549, 0.5699, 0.4862],
          [0.4844, 0.4116, 0.5675,  ..., 0.4325, 0.5593, 0.5583],
          [0.5315, 0.3739, 0.3693,  ..., 0.3337, 0.4244, 0.5655]],

         [[0.3748, 0.5917, 0.5598,  ..., 0.5605, 0.4518, 0.4187],
          [0.4978, 0.5433, 0.6645,  ..., 0.5273, 0.4417, 0.5983],
          [0.3576, 0.6379, 0.4173,  ..., 0.4855, 0.4865, 0.4729],
          [0.3789, 0.4998, 0.4944,  ..., 0.4962, 0.3141, 0.5479]]],


        [[[0.5289, 0.5564, 0.5301,  ..., 0.4450, 0.6826, 0.4249],
          [0.4426, 0.6567, 0.6567,  ..., 0.3711, 0.3585, 0.4121],
          [0.5718, 0.5071, 0.3730,  ..., 0.6628, 0.5108, 0.4712],
          [0.4031, 0.4846, 0.7356,  ..., 0.5946, 0.5059, 0.4993]],

         [[0.5530, 0.3826, 0.5765,  ..., 0.4620, 0.4739, 0.5898],
          [0.5148, 0.6007, 0.3882,  ..., 0.5508, 0.3817, 0.4429],
          [0.4154, 0.4216, 0.3693,  ..., 0.6460, 0.4211, 0.7402],
          [0.4776, 0.5290, 0.5104,  ..., 0.5903, 0.5799, 0.4762]],

         [[0.5552, 0.6334, 0.4450,  ..., 0.6567, 0.4450, 0.3844],
          [0.5837, 0.5879, 0.4144,  ..., 0.6697, 0.4637, 0.3748],
          [0.3812, 0.4894, 0.5210,  ..., 0.5263, 0.4263, 0.5047],
          [0.4470, 0.5746, 0.4981,  ..., 0.5438, 0.5297, 0.5139]],

         ...,

         [[0.6100, 0.6160, 0.4846,  ..., 0.4254, 0.5164, 0.5121],
          [0.5804, 0.5832, 0.5884,  ..., 0.4579, 0.5162, 0.5210],
          [0.4168, 0.4579, 0.5660,  ..., 0.5206, 0.5307, 0.3984],
          [0.5034, 0.5450, 0.4301,  ..., 0.5074, 0.5974, 0.6637]],

         [[0.4709, 0.3208, 0.4859,  ..., 0.4182, 0.4359, 0.5001],
          [0.5164, 0.4216, 0.3937,  ..., 0.5765, 0.5383, 0.6179],
          [0.4644, 0.6379, 0.4841,  ..., 0.4756, 0.5655, 0.5294],
          [0.5344, 0.4932, 0.5311,  ..., 0.5315, 0.5136, 0.5837]],

         [[0.5576, 0.6783, 0.4158,  ..., 0.6091, 0.4987, 0.5117],
          [0.4847, 0.6433, 0.3785,  ..., 0.6280, 0.5467, 0.5446],
          [0.4984, 0.3567, 0.6146,  ..., 0.4874, 0.6270, 0.3149],
          [0.4325, 0.3016, 0.4838,  ..., 0.4886, 0.5358, 0.5603]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0030, -0.0050,  0.0030, -0.0030,  0.0030,  0.0030, -0.0050, -0.0050,
         0.0030,  0.0050], device='cuda:0')
selected experts tensor([1605, 1781, 1631, 1594, 1647, 1638, 1608, 1559, 1602, 1719],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5650, 0.4769, 0.5279,  ..., 0.4485, 0.4653, 0.5383],
          [0.6159, 0.4986, 0.5155,  ..., 0.5596, 0.4404, 0.4679],
          [0.5251, 0.4251, 0.6647,  ..., 0.5432, 0.6000, 0.4953],
          [0.5082, 0.3911, 0.4180,  ..., 0.6876, 0.4409, 0.5359]],

         [[0.4939, 0.5020, 0.4879,  ..., 0.3762, 0.3049, 0.4052],
          [0.4644, 0.4756, 0.4405,  ..., 0.3625, 0.4963, 0.3808],
          [0.4111, 0.6115, 0.6129,  ..., 0.4151, 0.4242, 0.4772],
          [0.4012, 0.3943, 0.3794,  ..., 0.7175, 0.3973, 0.6338]],

         [[0.5399, 0.4684, 0.4665,  ..., 0.3836, 0.3743, 0.4541],
          [0.3656, 0.4037, 0.5296,  ..., 0.4546, 0.4694, 0.5244],
          [0.5297, 0.7072, 0.5768,  ..., 0.4449, 0.4933, 0.5049],
          [0.4035, 0.4490, 0.4437,  ..., 0.5725, 0.4921, 0.6175]],

         ...,

         [[0.4258, 0.5768, 0.4338,  ..., 0.5333, 0.5285, 0.4789],
          [0.4349, 0.5586, 0.5682,  ..., 0.4614, 0.4604, 0.5933],
          [0.6034, 0.4618, 0.6275,  ..., 0.5286, 0.4869, 0.4768],
          [0.6679, 0.3850, 0.4730,  ..., 0.5980, 0.4498, 0.5210]],

         [[0.4722, 0.5291, 0.4597,  ..., 0.4582, 0.5313, 0.6142],
          [0.5032, 0.4928, 0.2655,  ..., 0.5159, 0.5050, 0.5279],
          [0.5617, 0.5754, 0.4271,  ..., 0.3643, 0.4057, 0.4818],
          [0.5554, 0.4645, 0.6302,  ..., 0.5105, 0.4653, 0.4751]],

         [[0.3224, 0.3598, 0.4890,  ..., 0.3218, 0.5382, 0.5536],
          [0.3969, 0.5734, 0.4950,  ..., 0.5933, 0.4445, 0.5653],
          [0.6196, 0.5083, 0.5881,  ..., 0.3925, 0.4549, 0.3209],
          [0.4725, 0.4519, 0.4309,  ..., 0.4066, 0.4409, 0.5269]]],


        [[[0.4239, 0.6147, 0.4446,  ..., 0.4749, 0.4323, 0.3607],
          [0.4339, 0.4690, 0.5834,  ..., 0.5914, 0.4337, 0.5952],
          [0.6565, 0.6311, 0.4993,  ..., 0.5730, 0.4735, 0.5054],
          [0.6495, 0.4328, 0.3827,  ..., 0.5001, 0.5239, 0.5337]],

         [[0.4277, 0.3348, 0.5089,  ..., 0.3553, 0.4638, 0.5188],
          [0.4287, 0.4555, 0.5591,  ..., 0.4763, 0.4660, 0.4570],
          [0.5295, 0.5877, 0.4662,  ..., 0.4695, 0.4600, 0.4223],
          [0.5307, 0.4374, 0.4639,  ..., 0.5985, 0.6233, 0.5677]],

         [[0.5235, 0.6055, 0.4772,  ..., 0.5262, 0.3653, 0.4706],
          [0.5798, 0.3869, 0.4531,  ..., 0.5246, 0.4342, 0.6031],
          [0.5186, 0.5526, 0.6184,  ..., 0.5396, 0.4721, 0.4142],
          [0.4950, 0.3698, 0.3906,  ..., 0.5999, 0.4147, 0.5057]],

         ...,

         [[0.4578, 0.5143, 0.4971,  ..., 0.6008, 0.5313, 0.4730],
          [0.5578, 0.5179, 0.6613,  ..., 0.3517, 0.5223, 0.4646],
          [0.5063, 0.5581, 0.4589,  ..., 0.5730, 0.3908, 0.4473],
          [0.3876, 0.5070, 0.4290,  ..., 0.5015, 0.4704, 0.4444]],

         [[0.5207, 0.6092, 0.5216,  ..., 0.5115, 0.5273, 0.3813],
          [0.4867, 0.4990, 0.5065,  ..., 0.3892, 0.4006, 0.5451],
          [0.4277, 0.5644, 0.4845,  ..., 0.4718, 0.5101, 0.4185],
          [0.6450, 0.4151, 0.3730,  ..., 0.4815, 0.4783, 0.5222]],

         [[0.5655, 0.4061, 0.3804,  ..., 0.4446, 0.5090, 0.4957],
          [0.5093, 0.5928, 0.5644,  ..., 0.5403, 0.3644, 0.5265],
          [0.5547, 0.4944, 0.5572,  ..., 0.5267, 0.5944, 0.4773],
          [0.5206, 0.5054, 0.4980,  ..., 0.6293, 0.4390, 0.4767]]]],
       device='cuda:0')
tensor([[[[0.5660, 0.4819, 0.5329,  ..., 0.4535, 0.4603, 0.5433],
          [0.6169, 0.5036, 0.5205,  ..., 0.5646, 0.4354, 0.4729],
          [0.5261, 0.4301, 0.6697,  ..., 0.5482, 0.5950, 0.5003],
          [0.5092, 0.3961, 0.4230,  ..., 0.6926, 0.4359, 0.5409]],

         [[0.4949, 0.5070, 0.4929,  ..., 0.3812, 0.2999, 0.4102],
          [0.4654, 0.4806, 0.4455,  ..., 0.3675, 0.4913, 0.3858],
          [0.4121, 0.6165, 0.6179,  ..., 0.4201, 0.4192, 0.4822],
          [0.4022, 0.3993, 0.3844,  ..., 0.7225, 0.3923, 0.6388]],

         [[0.5409, 0.4734, 0.4715,  ..., 0.3886, 0.3693, 0.4591],
          [0.3666, 0.4087, 0.5346,  ..., 0.4596, 0.4644, 0.5294],
          [0.5307, 0.7122, 0.5818,  ..., 0.4499, 0.4883, 0.5099],
          [0.4045, 0.4540, 0.4487,  ..., 0.5775, 0.4871, 0.6225]],

         ...,

         [[0.4268, 0.5818, 0.4388,  ..., 0.5383, 0.5235, 0.4839],
          [0.4359, 0.5636, 0.5732,  ..., 0.4664, 0.4554, 0.5983],
          [0.6044, 0.4668, 0.6325,  ..., 0.5336, 0.4819, 0.4818],
          [0.6689, 0.3900, 0.4780,  ..., 0.6030, 0.4448, 0.5260]],

         [[0.4732, 0.5341, 0.4647,  ..., 0.4632, 0.5263, 0.6192],
          [0.5042, 0.4978, 0.2705,  ..., 0.5209, 0.5000, 0.5329],
          [0.5627, 0.5804, 0.4321,  ..., 0.3693, 0.4007, 0.4868],
          [0.5564, 0.4695, 0.6352,  ..., 0.5155, 0.4603, 0.4801]],

         [[0.3234, 0.3648, 0.4940,  ..., 0.3268, 0.5332, 0.5586],
          [0.3979, 0.5784, 0.5000,  ..., 0.5983, 0.4395, 0.5703],
          [0.6206, 0.5133, 0.5931,  ..., 0.3975, 0.4499, 0.3259],
          [0.4735, 0.4569, 0.4359,  ..., 0.4116, 0.4359, 0.5319]]],


        [[[0.4249, 0.6197, 0.4496,  ..., 0.4799, 0.4273, 0.3657],
          [0.4349, 0.4740, 0.5884,  ..., 0.5964, 0.4287, 0.6002],
          [0.6575, 0.6361, 0.5043,  ..., 0.5780, 0.4685, 0.5104],
          [0.6505, 0.4378, 0.3877,  ..., 0.5051, 0.5189, 0.5387]],

         [[0.4287, 0.3398, 0.5139,  ..., 0.3603, 0.4588, 0.5238],
          [0.4297, 0.4605, 0.5641,  ..., 0.4813, 0.4610, 0.4620],
          [0.5305, 0.5927, 0.4712,  ..., 0.4745, 0.4550, 0.4273],
          [0.5317, 0.4424, 0.4689,  ..., 0.6035, 0.6183, 0.5727]],

         [[0.5245, 0.6105, 0.4822,  ..., 0.5312, 0.3603, 0.4756],
          [0.5808, 0.3919, 0.4581,  ..., 0.5296, 0.4292, 0.6081],
          [0.5196, 0.5576, 0.6234,  ..., 0.5446, 0.4671, 0.4192],
          [0.4960, 0.3748, 0.3956,  ..., 0.6049, 0.4097, 0.5107]],

         ...,

         [[0.4588, 0.5193, 0.5021,  ..., 0.6058, 0.5263, 0.4780],
          [0.5588, 0.5229, 0.6663,  ..., 0.3567, 0.5173, 0.4696],
          [0.5073, 0.5631, 0.4639,  ..., 0.5780, 0.3858, 0.4523],
          [0.3886, 0.5120, 0.4340,  ..., 0.5065, 0.4654, 0.4494]],

         [[0.5217, 0.6142, 0.5266,  ..., 0.5165, 0.5223, 0.3863],
          [0.4877, 0.5040, 0.5115,  ..., 0.3942, 0.3956, 0.5501],
          [0.4287, 0.5694, 0.4895,  ..., 0.4768, 0.5051, 0.4235],
          [0.6460, 0.4201, 0.3780,  ..., 0.4865, 0.4733, 0.5272]],

         [[0.5665, 0.4111, 0.3854,  ..., 0.4496, 0.5040, 0.5007],
          [0.5103, 0.5978, 0.5694,  ..., 0.5453, 0.3594, 0.5315],
          [0.5557, 0.4994, 0.5622,  ..., 0.5317, 0.5894, 0.4823],
          [0.5216, 0.5104, 0.5030,  ..., 0.6343, 0.4340, 0.4817]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0010, -0.0050, -0.0050,  0.0050, -0.0030,  0.0030, -0.0010, -0.0050,
         0.0050, -0.0050], device='cuda:0')
selected experts tensor([1434, 2018, 1812, 1577, 1472, 1651, 1751, 1733,  798, 2138],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.4168, 0.5304, 0.6881,  ..., 0.5038, 0.5425, 0.4370],
          [0.3887, 0.4898, 0.5531,  ..., 0.4274, 0.6391, 0.4799],
          [0.5651, 0.5509, 0.3552,  ..., 0.4103, 0.6805, 0.5587],
          [0.6776, 0.4818, 0.4284,  ..., 0.5886, 0.4250, 0.6463]],

         [[0.4579, 0.4666, 0.6355,  ..., 0.4303, 0.6981, 0.4869],
          [0.4159, 0.5410, 0.5729,  ..., 0.3723, 0.6805, 0.6125],
          [0.6189, 0.4846, 0.5321,  ..., 0.5430, 0.6570, 0.6204],
          [0.6087, 0.5078, 0.4621,  ..., 0.3888, 0.4921, 0.6436]],

         [[0.4942, 0.5078, 0.7356,  ..., 0.4686, 0.5938, 0.4399],
          [0.4316, 0.4412, 0.6116,  ..., 0.4151, 0.6032, 0.5218],
          [0.5521, 0.4681, 0.5671,  ..., 0.4601, 0.5125, 0.6074],
          [0.5596, 0.5932, 0.4895,  ..., 0.5719, 0.4725, 0.5647]],

         ...,

         [[0.5010, 0.5010, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5030,  ..., 0.5030, 0.5030, 0.5030]],

         [[0.5010, 0.5010, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5030,  ..., 0.5030, 0.5030, 0.5030]],

         [[0.5010, 0.5010, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5030,  ..., 0.5030, 0.5030, 0.5030]]],


        [[[0.4502, 0.5557, 0.5563,  ..., 0.4444, 0.5334, 0.5700],
          [0.5101, 0.4787, 0.5872,  ..., 0.3472, 0.6088, 0.4686],
          [0.4017, 0.3901, 0.5186,  ..., 0.6847, 0.6736, 0.5320],
          [0.5610, 0.5138, 0.4798,  ..., 0.5608, 0.4589, 0.6139]],

         [[0.3947, 0.4601, 0.6623,  ..., 0.5625, 0.5223, 0.4785],
          [0.3034, 0.4140, 0.4706,  ..., 0.4127, 0.6490, 0.5456],
          [0.4972, 0.4688, 0.5138,  ..., 0.4047, 0.5909, 0.6093],
          [0.6105, 0.5036, 0.4708,  ..., 0.5900, 0.4370, 0.5786]],

         [[0.3785, 0.5547, 0.6623,  ..., 0.5084, 0.5580, 0.4488],
          [0.3845, 0.3878, 0.5582,  ..., 0.3385, 0.5705, 0.6535],
          [0.5799, 0.4492, 0.5185,  ..., 0.4531, 0.6906, 0.4909],
          [0.4905, 0.5057, 0.4843,  ..., 0.5791, 0.4592, 0.6418]],

         ...,

         [[0.5010, 0.5010, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5030,  ..., 0.5030, 0.5030, 0.5030]],

         [[0.5010, 0.5010, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5030,  ..., 0.5030, 0.5030, 0.5030]],

         [[0.5010, 0.5010, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5030,  ..., 0.5030, 0.5030, 0.5030]]]],
       device='cuda:0')
tensor([[[[0.4158, 0.5294, 0.6851,  ..., 0.5008, 0.5395, 0.4340],
          [0.3877, 0.4888, 0.5501,  ..., 0.4244, 0.6361, 0.4769],
          [0.5641, 0.5499, 0.3522,  ..., 0.4073, 0.6775, 0.5557],
          [0.6766, 0.4808, 0.4254,  ..., 0.5856, 0.4220, 0.6433]],

         [[0.4569, 0.4656, 0.6325,  ..., 0.4273, 0.6951, 0.4839],
          [0.4149, 0.5400, 0.5699,  ..., 0.3693, 0.6775, 0.6095],
          [0.6179, 0.4836, 0.5291,  ..., 0.5400, 0.6540, 0.6174],
          [0.6077, 0.5068, 0.4591,  ..., 0.3858, 0.4891, 0.6406]],

         [[0.4932, 0.5068, 0.7326,  ..., 0.4656, 0.5908, 0.4369],
          [0.4306, 0.4402, 0.6086,  ..., 0.4121, 0.6002, 0.5188],
          [0.5511, 0.4671, 0.5641,  ..., 0.4571, 0.5095, 0.6044],
          [0.5586, 0.5922, 0.4865,  ..., 0.5689, 0.4695, 0.5617]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.4492, 0.5547, 0.5533,  ..., 0.4414, 0.5304, 0.5670],
          [0.5091, 0.4777, 0.5842,  ..., 0.3442, 0.6058, 0.4656],
          [0.4007, 0.3891, 0.5156,  ..., 0.6817, 0.6706, 0.5290],
          [0.5600, 0.5128, 0.4768,  ..., 0.5578, 0.4559, 0.6109]],

         [[0.3937, 0.4591, 0.6593,  ..., 0.5595, 0.5193, 0.4755],
          [0.3024, 0.4130, 0.4676,  ..., 0.4097, 0.6460, 0.5426],
          [0.4962, 0.4678, 0.5108,  ..., 0.4017, 0.5879, 0.6063],
          [0.6095, 0.5026, 0.4678,  ..., 0.5870, 0.4340, 0.5756]],

         [[0.3775, 0.5537, 0.6593,  ..., 0.5054, 0.5550, 0.4458],
          [0.3835, 0.3868, 0.5552,  ..., 0.3355, 0.5675, 0.6505],
          [0.5789, 0.4482, 0.5155,  ..., 0.4501, 0.6876, 0.4879],
          [0.4895, 0.5047, 0.4813,  ..., 0.5761, 0.4562, 0.6388]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:112: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
  fig = plt.figure(figsize=self.figsize)
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([0.0010, 0.0010, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,
        0.0030], device='cuda:0')
selected experts tensor([ 787,  544, 2765, 2490,  936,  995,  754,  403, 1524, 1090],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6774, 0.3265, 0.3731,  ..., 0.5575, 0.4176, 0.4290],
          [0.6445, 0.5259, 0.4628,  ..., 0.4870, 0.3988, 0.5702],
          [0.4906, 0.4954, 0.4164,  ..., 0.5563, 0.4290, 0.5057],
          [0.7126, 0.4578, 0.5218,  ..., 0.4104, 0.3951, 0.3237]],

         [[0.6774, 0.3265, 0.3731,  ..., 0.5575, 0.4176, 0.4290],
          [0.6445, 0.5259, 0.4628,  ..., 0.4870, 0.3988, 0.5702],
          [0.4906, 0.4954, 0.4164,  ..., 0.5563, 0.4290, 0.5057],
          [0.7126, 0.4578, 0.5218,  ..., 0.4104, 0.3951, 0.3237]],

         [[0.4122, 0.5287, 0.6795,  ..., 0.6410, 0.5213, 0.4714],
          [0.5545, 0.5312, 0.5570,  ..., 0.3963, 0.5225, 0.4076],
          [0.4531, 0.5094, 0.4207,  ..., 0.2608, 0.4984, 0.3509],
          [0.5429, 0.4809, 0.3902,  ..., 0.3875, 0.5044, 0.5360]],

         ...,

         [[0.5487, 0.5136, 0.4572,  ..., 0.3291, 0.5730, 0.5806],
          [0.4756, 0.4277, 0.4122,  ..., 0.5307, 0.5314, 0.5398],
          [0.4405, 0.4950, 0.4905,  ..., 0.5573, 0.5887, 0.6420],
          [0.5061, 0.4610, 0.5025,  ..., 0.5772, 0.5481, 0.5452]],

         [[0.4579, 0.5826, 0.5335,  ..., 0.3706, 0.5433, 0.4597],
          [0.5081, 0.4310, 0.6129,  ..., 0.4217, 0.6723, 0.4478],
          [0.4708, 0.4754, 0.3809,  ..., 0.5273, 0.5365, 0.6348],
          [0.3789, 0.5616, 0.5615,  ..., 0.5410, 0.6475, 0.5046]],

         [[0.3729, 0.5257, 0.5237,  ..., 0.3940, 0.6137, 0.6157],
          [0.5937, 0.4291, 0.3740,  ..., 0.4232, 0.5170, 0.5849],
          [0.3826, 0.4091, 0.5327,  ..., 0.4351, 0.3895, 0.4680],
          [0.4027, 0.6140, 0.4480,  ..., 0.5805, 0.6376, 0.5839]]],


        [[[0.4056, 0.5645, 0.4093,  ..., 0.5848, 0.3955, 0.4057],
          [0.5624, 0.5106, 0.5137,  ..., 0.5734, 0.5646, 0.4062],
          [0.4775, 0.4905, 0.4975,  ..., 0.5196, 0.4590, 0.4333],
          [0.4502, 0.5110, 0.4389,  ..., 0.5630, 0.4949, 0.5995]],

         [[0.4470, 0.4229, 0.5909,  ..., 0.4270, 0.4634, 0.4466],
          [0.3933, 0.5595, 0.4990,  ..., 0.3742, 0.6257, 0.4670],
          [0.5480, 0.7174, 0.5420,  ..., 0.6292, 0.4067, 0.3777],
          [0.5615, 0.5650, 0.5899,  ..., 0.4327, 0.4651, 0.5437]],

         [[0.4857, 0.3343, 0.5279,  ..., 0.4236, 0.4765, 0.4430],
          [0.5244, 0.5510, 0.5162,  ..., 0.4993, 0.3645, 0.6321],
          [0.4591, 0.6564, 0.6578,  ..., 0.5815, 0.5118, 0.5806],
          [0.5002, 0.5027, 0.5909,  ..., 0.5172, 0.4295, 0.4252]],

         ...,

         [[0.5796, 0.4411, 0.6212,  ..., 0.6491, 0.5293, 0.5192],
          [0.4984, 0.5056, 0.6087,  ..., 0.5606, 0.3717, 0.5502],
          [0.4504, 0.5167, 0.3874,  ..., 0.3023, 0.6053, 0.5678],
          [0.2997, 0.5120, 0.5814,  ..., 0.4423, 0.5701, 0.6171]],

         [[0.5075, 0.4612, 0.5225,  ..., 0.4679, 0.4082, 0.5045],
          [0.4465, 0.6440, 0.5174,  ..., 0.3733, 0.4110, 0.3211],
          [0.4980, 0.6163, 0.5400,  ..., 0.5322, 0.5054, 0.4687],
          [0.4936, 0.4684, 0.6453,  ..., 0.5068, 0.5025, 0.6157]],

         [[0.4492, 0.5147, 0.4848,  ..., 0.4614, 0.3663, 0.4638],
          [0.5819, 0.5457, 0.5427,  ..., 0.3607, 0.5136, 0.5981],
          [0.4218, 0.4349, 0.5080,  ..., 0.5234, 0.6010, 0.5591],
          [0.4458, 0.6103, 0.4264,  ..., 0.4992, 0.5754, 0.6009]]]],
       device='cuda:0')
tensor([[[[0.6834, 0.3285, 0.3711,  ..., 0.5535, 0.4116, 0.4330],
          [0.6505, 0.5279, 0.4608,  ..., 0.4830, 0.3928, 0.5742],
          [0.4966, 0.4974, 0.4144,  ..., 0.5523, 0.4230, 0.5097],
          [0.7186, 0.4598, 0.5198,  ..., 0.4064, 0.3891, 0.3277]],

         [[0.6834, 0.3285, 0.3711,  ..., 0.5535, 0.4116, 0.4330],
          [0.6505, 0.5279, 0.4608,  ..., 0.4830, 0.3928, 0.5742],
          [0.4966, 0.4974, 0.4144,  ..., 0.5523, 0.4230, 0.5097],
          [0.7186, 0.4598, 0.5198,  ..., 0.4064, 0.3891, 0.3277]],

         [[0.4182, 0.5307, 0.6775,  ..., 0.6370, 0.5153, 0.4754],
          [0.5605, 0.5332, 0.5550,  ..., 0.3923, 0.5165, 0.4116],
          [0.4591, 0.5114, 0.4187,  ..., 0.2568, 0.4924, 0.3549],
          [0.5489, 0.4829, 0.3882,  ..., 0.3835, 0.4984, 0.5400]],

         ...,

         [[0.5547, 0.5156, 0.4552,  ..., 0.3251, 0.5670, 0.5846],
          [0.4816, 0.4297, 0.4102,  ..., 0.5267, 0.5254, 0.5438],
          [0.4465, 0.4970, 0.4885,  ..., 0.5533, 0.5827, 0.6460],
          [0.5121, 0.4630, 0.5005,  ..., 0.5732, 0.5421, 0.5492]],

         [[0.4639, 0.5846, 0.5315,  ..., 0.3666, 0.5373, 0.4637],
          [0.5141, 0.4330, 0.6109,  ..., 0.4177, 0.6663, 0.4518],
          [0.4768, 0.4774, 0.3789,  ..., 0.5233, 0.5305, 0.6388],
          [0.3849, 0.5636, 0.5595,  ..., 0.5370, 0.6415, 0.5086]],

         [[0.3789, 0.5277, 0.5217,  ..., 0.3900, 0.6077, 0.6197],
          [0.5997, 0.4311, 0.3720,  ..., 0.4192, 0.5110, 0.5889],
          [0.3886, 0.4111, 0.5307,  ..., 0.4311, 0.3835, 0.4720],
          [0.4087, 0.6160, 0.4460,  ..., 0.5765, 0.6316, 0.5879]]],


        [[[0.4116, 0.5665, 0.4073,  ..., 0.5808, 0.3895, 0.4097],
          [0.5684, 0.5126, 0.5117,  ..., 0.5694, 0.5586, 0.4102],
          [0.4835, 0.4925, 0.4955,  ..., 0.5156, 0.4530, 0.4373],
          [0.4562, 0.5130, 0.4369,  ..., 0.5590, 0.4889, 0.6035]],

         [[0.4530, 0.4249, 0.5889,  ..., 0.4230, 0.4574, 0.4506],
          [0.3993, 0.5615, 0.4970,  ..., 0.3702, 0.6197, 0.4710],
          [0.5540, 0.7194, 0.5400,  ..., 0.6252, 0.4007, 0.3817],
          [0.5675, 0.5670, 0.5879,  ..., 0.4287, 0.4591, 0.5477]],

         [[0.4917, 0.3363, 0.5259,  ..., 0.4196, 0.4705, 0.4470],
          [0.5304, 0.5530, 0.5142,  ..., 0.4953, 0.3585, 0.6361],
          [0.4651, 0.6584, 0.6558,  ..., 0.5775, 0.5058, 0.5846],
          [0.5062, 0.5047, 0.5889,  ..., 0.5132, 0.4235, 0.4292]],

         ...,

         [[0.5856, 0.4431, 0.6192,  ..., 0.6451, 0.5233, 0.5232],
          [0.5044, 0.5076, 0.6067,  ..., 0.5566, 0.3657, 0.5542],
          [0.4564, 0.5187, 0.3854,  ..., 0.2983, 0.5993, 0.5718],
          [0.3057, 0.5140, 0.5794,  ..., 0.4383, 0.5641, 0.6211]],

         [[0.5135, 0.4632, 0.5205,  ..., 0.4639, 0.4022, 0.5085],
          [0.4525, 0.6460, 0.5154,  ..., 0.3693, 0.4050, 0.3251],
          [0.5040, 0.6183, 0.5380,  ..., 0.5282, 0.4994, 0.4727],
          [0.4996, 0.4704, 0.6433,  ..., 0.5028, 0.4965, 0.6197]],

         [[0.4552, 0.5167, 0.4828,  ..., 0.4574, 0.3603, 0.4678],
          [0.5879, 0.5477, 0.5407,  ..., 0.3567, 0.5076, 0.6021],
          [0.4278, 0.4369, 0.5060,  ..., 0.5194, 0.5950, 0.5631],
          [0.4518, 0.6123, 0.4244,  ..., 0.4952, 0.5694, 0.6049]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0060, -0.0020,  0.0020,  0.0040,  0.0040,  0.0060, -0.0040,  0.0040,
         0.0060, -0.0040], device='cuda:0')
selected experts tensor([1665, 1679, 1610, 1567, 1616, 1816, 1739, 1511, 1552, 1629],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4469, 0.5696, 0.5275,  ..., 0.4095, 0.6966, 0.4322],
          [0.4650, 0.4113, 0.3742,  ..., 0.2484, 0.5568, 0.4594],
          [0.4699, 0.4996, 0.4883,  ..., 0.4133, 0.4156, 0.5090],
          [0.4807, 0.4422, 0.3940,  ..., 0.6162, 0.5083, 0.4365]],

         [[0.4469, 0.5696, 0.5275,  ..., 0.4095, 0.6966, 0.4322],
          [0.4650, 0.4113, 0.3742,  ..., 0.2484, 0.5568, 0.4594],
          [0.4699, 0.4996, 0.4883,  ..., 0.4133, 0.4156, 0.5090],
          [0.4807, 0.4422, 0.3940,  ..., 0.6162, 0.5083, 0.4365]],

         [[0.5138, 0.5272, 0.7304,  ..., 0.5483, 0.5743, 0.4857],
          [0.4575, 0.5705, 0.5616,  ..., 0.5601, 0.3351, 0.5449],
          [0.5633, 0.3498, 0.3973,  ..., 0.6819, 0.5286, 0.6075],
          [0.5396, 0.5151, 0.5116,  ..., 0.5759, 0.5401, 0.4005]],

         ...,

         [[0.5316, 0.3615, 0.4939,  ..., 0.6051, 0.4279, 0.6084],
          [0.4604, 0.5881, 0.5144,  ..., 0.4888, 0.5900, 0.5225],
          [0.5729, 0.5405, 0.5227,  ..., 0.6157, 0.3797, 0.6196],
          [0.5161, 0.5281, 0.5762,  ..., 0.5347, 0.4189, 0.4716]],

         [[0.4638, 0.6480, 0.5580,  ..., 0.4454, 0.4327, 0.4132],
          [0.5544, 0.5383, 0.5640,  ..., 0.3708, 0.4024, 0.3788],
          [0.5096, 0.3905, 0.5152,  ..., 0.6321, 0.5224, 0.6598],
          [0.5858, 0.6706, 0.4380,  ..., 0.5702, 0.4773, 0.4127]],

         [[0.5900, 0.5581, 0.4337,  ..., 0.5212, 0.4570, 0.5233],
          [0.5437, 0.6114, 0.5413,  ..., 0.6018, 0.3421, 0.3447],
          [0.4972, 0.4700, 0.4943,  ..., 0.2918, 0.4580, 0.3959],
          [0.6140, 0.3277, 0.5700,  ..., 0.3599, 0.5592, 0.4375]]],


        [[[0.4607, 0.4400, 0.4270,  ..., 0.5558, 0.4071, 0.4657],
          [0.5657, 0.6100, 0.3553,  ..., 0.5744, 0.4706, 0.5386],
          [0.4488, 0.5600, 0.5810,  ..., 0.5352, 0.5164, 0.5519],
          [0.5734, 0.4390, 0.3447,  ..., 0.5639, 0.6589, 0.4251]],

         [[0.5415, 0.3543, 0.5211,  ..., 0.4650, 0.3806, 0.4894],
          [0.5762, 0.4194, 0.4665,  ..., 0.4190, 0.4493, 0.5190],
          [0.4246, 0.6082, 0.4413,  ..., 0.3879, 0.4846, 0.4643],
          [0.3724, 0.3660, 0.4289,  ..., 0.5468, 0.3670, 0.4908]],

         [[0.3274, 0.5444, 0.5440,  ..., 0.4423, 0.4721, 0.5801],
          [0.5144, 0.3697, 0.4876,  ..., 0.5015, 0.3949, 0.5990],
          [0.5896, 0.4941, 0.3922,  ..., 0.5011, 0.4933, 0.4517],
          [0.6065, 0.4289, 0.4573,  ..., 0.4969, 0.4873, 0.6219]],

         ...,

         [[0.5423, 0.4891, 0.6419,  ..., 0.4081, 0.4847, 0.6103],
          [0.3889, 0.5371, 0.4607,  ..., 0.6675, 0.5671, 0.4113],
          [0.5236, 0.3525, 0.4809,  ..., 0.5256, 0.5546, 0.3334],
          [0.4614, 0.5729, 0.5430,  ..., 0.5384, 0.5585, 0.5233]],

         [[0.3652, 0.5480, 0.5294,  ..., 0.4648, 0.4241, 0.6098],
          [0.6482, 0.5993, 0.4505,  ..., 0.4147, 0.5724, 0.6075],
          [0.4522, 0.4606, 0.4786,  ..., 0.4798, 0.5796, 0.5938],
          [0.5957, 0.4023, 0.4996,  ..., 0.4962, 0.4341, 0.5575]],

         [[0.5616, 0.6723, 0.4198,  ..., 0.6051, 0.5025, 0.5158],
          [0.4887, 0.6373, 0.3825,  ..., 0.6240, 0.5510, 0.5486],
          [0.5023, 0.3507, 0.6186,  ..., 0.4833, 0.6310, 0.3189],
          [0.4365, 0.2956, 0.4876,  ..., 0.4846, 0.5398, 0.5643]]]],
       device='cuda:0')
tensor([[[[0.4429, 0.5756, 0.5235,  ..., 0.4135, 0.6926, 0.4282],
          [0.4610, 0.4173, 0.3702,  ..., 0.2524, 0.5528, 0.4554],
          [0.4659, 0.5056, 0.4843,  ..., 0.4173, 0.4116, 0.5050],
          [0.4767, 0.4482, 0.3900,  ..., 0.6202, 0.5043, 0.4325]],

         [[0.4429, 0.5756, 0.5235,  ..., 0.4135, 0.6926, 0.4282],
          [0.4610, 0.4173, 0.3702,  ..., 0.2524, 0.5528, 0.4554],
          [0.4659, 0.5056, 0.4843,  ..., 0.4173, 0.4116, 0.5050],
          [0.4767, 0.4482, 0.3900,  ..., 0.6202, 0.5043, 0.4325]],

         [[0.5098, 0.5332, 0.7264,  ..., 0.5523, 0.5703, 0.4817],
          [0.4535, 0.5765, 0.5576,  ..., 0.5641, 0.3311, 0.5409],
          [0.5593, 0.3558, 0.3933,  ..., 0.6859, 0.5246, 0.6035],
          [0.5356, 0.5211, 0.5076,  ..., 0.5799, 0.5361, 0.3965]],

         ...,

         [[0.5276, 0.3675, 0.4899,  ..., 0.6091, 0.4239, 0.6044],
          [0.4564, 0.5941, 0.5104,  ..., 0.4928, 0.5860, 0.5185],
          [0.5689, 0.5465, 0.5187,  ..., 0.6197, 0.3757, 0.6156],
          [0.5121, 0.5341, 0.5722,  ..., 0.5387, 0.4149, 0.4676]],

         [[0.4598, 0.6540, 0.5540,  ..., 0.4494, 0.4287, 0.4092],
          [0.5504, 0.5443, 0.5600,  ..., 0.3748, 0.3984, 0.3748],
          [0.5056, 0.3965, 0.5112,  ..., 0.6361, 0.5184, 0.6558],
          [0.5818, 0.6766, 0.4340,  ..., 0.5742, 0.4733, 0.4087]],

         [[0.5860, 0.5641, 0.4297,  ..., 0.5252, 0.4530, 0.5193],
          [0.5397, 0.6174, 0.5373,  ..., 0.6058, 0.3381, 0.3407],
          [0.4932, 0.4760, 0.4903,  ..., 0.2958, 0.4540, 0.3919],
          [0.6100, 0.3337, 0.5660,  ..., 0.3639, 0.5552, 0.4335]]],


        [[[0.4567, 0.4460, 0.4230,  ..., 0.5598, 0.4031, 0.4617],
          [0.5617, 0.6160, 0.3513,  ..., 0.5784, 0.4666, 0.5346],
          [0.4448, 0.5660, 0.5770,  ..., 0.5392, 0.5124, 0.5479],
          [0.5694, 0.4450, 0.3407,  ..., 0.5679, 0.6549, 0.4211]],

         [[0.5375, 0.3603, 0.5171,  ..., 0.4690, 0.3766, 0.4854],
          [0.5722, 0.4254, 0.4625,  ..., 0.4230, 0.4453, 0.5150],
          [0.4206, 0.6142, 0.4373,  ..., 0.3919, 0.4806, 0.4603],
          [0.3684, 0.3720, 0.4249,  ..., 0.5508, 0.3630, 0.4868]],

         [[0.3234, 0.5504, 0.5400,  ..., 0.4463, 0.4681, 0.5761],
          [0.5104, 0.3757, 0.4836,  ..., 0.5055, 0.3909, 0.5950],
          [0.5856, 0.5001, 0.3882,  ..., 0.5051, 0.4893, 0.4477],
          [0.6025, 0.4349, 0.4533,  ..., 0.5009, 0.4833, 0.6179]],

         ...,

         [[0.5383, 0.4951, 0.6379,  ..., 0.4121, 0.4807, 0.6063],
          [0.3849, 0.5431, 0.4567,  ..., 0.6715, 0.5631, 0.4073],
          [0.5196, 0.3585, 0.4769,  ..., 0.5296, 0.5506, 0.3294],
          [0.4574, 0.5789, 0.5390,  ..., 0.5424, 0.5545, 0.5193]],

         [[0.3612, 0.5540, 0.5254,  ..., 0.4688, 0.4201, 0.6058],
          [0.6442, 0.6053, 0.4465,  ..., 0.4187, 0.5684, 0.6035],
          [0.4482, 0.4666, 0.4746,  ..., 0.4838, 0.5756, 0.5898],
          [0.5917, 0.4083, 0.4956,  ..., 0.5002, 0.4301, 0.5535]],

         [[0.5576, 0.6783, 0.4158,  ..., 0.6091, 0.4985, 0.5118],
          [0.4847, 0.6433, 0.3785,  ..., 0.6280, 0.5470, 0.5446],
          [0.4983, 0.3567, 0.6146,  ..., 0.4873, 0.6270, 0.3149],
          [0.4325, 0.3016, 0.4836,  ..., 0.4886, 0.5358, 0.5603]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0040, -0.0060,  0.0040, -0.0020,  0.0020,  0.0040, -0.0040, -0.0040,
         0.0040,  0.0040], device='cuda:0')
selected experts tensor([1580, 1695, 1594, 1606, 1654, 1756, 1660, 1597, 1679, 1563],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5189, 0.5581, 0.4285,  ..., 0.4550, 0.4039, 0.4685],
          [0.4630, 0.5843, 0.4403,  ..., 0.6373, 0.4124, 0.5264],
          [0.4504, 0.5886, 0.5378,  ..., 0.5658, 0.5204, 0.5043],
          [0.6188, 0.3734, 0.3277,  ..., 0.5658, 0.4901, 0.5323]],

         [[0.5181, 0.5581, 0.4270,  ..., 0.4553, 0.4030, 0.4689],
          [0.4625, 0.5838, 0.4395,  ..., 0.6382, 0.4110, 0.5269],
          [0.4492, 0.5900, 0.5386,  ..., 0.5667, 0.5219, 0.5046],
          [0.6202, 0.3715, 0.3277,  ..., 0.5658, 0.4901, 0.5320]],

         [[0.4022, 0.5516, 0.4366,  ..., 0.3933, 0.5012, 0.5545],
          [0.5319, 0.5772, 0.5155,  ..., 0.3938, 0.4044, 0.5662],
          [0.3621, 0.4579, 0.4712,  ..., 0.4222, 0.4535, 0.5325],
          [0.4254, 0.3826, 0.4265,  ..., 0.6301, 0.5849, 0.5605]],

         ...,

         [[0.3970, 0.5567, 0.5767,  ..., 0.4519, 0.4920, 0.4847],
          [0.4793, 0.6045, 0.3738,  ..., 0.4560, 0.4491, 0.5177],
          [0.5004, 0.4801, 0.5434,  ..., 0.3679, 0.5101, 0.5066],
          [0.4177, 0.3321, 0.5667,  ..., 0.5281, 0.5371, 0.4698]],

         [[0.5053, 0.4791, 0.5672,  ..., 0.4477, 0.4510, 0.3615],
          [0.4297, 0.4275, 0.4023,  ..., 0.5530, 0.3771, 0.4570],
          [0.4946, 0.4909, 0.4870,  ..., 0.4842, 0.5564, 0.4975],
          [0.4817, 0.4516, 0.5605,  ..., 0.4514, 0.5254, 0.3688]],

         [[0.4707, 0.4333, 0.4959,  ..., 0.5591, 0.4884, 0.5359],
          [0.5020, 0.5686, 0.4632,  ..., 0.4151, 0.2890, 0.5591],
          [0.5603, 0.4935, 0.6663,  ..., 0.4871, 0.4917, 0.4861],
          [0.5020, 0.3957, 0.5201,  ..., 0.5720, 0.4494, 0.5073]]],


        [[[0.5151, 0.5102, 0.6003,  ..., 0.3966, 0.3627, 0.3409],
          [0.4751, 0.4885, 0.3817,  ..., 0.5453, 0.5725, 0.3891],
          [0.6325, 0.4855, 0.7220,  ..., 0.3606, 0.4342, 0.4729],
          [0.5508, 0.3748, 0.4371,  ..., 0.5800, 0.5559, 0.6620]],

         [[0.4017, 0.5567, 0.4285,  ..., 0.4456, 0.3354, 0.4692],
          [0.6451, 0.5036, 0.5369,  ..., 0.4786, 0.4002, 0.6900],
          [0.5651, 0.4881, 0.4832,  ..., 0.4668, 0.3502, 0.4424],
          [0.4504, 0.3738, 0.4893,  ..., 0.5782, 0.6262, 0.6109]],

         [[0.5021, 0.4468, 0.3534,  ..., 0.3243, 0.5288, 0.4645],
          [0.4704, 0.4352, 0.4056,  ..., 0.3365, 0.6029, 0.5039],
          [0.4400, 0.6409, 0.5199,  ..., 0.3933, 0.4352, 0.5063],
          [0.4608, 0.3471, 0.3208,  ..., 0.5576, 0.5214, 0.5947]],

         ...,

         [[0.4216, 0.4146, 0.4511,  ..., 0.5149, 0.3645, 0.4018],
          [0.4689, 0.5772, 0.4753,  ..., 0.5639, 0.4049, 0.4686],
          [0.5547, 0.4664, 0.4089,  ..., 0.5286, 0.2540, 0.3435],
          [0.6146, 0.3106, 0.3373,  ..., 0.5405, 0.4973, 0.4511]],

         [[0.4896, 0.4591, 0.3606,  ..., 0.4009, 0.5044, 0.3794],
          [0.4111, 0.5791, 0.5427,  ..., 0.5672, 0.4072, 0.4519],
          [0.3872, 0.5571, 0.5482,  ..., 0.4797, 0.4721, 0.4313],
          [0.5523, 0.5218, 0.3182,  ..., 0.5347, 0.5066, 0.3859]],

         [[0.5665, 0.4037, 0.3798,  ..., 0.4441, 0.5092, 0.4942],
          [0.5095, 0.5918, 0.5639,  ..., 0.5378, 0.3627, 0.5259],
          [0.5542, 0.4946, 0.5571,  ..., 0.5259, 0.5968, 0.4773],
          [0.5229, 0.5035, 0.4965,  ..., 0.6301, 0.4400, 0.4768]]]],
       device='cuda:0')
tensor([[[[0.5189, 0.5641, 0.4345,  ..., 0.4610, 0.3979, 0.4745],
          [0.4630, 0.5903, 0.4463,  ..., 0.6433, 0.4064, 0.5324],
          [0.4504, 0.5946, 0.5438,  ..., 0.5718, 0.5144, 0.5103],
          [0.6188, 0.3794, 0.3337,  ..., 0.5718, 0.4841, 0.5383]],

         [[0.5181, 0.5641, 0.4330,  ..., 0.4613, 0.3970, 0.4749],
          [0.4625, 0.5898, 0.4455,  ..., 0.6442, 0.4050, 0.5329],
          [0.4492, 0.5960, 0.5446,  ..., 0.5727, 0.5159, 0.5106],
          [0.6202, 0.3775, 0.3337,  ..., 0.5718, 0.4841, 0.5380]],

         [[0.4022, 0.5576, 0.4426,  ..., 0.3993, 0.4952, 0.5605],
          [0.5319, 0.5832, 0.5215,  ..., 0.3998, 0.3984, 0.5722],
          [0.3621, 0.4639, 0.4772,  ..., 0.4282, 0.4475, 0.5385],
          [0.4254, 0.3886, 0.4325,  ..., 0.6361, 0.5789, 0.5665]],

         ...,

         [[0.3970, 0.5627, 0.5827,  ..., 0.4579, 0.4860, 0.4907],
          [0.4793, 0.6105, 0.3798,  ..., 0.4620, 0.4431, 0.5237],
          [0.5004, 0.4861, 0.5494,  ..., 0.3739, 0.5041, 0.5126],
          [0.4177, 0.3381, 0.5727,  ..., 0.5341, 0.5311, 0.4758]],

         [[0.5053, 0.4851, 0.5732,  ..., 0.4537, 0.4450, 0.3675],
          [0.4297, 0.4335, 0.4083,  ..., 0.5590, 0.3711, 0.4630],
          [0.4946, 0.4969, 0.4930,  ..., 0.4902, 0.5504, 0.5035],
          [0.4817, 0.4576, 0.5665,  ..., 0.4574, 0.5194, 0.3748]],

         [[0.4707, 0.4393, 0.5019,  ..., 0.5651, 0.4824, 0.5419],
          [0.5020, 0.5746, 0.4692,  ..., 0.4211, 0.2830, 0.5651],
          [0.5603, 0.4995, 0.6723,  ..., 0.4931, 0.4857, 0.4921],
          [0.5020, 0.4017, 0.5261,  ..., 0.5780, 0.4434, 0.5133]]],


        [[[0.5151, 0.5162, 0.6063,  ..., 0.4026, 0.3567, 0.3469],
          [0.4751, 0.4945, 0.3877,  ..., 0.5513, 0.5665, 0.3951],
          [0.6325, 0.4915, 0.7280,  ..., 0.3666, 0.4282, 0.4789],
          [0.5508, 0.3808, 0.4431,  ..., 0.5860, 0.5499, 0.6680]],

         [[0.4017, 0.5627, 0.4345,  ..., 0.4516, 0.3294, 0.4752],
          [0.6451, 0.5096, 0.5429,  ..., 0.4846, 0.3942, 0.6960],
          [0.5651, 0.4941, 0.4892,  ..., 0.4728, 0.3442, 0.4484],
          [0.4504, 0.3798, 0.4953,  ..., 0.5842, 0.6202, 0.6169]],

         [[0.5021, 0.4528, 0.3594,  ..., 0.3303, 0.5228, 0.4705],
          [0.4704, 0.4412, 0.4116,  ..., 0.3425, 0.5969, 0.5099],
          [0.4400, 0.6469, 0.5259,  ..., 0.3993, 0.4292, 0.5123],
          [0.4608, 0.3531, 0.3268,  ..., 0.5636, 0.5154, 0.6007]],

         ...,

         [[0.4216, 0.4206, 0.4571,  ..., 0.5209, 0.3585, 0.4078],
          [0.4689, 0.5832, 0.4813,  ..., 0.5699, 0.3989, 0.4746],
          [0.5547, 0.4724, 0.4149,  ..., 0.5346, 0.2480, 0.3495],
          [0.6146, 0.3166, 0.3433,  ..., 0.5465, 0.4913, 0.4571]],

         [[0.4896, 0.4651, 0.3666,  ..., 0.4069, 0.4984, 0.3854],
          [0.4111, 0.5851, 0.5487,  ..., 0.5732, 0.4012, 0.4579],
          [0.3872, 0.5631, 0.5542,  ..., 0.4857, 0.4661, 0.4373],
          [0.5523, 0.5278, 0.3242,  ..., 0.5407, 0.5006, 0.3919]],

         [[0.5665, 0.4097, 0.3858,  ..., 0.4501, 0.5032, 0.5002],
          [0.5095, 0.5978, 0.5699,  ..., 0.5438, 0.3567, 0.5319],
          [0.5542, 0.5006, 0.5631,  ..., 0.5319, 0.5908, 0.4833],
          [0.5229, 0.5095, 0.5025,  ..., 0.6361, 0.4340, 0.4828]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:112: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
  fig = plt.figure(figsize=self.figsize)
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([ 0.0000, -0.0060, -0.0060,  0.0060, -0.0020,  0.0020, -0.0020, -0.0060,
         0.0060, -0.0060], device='cuda:0')
selected experts tensor([1528, 1712, 1699, 2055, 1626, 1773, 1490, 1629, 1049, 1823],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040]],

         [[0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040]],

         [[0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040]],

         ...,

         [[0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040]],

         [[0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040]],

         [[0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040]]],


        [[[0.4746, 0.5076, 0.6345,  ..., 0.4619, 0.4526, 0.5517],
          [0.4509, 0.4902, 0.3967,  ..., 0.5396, 0.6318, 0.5322],
          [0.3641, 0.4793, 0.5288,  ..., 0.3917, 0.4635, 0.4913],
          [0.5328, 0.5262, 0.5315,  ..., 0.5442, 0.4070, 0.5299]],

         [[0.5007, 0.6003, 0.5795,  ..., 0.4019, 0.6013, 0.5084],
          [0.6263, 0.6152, 0.4413,  ..., 0.4841, 0.6542, 0.5352],
          [0.4374, 0.4245, 0.4872,  ..., 0.5214, 0.5470, 0.5423],
          [0.5947, 0.5390, 0.5880,  ..., 0.5206, 0.7481, 0.5239]],

         [[0.4953, 0.6031, 0.5417,  ..., 0.5986, 0.4667, 0.5490],
          [0.4793, 0.3837, 0.4703,  ..., 0.3788, 0.6300, 0.5962],
          [0.4937, 0.4596, 0.5266,  ..., 0.4024, 0.4701, 0.5990],
          [0.6092, 0.4836, 0.4971,  ..., 0.7731, 0.5083, 0.6154]],

         ...,

         [[0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040]],

         [[0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040]],

         [[0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5020, 0.5040]]]],
       device='cuda:0')
tensor([[[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.4726, 0.5056, 0.6325,  ..., 0.4579, 0.4506, 0.5477],
          [0.4489, 0.4882, 0.3947,  ..., 0.5356, 0.6298, 0.5282],
          [0.3621, 0.4773, 0.5268,  ..., 0.3877, 0.4615, 0.4873],
          [0.5308, 0.5242, 0.5295,  ..., 0.5402, 0.4050, 0.5259]],

         [[0.4987, 0.5983, 0.5775,  ..., 0.3979, 0.5993, 0.5044],
          [0.6243, 0.6132, 0.4393,  ..., 0.4801, 0.6522, 0.5312],
          [0.4354, 0.4225, 0.4852,  ..., 0.5174, 0.5450, 0.5383],
          [0.5927, 0.5370, 0.5860,  ..., 0.5166, 0.7461, 0.5199]],

         [[0.4933, 0.6011, 0.5397,  ..., 0.5946, 0.4647, 0.5450],
          [0.4773, 0.3817, 0.4683,  ..., 0.3748, 0.6280, 0.5922],
          [0.4917, 0.4576, 0.5246,  ..., 0.3984, 0.4681, 0.5950],
          [0.6072, 0.4816, 0.4951,  ..., 0.7691, 0.5063, 0.6114]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0020, 0.0020, 0.0020, 0.0020, 0.0040, 0.0040, 0.0040, 0.0040, 0.0020,
        0.0040], device='cuda:0')
selected experts tensor([ 501,  353,  594,  476, 4211, 3994,  538,  272,  750,  599],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4586, 0.3745, 0.5493,  ..., 0.5245, 0.5194, 0.4675],
          [0.6417, 0.4977, 0.5938,  ..., 0.5098, 0.4976, 0.3810],
          [0.4373, 0.5086, 0.4327,  ..., 0.5787, 0.5897, 0.5573],
          [0.3311, 0.6358, 0.6710,  ..., 0.4901, 0.2972, 0.3672]],

         [[0.3198, 0.5078, 0.6162,  ..., 0.4767, 0.5117, 0.6331],
          [0.3933, 0.3838, 0.6186,  ..., 0.4844, 0.4439, 0.6116],
          [0.5729, 0.4372, 0.6813,  ..., 0.5834, 0.6091, 0.6088],
          [0.4572, 0.4233, 0.5325,  ..., 0.4275, 0.3574, 0.2714]],

         [[0.6210, 0.5611, 0.5895,  ..., 0.4766, 0.4205, 0.4442],
          [0.4150, 0.3212, 0.4483,  ..., 0.6275, 0.5725, 0.5625],
          [0.4793, 0.4750, 0.4584,  ..., 0.5292, 0.5254, 0.3636],
          [0.4530, 0.4447, 0.4798,  ..., 0.5877, 0.4554, 0.3672]],

         ...,

         [[0.4509, 0.5073, 0.5153,  ..., 0.5715, 0.5269, 0.4867],
          [0.5276, 0.5138, 0.5924,  ..., 0.6238, 0.3998, 0.6185],
          [0.5232, 0.4880, 0.5246,  ..., 0.4932, 0.4499, 0.5041],
          [0.4320, 0.3360, 0.6364,  ..., 0.4308, 0.4115, 0.5233]],

         [[0.5686, 0.6295, 0.4919,  ..., 0.7345, 0.4031, 0.3645],
          [0.6662, 0.4929, 0.4640,  ..., 0.5558, 0.5663, 0.5972],
          [0.5262, 0.3681, 0.3925,  ..., 0.4043, 0.5964, 0.4909],
          [0.4472, 0.4886, 0.5366,  ..., 0.4419, 0.3219, 0.5237]],

         [[0.6730, 0.6439, 0.5434,  ..., 0.5365, 0.4651, 0.5408],
          [0.5667, 0.4680, 0.4616,  ..., 0.3964, 0.4477, 0.5859],
          [0.4098, 0.6394, 0.4165,  ..., 0.5270, 0.5912, 0.3884],
          [0.5549, 0.7534, 0.5943,  ..., 0.6320, 0.3655, 0.4454]]],


        [[[0.6058, 0.4157, 0.4967,  ..., 0.5112, 0.5431, 0.6888],
          [0.4977, 0.4720, 0.4432,  ..., 0.5324, 0.3956, 0.6385],
          [0.3849, 0.4100, 0.3714,  ..., 0.4308, 0.3329, 0.6213],
          [0.4504, 0.5032, 0.5106,  ..., 0.5167, 0.5045, 0.4372]],

         [[0.4222, 0.4243, 0.5589,  ..., 0.5479, 0.5629, 0.3778],
          [0.5516, 0.5503, 0.4509,  ..., 0.5162, 0.6946, 0.5138],
          [0.5266, 0.5040, 0.5237,  ..., 0.3848, 0.5653, 0.5849],
          [0.4516, 0.4541, 0.4672,  ..., 0.5391, 0.4962, 0.6349]],

         [[0.5467, 0.5430, 0.6291,  ..., 0.4829, 0.5663, 0.6466],
          [0.5923, 0.5079, 0.4019,  ..., 0.5544, 0.5783, 0.5411],
          [0.4074, 0.3884, 0.6382,  ..., 0.5140, 0.6955, 0.4600],
          [0.4390, 0.4869, 0.3902,  ..., 0.5612, 0.4477, 0.4722]],

         ...,

         [[0.5230, 0.5126, 0.6436,  ..., 0.6330, 0.5615, 0.4844],
          [0.3793, 0.4979, 0.5410,  ..., 0.6429, 0.6091, 0.5774],
          [0.5672, 0.4626, 0.4420,  ..., 0.4597, 0.5930, 0.3801],
          [0.5175, 0.5104, 0.5905,  ..., 0.5166, 0.5138, 0.4252]],

         [[0.5162, 0.4100, 0.4580,  ..., 0.3959, 0.6313, 0.4575],
          [0.4526, 0.4927, 0.3420,  ..., 0.3941, 0.5897, 0.4267],
          [0.3751, 0.4517, 0.3187,  ..., 0.5510, 0.4120, 0.3510],
          [0.5347, 0.4718, 0.4351,  ..., 0.4549, 0.5150, 0.4981]],

         [[0.6730, 0.6439, 0.5434,  ..., 0.5365, 0.4651, 0.5408],
          [0.5667, 0.4680, 0.4616,  ..., 0.3964, 0.4477, 0.5859],
          [0.4098, 0.6394, 0.4165,  ..., 0.5270, 0.5912, 0.3884],
          [0.5549, 0.7534, 0.5943,  ..., 0.6320, 0.3655, 0.4454]]]],
       device='cuda:0')
tensor([[[[0.4656, 0.3775, 0.5463,  ..., 0.5195, 0.5124, 0.4705],
          [0.6487, 0.5007, 0.5908,  ..., 0.5048, 0.4906, 0.3840],
          [0.4443, 0.5116, 0.4297,  ..., 0.5737, 0.5827, 0.5603],
          [0.3381, 0.6388, 0.6680,  ..., 0.4851, 0.2902, 0.3702]],

         [[0.3268, 0.5108, 0.6132,  ..., 0.4717, 0.5047, 0.6361],
          [0.4003, 0.3868, 0.6156,  ..., 0.4794, 0.4369, 0.6146],
          [0.5799, 0.4402, 0.6783,  ..., 0.5784, 0.6021, 0.6118],
          [0.4642, 0.4263, 0.5295,  ..., 0.4225, 0.3504, 0.2744]],

         [[0.6280, 0.5641, 0.5865,  ..., 0.4716, 0.4135, 0.4472],
          [0.4220, 0.3242, 0.4453,  ..., 0.6225, 0.5655, 0.5655],
          [0.4863, 0.4780, 0.4554,  ..., 0.5242, 0.5184, 0.3666],
          [0.4600, 0.4477, 0.4768,  ..., 0.5827, 0.4484, 0.3702]],

         ...,

         [[0.4579, 0.5103, 0.5123,  ..., 0.5665, 0.5199, 0.4897],
          [0.5346, 0.5168, 0.5894,  ..., 0.6188, 0.3928, 0.6215],
          [0.5302, 0.4910, 0.5216,  ..., 0.4882, 0.4429, 0.5071],
          [0.4390, 0.3390, 0.6334,  ..., 0.4258, 0.4045, 0.5263]],

         [[0.5756, 0.6325, 0.4889,  ..., 0.7295, 0.3961, 0.3675],
          [0.6732, 0.4959, 0.4610,  ..., 0.5508, 0.5593, 0.6002],
          [0.5332, 0.3711, 0.3895,  ..., 0.3993, 0.5894, 0.4939],
          [0.4542, 0.4916, 0.5336,  ..., 0.4369, 0.3149, 0.5267]],

         [[0.6800, 0.6469, 0.5404,  ..., 0.5315, 0.4581, 0.5438],
          [0.5737, 0.4710, 0.4586,  ..., 0.3914, 0.4407, 0.5889],
          [0.4168, 0.6424, 0.4135,  ..., 0.5220, 0.5842, 0.3914],
          [0.5619, 0.7564, 0.5913,  ..., 0.6270, 0.3585, 0.4484]]],


        [[[0.6128, 0.4187, 0.4937,  ..., 0.5062, 0.5361, 0.6918],
          [0.5047, 0.4750, 0.4402,  ..., 0.5274, 0.3886, 0.6415],
          [0.3919, 0.4130, 0.3684,  ..., 0.4258, 0.3259, 0.6243],
          [0.4574, 0.5062, 0.5076,  ..., 0.5117, 0.4975, 0.4402]],

         [[0.4292, 0.4273, 0.5559,  ..., 0.5429, 0.5559, 0.3808],
          [0.5586, 0.5533, 0.4479,  ..., 0.5112, 0.6876, 0.5168],
          [0.5336, 0.5070, 0.5207,  ..., 0.3798, 0.5583, 0.5879],
          [0.4586, 0.4571, 0.4642,  ..., 0.5341, 0.4892, 0.6379]],

         [[0.5537, 0.5460, 0.6261,  ..., 0.4779, 0.5593, 0.6496],
          [0.5993, 0.5109, 0.3989,  ..., 0.5494, 0.5713, 0.5441],
          [0.4144, 0.3914, 0.6352,  ..., 0.5090, 0.6885, 0.4630],
          [0.4460, 0.4899, 0.3872,  ..., 0.5562, 0.4407, 0.4752]],

         ...,

         [[0.5300, 0.5156, 0.6406,  ..., 0.6280, 0.5545, 0.4874],
          [0.3863, 0.5009, 0.5380,  ..., 0.6379, 0.6021, 0.5804],
          [0.5742, 0.4656, 0.4390,  ..., 0.4547, 0.5860, 0.3831],
          [0.5245, 0.5134, 0.5875,  ..., 0.5116, 0.5068, 0.4282]],

         [[0.5232, 0.4130, 0.4550,  ..., 0.3909, 0.6243, 0.4605],
          [0.4596, 0.4957, 0.3390,  ..., 0.3891, 0.5827, 0.4297],
          [0.3821, 0.4547, 0.3157,  ..., 0.5460, 0.4050, 0.3540],
          [0.5417, 0.4748, 0.4321,  ..., 0.4499, 0.5080, 0.5011]],

         [[0.6800, 0.6469, 0.5404,  ..., 0.5315, 0.4581, 0.5438],
          [0.5737, 0.4710, 0.4586,  ..., 0.3914, 0.4407, 0.5889],
          [0.4168, 0.6424, 0.4135,  ..., 0.5220, 0.5842, 0.3914],
          [0.5619, 0.7564, 0.5913,  ..., 0.6270, 0.3585, 0.4484]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0070, -0.0030,  0.0030,  0.0050,  0.0050,  0.0050, -0.0050,  0.0050,
         0.0070, -0.0030], device='cuda:0')
selected experts tensor([1576, 1577, 1651, 1629, 1631, 1776, 1691, 1560, 1571, 1722],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4940, 0.4284, 0.5787,  ..., 0.6014, 0.4485, 0.5227],
          [0.5604, 0.4603, 0.5418,  ..., 0.4358, 0.5030, 0.5121],
          [0.5274, 0.5264, 0.4227,  ..., 0.4367, 0.4735, 0.5048],
          [0.3899, 0.3198, 0.4467,  ..., 0.4281, 0.5003, 0.5420]],

         [[0.5167, 0.5242, 0.4631,  ..., 0.5348, 0.4493, 0.5127],
          [0.5078, 0.5059, 0.5493,  ..., 0.4797, 0.4250, 0.4043],
          [0.5124, 0.4651, 0.6033,  ..., 0.4057, 0.5538, 0.4602],
          [0.5238, 0.5221, 0.4561,  ..., 0.5176, 0.4151, 0.4366]],

         [[0.6122, 0.5229, 0.3492,  ..., 0.5435, 0.6490, 0.6047],
          [0.6159, 0.4574, 0.4768,  ..., 0.4767, 0.5857, 0.5389],
          [0.4467, 0.4736, 0.5483,  ..., 0.3094, 0.3651, 0.3807],
          [0.4299, 0.4400, 0.5095,  ..., 0.3996, 0.4389, 0.5345]],

         ...,

         [[0.4280, 0.3900, 0.3680,  ..., 0.4205, 0.3888, 0.6206],
          [0.5887, 0.3551, 0.5571,  ..., 0.5606, 0.3732, 0.5118],
          [0.4850, 0.4265, 0.5033,  ..., 0.6545, 0.4587, 0.4015],
          [0.5491, 0.6497, 0.5510,  ..., 0.5654, 0.5512, 0.4371]],

         [[0.4992, 0.4352, 0.5396,  ..., 0.3690, 0.4611, 0.4513],
          [0.4959, 0.3012, 0.5820,  ..., 0.4343, 0.4423, 0.3973],
          [0.5260, 0.4723, 0.5271,  ..., 0.5707, 0.5957, 0.5655],
          [0.3871, 0.5182, 0.6247,  ..., 0.4850, 0.5057, 0.4709]],

         [[0.6019, 0.6164, 0.4723,  ..., 0.5635, 0.4951, 0.4638],
          [0.4979, 0.4626, 0.4323,  ..., 0.6331, 0.4279, 0.3835],
          [0.4575, 0.7036, 0.4433,  ..., 0.4837, 0.4476, 0.5710],
          [0.5772, 0.5182, 0.3881,  ..., 0.4753, 0.5476, 0.4332]]],


        [[[0.3858, 0.5504, 0.4537,  ..., 0.4814, 0.5381, 0.6182],
          [0.5877, 0.5195, 0.5335,  ..., 0.4636, 0.4757, 0.5328],
          [0.4629, 0.4885, 0.3992,  ..., 0.4771, 0.3760, 0.4927],
          [0.4081, 0.4472, 0.5811,  ..., 0.5464, 0.5017, 0.5849]],

         [[0.4142, 0.4991, 0.6320,  ..., 0.4585, 0.3606, 0.4976],
          [0.4832, 0.4990, 0.5503,  ..., 0.3555, 0.4389, 0.3327],
          [0.5028, 0.5366, 0.3361,  ..., 0.6185, 0.5386, 0.3761],
          [0.4491, 0.5179, 0.6024,  ..., 0.4842, 0.3455, 0.6155]],

         [[0.6103, 0.4409, 0.3743,  ..., 0.4963, 0.5150, 0.5691],
          [0.4784, 0.4611, 0.5428,  ..., 0.7387, 0.6139, 0.5607],
          [0.6366, 0.6016, 0.5238,  ..., 0.5127, 0.4623, 0.6005],
          [0.3707, 0.3919, 0.3405,  ..., 0.5232, 0.5256, 0.4542]],

         ...,

         [[0.4626, 0.4888, 0.4395,  ..., 0.3810, 0.5473, 0.6510],
          [0.3510, 0.3914, 0.5231,  ..., 0.5408, 0.4308, 0.5100],
          [0.5345, 0.5422, 0.5720,  ..., 0.6000, 0.4298, 0.3825],
          [0.4951, 0.5923, 0.4854,  ..., 0.5630, 0.4284, 0.5114]],

         [[0.5454, 0.4804, 0.6033,  ..., 0.6501, 0.4960, 0.5001],
          [0.4670, 0.3923, 0.5643,  ..., 0.4566, 0.3247, 0.5631],
          [0.5503, 0.5876, 0.4773,  ..., 0.4537, 0.5362, 0.4227],
          [0.5333, 0.5134, 0.4981,  ..., 0.6987, 0.7088, 0.4332]],

         [[0.6019, 0.6164, 0.4723,  ..., 0.5635, 0.4951, 0.4638],
          [0.4979, 0.4626, 0.4323,  ..., 0.6331, 0.4279, 0.3835],
          [0.4575, 0.7036, 0.4433,  ..., 0.4837, 0.4476, 0.5710],
          [0.5772, 0.5182, 0.3881,  ..., 0.4753, 0.5476, 0.4332]]]],
       device='cuda:0')
tensor([[[[0.4890, 0.4354, 0.5737,  ..., 0.6044, 0.4455, 0.5177],
          [0.5554, 0.4673, 0.5368,  ..., 0.4388, 0.5000, 0.5071],
          [0.5224, 0.5334, 0.4177,  ..., 0.4397, 0.4705, 0.4998],
          [0.3849, 0.3268, 0.4417,  ..., 0.4311, 0.4973, 0.5370]],

         [[0.5117, 0.5312, 0.4581,  ..., 0.5378, 0.4463, 0.5077],
          [0.5028, 0.5129, 0.5443,  ..., 0.4827, 0.4220, 0.3993],
          [0.5074, 0.4721, 0.5983,  ..., 0.4087, 0.5508, 0.4552],
          [0.5188, 0.5291, 0.4511,  ..., 0.5206, 0.4121, 0.4316]],

         [[0.6072, 0.5299, 0.3442,  ..., 0.5465, 0.6460, 0.5997],
          [0.6109, 0.4644, 0.4718,  ..., 0.4797, 0.5827, 0.5339],
          [0.4417, 0.4806, 0.5433,  ..., 0.3124, 0.3621, 0.3757],
          [0.4249, 0.4470, 0.5045,  ..., 0.4026, 0.4359, 0.5295]],

         ...,

         [[0.4230, 0.3970, 0.3630,  ..., 0.4235, 0.3858, 0.6156],
          [0.5837, 0.3621, 0.5521,  ..., 0.5636, 0.3702, 0.5068],
          [0.4800, 0.4335, 0.4983,  ..., 0.6575, 0.4557, 0.3965],
          [0.5441, 0.6567, 0.5460,  ..., 0.5684, 0.5482, 0.4321]],

         [[0.4942, 0.4422, 0.5346,  ..., 0.3720, 0.4581, 0.4463],
          [0.4909, 0.3082, 0.5770,  ..., 0.4373, 0.4393, 0.3923],
          [0.5210, 0.4793, 0.5221,  ..., 0.5737, 0.5927, 0.5605],
          [0.3821, 0.5252, 0.6197,  ..., 0.4880, 0.5027, 0.4659]],

         [[0.5969, 0.6234, 0.4673,  ..., 0.5665, 0.4921, 0.4588],
          [0.4929, 0.4696, 0.4273,  ..., 0.6361, 0.4249, 0.3785],
          [0.4525, 0.7106, 0.4383,  ..., 0.4867, 0.4446, 0.5660],
          [0.5722, 0.5252, 0.3831,  ..., 0.4783, 0.5446, 0.4282]]],


        [[[0.3808, 0.5574, 0.4487,  ..., 0.4844, 0.5351, 0.6132],
          [0.5827, 0.5265, 0.5285,  ..., 0.4666, 0.4727, 0.5278],
          [0.4579, 0.4955, 0.3942,  ..., 0.4801, 0.3730, 0.4877],
          [0.4031, 0.4542, 0.5761,  ..., 0.5494, 0.4987, 0.5799]],

         [[0.4092, 0.5061, 0.6270,  ..., 0.4615, 0.3576, 0.4926],
          [0.4782, 0.5060, 0.5453,  ..., 0.3585, 0.4359, 0.3277],
          [0.4978, 0.5436, 0.3311,  ..., 0.6215, 0.5356, 0.3711],
          [0.4441, 0.5249, 0.5974,  ..., 0.4872, 0.3425, 0.6105]],

         [[0.6053, 0.4479, 0.3693,  ..., 0.4993, 0.5120, 0.5641],
          [0.4734, 0.4681, 0.5378,  ..., 0.7417, 0.6109, 0.5557],
          [0.6316, 0.6086, 0.5188,  ..., 0.5157, 0.4593, 0.5955],
          [0.3657, 0.3989, 0.3355,  ..., 0.5262, 0.5226, 0.4492]],

         ...,

         [[0.4576, 0.4958, 0.4345,  ..., 0.3840, 0.5443, 0.6460],
          [0.3460, 0.3984, 0.5181,  ..., 0.5438, 0.4278, 0.5050],
          [0.5295, 0.5492, 0.5670,  ..., 0.6030, 0.4268, 0.3775],
          [0.4901, 0.5993, 0.4804,  ..., 0.5660, 0.4254, 0.5064]],

         [[0.5404, 0.4874, 0.5983,  ..., 0.6531, 0.4930, 0.4951],
          [0.4620, 0.3993, 0.5593,  ..., 0.4596, 0.3217, 0.5581],
          [0.5453, 0.5946, 0.4723,  ..., 0.4567, 0.5332, 0.4177],
          [0.5283, 0.5204, 0.4931,  ..., 0.7017, 0.7058, 0.4282]],

         [[0.5969, 0.6234, 0.4673,  ..., 0.5665, 0.4921, 0.4588],
          [0.4929, 0.4696, 0.4273,  ..., 0.6361, 0.4249, 0.3785],
          [0.4525, 0.7106, 0.4383,  ..., 0.4867, 0.4446, 0.5660],
          [0.5722, 0.5252, 0.3831,  ..., 0.4783, 0.5446, 0.4282]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0050, -0.0070,  0.0050, -0.0010,  0.0010,  0.0030, -0.0050, -0.0030,
         0.0030,  0.0050], device='cuda:0')
selected experts tensor([1725, 1657, 1737, 1721, 1483, 1611, 1728, 1615, 1554, 1553],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4647, 0.4921, 0.5390,  ..., 0.3929, 0.5094, 0.4765],
          [0.4895, 0.5231, 0.4788,  ..., 0.4711, 0.5075, 0.5705],
          [0.6577, 0.2848, 0.5009,  ..., 0.4950, 0.4690, 0.5393],
          [0.4422, 0.5609, 0.4888,  ..., 0.4042, 0.5084, 0.5629]],

         [[0.5794, 0.5380, 0.5078,  ..., 0.2868, 0.3700, 0.3994],
          [0.3952, 0.5090, 0.4540,  ..., 0.5550, 0.4496, 0.4896],
          [0.6699, 0.4740, 0.4882,  ..., 0.4304, 0.6133, 0.4989],
          [0.5641, 0.5686, 0.4658,  ..., 0.4442, 0.4721, 0.5913]],

         [[0.3758, 0.6773, 0.4378,  ..., 0.5796, 0.3993, 0.4074],
          [0.4873, 0.6210, 0.6688,  ..., 0.5492, 0.4224, 0.5724],
          [0.6524, 0.5274, 0.4577,  ..., 0.6302, 0.5333, 0.5125],
          [0.5904, 0.4907, 0.4371,  ..., 0.6050, 0.5280, 0.4706]],

         ...,

         [[0.3667, 0.3605, 0.5082,  ..., 0.4052, 0.4073, 0.5395],
          [0.4379, 0.5259, 0.5104,  ..., 0.6064, 0.4262, 0.2970],
          [0.6170, 0.6705, 0.6136,  ..., 0.5096, 0.5706, 0.4227],
          [0.3514, 0.5034, 0.5710,  ..., 0.5016, 0.4975, 0.6086]],

         [[0.4640, 0.4876, 0.4484,  ..., 0.5572, 0.5152, 0.6939],
          [0.6638, 0.4046, 0.5351,  ..., 0.5294, 0.3373, 0.6627],
          [0.6017, 0.4885, 0.4426,  ..., 0.4597, 0.4238, 0.4749],
          [0.4788, 0.3267, 0.5405,  ..., 0.5200, 0.5516, 0.5489]],

         [[0.4458, 0.4526, 0.3578,  ..., 0.6437, 0.4781, 0.3578],
          [0.5538, 0.6255, 0.7100,  ..., 0.5417, 0.4059, 0.5209],
          [0.5946, 0.4910, 0.5419,  ..., 0.5644, 0.5203, 0.5472],
          [0.6063, 0.4402, 0.4208,  ..., 0.4309, 0.5101, 0.6039]]],


        [[[0.6603, 0.4122, 0.4155,  ..., 0.4512, 0.3673, 0.4203],
          [0.3712, 0.4842, 0.4446,  ..., 0.4671, 0.4520, 0.3825],
          [0.5040, 0.5828, 0.5918,  ..., 0.4328, 0.4520, 0.5356],
          [0.5069, 0.4497, 0.4513,  ..., 0.4804, 0.4300, 0.4972]],

         [[0.4871, 0.5103, 0.4407,  ..., 0.5311, 0.5027, 0.5081],
          [0.5451, 0.3660, 0.4941,  ..., 0.6621, 0.4228, 0.6372],
          [0.5932, 0.4241, 0.5800,  ..., 0.4444, 0.5366, 0.5091],
          [0.5115, 0.5494, 0.3947,  ..., 0.4606, 0.5172, 0.5322]],

         [[0.4288, 0.5414, 0.4441,  ..., 0.4405, 0.3548, 0.4308],
          [0.5230, 0.5029, 0.4860,  ..., 0.3625, 0.5554, 0.4737],
          [0.3550, 0.5419, 0.5652,  ..., 0.4400, 0.3745, 0.3952],
          [0.4131, 0.3328, 0.3479,  ..., 0.5938, 0.4031, 0.5237]],

         ...,

         [[0.4288, 0.4426, 0.4325,  ..., 0.2966, 0.3984, 0.4480],
          [0.4164, 0.4829, 0.5993,  ..., 0.4490, 0.4787, 0.6390],
          [0.4832, 0.5899, 0.4824,  ..., 0.5301, 0.4148, 0.4217],
          [0.4803, 0.4925, 0.5197,  ..., 0.4999, 0.3628, 0.7271]],

         [[0.5136, 0.5376, 0.4275,  ..., 0.5099, 0.4607, 0.3975],
          [0.6308, 0.5913, 0.6264,  ..., 0.5758, 0.4200, 0.5767],
          [0.4297, 0.4453, 0.4651,  ..., 0.6481, 0.5232, 0.5308],
          [0.4374, 0.6044, 0.6914,  ..., 0.5806, 0.4808, 0.4863]],

         [[0.4458, 0.4526, 0.3578,  ..., 0.6437, 0.4781, 0.3578],
          [0.5538, 0.6255, 0.7100,  ..., 0.5417, 0.4059, 0.5209],
          [0.5946, 0.4910, 0.5419,  ..., 0.5644, 0.5203, 0.5472],
          [0.6063, 0.4402, 0.4208,  ..., 0.4309, 0.5101, 0.6039]]]],
       device='cuda:0')
tensor([[[[0.4637, 0.4991, 0.5460,  ..., 0.3979, 0.5024, 0.4835],
          [0.4885, 0.5301, 0.4858,  ..., 0.4761, 0.5005, 0.5775],
          [0.6567, 0.2918, 0.5079,  ..., 0.5000, 0.4620, 0.5463],
          [0.4412, 0.5679, 0.4958,  ..., 0.4092, 0.5014, 0.5699]],

         [[0.5784, 0.5450, 0.5148,  ..., 0.2918, 0.3630, 0.4064],
          [0.3942, 0.5160, 0.4610,  ..., 0.5600, 0.4426, 0.4966],
          [0.6689, 0.4810, 0.4952,  ..., 0.4354, 0.6063, 0.5059],
          [0.5631, 0.5756, 0.4728,  ..., 0.4492, 0.4651, 0.5983]],

         [[0.3748, 0.6843, 0.4448,  ..., 0.5846, 0.3923, 0.4144],
          [0.4863, 0.6280, 0.6758,  ..., 0.5542, 0.4154, 0.5794],
          [0.6514, 0.5344, 0.4647,  ..., 0.6352, 0.5263, 0.5195],
          [0.5894, 0.4977, 0.4441,  ..., 0.6100, 0.5210, 0.4776]],

         ...,

         [[0.3657, 0.3675, 0.5152,  ..., 0.4102, 0.4003, 0.5465],
          [0.4369, 0.5329, 0.5174,  ..., 0.6114, 0.4192, 0.3040],
          [0.6160, 0.6775, 0.6206,  ..., 0.5146, 0.5636, 0.4297],
          [0.3504, 0.5104, 0.5780,  ..., 0.5066, 0.4905, 0.6156]],

         [[0.4630, 0.4946, 0.4554,  ..., 0.5622, 0.5082, 0.7009],
          [0.6628, 0.4116, 0.5421,  ..., 0.5344, 0.3303, 0.6697],
          [0.6007, 0.4955, 0.4496,  ..., 0.4647, 0.4168, 0.4819],
          [0.4778, 0.3337, 0.5475,  ..., 0.5250, 0.5446, 0.5559]],

         [[0.4448, 0.4596, 0.3648,  ..., 0.6487, 0.4711, 0.3648],
          [0.5528, 0.6325, 0.7170,  ..., 0.5467, 0.3989, 0.5279],
          [0.5936, 0.4980, 0.5489,  ..., 0.5694, 0.5133, 0.5542],
          [0.6053, 0.4472, 0.4278,  ..., 0.4359, 0.5031, 0.6109]]],


        [[[0.6593, 0.4192, 0.4225,  ..., 0.4562, 0.3603, 0.4273],
          [0.3702, 0.4912, 0.4516,  ..., 0.4721, 0.4450, 0.3895],
          [0.5030, 0.5898, 0.5988,  ..., 0.4378, 0.4450, 0.5426],
          [0.5059, 0.4567, 0.4583,  ..., 0.4854, 0.4230, 0.5042]],

         [[0.4861, 0.5173, 0.4477,  ..., 0.5361, 0.4957, 0.5151],
          [0.5441, 0.3730, 0.5011,  ..., 0.6671, 0.4158, 0.6442],
          [0.5922, 0.4311, 0.5870,  ..., 0.4494, 0.5296, 0.5161],
          [0.5105, 0.5564, 0.4017,  ..., 0.4656, 0.5102, 0.5392]],

         [[0.4278, 0.5484, 0.4511,  ..., 0.4455, 0.3478, 0.4378],
          [0.5220, 0.5099, 0.4930,  ..., 0.3675, 0.5484, 0.4807],
          [0.3540, 0.5489, 0.5722,  ..., 0.4450, 0.3675, 0.4022],
          [0.4121, 0.3398, 0.3549,  ..., 0.5988, 0.3961, 0.5307]],

         ...,

         [[0.4278, 0.4496, 0.4395,  ..., 0.3016, 0.3914, 0.4550],
          [0.4154, 0.4899, 0.6063,  ..., 0.4540, 0.4717, 0.6460],
          [0.4822, 0.5969, 0.4894,  ..., 0.5351, 0.4078, 0.4287],
          [0.4793, 0.4995, 0.5267,  ..., 0.5049, 0.3558, 0.7341]],

         [[0.5126, 0.5446, 0.4345,  ..., 0.5149, 0.4537, 0.4045],
          [0.6298, 0.5983, 0.6334,  ..., 0.5808, 0.4130, 0.5837],
          [0.4287, 0.4523, 0.4721,  ..., 0.6531, 0.5162, 0.5378],
          [0.4364, 0.6114, 0.6984,  ..., 0.5856, 0.4738, 0.4933]],

         [[0.4448, 0.4596, 0.3648,  ..., 0.6487, 0.4711, 0.3648],
          [0.5528, 0.6325, 0.7170,  ..., 0.5467, 0.3989, 0.5279],
          [0.5936, 0.4980, 0.5489,  ..., 0.5694, 0.5133, 0.5542],
          [0.6053, 0.4472, 0.4278,  ..., 0.4359, 0.5031, 0.6109]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0010, -0.0070, -0.0070,  0.0050, -0.0010,  0.0010, -0.0010, -0.0050,
         0.0070, -0.0070], device='cuda:0')
selected experts tensor([1858, 1596, 1569, 1790, 1901, 1669, 1632, 1711,  781, 1877],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.4647, 0.6125, 0.4936,  ..., 0.4428, 0.3455, 0.5558],
          [0.5089, 0.5123, 0.4117,  ..., 0.4971, 0.6204, 0.5137],
          [0.4647, 0.3732, 0.5167,  ..., 0.4469, 0.4633, 0.4294],
          [0.6328, 0.4701, 0.4165,  ..., 0.4968, 0.5132, 0.6150]],

         [[0.5700, 0.5168, 0.6400,  ..., 0.5218, 0.4681, 0.5568],
          [0.3902, 0.4526, 0.5129,  ..., 0.4573, 0.6614, 0.5665],
          [0.4274, 0.4193, 0.3810,  ..., 0.4894, 0.5524, 0.6141],
          [0.4611, 0.4370, 0.3481,  ..., 0.5532, 0.5128, 0.5396]],

         [[0.5396, 0.5493, 0.6701,  ..., 0.5686, 0.6111, 0.6302],
          [0.5838, 0.5347, 0.3204,  ..., 0.4998, 0.7294, 0.5362],
          [0.4415, 0.4604, 0.3508,  ..., 0.4419, 0.6463, 0.5806],
          [0.4437, 0.5519, 0.4447,  ..., 0.5677, 0.4907, 0.5350]],

         ...,

         [[0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5030, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5030, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5030, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5030, 0.5050]],

         [[0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5030, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5030, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5030, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5030, 0.5050]],

         [[0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5030, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5030, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5030, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5030, 0.5050]]],


        [[[0.6135, 0.5354, 0.6195,  ..., 0.4772, 0.6889, 0.3927],
          [0.4981, 0.4070, 0.4677,  ..., 0.4237, 0.4609, 0.4996],
          [0.4770, 0.4886, 0.5519,  ..., 0.4740, 0.5767, 0.7075],
          [0.5551, 0.6186, 0.3606,  ..., 0.4467, 0.5502, 0.5925]],

         [[0.4882, 0.5881, 0.5420,  ..., 0.4667, 0.3162, 0.5462],
          [0.5909, 0.4959, 0.6588,  ..., 0.4867, 0.6418, 0.6080],
          [0.3669, 0.3741, 0.4946,  ..., 0.5777, 0.5657, 0.5423],
          [0.5085, 0.4136, 0.4231,  ..., 0.5820, 0.5524, 0.4467]],

         [[0.4729, 0.4899, 0.7120,  ..., 0.4847, 0.6597, 0.4500],
          [0.4160, 0.5738, 0.4028,  ..., 0.4817, 0.5369, 0.5583],
          [0.4042, 0.4524, 0.5320,  ..., 0.3599, 0.6088, 0.4760],
          [0.5709, 0.7047, 0.4440,  ..., 0.3798, 0.4643, 0.6173]],

         ...,

         [[0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5030, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5030, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5030, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5030, 0.5050]],

         [[0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5030, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5030, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5030, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5030, 0.5050]],

         [[0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5030, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5030, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5030, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5030, 0.5050]]]],
       device='cuda:0')/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([[[[0.4617, 0.6095, 0.4906,  ..., 0.4378, 0.3425, 0.5508],
          [0.5059, 0.5093, 0.4087,  ..., 0.4921, 0.6174, 0.5087],
          [0.4617, 0.3702, 0.5137,  ..., 0.4419, 0.4603, 0.4244],
          [0.6298, 0.4671, 0.4135,  ..., 0.4918, 0.5102, 0.6100]],

         [[0.5670, 0.5138, 0.6370,  ..., 0.5168, 0.4651, 0.5518],
          [0.3872, 0.4496, 0.5099,  ..., 0.4523, 0.6584, 0.5615],
          [0.4244, 0.4163, 0.3780,  ..., 0.4844, 0.5494, 0.6091],
          [0.4581, 0.4340, 0.3451,  ..., 0.5482, 0.5098, 0.5346]],

         [[0.5366, 0.5463, 0.6671,  ..., 0.5636, 0.6081, 0.6252],
          [0.5808, 0.5317, 0.3174,  ..., 0.4948, 0.7264, 0.5312],
          [0.4385, 0.4574, 0.3478,  ..., 0.4369, 0.6433, 0.5756],
          [0.4407, 0.5489, 0.4417,  ..., 0.5627, 0.4877, 0.5300]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.6105, 0.5324, 0.6165,  ..., 0.4722, 0.6859, 0.3877],
          [0.4951, 0.4040, 0.4647,  ..., 0.4187, 0.4579, 0.4946],
          [0.4740, 0.4856, 0.5489,  ..., 0.4690, 0.5737, 0.7025],
          [0.5521, 0.6156, 0.3576,  ..., 0.4417, 0.5472, 0.5875]],

         [[0.4852, 0.5851, 0.5390,  ..., 0.4617, 0.3132, 0.5412],
          [0.5879, 0.4929, 0.6558,  ..., 0.4817, 0.6388, 0.6030],
          [0.3639, 0.3711, 0.4916,  ..., 0.5727, 0.5627, 0.5373],
          [0.5055, 0.4106, 0.4201,  ..., 0.5770, 0.5494, 0.4417]],

         [[0.4699, 0.4869, 0.7090,  ..., 0.4797, 0.6567, 0.4450],
          [0.4130, 0.5708, 0.3998,  ..., 0.4767, 0.5339, 0.5533],
          [0.4012, 0.4494, 0.5290,  ..., 0.3549, 0.6058, 0.4710],
          [0.5679, 0.7017, 0.4410,  ..., 0.3748, 0.4613, 0.6123]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0050, 0.0050, 0.0030,
        0.0050], device='cuda:0')
selected experts tensor([ 615,  673, 1222,  977, 1122,  871, 2498, 1835, 1424, 1051],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5591, 0.4566, 0.6717,  ..., 0.5063, 0.5936, 0.5625],
          [0.6283, 0.5297, 0.5262,  ..., 0.4975, 0.5983, 0.4616],
          [0.3453, 0.5616, 0.4302,  ..., 0.4309, 0.5586, 0.4834],
          [0.6364, 0.4390, 0.6595,  ..., 0.4780, 0.5639, 0.3385]],

         [[0.3489, 0.2834, 0.5186,  ..., 0.3937, 0.5125, 0.3509],
          [0.5653, 0.5030, 0.4117,  ..., 0.3371, 0.5731, 0.4944],
          [0.4275, 0.4291, 0.4911,  ..., 0.6484, 0.2619, 0.4775],
          [0.4032, 0.5831, 0.5096,  ..., 0.6903, 0.3719, 0.5687]],

         [[0.5567, 0.6005, 0.5371,  ..., 0.6801, 0.5552, 0.5232],
          [0.5838, 0.3646, 0.4495,  ..., 0.6340, 0.4756, 0.4314],
          [0.6256, 0.4724, 0.5230,  ..., 0.6229, 0.5144, 0.5030],
          [0.4165, 0.5168, 0.3750,  ..., 0.6430, 0.4957, 0.6230]],

         ...,

         [[0.6471, 0.5549, 0.5998,  ..., 0.5375, 0.4895, 0.4789],
          [0.5490, 0.5235, 0.5800,  ..., 0.5082, 0.5740, 0.4947],
          [0.4776, 0.4305, 0.5776,  ..., 0.6155, 0.5974, 0.5191],
          [0.5933, 0.5224, 0.3498,  ..., 0.6169, 0.6655, 0.5147]],

         [[0.6740, 0.6449, 0.5424,  ..., 0.5371, 0.4661, 0.5401],
          [0.5682, 0.4689, 0.4606,  ..., 0.3974, 0.4487, 0.5849],
          [0.4108, 0.6404, 0.4155,  ..., 0.5281, 0.5922, 0.3869],
          [0.5562, 0.7544, 0.5933,  ..., 0.6330, 0.3665, 0.4444]],

         [[0.4407, 0.4215, 0.6272,  ..., 0.6358, 0.5656, 0.4919],
          [0.4681, 0.4503, 0.4442,  ..., 0.4535, 0.4634, 0.5924],
          [0.4714, 0.3682, 0.4743,  ..., 0.5653, 0.6812, 0.3635],
          [0.5269, 0.4363, 0.5407,  ..., 0.3923, 0.4528, 0.4755]]],


        [[[0.5810, 0.4433, 0.4970,  ..., 0.3511, 0.5334, 0.4357],
          [0.5853, 0.6940, 0.5562,  ..., 0.3886, 0.3137, 0.4854],
          [0.4837, 0.5418, 0.4708,  ..., 0.3573, 0.3719, 0.5507],
          [0.4156, 0.5348, 0.4794,  ..., 0.5211, 0.5533, 0.3772]],

         [[0.5110, 0.5111, 0.3427,  ..., 0.3708, 0.5817, 0.5591],
          [0.4132, 0.4807, 0.5051,  ..., 0.5085, 0.3531, 0.6276],
          [0.3896, 0.4873, 0.5304,  ..., 0.5641, 0.5436, 0.3977],
          [0.4194, 0.3931, 0.5918,  ..., 0.5549, 0.3566, 0.5279]],

         [[0.4958, 0.3806, 0.5866,  ..., 0.4309, 0.5465, 0.5301],
          [0.4837, 0.5005, 0.4207,  ..., 0.5326, 0.5181, 0.4738],
          [0.4403, 0.3637, 0.4674,  ..., 0.3690, 0.4022, 0.3888],
          [0.4080, 0.5954, 0.4732,  ..., 0.5920, 0.7210, 0.4142]],

         ...,

         [[0.6774, 0.3265, 0.3740,  ..., 0.5595, 0.4201, 0.4290],
          [0.6445, 0.5260, 0.4628,  ..., 0.4890, 0.4008, 0.5706],
          [0.4906, 0.4954, 0.4164,  ..., 0.5583, 0.4310, 0.5057],
          [0.7126, 0.4578, 0.5216,  ..., 0.4129, 0.3971, 0.3237]],

         [[0.6550, 0.4176, 0.4606,  ..., 0.5139, 0.5622, 0.5389],
          [0.4494, 0.4707, 0.4850,  ..., 0.4614, 0.4396, 0.5131],
          [0.4680, 0.5522, 0.5757,  ..., 0.3538, 0.5482, 0.4161],
          [0.5490, 0.4725, 0.4302,  ..., 0.4954, 0.4783, 0.5783]],

         [[0.4848, 0.6413, 0.3892,  ..., 0.4433, 0.4818, 0.4029],
          [0.4945, 0.3778, 0.6743,  ..., 0.5830, 0.6087, 0.4502],
          [0.4915, 0.4936, 0.5838,  ..., 0.5958, 0.4521, 0.4916],
          [0.5724, 0.5760, 0.5890,  ..., 0.6109, 0.4930, 0.3804]]]],
       device='cuda:0')
tensor([[[[0.5651, 0.4586, 0.6697,  ..., 0.5003, 0.5856, 0.5665],
          [0.6343, 0.5317, 0.5242,  ..., 0.4915, 0.5903, 0.4656],
          [0.3513, 0.5636, 0.4282,  ..., 0.4249, 0.5506, 0.4874],
          [0.6424, 0.4410, 0.6575,  ..., 0.4720, 0.5559, 0.3425]],

         [[0.3549, 0.2854, 0.5166,  ..., 0.3877, 0.5045, 0.3549],
          [0.5713, 0.5050, 0.4097,  ..., 0.3311, 0.5651, 0.4984],
          [0.4335, 0.4311, 0.4891,  ..., 0.6424, 0.2539, 0.4815],
          [0.4092, 0.5851, 0.5076,  ..., 0.6843, 0.3639, 0.5727]],

         [[0.5627, 0.6025, 0.5351,  ..., 0.6741, 0.5472, 0.5272],
          [0.5898, 0.3666, 0.4475,  ..., 0.6280, 0.4676, 0.4354],
          [0.6316, 0.4744, 0.5210,  ..., 0.6169, 0.5064, 0.5070],
          [0.4225, 0.5188, 0.3730,  ..., 0.6370, 0.4877, 0.6270]],

         ...,

         [[0.6531, 0.5569, 0.5978,  ..., 0.5315, 0.4815, 0.4829],
          [0.5550, 0.5255, 0.5780,  ..., 0.5022, 0.5660, 0.4987],
          [0.4836, 0.4325, 0.5756,  ..., 0.6095, 0.5894, 0.5231],
          [0.5993, 0.5244, 0.3478,  ..., 0.6109, 0.6575, 0.5187]],

         [[0.6800, 0.6469, 0.5404,  ..., 0.5311, 0.4581, 0.5441],
          [0.5742, 0.4709, 0.4586,  ..., 0.3914, 0.4407, 0.5889],
          [0.4168, 0.6424, 0.4135,  ..., 0.5221, 0.5842, 0.3909],
          [0.5622, 0.7564, 0.5913,  ..., 0.6270, 0.3585, 0.4484]],

         [[0.4467, 0.4235, 0.6252,  ..., 0.6298, 0.5576, 0.4959],
          [0.4741, 0.4523, 0.4422,  ..., 0.4475, 0.4554, 0.5964],
          [0.4774, 0.3702, 0.4723,  ..., 0.5593, 0.6732, 0.3675],
          [0.5329, 0.4383, 0.5387,  ..., 0.3863, 0.4448, 0.4795]]],


        [[[0.5870, 0.4453, 0.4950,  ..., 0.3451, 0.5254, 0.4397],
          [0.5913, 0.6960, 0.5542,  ..., 0.3826, 0.3057, 0.4894],
          [0.4897, 0.5438, 0.4688,  ..., 0.3513, 0.3639, 0.5547],
          [0.4216, 0.5368, 0.4774,  ..., 0.5151, 0.5453, 0.3812]],

         [[0.5170, 0.5131, 0.3407,  ..., 0.3648, 0.5737, 0.5631],
          [0.4192, 0.4827, 0.5031,  ..., 0.5025, 0.3451, 0.6316],
          [0.3956, 0.4893, 0.5284,  ..., 0.5581, 0.5356, 0.4017],
          [0.4254, 0.3951, 0.5898,  ..., 0.5489, 0.3486, 0.5319]],

         [[0.5018, 0.3826, 0.5846,  ..., 0.4249, 0.5385, 0.5341],
          [0.4897, 0.5025, 0.4187,  ..., 0.5266, 0.5101, 0.4778],
          [0.4463, 0.3657, 0.4654,  ..., 0.3630, 0.3942, 0.3928],
          [0.4140, 0.5974, 0.4712,  ..., 0.5860, 0.7130, 0.4182]],

         ...,

         [[0.6834, 0.3285, 0.3720,  ..., 0.5535, 0.4121, 0.4330],
          [0.6505, 0.5280, 0.4608,  ..., 0.4830, 0.3928, 0.5746],
          [0.4966, 0.4974, 0.4144,  ..., 0.5523, 0.4230, 0.5097],
          [0.7186, 0.4598, 0.5196,  ..., 0.4069, 0.3891, 0.3277]],

         [[0.6610, 0.4196, 0.4586,  ..., 0.5079, 0.5542, 0.5429],
          [0.4554, 0.4727, 0.4830,  ..., 0.4554, 0.4316, 0.5171],
          [0.4740, 0.5542, 0.5737,  ..., 0.3478, 0.5402, 0.4201],
          [0.5550, 0.4745, 0.4282,  ..., 0.4894, 0.4703, 0.5823]],

         [[0.4908, 0.6433, 0.3872,  ..., 0.4373, 0.4738, 0.4069],
          [0.5005, 0.3798, 0.6723,  ..., 0.5770, 0.6007, 0.4542],
          [0.4975, 0.4956, 0.5818,  ..., 0.5898, 0.4441, 0.4956],
          [0.5784, 0.5780, 0.5870,  ..., 0.6049, 0.4850, 0.3844]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0060, -0.0020,  0.0020,  0.0060,  0.0060,  0.0040, -0.0060,  0.0060,
         0.0080, -0.0040], device='cuda:0')
selected experts tensor([1758, 1574, 1583, 1771, 1704, 1581, 1671, 1526, 1645, 1571],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5483, 0.4491, 0.3509,  ..., 0.5452, 0.3291, 0.5701],
          [0.5161, 0.3223, 0.3206,  ..., 0.4781, 0.5597, 0.5392],
          [0.5739, 0.4479, 0.3625,  ..., 0.3973, 0.7049, 0.4651],
          [0.4503, 0.6389, 0.4505,  ..., 0.4440, 0.6365, 0.5143]],

         [[0.4507, 0.4886, 0.3625,  ..., 0.4011, 0.7049, 0.5433],
          [0.6589, 0.6712, 0.4936,  ..., 0.5355, 0.5777, 0.5830],
          [0.6545, 0.3904, 0.5271,  ..., 0.5726, 0.3770, 0.5316],
          [0.4872, 0.5547, 0.5053,  ..., 0.5178, 0.4052, 0.5214]],

         [[0.4430, 0.4198, 0.4730,  ..., 0.5926, 0.5971, 0.4810],
          [0.5186, 0.5908, 0.4394,  ..., 0.3945, 0.6347, 0.5354],
          [0.6209, 0.4342, 0.5268,  ..., 0.5850, 0.5829, 0.5127],
          [0.4850, 0.5077, 0.4994,  ..., 0.4605, 0.5261, 0.5272]],

         ...,

         [[0.4503, 0.4337, 0.3652,  ..., 0.4816, 0.5503, 0.4443],
          [0.4462, 0.5177, 0.5297,  ..., 0.4600, 0.5214, 0.5294],
          [0.4457, 0.5556, 0.4094,  ..., 0.4181, 0.5252, 0.4035],
          [0.4066, 0.4390, 0.5476,  ..., 0.4101, 0.3164, 0.6076]],

         [[0.6009, 0.6154, 0.4711,  ..., 0.5645, 0.4961, 0.4651],
          [0.4970, 0.4616, 0.4318,  ..., 0.6341, 0.4289, 0.3845],
          [0.4565, 0.7026, 0.4423,  ..., 0.4847, 0.4486, 0.5725],
          [0.5762, 0.5175, 0.3871,  ..., 0.4763, 0.5486, 0.4342]],

         [[0.4736, 0.5271, 0.3473,  ..., 0.4738, 0.5839, 0.4195],
          [0.4478, 0.4470, 0.4175,  ..., 0.4532, 0.5650, 0.3345],
          [0.6849, 0.4749, 0.4643,  ..., 0.4224, 0.6329, 0.5086],
          [0.3982, 0.5652, 0.3189,  ..., 0.4901, 0.5150, 0.4470]]],


        [[[0.4452, 0.5320, 0.6042,  ..., 0.5476, 0.5288, 0.5612],
          [0.3917, 0.5185, 0.5719,  ..., 0.5525, 0.4151, 0.6783],
          [0.4260, 0.5969, 0.5590,  ..., 0.5935, 0.4365, 0.5544],
          [0.5541, 0.4231, 0.4356,  ..., 0.5032, 0.4892, 0.3955]],

         [[0.5820, 0.4621, 0.4969,  ..., 0.4874, 0.4830, 0.4801],
          [0.6491, 0.5880, 0.5068,  ..., 0.6848, 0.5393, 0.4414],
          [0.4423, 0.3984, 0.5602,  ..., 0.2996, 0.3884, 0.5954],
          [0.4777, 0.5757, 0.6410,  ..., 0.5214, 0.4967, 0.4797]],

         [[0.5967, 0.4330, 0.4981,  ..., 0.4717, 0.5548, 0.4510],
          [0.4503, 0.6076, 0.4433,  ..., 0.4798, 0.5398, 0.5438],
          [0.6149, 0.5424, 0.4872,  ..., 0.4515, 0.5291, 0.3699],
          [0.4546, 0.6344, 0.4575,  ..., 0.6057, 0.5676, 0.4588]],

         ...,

         [[0.4469, 0.5681, 0.5275,  ..., 0.4110, 0.6966, 0.4347],
          [0.4650, 0.4093, 0.3742,  ..., 0.2504, 0.5568, 0.4617],
          [0.4699, 0.4976, 0.4883,  ..., 0.4153, 0.4156, 0.5110],
          [0.4807, 0.4402, 0.3945,  ..., 0.6182, 0.5083, 0.4385]],

         [[0.6500, 0.5609, 0.5325,  ..., 0.4906, 0.4857, 0.4537],
          [0.6103, 0.5460, 0.4327,  ..., 0.5611, 0.4076, 0.5617],
          [0.4341, 0.4827, 0.4466,  ..., 0.5741, 0.5045, 0.3717],
          [0.5839, 0.5324, 0.4809,  ..., 0.5069, 0.5183, 0.3923]],

         [[0.4361, 0.4652, 0.5291,  ..., 0.5152, 0.4356, 0.6376],
          [0.4450, 0.4317, 0.5253,  ..., 0.4863, 0.4409, 0.5433],
          [0.6079, 0.6025, 0.4308,  ..., 0.5750, 0.4862, 0.5377],
          [0.4389, 0.4738, 0.5107,  ..., 0.3484, 0.4839, 0.4309]]]],
       device='cuda:0')
tensor([[[[0.5443, 0.4571, 0.3469,  ..., 0.5472, 0.3251, 0.5641],
          [0.5121, 0.3303, 0.3166,  ..., 0.4801, 0.5557, 0.5332],
          [0.5699, 0.4559, 0.3585,  ..., 0.3993, 0.7009, 0.4591],
          [0.4463, 0.6469, 0.4465,  ..., 0.4460, 0.6325, 0.5083]],

         [[0.4467, 0.4966, 0.3585,  ..., 0.4031, 0.7009, 0.5373],
          [0.6549, 0.6792, 0.4896,  ..., 0.5375, 0.5737, 0.5770],
          [0.6505, 0.3984, 0.5231,  ..., 0.5746, 0.3730, 0.5256],
          [0.4832, 0.5627, 0.5013,  ..., 0.5198, 0.4012, 0.5154]],

         [[0.4390, 0.4278, 0.4690,  ..., 0.5946, 0.5931, 0.4750],
          [0.5146, 0.5988, 0.4354,  ..., 0.3965, 0.6307, 0.5294],
          [0.6169, 0.4422, 0.5228,  ..., 0.5870, 0.5789, 0.5067],
          [0.4810, 0.5157, 0.4954,  ..., 0.4625, 0.5221, 0.5212]],

         ...,

         [[0.4463, 0.4417, 0.3612,  ..., 0.4836, 0.5463, 0.4383],
          [0.4422, 0.5257, 0.5257,  ..., 0.4620, 0.5174, 0.5234],
          [0.4417, 0.5636, 0.4054,  ..., 0.4201, 0.5212, 0.3975],
          [0.4026, 0.4470, 0.5436,  ..., 0.4121, 0.3124, 0.6016]],

         [[0.5969, 0.6234, 0.4671,  ..., 0.5665, 0.4921, 0.4591],
          [0.4930, 0.4696, 0.4278,  ..., 0.6361, 0.4249, 0.3785],
          [0.4525, 0.7106, 0.4383,  ..., 0.4867, 0.4446, 0.5665],
          [0.5722, 0.5255, 0.3831,  ..., 0.4783, 0.5446, 0.4282]],

         [[0.4696, 0.5351, 0.3433,  ..., 0.4758, 0.5799, 0.4135],
          [0.4438, 0.4550, 0.4135,  ..., 0.4552, 0.5610, 0.3285],
          [0.6809, 0.4829, 0.4603,  ..., 0.4244, 0.6289, 0.5026],
          [0.3942, 0.5732, 0.3149,  ..., 0.4921, 0.5110, 0.4410]]],


        [[[0.4412, 0.5400, 0.6002,  ..., 0.5496, 0.5248, 0.5552],
          [0.3877, 0.5265, 0.5679,  ..., 0.5545, 0.4111, 0.6723],
          [0.4220, 0.6049, 0.5550,  ..., 0.5955, 0.4325, 0.5484],
          [0.5501, 0.4311, 0.4316,  ..., 0.5052, 0.4852, 0.3895]],

         [[0.5780, 0.4701, 0.4929,  ..., 0.4894, 0.4790, 0.4741],
          [0.6451, 0.5960, 0.5028,  ..., 0.6868, 0.5353, 0.4354],
          [0.4383, 0.4064, 0.5562,  ..., 0.3016, 0.3844, 0.5894],
          [0.4737, 0.5837, 0.6370,  ..., 0.5234, 0.4927, 0.4737]],

         [[0.5927, 0.4410, 0.4941,  ..., 0.4737, 0.5508, 0.4450],
          [0.4463, 0.6156, 0.4393,  ..., 0.4818, 0.5358, 0.5378],
          [0.6109, 0.5504, 0.4832,  ..., 0.4535, 0.5251, 0.3639],
          [0.4506, 0.6424, 0.4535,  ..., 0.6077, 0.5636, 0.4528]],

         ...,

         [[0.4429, 0.5761, 0.5235,  ..., 0.4130, 0.6926, 0.4287],
          [0.4610, 0.4173, 0.3702,  ..., 0.2524, 0.5528, 0.4557],
          [0.4659, 0.5056, 0.4843,  ..., 0.4173, 0.4116, 0.5050],
          [0.4767, 0.4482, 0.3905,  ..., 0.6202, 0.5043, 0.4325]],

         [[0.6460, 0.5689, 0.5285,  ..., 0.4926, 0.4817, 0.4477],
          [0.6063, 0.5540, 0.4287,  ..., 0.5631, 0.4036, 0.5557],
          [0.4301, 0.4907, 0.4426,  ..., 0.5761, 0.5005, 0.3657],
          [0.5799, 0.5404, 0.4769,  ..., 0.5089, 0.5143, 0.3863]],

         [[0.4321, 0.4732, 0.5251,  ..., 0.5172, 0.4316, 0.6316],
          [0.4410, 0.4397, 0.5213,  ..., 0.4883, 0.4369, 0.5373],
          [0.6039, 0.6105, 0.4268,  ..., 0.5770, 0.4822, 0.5317],
          [0.4349, 0.4818, 0.5067,  ..., 0.3504, 0.4799, 0.4249]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0040, -0.0080,  0.0040, -0.0020,  0.0020,  0.0040, -0.0060, -0.0020,
         0.0040,  0.0060], device='cuda:0')
selected experts tensor([1615, 1610, 1665, 1594, 1575, 1629, 1623, 1704, 1715, 1654],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4735, 0.5071, 0.5274,  ..., 0.3356, 0.4555, 0.3876],
          [0.4562, 0.5262, 0.4662,  ..., 0.5108, 0.3496, 0.3923],
          [0.4325, 0.6072, 0.4950,  ..., 0.4962, 0.4120, 0.5140],
          [0.4330, 0.3606, 0.4117,  ..., 0.6816, 0.3846, 0.5068]],

         [[0.5808, 0.5838, 0.3896,  ..., 0.4390, 0.3782, 0.4083],
          [0.5063, 0.5342, 0.4618,  ..., 0.4678, 0.4761, 0.4145],
          [0.4475, 0.6142, 0.5624,  ..., 0.5557, 0.5008, 0.5373],
          [0.5380, 0.3720, 0.4113,  ..., 0.5349, 0.5118, 0.6425]],

         [[0.4754, 0.4194, 0.5412,  ..., 0.6183, 0.3897, 0.4337],
          [0.4789, 0.5720, 0.4725,  ..., 0.5320, 0.5003, 0.4436],
          [0.5665, 0.4208, 0.6238,  ..., 0.5124, 0.4769, 0.5168],
          [0.5842, 0.5222, 0.5149,  ..., 0.4707, 0.4926, 0.5599]],

         ...,

         [[0.6352, 0.4724, 0.5535,  ..., 0.5301, 0.5362, 0.5771],
          [0.6783, 0.4630, 0.5257,  ..., 0.5567, 0.3505, 0.4950],
          [0.3780, 0.4981, 0.4716,  ..., 0.4789, 0.4932, 0.4692],
          [0.5879, 0.4661, 0.6082,  ..., 0.5130, 0.4630, 0.5532]],

         [[0.4429, 0.3980, 0.4543,  ..., 0.5691, 0.4784, 0.5399],
          [0.4154, 0.5003, 0.4521,  ..., 0.4891, 0.5186, 0.4833],
          [0.4981, 0.5979, 0.5521,  ..., 0.3976, 0.4334, 0.3550],
          [0.5023, 0.3738, 0.5004,  ..., 0.4866, 0.4224, 0.6020]],

         [[0.6505, 0.4657, 0.5269,  ..., 0.4309, 0.4338, 0.5733],
          [0.2967, 0.5739, 0.4184,  ..., 0.4061, 0.4841, 0.4448],
          [0.5615, 0.5072, 0.5511,  ..., 0.3863, 0.5158, 0.4140],
          [0.4441, 0.3269, 0.4974,  ..., 0.4737, 0.3611, 0.3956]]],


        [[[0.2674, 0.5758, 0.4009,  ..., 0.4280, 0.4338, 0.5358],
          [0.4273, 0.4232, 0.5410,  ..., 0.4261, 0.4027, 0.5063],
          [0.4936, 0.4241, 0.4601,  ..., 0.4318, 0.4286, 0.4518],
          [0.4026, 0.4289, 0.4477,  ..., 0.6031, 0.6725, 0.4537]],

         [[0.3311, 0.5786, 0.5301,  ..., 0.5432, 0.3540, 0.5599],
          [0.5746, 0.5357, 0.4151,  ..., 0.3715, 0.4862, 0.4126],
          [0.5409, 0.5819, 0.4955,  ..., 0.5106, 0.4946, 0.4728],
          [0.4852, 0.4799, 0.3148,  ..., 0.5298, 0.4367, 0.5728]],

         [[0.4045, 0.5269, 0.5653,  ..., 0.5301, 0.4425, 0.4735],
          [0.6118, 0.5267, 0.6611,  ..., 0.3269, 0.4353, 0.6245],
          [0.3460, 0.6480, 0.5506,  ..., 0.4606, 0.3800, 0.4121],
          [0.4405, 0.3480, 0.4352,  ..., 0.5696, 0.5499, 0.4837]],

         ...,

         [[0.4649, 0.4545, 0.5686,  ..., 0.3962, 0.4262, 0.5329],
          [0.4637, 0.4410, 0.6908,  ..., 0.5487, 0.3348, 0.5676],
          [0.5195, 0.4325, 0.5979,  ..., 0.3877, 0.4078, 0.5193],
          [0.4903, 0.5139, 0.4669,  ..., 0.5068, 0.5487, 0.6729]],

         [[0.4431, 0.5923, 0.3840,  ..., 0.6620, 0.3897, 0.4298],
          [0.6388, 0.5106, 0.5653,  ..., 0.5279, 0.3819, 0.6043],
          [0.6343, 0.5105, 0.6137,  ..., 0.5276, 0.6180, 0.4327],
          [0.5746, 0.3651, 0.4265,  ..., 0.5720, 0.3728, 0.4956]],

         [[0.3337, 0.4641, 0.4679,  ..., 0.4075, 0.6231, 0.5566],
          [0.6487, 0.5080, 0.5166,  ..., 0.4533, 0.3470, 0.5008],
          [0.5576, 0.4555, 0.5993,  ..., 0.4686, 0.4182, 0.4395],
          [0.4373, 0.4504, 0.5424,  ..., 0.5886, 0.6016, 0.5337]]]],
       device='cuda:0')
tensor([[[[0.4735, 0.5131, 0.5334,  ..., 0.3416, 0.4475, 0.3956],
          [0.4562, 0.5322, 0.4722,  ..., 0.5168, 0.3416, 0.4003],
          [0.4325, 0.6132, 0.5010,  ..., 0.5022, 0.4040, 0.5220],
          [0.4330, 0.3666, 0.4177,  ..., 0.6876, 0.3766, 0.5148]],

         [[0.5808, 0.5898, 0.3956,  ..., 0.4450, 0.3702, 0.4163],
          [0.5063, 0.5402, 0.4678,  ..., 0.4738, 0.4681, 0.4225],
          [0.4475, 0.6202, 0.5684,  ..., 0.5617, 0.4928, 0.5453],
          [0.5380, 0.3780, 0.4173,  ..., 0.5409, 0.5038, 0.6505]],

         [[0.4754, 0.4254, 0.5472,  ..., 0.6243, 0.3817, 0.4417],
          [0.4789, 0.5780, 0.4785,  ..., 0.5380, 0.4923, 0.4516],
          [0.5665, 0.4268, 0.6298,  ..., 0.5184, 0.4689, 0.5248],
          [0.5842, 0.5282, 0.5209,  ..., 0.4767, 0.4846, 0.5679]],

         ...,

         [[0.6352, 0.4784, 0.5595,  ..., 0.5361, 0.5282, 0.5851],
          [0.6783, 0.4690, 0.5317,  ..., 0.5627, 0.3425, 0.5030],
          [0.3780, 0.5041, 0.4776,  ..., 0.4849, 0.4852, 0.4772],
          [0.5879, 0.4721, 0.6142,  ..., 0.5190, 0.4550, 0.5612]],

         [[0.4429, 0.4040, 0.4603,  ..., 0.5751, 0.4704, 0.5479],
          [0.4154, 0.5063, 0.4581,  ..., 0.4951, 0.5106, 0.4913],
          [0.4981, 0.6039, 0.5581,  ..., 0.4036, 0.4254, 0.3630],
          [0.5023, 0.3798, 0.5064,  ..., 0.4926, 0.4144, 0.6100]],

         [[0.6505, 0.4717, 0.5329,  ..., 0.4369, 0.4258, 0.5813],
          [0.2967, 0.5799, 0.4244,  ..., 0.4121, 0.4761, 0.4528],
          [0.5615, 0.5132, 0.5571,  ..., 0.3923, 0.5078, 0.4220],
          [0.4441, 0.3329, 0.5034,  ..., 0.4797, 0.3531, 0.4036]]],


        [[[0.2674, 0.5818, 0.4069,  ..., 0.4340, 0.4258, 0.5438],
          [0.4273, 0.4292, 0.5470,  ..., 0.4321, 0.3947, 0.5143],
          [0.4936, 0.4301, 0.4661,  ..., 0.4378, 0.4206, 0.4598],
          [0.4026, 0.4349, 0.4537,  ..., 0.6091, 0.6645, 0.4617]],

         [[0.3311, 0.5846, 0.5361,  ..., 0.5492, 0.3460, 0.5679],
          [0.5746, 0.5417, 0.4211,  ..., 0.3775, 0.4782, 0.4206],
          [0.5409, 0.5879, 0.5015,  ..., 0.5166, 0.4866, 0.4808],
          [0.4852, 0.4859, 0.3208,  ..., 0.5358, 0.4287, 0.5808]],

         [[0.4045, 0.5329, 0.5713,  ..., 0.5361, 0.4345, 0.4815],
          [0.6118, 0.5327, 0.6671,  ..., 0.3329, 0.4273, 0.6325],
          [0.3460, 0.6540, 0.5566,  ..., 0.4666, 0.3720, 0.4201],
          [0.4405, 0.3540, 0.4412,  ..., 0.5756, 0.5419, 0.4917]],

         ...,

         [[0.4649, 0.4605, 0.5746,  ..., 0.4022, 0.4182, 0.5409],
          [0.4637, 0.4470, 0.6968,  ..., 0.5547, 0.3268, 0.5756],
          [0.5195, 0.4385, 0.6039,  ..., 0.3937, 0.3998, 0.5273],
          [0.4903, 0.5199, 0.4729,  ..., 0.5128, 0.5407, 0.6809]],

         [[0.4431, 0.5983, 0.3900,  ..., 0.6680, 0.3817, 0.4378],
          [0.6388, 0.5166, 0.5713,  ..., 0.5339, 0.3739, 0.6123],
          [0.6343, 0.5165, 0.6197,  ..., 0.5336, 0.6100, 0.4407],
          [0.5746, 0.3711, 0.4325,  ..., 0.5780, 0.3648, 0.5036]],

         [[0.3337, 0.4701, 0.4739,  ..., 0.4135, 0.6151, 0.5646],
          [0.6487, 0.5140, 0.5226,  ..., 0.4593, 0.3390, 0.5088],
          [0.5576, 0.4615, 0.6053,  ..., 0.4746, 0.4102, 0.4475],
          [0.4373, 0.4564, 0.5484,  ..., 0.5946, 0.5936, 0.5417]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0000e+00, -6.0000e-03, -6.0000e-03,  4.0000e-03, -2.0000e-03,
        -2.3283e-10,  0.0000e+00, -6.0000e-03,  8.0000e-03, -8.0000e-03],
       device='cuda:0')
selected experts tensor([1363, 1725, 1863, 1559, 1826, 1758, 1406, 1836,  919, 2129],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.4997, 0.5473, 0.7065,  ..., 0.4375, 0.5161, 0.5991],
          [0.5561, 0.5146, 0.4532,  ..., 0.3197, 0.5151, 0.4986],
          [0.3535, 0.5762, 0.4076,  ..., 0.4735, 0.7206, 0.6006],
          [0.5342, 0.5681, 0.4104,  ..., 0.5606, 0.3892, 0.6385]],

         [[0.4094, 0.6070, 0.6650,  ..., 0.4435, 0.5352, 0.5496],
          [0.5107, 0.4857, 0.4476,  ..., 0.4123, 0.6194, 0.5098],
          [0.5401, 0.3861, 0.5364,  ..., 0.6079, 0.6031, 0.4999],
          [0.5127, 0.5369, 0.3064,  ..., 0.5527, 0.5771, 0.6493]],

         [[0.4643, 0.5166, 0.6042,  ..., 0.4875, 0.4178, 0.4342],
          [0.4829, 0.6042, 0.4858,  ..., 0.3299, 0.5029, 0.4091],
          [0.6023, 0.4991, 0.5919,  ..., 0.5592, 0.7346, 0.4361],
          [0.6075, 0.5435, 0.2729,  ..., 0.3679, 0.4652, 0.6670]],

         ...,

         [[0.5040, 0.5040, 0.5040,  ..., 0.5040, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5040, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5040, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5040, 0.5020, 0.5060]],

         [[0.5040, 0.5040, 0.5040,  ..., 0.5040, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5040, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5040, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5040, 0.5020, 0.5060]],

         [[0.5040, 0.5040, 0.5040,  ..., 0.5040, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5040, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5040, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5040, 0.5020, 0.5060]]],


        [[[0.4643, 0.6685, 0.6642,  ..., 0.5250, 0.4688, 0.4631],
          [0.4184, 0.5924, 0.5953,  ..., 0.4766, 0.6726, 0.6330],
          [0.4653, 0.5191, 0.5289,  ..., 0.3843, 0.6115, 0.5489],
          [0.5630, 0.3806, 0.4500,  ..., 0.5273, 0.4587, 0.5249]],

         [[0.5032, 0.5834, 0.5834,  ..., 0.4425, 0.6162, 0.5276],
          [0.4789, 0.5886, 0.5130,  ..., 0.5507, 0.5690, 0.4619],
          [0.3788, 0.4728, 0.4945,  ..., 0.4696, 0.5876, 0.4314],
          [0.6145, 0.6482, 0.4024,  ..., 0.4772, 0.4037, 0.5711]],

         [[0.5269, 0.5700, 0.6772,  ..., 0.4251, 0.5512, 0.6234],
          [0.4912, 0.4990, 0.4433,  ..., 0.4930, 0.6820, 0.4793],
          [0.3607, 0.4687, 0.3770,  ..., 0.3580, 0.6143, 0.5237],
          [0.5235, 0.5990, 0.3871,  ..., 0.6065, 0.4719, 0.4342]],

         ...,

         [[0.5040, 0.5040, 0.5040,  ..., 0.5040, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5040, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5040, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5040, 0.5020, 0.5060]],

         [[0.5040, 0.5040, 0.5040,  ..., 0.5040, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5040, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5040, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5040, 0.5020, 0.5060]],

         [[0.5040, 0.5040, 0.5040,  ..., 0.5040, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5040, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5040, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5040, 0.5020, 0.5060]]]],
       device='cuda:0')
tensor([[[[0.4957, 0.5433, 0.7025,  ..., 0.4335, 0.5141, 0.5931],
          [0.5521, 0.5106, 0.4492,  ..., 0.3157, 0.5131, 0.4926],
          [0.3495, 0.5722, 0.4036,  ..., 0.4695, 0.7186, 0.5946],
          [0.5302, 0.5641, 0.4064,  ..., 0.5566, 0.3872, 0.6325]],

         [[0.4054, 0.6030, 0.6610,  ..., 0.4395, 0.5332, 0.5436],
          [0.5067, 0.4817, 0.4436,  ..., 0.4083, 0.6174, 0.5038],
          [0.5361, 0.3821, 0.5324,  ..., 0.6039, 0.6011, 0.4939],
          [0.5087, 0.5329, 0.3024,  ..., 0.5487, 0.5751, 0.6433]],

         [[0.4603, 0.5126, 0.6002,  ..., 0.4835, 0.4158, 0.4282],
          [0.4789, 0.6002, 0.4818,  ..., 0.3259, 0.5009, 0.4031],
          [0.5983, 0.4951, 0.5879,  ..., 0.5552, 0.7326, 0.4301],
          [0.6035, 0.5395, 0.2689,  ..., 0.3639, 0.4632, 0.6610]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.4603, 0.6645, 0.6602,  ..., 0.5210, 0.4668, 0.4571],
          [0.4144, 0.5884, 0.5913,  ..., 0.4726, 0.6706, 0.6270],
          [0.4613, 0.5151, 0.5249,  ..., 0.3803, 0.6095, 0.5429],
          [0.5590, 0.3766, 0.4460,  ..., 0.5233, 0.4567, 0.5189]],

         [[0.4992, 0.5794, 0.5794,  ..., 0.4385, 0.6142, 0.5216],
          [0.4749, 0.5846, 0.5090,  ..., 0.5467, 0.5670, 0.4559],
          [0.3748, 0.4688, 0.4905,  ..., 0.4656, 0.5856, 0.4254],
          [0.6105, 0.6442, 0.3984,  ..., 0.4732, 0.4017, 0.5651]],

         [[0.5229, 0.5660, 0.6732,  ..., 0.4211, 0.5492, 0.6174],
          [0.4872, 0.4950, 0.4393,  ..., 0.4890, 0.6800, 0.4733],
          [0.3567, 0.4647, 0.3730,  ..., 0.3540, 0.6123, 0.5177],
          [0.5195, 0.5950, 0.3831,  ..., 0.6025, 0.4699, 0.4282]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0020,
        0.0060], device='cuda:0')
selected experts tensor([2463,  606, 1017,  543,  978, 1004,  960,  490, 1487, 2740],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4589, 0.4735, 0.4298,  ..., 0.5831, 0.5788, 0.4281],
          [0.5417, 0.3729, 0.4718,  ..., 0.4707, 0.2775, 0.5008],
          [0.4160, 0.5698, 0.4033,  ..., 0.5680, 0.5118, 0.5464],
          [0.5772, 0.5341, 0.3898,  ..., 0.5428, 0.5455, 0.4609]],

         [[0.3863, 0.4077, 0.6675,  ..., 0.5792, 0.5163, 0.5774],
          [0.4567, 0.4134, 0.6561,  ..., 0.3664, 0.4624, 0.4570],
          [0.3761, 0.4805, 0.4117,  ..., 0.4997, 0.6350, 0.5146],
          [0.4203, 0.5879, 0.3769,  ..., 0.3800, 0.6258, 0.6905]],

         [[0.3533, 0.3371, 0.5072,  ..., 0.5179, 0.3682, 0.5012],
          [0.6282, 0.6085, 0.4127,  ..., 0.5126, 0.5549, 0.4597],
          [0.5218, 0.6696, 0.4355,  ..., 0.5375, 0.3754, 0.4423],
          [0.4941, 0.4963, 0.4198,  ..., 0.4763, 0.5646, 0.5797]],

         ...,

         [[0.6636, 0.6414, 0.3624,  ..., 0.4021, 0.5027, 0.5892],
          [0.5339, 0.3530, 0.4565,  ..., 0.3691, 0.5074, 0.5204],
          [0.3825, 0.4981, 0.5853,  ..., 0.4049, 0.5368, 0.4442],
          [0.4371, 0.5348, 0.4694,  ..., 0.5341, 0.4949, 0.4387]],

         [[0.4586, 0.5798, 0.5514,  ..., 0.5428, 0.4305, 0.5411],
          [0.5566, 0.4445, 0.6241,  ..., 0.4615, 0.4751, 0.4491],
          [0.4203, 0.4474, 0.4769,  ..., 0.5878, 0.4670, 0.6009],
          [0.4521, 0.3904, 0.5685,  ..., 0.3993, 0.5479, 0.4257]],

         [[0.4051, 0.2588, 0.5158,  ..., 0.3364, 0.4629, 0.4763],
          [0.4555, 0.4819, 0.2658,  ..., 0.6584, 0.4656, 0.4295],
          [0.4972, 0.4872, 0.5304,  ..., 0.3682, 0.5028, 0.3875],
          [0.4681, 0.4277, 0.4384,  ..., 0.5247, 0.4352, 0.5336]]],


        [[[0.3020, 0.4296, 0.4165,  ..., 0.5653, 0.6431, 0.4276],
          [0.4446, 0.5593, 0.5340,  ..., 0.4542, 0.5916, 0.4929],
          [0.3877, 0.5617, 0.6418,  ..., 0.3583, 0.6290, 0.5353],
          [0.5833, 0.6076, 0.6463,  ..., 0.5530, 0.4357, 0.3663]],

         [[0.4641, 0.5214, 0.5606,  ..., 0.5578, 0.7021, 0.5094],
          [0.4390, 0.6251, 0.5211,  ..., 0.4997, 0.6179, 0.4334],
          [0.3121, 0.5617, 0.6658,  ..., 0.4881, 0.5224, 0.4233],
          [0.6972, 0.4801, 0.5156,  ..., 0.4167, 0.4897, 0.3963]],

         [[0.4837, 0.6423, 0.3902,  ..., 0.4443, 0.4808, 0.4039],
          [0.4935, 0.3784, 0.6753,  ..., 0.5840, 0.6077, 0.4512],
          [0.4906, 0.4947, 0.5853,  ..., 0.5968, 0.4511, 0.4926],
          [0.5714, 0.5770, 0.5900,  ..., 0.6119, 0.4921, 0.3814]],

         ...,

         [[0.4717, 0.6441, 0.5010,  ..., 0.5685, 0.4658, 0.6624],
          [0.4681, 0.5722, 0.5786,  ..., 0.4741, 0.6733, 0.6995],
          [0.4260, 0.4092, 0.5476,  ..., 0.6422, 0.4040, 0.4976],
          [0.3669, 0.5515, 0.5947,  ..., 0.4143, 0.2980, 0.3290]],

         [[0.5234, 0.5954, 0.5690,  ..., 0.5518, 0.4874, 0.6107],
          [0.2666, 0.4766, 0.5340,  ..., 0.4683, 0.4922, 0.5635],
          [0.5264, 0.4870, 0.5170,  ..., 0.6123, 0.5287, 0.5507],
          [0.5672, 0.5746, 0.5289,  ..., 0.3873, 0.5491, 0.5769]],

         [[0.5979, 0.5665, 0.4317,  ..., 0.4530, 0.4658, 0.6492],
          [0.4914, 0.4092, 0.4355,  ..., 0.4007, 0.4439, 0.5136],
          [0.5581, 0.3539, 0.4241,  ..., 0.4096, 0.5511, 0.5625],
          [0.5571, 0.4837, 0.5090,  ..., 0.4624, 0.5826, 0.5625]]]],
       device='cuda:0')
tensor([[[[0.4659, 0.4745, 0.4268,  ..., 0.5761, 0.5718, 0.4311],
          [0.5487, 0.3739, 0.4688,  ..., 0.4637, 0.2705, 0.5038],
          [0.4230, 0.5708, 0.4003,  ..., 0.5610, 0.5048, 0.5494],
          [0.5842, 0.5351, 0.3868,  ..., 0.5358, 0.5385, 0.4639]],

         [[0.3933, 0.4087, 0.6645,  ..., 0.5722, 0.5093, 0.5804],
          [0.4637, 0.4144, 0.6531,  ..., 0.3594, 0.4554, 0.4600],
          [0.3831, 0.4815, 0.4087,  ..., 0.4927, 0.6280, 0.5176],
          [0.4273, 0.5889, 0.3739,  ..., 0.3730, 0.6188, 0.6935]],

         [[0.3603, 0.3381, 0.5042,  ..., 0.5109, 0.3612, 0.5042],
          [0.6352, 0.6095, 0.4097,  ..., 0.5056, 0.5479, 0.4627],
          [0.5288, 0.6706, 0.4325,  ..., 0.5305, 0.3684, 0.4453],
          [0.5011, 0.4973, 0.4168,  ..., 0.4693, 0.5576, 0.5827]],

         ...,

         [[0.6706, 0.6424, 0.3594,  ..., 0.3951, 0.4957, 0.5922],
          [0.5409, 0.3540, 0.4535,  ..., 0.3621, 0.5004, 0.5234],
          [0.3895, 0.4991, 0.5823,  ..., 0.3979, 0.5297, 0.4472],
          [0.4441, 0.5358, 0.4664,  ..., 0.5271, 0.4879, 0.4417]],

         [[0.4656, 0.5808, 0.5484,  ..., 0.5358, 0.4235, 0.5441],
          [0.5636, 0.4455, 0.6211,  ..., 0.4545, 0.4681, 0.4521],
          [0.4273, 0.4484, 0.4739,  ..., 0.5808, 0.4600, 0.6039],
          [0.4591, 0.3914, 0.5655,  ..., 0.3923, 0.5409, 0.4287]],

         [[0.4121, 0.2598, 0.5128,  ..., 0.3294, 0.4559, 0.4793],
          [0.4625, 0.4829, 0.2628,  ..., 0.6514, 0.4586, 0.4325],
          [0.5042, 0.4882, 0.5274,  ..., 0.3612, 0.4958, 0.3905],
          [0.4751, 0.4287, 0.4354,  ..., 0.5177, 0.4282, 0.5366]]],


        [[[0.3090, 0.4306, 0.4135,  ..., 0.5583, 0.6361, 0.4306],
          [0.4516, 0.5603, 0.5310,  ..., 0.4472, 0.5846, 0.4959],
          [0.3947, 0.5627, 0.6388,  ..., 0.3513, 0.6220, 0.5383],
          [0.5903, 0.6086, 0.6433,  ..., 0.5460, 0.4287, 0.3693]],

         [[0.4711, 0.5224, 0.5576,  ..., 0.5508, 0.6951, 0.5124],
          [0.4460, 0.6261, 0.5181,  ..., 0.4927, 0.6109, 0.4364],
          [0.3191, 0.5627, 0.6628,  ..., 0.4811, 0.5154, 0.4263],
          [0.7042, 0.4811, 0.5126,  ..., 0.4097, 0.4827, 0.3993]],

         [[0.4907, 0.6433, 0.3872,  ..., 0.4373, 0.4738, 0.4069],
          [0.5005, 0.3794, 0.6723,  ..., 0.5770, 0.6007, 0.4542],
          [0.4976, 0.4957, 0.5823,  ..., 0.5898, 0.4441, 0.4956],
          [0.5784, 0.5780, 0.5870,  ..., 0.6049, 0.4851, 0.3844]],

         ...,

         [[0.4787, 0.6451, 0.4980,  ..., 0.5615, 0.4588, 0.6654],
          [0.4751, 0.5732, 0.5756,  ..., 0.4671, 0.6663, 0.7025],
          [0.4330, 0.4102, 0.5446,  ..., 0.6352, 0.3970, 0.5006],
          [0.3739, 0.5525, 0.5917,  ..., 0.4073, 0.2910, 0.3320]],

         [[0.5304, 0.5964, 0.5660,  ..., 0.5448, 0.4804, 0.6137],
          [0.2736, 0.4776, 0.5310,  ..., 0.4613, 0.4852, 0.5665],
          [0.5334, 0.4880, 0.5140,  ..., 0.6053, 0.5217, 0.5537],
          [0.5742, 0.5756, 0.5259,  ..., 0.3803, 0.5421, 0.5799]],

         [[0.6049, 0.5675, 0.4287,  ..., 0.4460, 0.4588, 0.6522],
          [0.4984, 0.4102, 0.4325,  ..., 0.3937, 0.4369, 0.5166],
          [0.5651, 0.3549, 0.4211,  ..., 0.4026, 0.5441, 0.5655],
          [0.5641, 0.4847, 0.5060,  ..., 0.4554, 0.5756, 0.5655]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0070, -0.0010,  0.0030,  0.0050,  0.0050,  0.0050, -0.0070,  0.0070,
         0.0070, -0.0030], device='cuda:0')
selected experts tensor([1776, 1635, 1568, 1681, 1592, 1578, 1632, 1683, 1563, 1676],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4304, 0.5227, 0.4293,  ..., 0.3537, 0.4640, 0.5337],
          [0.4313, 0.5257, 0.5738,  ..., 0.5304, 0.5053, 0.5181],
          [0.4213, 0.5409, 0.4288,  ..., 0.4067, 0.7247, 0.4515],
          [0.4878, 0.4117, 0.4985,  ..., 0.4937, 0.4589, 0.3267]],

         [[0.6357, 0.4770, 0.5524,  ..., 0.5428, 0.5478, 0.4147],
          [0.3662, 0.5847, 0.4830,  ..., 0.5481, 0.5490, 0.3761],
          [0.4917, 0.4441, 0.5218,  ..., 0.5159, 0.4365, 0.4626],
          [0.4692, 0.4695, 0.5140,  ..., 0.5227, 0.3678, 0.5551]],

         [[0.5681, 0.5170, 0.4664,  ..., 0.5445, 0.4399, 0.4347],
          [0.6608, 0.2824, 0.4005,  ..., 0.3035, 0.5853, 0.3816],
          [0.5401, 0.4506, 0.5531,  ..., 0.5906, 0.4389, 0.4175],
          [0.5171, 0.4898, 0.4146,  ..., 0.4267, 0.5625, 0.6071]],

         ...,

         [[0.5163, 0.6191, 0.5529,  ..., 0.5187, 0.6445, 0.5249],
          [0.4213, 0.6653, 0.6046,  ..., 0.5868, 0.3660, 0.6519],
          [0.4704, 0.4781, 0.4203,  ..., 0.5219, 0.5004, 0.6066],
          [0.6311, 0.4174, 0.5112,  ..., 0.6349, 0.4042, 0.5479]],

         [[0.5166, 0.4774, 0.4415,  ..., 0.5159, 0.4466, 0.4246],
          [0.5283, 0.5004, 0.6570,  ..., 0.4091, 0.5447, 0.3132],
          [0.6164, 0.4446, 0.5230,  ..., 0.3764, 0.5618, 0.4692],
          [0.4665, 0.5843, 0.5147,  ..., 0.3764, 0.3196, 0.5284]],

         [[0.6564, 0.5103, 0.5278,  ..., 0.4883, 0.5848, 0.5061],
          [0.5365, 0.4911, 0.3921,  ..., 0.3019, 0.4747, 0.5488],
          [0.4631, 0.5281, 0.6727,  ..., 0.4416, 0.5262, 0.5763],
          [0.5500, 0.4552, 0.5483,  ..., 0.6762, 0.4553, 0.5389]]],


        [[[0.5206, 0.4251, 0.5342,  ..., 0.3510, 0.4722, 0.4614],
          [0.4718, 0.2832, 0.4399,  ..., 0.5587, 0.4940, 0.3945],
          [0.5830, 0.5037, 0.6328,  ..., 0.4464, 0.5151, 0.5394],
          [0.4261, 0.4608, 0.4473,  ..., 0.6376, 0.4874, 0.4001]],

         [[0.6483, 0.3905, 0.5156,  ..., 0.4353, 0.5791, 0.5542],
          [0.3617, 0.4821, 0.7039,  ..., 0.5958, 0.4399, 0.6302],
          [0.4213, 0.3669, 0.4384,  ..., 0.4580, 0.4141, 0.5858],
          [0.5669, 0.5605, 0.4336,  ..., 0.5065, 0.4061, 0.2801]],

         [[0.4371, 0.4664, 0.5281,  ..., 0.5143, 0.4346, 0.6366],
          [0.4460, 0.4327, 0.5243,  ..., 0.4852, 0.4399, 0.5423],
          [0.6089, 0.6035, 0.4298,  ..., 0.5740, 0.4852, 0.5367],
          [0.4399, 0.4748, 0.5096,  ..., 0.3474, 0.4829, 0.4299]],

         ...,

         [[0.4133, 0.4455, 0.5150,  ..., 0.6403, 0.4865, 0.4566],
          [0.6687, 0.4467, 0.4293,  ..., 0.5778, 0.4672, 0.4687],
          [0.4780, 0.5595, 0.5179,  ..., 0.5649, 0.4652, 0.5643],
          [0.5250, 0.5063, 0.3420,  ..., 0.3273, 0.5053, 0.6816]],

         [[0.5469, 0.5458, 0.4512,  ..., 0.5750, 0.5211, 0.5249],
          [0.5537, 0.4603, 0.2972,  ..., 0.6222, 0.3642, 0.4251],
          [0.6366, 0.4550, 0.5473,  ..., 0.5731, 0.5413, 0.6089],
          [0.5901, 0.5112, 0.3760,  ..., 0.4072, 0.6162, 0.3973]],

         [[0.5161, 0.5828, 0.4803,  ..., 0.6222, 0.5645, 0.5811],
          [0.5681, 0.4146, 0.5729,  ..., 0.4363, 0.4604, 0.4638],
          [0.5633, 0.4446, 0.6472,  ..., 0.3875, 0.4915, 0.5825],
          [0.5110, 0.4628, 0.4623,  ..., 0.6019, 0.5447, 0.4675]]]],
       device='cuda:0')
tensor([[[[0.4254, 0.5297, 0.4263,  ..., 0.3567, 0.4610, 0.5287],
          [0.4263, 0.5327, 0.5708,  ..., 0.5334, 0.5023, 0.5131],
          [0.4163, 0.5479, 0.4258,  ..., 0.4097, 0.7217, 0.4465],
          [0.4828, 0.4187, 0.4955,  ..., 0.4967, 0.4559, 0.3217]],

         [[0.6307, 0.4840, 0.5494,  ..., 0.5458, 0.5448, 0.4097],
          [0.3612, 0.5917, 0.4800,  ..., 0.5511, 0.5460, 0.3711],
          [0.4867, 0.4511, 0.5188,  ..., 0.5189, 0.4335, 0.4576],
          [0.4642, 0.4765, 0.5110,  ..., 0.5257, 0.3648, 0.5501]],

         [[0.5631, 0.5240, 0.4634,  ..., 0.5475, 0.4369, 0.4297],
          [0.6558, 0.2894, 0.3975,  ..., 0.3065, 0.5823, 0.3766],
          [0.5351, 0.4576, 0.5501,  ..., 0.5936, 0.4359, 0.4125],
          [0.5121, 0.4968, 0.4116,  ..., 0.4297, 0.5595, 0.6021]],

         ...,

         [[0.5113, 0.6261, 0.5499,  ..., 0.5217, 0.6415, 0.5199],
          [0.4163, 0.6723, 0.6016,  ..., 0.5898, 0.3630, 0.6469],
          [0.4654, 0.4851, 0.4173,  ..., 0.5249, 0.4974, 0.6016],
          [0.6261, 0.4244, 0.5082,  ..., 0.6379, 0.4012, 0.5429]],

         [[0.5116, 0.4844, 0.4385,  ..., 0.5189, 0.4436, 0.4196],
          [0.5233, 0.5074, 0.6540,  ..., 0.4121, 0.5417, 0.3082],
          [0.6114, 0.4516, 0.5200,  ..., 0.3794, 0.5588, 0.4642],
          [0.4615, 0.5913, 0.5117,  ..., 0.3794, 0.3166, 0.5234]],

         [[0.6514, 0.5173, 0.5248,  ..., 0.4913, 0.5818, 0.5011],
          [0.5315, 0.4981, 0.3891,  ..., 0.3049, 0.4717, 0.5438],
          [0.4581, 0.5351, 0.6697,  ..., 0.4446, 0.5232, 0.5713],
          [0.5450, 0.4622, 0.5453,  ..., 0.6792, 0.4523, 0.5339]]],


        [[[0.5156, 0.4321, 0.5312,  ..., 0.3540, 0.4692, 0.4564],
          [0.4668, 0.2902, 0.4369,  ..., 0.5617, 0.4910, 0.3895],
          [0.5780, 0.5107, 0.6298,  ..., 0.4494, 0.5121, 0.5344],
          [0.4211, 0.4678, 0.4443,  ..., 0.6406, 0.4844, 0.3951]],

         [[0.6433, 0.3975, 0.5126,  ..., 0.4383, 0.5761, 0.5492],
          [0.3567, 0.4891, 0.7009,  ..., 0.5988, 0.4369, 0.6252],
          [0.4163, 0.3739, 0.4354,  ..., 0.4610, 0.4111, 0.5808],
          [0.5619, 0.5675, 0.4306,  ..., 0.5095, 0.4031, 0.2751]],

         [[0.4321, 0.4734, 0.5251,  ..., 0.5173, 0.4316, 0.6316],
          [0.4410, 0.4397, 0.5213,  ..., 0.4882, 0.4369, 0.5373],
          [0.6039, 0.6105, 0.4268,  ..., 0.5770, 0.4822, 0.5317],
          [0.4349, 0.4818, 0.5066,  ..., 0.3504, 0.4799, 0.4249]],

         ...,

         [[0.4083, 0.4525, 0.5120,  ..., 0.6433, 0.4835, 0.4516],
          [0.6637, 0.4537, 0.4263,  ..., 0.5808, 0.4642, 0.4637],
          [0.4730, 0.5665, 0.5149,  ..., 0.5679, 0.4622, 0.5593],
          [0.5200, 0.5133, 0.3390,  ..., 0.3303, 0.5023, 0.6766]],

         [[0.5419, 0.5528, 0.4482,  ..., 0.5780, 0.5181, 0.5199],
          [0.5487, 0.4673, 0.2942,  ..., 0.6252, 0.3612, 0.4201],
          [0.6316, 0.4620, 0.5443,  ..., 0.5761, 0.5383, 0.6039],
          [0.5851, 0.5182, 0.3730,  ..., 0.4102, 0.6132, 0.3923]],

         [[0.5111, 0.5898, 0.4773,  ..., 0.6252, 0.5615, 0.5761],
          [0.5631, 0.4216, 0.5699,  ..., 0.4393, 0.4574, 0.4588],
          [0.5583, 0.4516, 0.6442,  ..., 0.3905, 0.4885, 0.5775],
          [0.5060, 0.4698, 0.4593,  ..., 0.6049, 0.5417, 0.4625]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0050, -0.0070,  0.0030, -0.0010,  0.0030,  0.0050, -0.0050, -0.0030,
         0.0030,  0.0050], device='cuda:0')
selected experts tensor([1630, 1706, 1523, 1595, 1670, 1793, 1643, 1714, 1566, 1544],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5046, 0.3812, 0.5557,  ..., 0.3669, 0.4948, 0.3522],
          [0.5073, 0.5383, 0.5056,  ..., 0.4340, 0.4239, 0.4800],
          [0.6524, 0.4956, 0.4736,  ..., 0.4811, 0.5386, 0.4457],
          [0.4355, 0.5126, 0.5067,  ..., 0.5951, 0.6324, 0.4981]],

         [[0.5356, 0.4494, 0.5431,  ..., 0.6104, 0.4263, 0.5799],
          [0.4815, 0.6182, 0.5412,  ..., 0.4409, 0.5347, 0.4387],
          [0.4747, 0.4887, 0.5866,  ..., 0.3337, 0.5717, 0.5752],
          [0.4968, 0.5513, 0.5187,  ..., 0.5941, 0.5860, 0.6555]],

         [[0.5043, 0.4709, 0.5207,  ..., 0.5118, 0.4415, 0.4707],
          [0.5842, 0.5298, 0.4944,  ..., 0.4429, 0.4983, 0.6659],
          [0.5918, 0.5196, 0.5069,  ..., 0.3138, 0.4533, 0.5637],
          [0.4335, 0.3849, 0.5605,  ..., 0.5561, 0.5256, 0.6153]],

         ...,

         [[0.6147, 0.5552, 0.4414,  ..., 0.5040, 0.5789, 0.4846],
          [0.5970, 0.5847, 0.5561,  ..., 0.5446, 0.4991, 0.3221],
          [0.4307, 0.4084, 0.5380,  ..., 0.3515, 0.5102, 0.5188],
          [0.5091, 0.5400, 0.7194,  ..., 0.5254, 0.5774, 0.3388]],

         [[0.4427, 0.4438, 0.5129,  ..., 0.4055, 0.3139, 0.5060],
          [0.5125, 0.6730, 0.4762,  ..., 0.4896, 0.5903, 0.5794],
          [0.6184, 0.5537, 0.5576,  ..., 0.4222, 0.5514, 0.4938],
          [0.4750, 0.4315, 0.5866,  ..., 0.5402, 0.6674, 0.6459]],

         [[0.5332, 0.4395, 0.5937,  ..., 0.4036, 0.5121, 0.4666],
          [0.5309, 0.6540, 0.6109,  ..., 0.5378, 0.4112, 0.5804],
          [0.5593, 0.4284, 0.5857,  ..., 0.5023, 0.4615, 0.3504],
          [0.5288, 0.3994, 0.6090,  ..., 0.4222, 0.4761, 0.4633]]],


        [[[0.4854, 0.5499, 0.5542,  ..., 0.5189, 0.4812, 0.5428],
          [0.5632, 0.5494, 0.5638,  ..., 0.5039, 0.3976, 0.5565],
          [0.5056, 0.3650, 0.5595,  ..., 0.4738, 0.5741, 0.4963],
          [0.4805, 0.4748, 0.3515,  ..., 0.5151, 0.4611, 0.5447]],

         [[0.3822, 0.4065, 0.5757,  ..., 0.4742, 0.3666, 0.5382],
          [0.5097, 0.6558, 0.5091,  ..., 0.4646, 0.4983, 0.5541],
          [0.4264, 0.5482, 0.5072,  ..., 0.4017, 0.5569, 0.5380],
          [0.4869, 0.4165, 0.5281,  ..., 0.6173, 0.2834, 0.5268]],

         [[0.4997, 0.4947, 0.5605,  ..., 0.3293, 0.5383, 0.5433],
          [0.4725, 0.4608, 0.5624,  ..., 0.4606, 0.3603, 0.5322],
          [0.3966, 0.4141, 0.6016,  ..., 0.5460, 0.2952, 0.4068],
          [0.4197, 0.3461, 0.4747,  ..., 0.5748, 0.5779, 0.5462]],

         ...,

         [[0.4918, 0.6030, 0.4032,  ..., 0.3984, 0.4918, 0.5251],
          [0.4664, 0.6300, 0.5566,  ..., 0.5395, 0.4358, 0.5818],
          [0.6115, 0.5137, 0.5125,  ..., 0.4766, 0.4856, 0.5197],
          [0.5723, 0.5229, 0.4634,  ..., 0.4368, 0.3264, 0.4697]],

         [[0.5373, 0.3835, 0.4359,  ..., 0.3372, 0.4829, 0.5766],
          [0.3790, 0.7163, 0.4641,  ..., 0.4443, 0.5038, 0.4192],
          [0.4732, 0.7076, 0.5819,  ..., 0.4824, 0.4239, 0.4868],
          [0.3712, 0.4662, 0.3037,  ..., 0.6228, 0.5598, 0.4064]],

         [[0.4826, 0.3497, 0.3825,  ..., 0.4773, 0.3523, 0.6512],
          [0.5322, 0.5705, 0.6016,  ..., 0.3542, 0.5434, 0.4890],
          [0.5283, 0.5585, 0.5581,  ..., 0.4870, 0.4439, 0.4998],
          [0.5984, 0.3678, 0.5535,  ..., 0.3891, 0.3765, 0.6061]]]],
       device='cuda:0')
tensor([[[[0.5036, 0.3882, 0.5627,  ..., 0.3739, 0.4858, 0.3612],
          [0.5063, 0.5453, 0.5126,  ..., 0.4410, 0.4149, 0.4890],
          [0.6514, 0.5026, 0.4806,  ..., 0.4881, 0.5296, 0.4547],
          [0.4345, 0.5196, 0.5137,  ..., 0.6021, 0.6234, 0.5071]],

         [[0.5346, 0.4564, 0.5501,  ..., 0.6174, 0.4173, 0.5889],
          [0.4805, 0.6252, 0.5482,  ..., 0.4479, 0.5257, 0.4477],
          [0.4737, 0.4957, 0.5936,  ..., 0.3407, 0.5627, 0.5842],
          [0.4958, 0.5583, 0.5257,  ..., 0.6011, 0.5770, 0.6645]],

         [[0.5033, 0.4779, 0.5277,  ..., 0.5188, 0.4325, 0.4797],
          [0.5832, 0.5368, 0.5014,  ..., 0.4499, 0.4893, 0.6749],
          [0.5908, 0.5266, 0.5139,  ..., 0.3208, 0.4443, 0.5727],
          [0.4325, 0.3919, 0.5675,  ..., 0.5631, 0.5166, 0.6243]],

         ...,

         [[0.6137, 0.5622, 0.4484,  ..., 0.5110, 0.5699, 0.4936],
          [0.5960, 0.5917, 0.5631,  ..., 0.5516, 0.4901, 0.3311],
          [0.4297, 0.4154, 0.5450,  ..., 0.3585, 0.5012, 0.5278],
          [0.5081, 0.5470, 0.7264,  ..., 0.5324, 0.5684, 0.3478]],

         [[0.4417, 0.4508, 0.5199,  ..., 0.4125, 0.3049, 0.5150],
          [0.5115, 0.6800, 0.4832,  ..., 0.4966, 0.5813, 0.5884],
          [0.6174, 0.5607, 0.5646,  ..., 0.4292, 0.5424, 0.5028],
          [0.4740, 0.4385, 0.5936,  ..., 0.5472, 0.6584, 0.6549]],

         [[0.5322, 0.4465, 0.6007,  ..., 0.4106, 0.5031, 0.4756],
          [0.5299, 0.6610, 0.6179,  ..., 0.5448, 0.4022, 0.5894],
          [0.5583, 0.4354, 0.5927,  ..., 0.5093, 0.4525, 0.3594],
          [0.5278, 0.4064, 0.6160,  ..., 0.4292, 0.4671, 0.4723]]],


        [[[0.4844, 0.5569, 0.5612,  ..., 0.5259, 0.4722, 0.5518],
          [0.5622, 0.5564, 0.5708,  ..., 0.5109, 0.3886, 0.5655],
          [0.5046, 0.3720, 0.5665,  ..., 0.4808, 0.5651, 0.5053],
          [0.4795, 0.4818, 0.3585,  ..., 0.5221, 0.4521, 0.5537]],

         [[0.3812, 0.4135, 0.5827,  ..., 0.4812, 0.3576, 0.5472],
          [0.5087, 0.6628, 0.5161,  ..., 0.4716, 0.4893, 0.5631],
          [0.4254, 0.5552, 0.5142,  ..., 0.4087, 0.5479, 0.5470],
          [0.4859, 0.4235, 0.5351,  ..., 0.6243, 0.2744, 0.5358]],

         [[0.4987, 0.5017, 0.5675,  ..., 0.3363, 0.5293, 0.5523],
          [0.4715, 0.4678, 0.5694,  ..., 0.4676, 0.3513, 0.5412],
          [0.3956, 0.4211, 0.6086,  ..., 0.5530, 0.2862, 0.4158],
          [0.4187, 0.3531, 0.4817,  ..., 0.5818, 0.5689, 0.5552]],

         ...,

         [[0.4908, 0.6100, 0.4102,  ..., 0.4054, 0.4828, 0.5341],
          [0.4654, 0.6370, 0.5636,  ..., 0.5465, 0.4268, 0.5908],
          [0.6105, 0.5207, 0.5195,  ..., 0.4836, 0.4766, 0.5287],
          [0.5713, 0.5299, 0.4704,  ..., 0.4438, 0.3174, 0.4787]],

         [[0.5363, 0.3905, 0.4429,  ..., 0.3442, 0.4739, 0.5856],
          [0.3780, 0.7233, 0.4711,  ..., 0.4513, 0.4948, 0.4282],
          [0.4722, 0.7146, 0.5889,  ..., 0.4894, 0.4149, 0.4958],
          [0.3702, 0.4732, 0.3107,  ..., 0.6298, 0.5508, 0.4154]],

         [[0.4816, 0.3567, 0.3895,  ..., 0.4843, 0.3433, 0.6602],
          [0.5312, 0.5775, 0.6086,  ..., 0.3612, 0.5344, 0.4980],
          [0.5273, 0.5655, 0.5651,  ..., 0.4940, 0.4349, 0.5088],
          [0.5974, 0.3748, 0.5605,  ..., 0.3961, 0.3675, 0.6151]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
[batch=10/40]:
	 Train time/batch: 9
	 Train time/sample: 18
	 Train time/batch_in_epoch: 9
	 Train time/sample_in_epoch: 18
	 Train time/token: 18432
	 Train time/token_in_epoch: 18432
	 Train memory/current_allocated_mem: 1.1145
	 Train memory/current_active_mem: 1.1145
	 Train memory/current_inactive_mem: 0.3325
	 Train memory/current_reserved_mem: 3.3638
	 Train memory/peak_allocated_mem: 2.7250
	 Train memory/peak_active_mem: 2.7250
	 Train memory/peak_inactive_mem: 0.5823
	 Train memory/peak_reserved_mem: 3.3638
	 Train memory/alloc_retries: 0
	 Train trainer/device_train_microbatch_size: 10
	 Train loss/train/total: 0.0053
	 Train metrics/train/LanguageCrossEntropy: 10.7643
	 Train metrics/train/LanguagePerplexity: 47303.4492
	 Train metrics/train/TokenAccuracy: 0.0000
	 Train time/train: 0.0227
	 Train time/val: 0.0000
	 Train time/total: 0.0227
	 Train lr-DecoupledAdamW/group0: 0.0000
	 Train time/remaining_estimate: 0.0429
	 Train metrics/shannon_entropy: 10.6359
	 Train metrics/batch_shannon_entropy: <wandb.sdk.data_types.table.Table object at 0x7097b028cfb0>
	 Train metrics/seq_shannon_entropy: CustomChart(table=<wandb.sdk.data_types.table.Table object at 0x70982f0f69c0>, spec=CustomChartSpec(spec_name='wandb/lineseries/v0', fields={'step': 'step', 'lineKey': 'lineKey', 'lineVal': 'lineVal'}, string_fields={'title': 'Per-Layer Shannon Entropy', 'xname': 'Block Index'}, key='', panel_type='Vega2', split_table=False))
	 Train metrics/exit_entropy: 0.6641
	 Train metrics/batch_exit_entropy: <wandb.sdk.data_types.table.Table object at 0x70984213f1a0>
	 Train metrics/seq_exit_entropy: CustomChart(table=<wandb.sdk.data_types.table.Table object at 0x709826d42870>, spec=CustomChartSpec(spec_name='wandb/lineseries/v0', fields={'step': 'step', 'lineKey': 'lineKey', 'lineVal': 'lineVal'}, string_fields={'title': 'Per-Layer Exit Entropy', 'xname': 'Block Index'}, key='', panel_type='Vega2', split_table=False))
	 Train expert_selection/ffn_layer: <wandb.sdk.data_types.image.Image object at 0x709821d97a40>
	 Train expert_selection/attn_o_layer: <wandb.sdk.data_types.image.Image object at 0x70981f5ef3b0>
	 Train expert_selection/attn_v_layer: <wandb.sdk.data_types.image.Image object at 0x709831794da0>
	 Train l2_norm/moment/model.transformer.router: 0.0000
	 Train l2_norm/param/model.transformer.router: 0.3633
	 Train l2_norm/update/model.transformer.router: 0.0001
	 Train l2_norm/grad/model.transformer.router: 0.0000
	 Train l2_norm/moment/model.transformer.tau: 0.0000
	 Train l2_norm/param/model.transformer.tau: 1.0000
	 Train l2_norm/update/model.transformer.tau: 0.0000
	 Train l2_norm/grad/model.transformer.tau: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.attention.v: 0.0006
	 Train l2_norm/param/model.transformer.layers.0.attention.v: 20.4715
	 Train l2_norm/update/model.transformer.layers.0.attention.v: 0.0078
	 Train l2_norm/grad/model.transformer.layers.0.attention.v: 0.0013
	 Train l2_norm/moment/model.transformer.layers.0.attention.o: 0.0005
	 Train l2_norm/param/model.transformer.layers.0.attention.o: 22.6947
	 Train l2_norm/update/model.transformer.layers.0.attention.o: 0.0079
	 Train l2_norm/grad/model.transformer.layers.0.attention.o: 0.0011
	 Train l2_norm/moment/model.transformer.layers.0.attention.sel_v: 0.0001
	 Train l2_norm/param/model.transformer.layers.0.attention.sel_v: 2.2340
	 Train l2_norm/update/model.transformer.layers.0.attention.sel_v: 0.0005
	 Train l2_norm/grad/model.transformer.layers.0.attention.sel_v: 0.0002
	 Train l2_norm/moment/model.transformer.layers.0.attention.sel_o: 0.0001
	 Train l2_norm/param/model.transformer.layers.0.attention.sel_o: 2.2493
	 Train l2_norm/update/model.transformer.layers.0.attention.sel_o: 0.0005
	 Train l2_norm/grad/model.transformer.layers.0.attention.sel_o: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.attention.q.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attention.q.weight: 6.4671
	 Train l2_norm/update/model.transformer.layers.0.attention.q.weight: 0.0024
	 Train l2_norm/grad/model.transformer.layers.0.attention.q.weight: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.attention.k.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attention.k.weight: 6.4731
	 Train l2_norm/update/model.transformer.layers.0.attention.k.weight: 0.0024
	 Train l2_norm/grad/model.transformer.layers.0.attention.k.weight: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.ffn.keys: 0.0004
	 Train l2_norm/param/model.transformer.layers.0.ffn.keys: 14.9961
	 Train l2_norm/update/model.transformer.layers.0.ffn.keys: 0.0064
	 Train l2_norm/grad/model.transformer.layers.0.ffn.keys: 0.0009
	 Train l2_norm/moment/model.transformer.layers.0.ffn.values: 0.0010
	 Train l2_norm/param/model.transformer.layers.0.ffn.values: 7.1902
	 Train l2_norm/update/model.transformer.layers.0.ffn.values: 0.0068
	 Train l2_norm/grad/model.transformer.layers.0.ffn.values: 0.0020
	 Train l2_norm/moment/model.transformer.layers.0.ffn.expert_sel: 0.0001
	 Train l2_norm/param/model.transformer.layers.0.ffn.expert_sel: 4.7509
	 Train l2_norm/update/model.transformer.layers.0.ffn.expert_sel: 0.0021
	 Train l2_norm/grad/model.transformer.layers.0.ffn.expert_sel: 0.0002
	 Train l2_norm/moment/model.transformer.layers.0.attn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attn_pre.weight: 20.2978
	 Train l2_norm/update/model.transformer.layers.0.attn_pre.weight: 0.0002
	 Train l2_norm/grad/model.transformer.layers.0.attn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.attn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attn_post.weight: 20.2978
	 Train l2_norm/update/model.transformer.layers.0.attn_post.weight: 0.0002
	 Train l2_norm/grad/model.transformer.layers.0.attn_post.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.ffn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.ffn_pre.weight: 20.2978
	 Train l2_norm/update/model.transformer.layers.0.ffn_pre.weight: 0.0002
	 Train l2_norm/grad/model.transformer.layers.0.ffn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.ffn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.ffn_post.weight: 20.2978
	 Train l2_norm/update/model.transformer.layers.0.ffn_post.weight: 0.0002
	 Train l2_norm/grad/model.transformer.layers.0.ffn_post.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.v: 0.0002
	 Train l2_norm/param/model.transformer.layers.1.attention.v: 20.4824
	 Train l2_norm/update/model.transformer.layers.1.attention.v: 0.0057
	 Train l2_norm/grad/model.transformer.layers.1.attention.v: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.o: 0.0001
	 Train l2_norm/param/model.transformer.layers.1.attention.o: 22.6890
	 Train l2_norm/update/model.transformer.layers.1.attention.o: 0.0053
	 Train l2_norm/grad/model.transformer.layers.1.attention.o: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.sel_v: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.sel_v: 2.2359
	 Train l2_norm/update/model.transformer.layers.1.attention.sel_v: 0.0003
	 Train l2_norm/grad/model.transformer.layers.1.attention.sel_v: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.sel_o: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.sel_o: 2.2254
	 Train l2_norm/update/model.transformer.layers.1.attention.sel_o: 0.0004
	 Train l2_norm/grad/model.transformer.layers.1.attention.sel_o: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.q.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.q.weight: 6.4888
	 Train l2_norm/update/model.transformer.layers.1.attention.q.weight: 0.0012
	 Train l2_norm/grad/model.transformer.layers.1.attention.q.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.k.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.k.weight: 6.4776
	 Train l2_norm/update/model.transformer.layers.1.attention.k.weight: 0.0012
	 Train l2_norm/grad/model.transformer.layers.1.attention.k.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn.keys: 0.0001
	 Train l2_norm/param/model.transformer.layers.1.ffn.keys: 15.0064
	 Train l2_norm/update/model.transformer.layers.1.ffn.keys: 0.0045
	 Train l2_norm/grad/model.transformer.layers.1.ffn.keys: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn.values: 0.0004
	 Train l2_norm/param/model.transformer.layers.1.ffn.values: 7.1641
	 Train l2_norm/update/model.transformer.layers.1.ffn.values: 0.0054
	 Train l2_norm/grad/model.transformer.layers.1.ffn.values: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn.expert_sel: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.ffn.expert_sel: 4.7382
	 Train l2_norm/update/model.transformer.layers.1.ffn.expert_sel: 0.0014
	 Train l2_norm/grad/model.transformer.layers.1.ffn.expert_sel: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attn_pre.weight: 20.2978
	 Train l2_norm/update/model.transformer.layers.1.attn_pre.weight: 0.0001
	 Train l2_norm/grad/model.transformer.layers.1.attn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attn_post.weight: 20.2978
	 Train l2_norm/update/model.transformer.layers.1.attn_post.weight: 0.0001
	 Train l2_norm/grad/model.transformer.layers.1.attn_post.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.ffn_pre.weight: 20.2978
	 Train l2_norm/update/model.transformer.layers.1.ffn_pre.weight: 0.0001
	 Train l2_norm/grad/model.transformer.layers.1.ffn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.ffn_post.weight: 20.2978
	 Train l2_norm/update/model.transformer.layers.1.ffn_post.weight: 0.0001
	 Train l2_norm/grad/model.transformer.layers.1.ffn_post.weight: 0.0000
	 Train l2_norm/moment/model.embedding.weight: 0.0002
	 Train l2_norm/param/model.embedding.weight: 221.7558
	 Train l2_norm/update/model.embedding.weight: 0.0047
	 Train l2_norm/grad/model.embedding.weight: 0.0005
	 Train l2_norm/moment/model.lm_head.weight: 0.0004
	 Train l2_norm/param/model.lm_head.weight: 127.9963
	 Train l2_norm/update/model.lm_head.weight: 0.0143
	 Train l2_norm/grad/model.lm_head.weight: 0.0009
	 Train l2_norm/moment/model.lm_head.bias: 0.0000
	 Train l2_norm/param/model.lm_head.bias: 6.2887
	 Train l2_norm/update/model.lm_head.bias: 0.0012
	 Train l2_norm/grad/model.lm_head.bias: 0.0000
	 Train l2_norm/moment/model.out_norm.weight: 0.0000
	 Train l2_norm/param/model.out_norm.weight: 20.2976
	 Train l2_norm/update/model.out_norm.weight: 0.0002
	 Train l2_norm/grad/model.out_norm.weight: 0.0000
	 Train l2_norm/grad/global: 0.0030
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([ 0.0010, -0.0070, -0.0070,  0.0050, -0.0030, -0.0010,  0.0010, -0.0070,
         0.0090, -0.0090], device='cuda:0')
selected experts tensor([1783, 1613, 1688, 1425, 1878, 1574, 1643, 1849,  894, 2037],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050],
          [0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050],
          [0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050],
          [0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050]],

         [[0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050],
          [0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050],
          [0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050],
          [0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050]],

         [[0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050],
          [0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050],
          [0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050],
          [0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050]],

         ...,

         [[0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050],
          [0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050],
          [0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050],
          [0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050]],

         [[0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050],
          [0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050],
          [0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050],
          [0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050]],

         [[0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050],
          [0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050],
          [0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050],
          [0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050]]],


        [[[0.5051, 0.5787, 0.5753,  ..., 0.4015, 0.5221, 0.5214],
          [0.5248, 0.5430, 0.4612,  ..., 0.5227, 0.5847, 0.5344],
          [0.3696, 0.5033, 0.5154,  ..., 0.4672, 0.6166, 0.4356],
          [0.5671, 0.4517, 0.3431,  ..., 0.5986, 0.4407, 0.5013]],

         [[0.4853, 0.5868, 0.5772,  ..., 0.3536, 0.4832, 0.5111],
          [0.4179, 0.4977, 0.5963,  ..., 0.4356, 0.6166, 0.5739],
          [0.3508, 0.4491, 0.3816,  ..., 0.4558, 0.4939, 0.4554],
          [0.6552, 0.4583, 0.3233,  ..., 0.5354, 0.4915, 0.6687]],

         [[0.4660, 0.6117, 0.6284,  ..., 0.4090, 0.6655, 0.5080],
          [0.4575, 0.5612, 0.4739,  ..., 0.4479, 0.5737, 0.5868],
          [0.3490, 0.5006, 0.5825,  ..., 0.3955, 0.5502, 0.4680],
          [0.5060, 0.4976, 0.4926,  ..., 0.4754, 0.5603, 0.5815]],

         ...,

         [[0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050],
          [0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050],
          [0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050],
          [0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050]],

         [[0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050],
          [0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050],
          [0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050],
          [0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050]],

         [[0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050],
          [0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050],
          [0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050],
          [0.5030, 0.5050, 0.5050,  ..., 0.5050, 0.5010, 0.5050]]]],
       device='cuda:0')
tensor([[[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5021, 0.5737, 0.5703,  ..., 0.3965, 0.5211, 0.5164],
          [0.5218, 0.5380, 0.4562,  ..., 0.5177, 0.5837, 0.5294],
          [0.3666, 0.4983, 0.5104,  ..., 0.4622, 0.6156, 0.4306],
          [0.5641, 0.4467, 0.3381,  ..., 0.5936, 0.4397, 0.4963]],

         [[0.4823, 0.5818, 0.5722,  ..., 0.3486, 0.4822, 0.5061],
          [0.4149, 0.4927, 0.5913,  ..., 0.4306, 0.6156, 0.5689],
          [0.3478, 0.4441, 0.3766,  ..., 0.4508, 0.4929, 0.4504],
          [0.6522, 0.4533, 0.3183,  ..., 0.5304, 0.4905, 0.6637]],

         [[0.4630, 0.6067, 0.6234,  ..., 0.4040, 0.6645, 0.5030],
          [0.4545, 0.5562, 0.4689,  ..., 0.4429, 0.5727, 0.5818],
          [0.3460, 0.4956, 0.5775,  ..., 0.3905, 0.5492, 0.4630],
          [0.5030, 0.4926, 0.4876,  ..., 0.4704, 0.5593, 0.5765]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0030, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0010,
        0.0050], device='cuda:0')
selected experts tensor([ 422, 3931, 3933,  379,  660,  750,  465,  324,  801,  623],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6236, 0.4508, 0.4855,  ..., 0.5159, 0.4458, 0.4156],
          [0.6703, 0.5139, 0.5500,  ..., 0.4838, 0.4584, 0.4842],
          [0.4943, 0.4593, 0.4380,  ..., 0.4523, 0.5118, 0.4607],
          [0.6380, 0.4135, 0.4270,  ..., 0.6731, 0.5569, 0.4582]],

         [[0.5479, 0.5617, 0.5990,  ..., 0.5332, 0.3417, 0.5188],
          [0.5964, 0.5908, 0.6374,  ..., 0.5073, 0.3549, 0.5711],
          [0.4801, 0.5875, 0.4852,  ..., 0.5486, 0.4806, 0.4927],
          [0.5035, 0.5641, 0.6464,  ..., 0.3932, 0.4928, 0.4442]],

         [[0.3965, 0.4627, 0.5339,  ..., 0.5883, 0.5764, 0.4517],
          [0.5003, 0.3107, 0.4294,  ..., 0.4641, 0.5931, 0.4233],
          [0.4322, 0.5832, 0.6061,  ..., 0.6358, 0.5735, 0.5849],
          [0.4241, 0.4163, 0.4760,  ..., 0.6141, 0.5562, 0.5490]],

         ...,

         [[0.4325, 0.5015, 0.4356,  ..., 0.5295, 0.5261, 0.4964],
          [0.5358, 0.3657, 0.4370,  ..., 0.5228, 0.5731, 0.5338],
          [0.7215, 0.3648, 0.4568,  ..., 0.4513, 0.5627, 0.5360],
          [0.5082, 0.4710, 0.5691,  ..., 0.4489, 0.5367, 0.4281]],

         [[0.5315, 0.5884, 0.5212,  ..., 0.5197, 0.4893, 0.4836],
          [0.5539, 0.4092, 0.5376,  ..., 0.4957, 0.4300, 0.4621],
          [0.4150, 0.4477, 0.4303,  ..., 0.4204, 0.5683, 0.6258],
          [0.4207, 0.5327, 0.4332,  ..., 0.5384, 0.4381, 0.5563]],

         [[0.5747, 0.5576, 0.6455,  ..., 0.4643, 0.4008, 0.4991],
          [0.4828, 0.5537, 0.3959,  ..., 0.6151, 0.5543, 0.6046],
          [0.5866, 0.4915, 0.4638,  ..., 0.4457, 0.4871, 0.6180],
          [0.3275, 0.4441, 0.4198,  ..., 0.5508, 0.3906, 0.4710]]],


        [[[0.4375, 0.5600, 0.4024,  ..., 0.4021, 0.5467, 0.5659],
          [0.5196, 0.5595, 0.4015,  ..., 0.6155, 0.5711, 0.5849],
          [0.3640, 0.5134, 0.6866,  ..., 0.4491, 0.4499, 0.4991],
          [0.4397, 0.3693, 0.5147,  ..., 0.5892, 0.5134, 0.5706]],

         [[0.2782, 0.6817, 0.4010,  ..., 0.5122, 0.5798, 0.5522],
          [0.4501, 0.3900, 0.6338,  ..., 0.5163, 0.5615, 0.4992],
          [0.5421, 0.4182, 0.6500,  ..., 0.3084, 0.5297, 0.7224],
          [0.5273, 0.5622, 0.5149,  ..., 0.4617, 0.5627, 0.6125]],

         [[0.4494, 0.5156, 0.4867,  ..., 0.4631, 0.3683, 0.4649],
          [0.5799, 0.5475, 0.5449,  ..., 0.3627, 0.5155, 0.5981],
          [0.4198, 0.4369, 0.5099,  ..., 0.5254, 0.6030, 0.5591],
          [0.4438, 0.6118, 0.4284,  ..., 0.5011, 0.5774, 0.6009]],

         ...,

         [[0.4060, 0.5179, 0.4767,  ..., 0.4745, 0.5343, 0.3907],
          [0.4150, 0.6183, 0.3779,  ..., 0.5508, 0.4420, 0.5754],
          [0.5421, 0.4734, 0.3231,  ..., 0.4930, 0.4410, 0.5697],
          [0.4535, 0.4235, 0.5327,  ..., 0.5605, 0.4598, 0.4137]],

         [[0.4655, 0.5482, 0.4029,  ..., 0.3771, 0.5588, 0.3837],
          [0.4482, 0.4810, 0.4747,  ..., 0.5944, 0.4881, 0.4488],
          [0.5259, 0.6109, 0.4791,  ..., 0.4612, 0.4439, 0.4476],
          [0.5361, 0.3594, 0.5910,  ..., 0.4600, 0.4955, 0.4076]],

         [[0.5115, 0.4810, 0.4151,  ..., 0.4290, 0.5056, 0.6065],
          [0.6020, 0.6044, 0.5801,  ..., 0.4247, 0.6026, 0.5104],
          [0.4169, 0.3549, 0.4568,  ..., 0.5930, 0.4220, 0.3297],
          [0.4750, 0.4940, 0.3815,  ..., 0.4839, 0.4158, 0.3977]]]],
       device='cuda:0')
tensor([[[[0.6316, 0.4508, 0.4815,  ..., 0.5099, 0.4378, 0.4196],
          [0.6783, 0.5139, 0.5460,  ..., 0.4778, 0.4504, 0.4882],
          [0.5023, 0.4593, 0.4340,  ..., 0.4463, 0.5038, 0.4647],
          [0.6460, 0.4135, 0.4230,  ..., 0.6671, 0.5489, 0.4622]],

         [[0.5559, 0.5617, 0.5950,  ..., 0.5272, 0.3337, 0.5228],
          [0.6044, 0.5908, 0.6334,  ..., 0.5013, 0.3469, 0.5751],
          [0.4881, 0.5875, 0.4812,  ..., 0.5426, 0.4726, 0.4967],
          [0.5115, 0.5641, 0.6424,  ..., 0.3872, 0.4848, 0.4482]],

         [[0.4045, 0.4627, 0.5299,  ..., 0.5823, 0.5684, 0.4557],
          [0.5083, 0.3107, 0.4254,  ..., 0.4581, 0.5851, 0.4273],
          [0.4402, 0.5832, 0.6021,  ..., 0.6298, 0.5655, 0.5889],
          [0.4321, 0.4163, 0.4720,  ..., 0.6081, 0.5482, 0.5530]],

         ...,

         [[0.4405, 0.5015, 0.4316,  ..., 0.5235, 0.5181, 0.5004],
          [0.5438, 0.3657, 0.4330,  ..., 0.5168, 0.5651, 0.5378],
          [0.7295, 0.3648, 0.4528,  ..., 0.4453, 0.5547, 0.5400],
          [0.5162, 0.4710, 0.5651,  ..., 0.4429, 0.5287, 0.4321]],

         [[0.5395, 0.5884, 0.5172,  ..., 0.5137, 0.4813, 0.4876],
          [0.5619, 0.4092, 0.5336,  ..., 0.4897, 0.4220, 0.4661],
          [0.4230, 0.4477, 0.4263,  ..., 0.4144, 0.5603, 0.6298],
          [0.4287, 0.5327, 0.4292,  ..., 0.5324, 0.4301, 0.5603]],

         [[0.5827, 0.5576, 0.6415,  ..., 0.4583, 0.3928, 0.5031],
          [0.4908, 0.5537, 0.3919,  ..., 0.6091, 0.5463, 0.6086],
          [0.5946, 0.4915, 0.4598,  ..., 0.4397, 0.4791, 0.6220],
          [0.3355, 0.4441, 0.4158,  ..., 0.5448, 0.3826, 0.4750]]],


        [[[0.4455, 0.5600, 0.3984,  ..., 0.3961, 0.5387, 0.5699],
          [0.5276, 0.5595, 0.3975,  ..., 0.6095, 0.5631, 0.5889],
          [0.3720, 0.5134, 0.6826,  ..., 0.4431, 0.4419, 0.5031],
          [0.4477, 0.3693, 0.5107,  ..., 0.5832, 0.5054, 0.5746]],

         [[0.2862, 0.6817, 0.3970,  ..., 0.5062, 0.5718, 0.5562],
          [0.4581, 0.3900, 0.6298,  ..., 0.5103, 0.5535, 0.5032],
          [0.5501, 0.4182, 0.6460,  ..., 0.3024, 0.5217, 0.7264],
          [0.5353, 0.5622, 0.5109,  ..., 0.4557, 0.5547, 0.6165]],

         [[0.4574, 0.5156, 0.4827,  ..., 0.4571, 0.3603, 0.4689],
          [0.5879, 0.5475, 0.5409,  ..., 0.3567, 0.5075, 0.6021],
          [0.4278, 0.4369, 0.5059,  ..., 0.5194, 0.5950, 0.5631],
          [0.4518, 0.6118, 0.4244,  ..., 0.4951, 0.5694, 0.6049]],

         ...,

         [[0.4140, 0.5179, 0.4727,  ..., 0.4685, 0.5263, 0.3947],
          [0.4230, 0.6183, 0.3739,  ..., 0.5448, 0.4340, 0.5794],
          [0.5501, 0.4734, 0.3191,  ..., 0.4870, 0.4330, 0.5737],
          [0.4615, 0.4235, 0.5287,  ..., 0.5545, 0.4518, 0.4177]],

         [[0.4735, 0.5482, 0.3989,  ..., 0.3711, 0.5508, 0.3877],
          [0.4562, 0.4810, 0.4707,  ..., 0.5884, 0.4801, 0.4528],
          [0.5339, 0.6109, 0.4751,  ..., 0.4552, 0.4359, 0.4516],
          [0.5441, 0.3594, 0.5870,  ..., 0.4540, 0.4875, 0.4116]],

         [[0.5195, 0.4810, 0.4111,  ..., 0.4230, 0.4976, 0.6105],
          [0.6100, 0.6044, 0.5761,  ..., 0.4187, 0.5946, 0.5144],
          [0.4249, 0.3549, 0.4528,  ..., 0.5870, 0.4140, 0.3337],
          [0.4830, 0.4940, 0.3775,  ..., 0.4779, 0.4078, 0.4017]]]],
       device='cuda:0', requires_grad=True)
tensor([-8.0000e-03,  2.3283e-10,  4.0000e-03,  4.0000e-03,  6.0000e-03,
         6.0000e-03, -6.0000e-03,  6.0000e-03,  8.0000e-03, -4.0000e-03],
       device='cuda:0')
selected experts tensor([1758, 1603, 1483, 1742, 1671, 1659, 1571, 1638, 1572, 1687],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4503, 0.5399, 0.4594,  ..., 0.5357, 0.5061, 0.5614],
          [0.5802, 0.3857, 0.7008,  ..., 0.5367, 0.5556, 0.6285],
          [0.2882, 0.6821, 0.5034,  ..., 0.5178, 0.5222, 0.3268],
          [0.6067, 0.4542, 0.6274,  ..., 0.4228, 0.5568, 0.4728]],

         [[0.5211, 0.5235, 0.5824,  ..., 0.3219, 0.5030, 0.4651],
          [0.4223, 0.5501, 0.6419,  ..., 0.6836, 0.3894, 0.4879],
          [0.3762, 0.4112, 0.5369,  ..., 0.4314, 0.6720, 0.5005],
          [0.3799, 0.4188, 0.6223,  ..., 0.5287, 0.4889, 0.5744]],

         [[0.5778, 0.4726, 0.5177,  ..., 0.6605, 0.6545, 0.4433],
          [0.3717, 0.5170, 0.5561,  ..., 0.4875, 0.6866, 0.3771],
          [0.4862, 0.4554, 0.5303,  ..., 0.5167, 0.6103, 0.6015],
          [0.4498, 0.5370, 0.3526,  ..., 0.5948, 0.6103, 0.6067]],

         ...,

         [[0.5022, 0.4850, 0.4478,  ..., 0.5882, 0.4556, 0.5920],
          [0.5299, 0.4689, 0.5478,  ..., 0.4979, 0.3912, 0.5141],
          [0.5744, 0.5931, 0.4524,  ..., 0.4577, 0.5503, 0.7102],
          [0.5247, 0.4863, 0.3679,  ..., 0.5924, 0.3670, 0.5022]],

         [[0.5032, 0.5738, 0.4175,  ..., 0.5778, 0.6410, 0.4839],
          [0.4610, 0.5091, 0.4824,  ..., 0.4377, 0.6131, 0.5204],
          [0.4157, 0.4890, 0.5045,  ..., 0.3332, 0.5505, 0.6385],
          [0.4578, 0.4231, 0.6473,  ..., 0.5522, 0.4052, 0.6099]],

         [[0.3762, 0.4358, 0.5585,  ..., 0.6553, 0.6840, 0.5107],
          [0.5353, 0.5239, 0.4898,  ..., 0.4468, 0.4585, 0.6618],
          [0.7003, 0.2579, 0.6214,  ..., 0.5541, 0.5250, 0.5902],
          [0.4285, 0.5652, 0.4265,  ..., 0.4937, 0.3968, 0.5811]]],


        [[[0.6995, 0.4743, 0.6668,  ..., 0.5427, 0.5573, 0.5377],
          [0.4063, 0.5016, 0.3987,  ..., 0.4599, 0.5232, 0.3790],
          [0.3555, 0.3899, 0.4891,  ..., 0.6046, 0.4469, 0.4414],
          [0.4309, 0.4646, 0.5047,  ..., 0.6240, 0.4704, 0.6860]],

         [[0.5126, 0.3718, 0.5278,  ..., 0.5130, 0.5986, 0.4731],
          [0.5132, 0.5181, 0.3616,  ..., 0.5687, 0.5748, 0.4818],
          [0.4850, 0.4378, 0.4989,  ..., 0.5435, 0.4222, 0.5614],
          [0.4726, 0.5303, 0.4495,  ..., 0.4553, 0.5316, 0.4200]],

         [[0.5636, 0.6703, 0.4198,  ..., 0.6041, 0.5020, 0.5181],
          [0.4907, 0.6362, 0.3825,  ..., 0.6240, 0.5510, 0.5506],
          [0.5042, 0.3487, 0.6186,  ..., 0.4834, 0.6310, 0.3209],
          [0.4381, 0.2936, 0.4878,  ..., 0.4846, 0.5396, 0.5660]],

         ...,

         [[0.6574, 0.5704, 0.6347,  ..., 0.3572, 0.5810, 0.5438],
          [0.5253, 0.5419, 0.7575,  ..., 0.5192, 0.5075, 0.4962],
          [0.7093, 0.6001, 0.5758,  ..., 0.5377, 0.4744, 0.7174],
          [0.5194, 0.5532, 0.4736,  ..., 0.4771, 0.3825, 0.5816]],

         [[0.4482, 0.4537, 0.6832,  ..., 0.3786, 0.5140, 0.5701],
          [0.5554, 0.6281, 0.5580,  ..., 0.4966, 0.4604, 0.5123],
          [0.5725, 0.4477, 0.4029,  ..., 0.4319, 0.4944, 0.4879],
          [0.4129, 0.4953, 0.3491,  ..., 0.4587, 0.4498, 0.4352]],

         [[0.5095, 0.3909, 0.5087,  ..., 0.4664, 0.4628, 0.4600],
          [0.5484, 0.4804, 0.5179,  ..., 0.5350, 0.3898, 0.5720],
          [0.5679, 0.6522, 0.4284,  ..., 0.5500, 0.4308, 0.6818],
          [0.4890, 0.4212, 0.5065,  ..., 0.4761, 0.5118, 0.4590]]]],
       device='cuda:0')
tensor([[[[0.4443, 0.5479, 0.4554,  ..., 0.5397, 0.5021, 0.5554],
          [0.5742, 0.3937, 0.6968,  ..., 0.5407, 0.5516, 0.6225],
          [0.2822, 0.6901, 0.4994,  ..., 0.5218, 0.5182, 0.3208],
          [0.6007, 0.4622, 0.6234,  ..., 0.4268, 0.5528, 0.4668]],

         [[0.5151, 0.5315, 0.5784,  ..., 0.3259, 0.4990, 0.4591],
          [0.4163, 0.5581, 0.6379,  ..., 0.6876, 0.3854, 0.4819],
          [0.3702, 0.4192, 0.5329,  ..., 0.4354, 0.6680, 0.4945],
          [0.3739, 0.4268, 0.6183,  ..., 0.5327, 0.4849, 0.5684]],

         [[0.5718, 0.4806, 0.5137,  ..., 0.6645, 0.6505, 0.4373],
          [0.3657, 0.5250, 0.5521,  ..., 0.4915, 0.6826, 0.3711],
          [0.4802, 0.4634, 0.5263,  ..., 0.5207, 0.6063, 0.5955],
          [0.4438, 0.5450, 0.3486,  ..., 0.5988, 0.6063, 0.6007]],

         ...,

         [[0.4962, 0.4930, 0.4438,  ..., 0.5922, 0.4516, 0.5860],
          [0.5239, 0.4769, 0.5438,  ..., 0.5019, 0.3872, 0.5081],
          [0.5684, 0.6011, 0.4484,  ..., 0.4617, 0.5463, 0.7042],
          [0.5187, 0.4943, 0.3639,  ..., 0.5964, 0.3630, 0.4962]],

         [[0.4972, 0.5818, 0.4135,  ..., 0.5818, 0.6370, 0.4779],
          [0.4550, 0.5171, 0.4784,  ..., 0.4417, 0.6091, 0.5144],
          [0.4097, 0.4970, 0.5005,  ..., 0.3372, 0.5465, 0.6325],
          [0.4518, 0.4311, 0.6433,  ..., 0.5562, 0.4012, 0.6039]],

         [[0.3702, 0.4438, 0.5545,  ..., 0.6593, 0.6800, 0.5047],
          [0.5293, 0.5319, 0.4858,  ..., 0.4508, 0.4545, 0.6558],
          [0.6943, 0.2659, 0.6174,  ..., 0.5581, 0.5210, 0.5842],
          [0.4225, 0.5732, 0.4225,  ..., 0.4977, 0.3928, 0.5751]]],


        [[[0.6935, 0.4823, 0.6628,  ..., 0.5467, 0.5533, 0.5317],
          [0.4003, 0.5096, 0.3947,  ..., 0.4639, 0.5192, 0.3730],
          [0.3495, 0.3979, 0.4851,  ..., 0.6086, 0.4429, 0.4354],
          [0.4249, 0.4726, 0.5007,  ..., 0.6280, 0.4664, 0.6800]],

         [[0.5066, 0.3798, 0.5238,  ..., 0.5170, 0.5946, 0.4671],
          [0.5072, 0.5261, 0.3576,  ..., 0.5727, 0.5708, 0.4758],
          [0.4790, 0.4458, 0.4949,  ..., 0.5475, 0.4182, 0.5554],
          [0.4666, 0.5383, 0.4455,  ..., 0.4593, 0.5276, 0.4140]],

         [[0.5576, 0.6783, 0.4158,  ..., 0.6081, 0.4980, 0.5121],
          [0.4847, 0.6442, 0.3785,  ..., 0.6280, 0.5470, 0.5446],
          [0.4982, 0.3567, 0.6146,  ..., 0.4874, 0.6270, 0.3149],
          [0.4321, 0.3016, 0.4838,  ..., 0.4886, 0.5356, 0.5600]],

         ...,

         [[0.6514, 0.5784, 0.6307,  ..., 0.3612, 0.5770, 0.5378],
          [0.5193, 0.5499, 0.7535,  ..., 0.5232, 0.5035, 0.4902],
          [0.7033, 0.6081, 0.5718,  ..., 0.5417, 0.4704, 0.7114],
          [0.5134, 0.5612, 0.4696,  ..., 0.4811, 0.3785, 0.5756]],

         [[0.4422, 0.4617, 0.6792,  ..., 0.3826, 0.5100, 0.5641],
          [0.5494, 0.6361, 0.5540,  ..., 0.5006, 0.4564, 0.5063],
          [0.5665, 0.4557, 0.3989,  ..., 0.4359, 0.4904, 0.4819],
          [0.4069, 0.5033, 0.3451,  ..., 0.4627, 0.4458, 0.4292]],

         [[0.5035, 0.3989, 0.5047,  ..., 0.4704, 0.4588, 0.4540],
          [0.5424, 0.4884, 0.5139,  ..., 0.5390, 0.3858, 0.5660],
          [0.5619, 0.6602, 0.4244,  ..., 0.5540, 0.4268, 0.6758],
          [0.4830, 0.4292, 0.5025,  ..., 0.4801, 0.5078, 0.4530]]]],
       device='cuda:0', requires_grad=True)
tensor([ 6.0000e-03, -8.0000e-03,  4.0000e-03,  2.3283e-10,  2.0000e-03,
         4.0000e-03, -6.0000e-03, -4.0000e-03,  4.0000e-03,  6.0000e-03],
       device='cuda:0')
selected experts tensor([1662, 1581, 1519, 1657, 1724, 1638, 1648, 1649, 1633, 1673],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4600, 0.4933, 0.5012,  ..., 0.4621, 0.4941, 0.4576],
          [0.4717, 0.4830, 0.4884,  ..., 0.5513, 0.4541, 0.4285],
          [0.4064, 0.6256, 0.6609,  ..., 0.5700, 0.4406, 0.3936],
          [0.4135, 0.3312, 0.3871,  ..., 0.6254, 0.4821, 0.5348]],

         [[0.4523, 0.5961, 0.2878,  ..., 0.4308, 0.4848, 0.4819],
          [0.4681, 0.4780, 0.5780,  ..., 0.4250, 0.4425, 0.5246],
          [0.5761, 0.3933, 0.4810,  ..., 0.4887, 0.4258, 0.4197],
          [0.3909, 0.4080, 0.5290,  ..., 0.5072, 0.6228, 0.5822]],

         [[0.4140, 0.4499, 0.4036,  ..., 0.5426, 0.2368, 0.2698],
          [0.5127, 0.4084, 0.5685,  ..., 0.4881, 0.4633, 0.5076],
          [0.5273, 0.5441, 0.5757,  ..., 0.6218, 0.4550, 0.5642],
          [0.5295, 0.5107, 0.3728,  ..., 0.6290, 0.3569, 0.5808]],

         ...,

         [[0.5713, 0.6256, 0.4938,  ..., 0.2878, 0.4187, 0.5034],
          [0.5287, 0.5923, 0.3728,  ..., 0.4669, 0.4183, 0.4197],
          [0.5463, 0.6123, 0.4603,  ..., 0.5237, 0.4028, 0.3954],
          [0.5770, 0.5230, 0.4759,  ..., 0.5880, 0.4306, 0.5560]],

         [[0.3877, 0.4694, 0.4946,  ..., 0.3695, 0.4577, 0.5338],
          [0.3933, 0.4660, 0.3622,  ..., 0.5945, 0.4330, 0.4601],
          [0.6584, 0.3915, 0.5818,  ..., 0.5341, 0.4368, 0.5319],
          [0.4340, 0.4275, 0.7322,  ..., 0.4951, 0.4500, 0.5765]],

         [[0.5306, 0.4347, 0.5818,  ..., 0.4334, 0.3472, 0.3521],
          [0.4622, 0.5502, 0.3946,  ..., 0.4198, 0.4187, 0.4411],
          [0.5022, 0.3552, 0.5847,  ..., 0.3586, 0.4282, 0.5261],
          [0.5770, 0.5178, 0.5609,  ..., 0.5383, 0.5097, 0.5166]]],


        [[[0.3407, 0.5843, 0.4758,  ..., 0.5580, 0.4625, 0.6776],
          [0.5699, 0.5639, 0.6034,  ..., 0.4739, 0.4538, 0.6785],
          [0.4910, 0.3720, 0.4334,  ..., 0.3895, 0.4759, 0.4990],
          [0.3775, 0.4604, 0.4627,  ..., 0.6131, 0.5490, 0.5229]],

         [[0.3594, 0.4357, 0.4145,  ..., 0.5623, 0.3569, 0.4699],
          [0.4817, 0.5562, 0.4150,  ..., 0.4332, 0.4669, 0.4135],
          [0.5552, 0.4694, 0.3993,  ..., 0.5580, 0.4996, 0.4855],
          [0.4739, 0.5025, 0.4988,  ..., 0.5491, 0.4768, 0.5179]],

         [[0.4999, 0.4175, 0.3904,  ..., 0.4935, 0.4033, 0.4974],
          [0.5450, 0.4613, 0.4399,  ..., 0.4758, 0.4601, 0.4715],
          [0.4263, 0.6183, 0.6299,  ..., 0.4269, 0.4781, 0.3912],
          [0.5203, 0.4942, 0.3989,  ..., 0.6522, 0.4519, 0.6125]],

         ...,

         [[0.4467, 0.5395, 0.6290,  ..., 0.5181, 0.5250, 0.5292],
          [0.5329, 0.6274, 0.5168,  ..., 0.4790, 0.3949, 0.5278],
          [0.5074, 0.5205, 0.6539,  ..., 0.3862, 0.3866, 0.5307],
          [0.5220, 0.4027, 0.4643,  ..., 0.6043, 0.5260, 0.5546]],

         [[0.3720, 0.3453, 0.4269,  ..., 0.4919, 0.5514, 0.5160],
          [0.3486, 0.6058, 0.4895,  ..., 0.4672, 0.5245, 0.6484],
          [0.4263, 0.5557, 0.6574,  ..., 0.4860, 0.3848, 0.6297],
          [0.3675, 0.4378, 0.3918,  ..., 0.4031, 0.4657, 0.5408]],

         [[0.4492, 0.4640, 0.5048,  ..., 0.3550, 0.3839, 0.5986],
          [0.5356, 0.5610, 0.4784,  ..., 0.5685, 0.4225, 0.4030],
          [0.4258, 0.4241, 0.6272,  ..., 0.5298, 0.5946, 0.4849],
          [0.4838, 0.5112, 0.5181,  ..., 0.6025, 0.3649, 0.5893]]]],
       device='cuda:0')
tensor([[[[0.4600, 0.4993, 0.5092,  ..., 0.4701, 0.4841, 0.4676],
          [0.4717, 0.4890, 0.4964,  ..., 0.5593, 0.4441, 0.4385],
          [0.4064, 0.6316, 0.6689,  ..., 0.5780, 0.4306, 0.4036],
          [0.4135, 0.3372, 0.3951,  ..., 0.6334, 0.4721, 0.5448]],

         [[0.4523, 0.6021, 0.2958,  ..., 0.4388, 0.4748, 0.4919],
          [0.4681, 0.4840, 0.5860,  ..., 0.4330, 0.4325, 0.5346],
          [0.5761, 0.3993, 0.4890,  ..., 0.4967, 0.4158, 0.4297],
          [0.3909, 0.4140, 0.5370,  ..., 0.5152, 0.6128, 0.5922]],

         [[0.4140, 0.4559, 0.4116,  ..., 0.5506, 0.2268, 0.2798],
          [0.5127, 0.4144, 0.5765,  ..., 0.4961, 0.4533, 0.5176],
          [0.5273, 0.5501, 0.5837,  ..., 0.6298, 0.4450, 0.5742],
          [0.5295, 0.5167, 0.3808,  ..., 0.6370, 0.3469, 0.5908]],

         ...,

         [[0.5713, 0.6316, 0.5018,  ..., 0.2958, 0.4087, 0.5134],
          [0.5287, 0.5983, 0.3808,  ..., 0.4749, 0.4083, 0.4297],
          [0.5463, 0.6183, 0.4683,  ..., 0.5317, 0.3928, 0.4054],
          [0.5770, 0.5290, 0.4839,  ..., 0.5960, 0.4206, 0.5660]],

         [[0.3877, 0.4754, 0.5026,  ..., 0.3775, 0.4477, 0.5438],
          [0.3933, 0.4720, 0.3702,  ..., 0.6025, 0.4230, 0.4701],
          [0.6584, 0.3975, 0.5898,  ..., 0.5421, 0.4268, 0.5419],
          [0.4340, 0.4335, 0.7402,  ..., 0.5031, 0.4400, 0.5865]],

         [[0.5306, 0.4407, 0.5898,  ..., 0.4414, 0.3372, 0.3621],
          [0.4622, 0.5562, 0.4026,  ..., 0.4278, 0.4087, 0.4511],
          [0.5022, 0.3612, 0.5927,  ..., 0.3666, 0.4182, 0.5361],
          [0.5770, 0.5238, 0.5689,  ..., 0.5463, 0.4997, 0.5266]]],


        [[[0.3407, 0.5903, 0.4838,  ..., 0.5660, 0.4525, 0.6876],
          [0.5699, 0.5699, 0.6114,  ..., 0.4819, 0.4438, 0.6885],
          [0.4910, 0.3780, 0.4414,  ..., 0.3975, 0.4659, 0.5090],
          [0.3775, 0.4664, 0.4707,  ..., 0.6211, 0.5390, 0.5329]],

         [[0.3594, 0.4417, 0.4225,  ..., 0.5703, 0.3469, 0.4799],
          [0.4817, 0.5622, 0.4230,  ..., 0.4412, 0.4569, 0.4235],
          [0.5552, 0.4754, 0.4073,  ..., 0.5660, 0.4896, 0.4955],
          [0.4739, 0.5085, 0.5068,  ..., 0.5571, 0.4668, 0.5279]],

         [[0.4999, 0.4235, 0.3984,  ..., 0.5015, 0.3933, 0.5074],
          [0.5450, 0.4673, 0.4479,  ..., 0.4838, 0.4501, 0.4815],
          [0.4263, 0.6243, 0.6379,  ..., 0.4349, 0.4681, 0.4012],
          [0.5203, 0.5002, 0.4069,  ..., 0.6602, 0.4419, 0.6225]],

         ...,

         [[0.4467, 0.5455, 0.6370,  ..., 0.5261, 0.5150, 0.5392],
          [0.5329, 0.6334, 0.5248,  ..., 0.4870, 0.3849, 0.5378],
          [0.5074, 0.5265, 0.6619,  ..., 0.3942, 0.3766, 0.5407],
          [0.5220, 0.4087, 0.4723,  ..., 0.6123, 0.5160, 0.5646]],

         [[0.3720, 0.3513, 0.4349,  ..., 0.4999, 0.5414, 0.5260],
          [0.3486, 0.6118, 0.4975,  ..., 0.4752, 0.5145, 0.6584],
          [0.4263, 0.5617, 0.6654,  ..., 0.4940, 0.3748, 0.6397],
          [0.3675, 0.4438, 0.3998,  ..., 0.4111, 0.4557, 0.5508]],

         [[0.4492, 0.4700, 0.5128,  ..., 0.3630, 0.3739, 0.6086],
          [0.5356, 0.5670, 0.4864,  ..., 0.5765, 0.4125, 0.4130],
          [0.4258, 0.4301, 0.6352,  ..., 0.5378, 0.5846, 0.4949],
          [0.4838, 0.5172, 0.5261,  ..., 0.6105, 0.3549, 0.5993]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0000e+00, -6.0000e-03, -8.0000e-03,  6.0000e-03, -4.0000e-03,
        -2.3283e-10,  0.0000e+00, -8.0000e-03,  1.0000e-02, -1.0000e-02],
       device='cuda:0')
selected experts tensor([1702, 1857, 1556, 1740, 1286, 1785, 1568, 1944,  774, 2172],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.5253, 0.6219, 0.6958,  ..., 0.5711, 0.6101, 0.4719],
          [0.5005, 0.5266, 0.5796,  ..., 0.4096, 0.4365, 0.5269],
          [0.4764, 0.6163, 0.4833,  ..., 0.4441, 0.6036, 0.4750],
          [0.5990, 0.5190, 0.3931,  ..., 0.5142, 0.4998, 0.6206]],

         [[0.5539, 0.5228, 0.5553,  ..., 0.5253, 0.5630, 0.4682],
          [0.5834, 0.5611, 0.4704,  ..., 0.4568, 0.5449, 0.4256],
          [0.5910, 0.5344, 0.5027,  ..., 0.4857, 0.7376, 0.6574],
          [0.5867, 0.4966, 0.3114,  ..., 0.5735, 0.4425, 0.4849]],

         [[0.3670, 0.4454, 0.6991,  ..., 0.4445, 0.5291, 0.5825],
          [0.3580, 0.4375, 0.6004,  ..., 0.4953, 0.5502, 0.6644],
          [0.4322, 0.4920, 0.4351,  ..., 0.6178, 0.5824, 0.5539],
          [0.5005, 0.5053, 0.4519,  ..., 0.4660, 0.4345, 0.6749]],

         ...,

         [[0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5020, 0.5060]],

         [[0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5020, 0.5060]],

         [[0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5020, 0.5060]]],


        [[[0.5633, 0.5347, 0.5577,  ..., 0.5221, 0.5144, 0.4218],
          [0.4977, 0.4725, 0.4829,  ..., 0.5687, 0.6281, 0.6183],
          [0.3779, 0.5053, 0.3155,  ..., 0.4590, 0.6101, 0.5911],
          [0.5008, 0.3917, 0.3526,  ..., 0.6330, 0.5044, 0.3993]],

         [[0.3779, 0.6338, 0.6209,  ..., 0.5123, 0.5656, 0.5076],
          [0.3959, 0.5512, 0.5307,  ..., 0.5428, 0.5804, 0.4982],
          [0.4770, 0.3421, 0.5943,  ..., 0.5547, 0.6115, 0.5821],
          [0.5990, 0.6065, 0.4394,  ..., 0.4556, 0.3596, 0.5392]],

         [[0.5010, 0.5606, 0.5553,  ..., 0.4748, 0.4796, 0.5607],
          [0.4241, 0.4488, 0.5235,  ..., 0.3493, 0.5480, 0.4707],
          [0.4874, 0.3571, 0.4994,  ..., 0.4961, 0.5742, 0.5106],
          [0.7065, 0.5180, 0.4337,  ..., 0.6457, 0.3677, 0.6860]],

         ...,

         [[0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5020, 0.5060]],

         [[0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5020, 0.5060]],

         [[0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5020, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5020, 0.5060]]]],
       device='cuda:0')/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([[[[0.5213, 0.6179, 0.6918,  ..., 0.5651, 0.6081, 0.4659],
          [0.4965, 0.5226, 0.5756,  ..., 0.4036, 0.4345, 0.5209],
          [0.4724, 0.6123, 0.4793,  ..., 0.4381, 0.6016, 0.4690],
          [0.5950, 0.5150, 0.3891,  ..., 0.5082, 0.4978, 0.6146]],

         [[0.5499, 0.5188, 0.5513,  ..., 0.5193, 0.5610, 0.4622],
          [0.5794, 0.5571, 0.4664,  ..., 0.4508, 0.5429, 0.4196],
          [0.5870, 0.5304, 0.4987,  ..., 0.4797, 0.7356, 0.6514],
          [0.5827, 0.4926, 0.3074,  ..., 0.5675, 0.4405, 0.4789]],

         [[0.3630, 0.4414, 0.6951,  ..., 0.4385, 0.5271, 0.5765],
          [0.3540, 0.4335, 0.5964,  ..., 0.4893, 0.5482, 0.6584],
          [0.4282, 0.4880, 0.4311,  ..., 0.6118, 0.5804, 0.5479],
          [0.4965, 0.5013, 0.4479,  ..., 0.4600, 0.4325, 0.6689]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5593, 0.5307, 0.5537,  ..., 0.5161, 0.5124, 0.4158],
          [0.4937, 0.4685, 0.4789,  ..., 0.5627, 0.6261, 0.6123],
          [0.3739, 0.5013, 0.3115,  ..., 0.4530, 0.6081, 0.5851],
          [0.4968, 0.3877, 0.3486,  ..., 0.6270, 0.5024, 0.3933]],

         [[0.3739, 0.6298, 0.6169,  ..., 0.5063, 0.5636, 0.5016],
          [0.3919, 0.5472, 0.5267,  ..., 0.5368, 0.5784, 0.4922],
          [0.4730, 0.3381, 0.5903,  ..., 0.5487, 0.6095, 0.5761],
          [0.5950, 0.6025, 0.4354,  ..., 0.4496, 0.3576, 0.5332]],

         [[0.4970, 0.5566, 0.5513,  ..., 0.4688, 0.4776, 0.5547],
          [0.4201, 0.4448, 0.5195,  ..., 0.3433, 0.5460, 0.4647],
          [0.4834, 0.3531, 0.4954,  ..., 0.4901, 0.5722, 0.5046],
          [0.7025, 0.5140, 0.4297,  ..., 0.6397, 0.3657, 0.6800]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0040, 0.0040, 0.0040, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0020,
        0.0060], device='cuda:0')
selected experts tensor([ 899,  783, 1187, 1433, 2073, 1417, 1068,  529, 1614, 1285],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4489, 0.5164, 0.4878,  ..., 0.4639, 0.3693, 0.4644],
          [0.5789, 0.5485, 0.5459,  ..., 0.3637, 0.5165, 0.5971],
          [0.4188, 0.4379, 0.5110,  ..., 0.5264, 0.6040, 0.5581],
          [0.4428, 0.6128, 0.4294,  ..., 0.5022, 0.5784, 0.5999]],

         [[0.5837, 0.5010, 0.3528,  ..., 0.4391, 0.4500, 0.4104],
          [0.5341, 0.5974, 0.3807,  ..., 0.4777, 0.3810, 0.4894],
          [0.4116, 0.5737, 0.4395,  ..., 0.3975, 0.4567, 0.6347],
          [0.6079, 0.4787, 0.5027,  ..., 0.4419, 0.5827, 0.5682]],

         [[0.4707, 0.5557, 0.3680,  ..., 0.4714, 0.6315, 0.4570],
          [0.5397, 0.3799, 0.4457,  ..., 0.4938, 0.4144, 0.4108],
          [0.4334, 0.4383, 0.5098,  ..., 0.4972, 0.4741, 0.5516],
          [0.3468, 0.5164, 0.4665,  ..., 0.7334, 0.5007, 0.5653]],

         ...,

         [[0.4566, 0.5475, 0.6739,  ..., 0.4617, 0.3281, 0.4896],
          [0.5879, 0.5526, 0.7001,  ..., 0.5006, 0.7477, 0.5495],
          [0.5447, 0.5057, 0.5362,  ..., 0.4504, 0.4310, 0.4910],
          [0.4605, 0.6690, 0.4631,  ..., 0.3919, 0.4853, 0.4352]],

         [[0.4818, 0.6452, 0.3927,  ..., 0.4439, 0.4827, 0.4019],
          [0.4914, 0.3804, 0.6773,  ..., 0.5840, 0.6097, 0.4490],
          [0.4886, 0.4966, 0.5868,  ..., 0.5968, 0.4531, 0.4905],
          [0.5694, 0.5790, 0.5920,  ..., 0.6119, 0.4942, 0.3794]],

         [[0.5954, 0.5937, 0.5167,  ..., 0.3914, 0.3675, 0.3322],
          [0.4698, 0.5665, 0.5038,  ..., 0.5146, 0.4358, 0.4309],
          [0.5452, 0.4846, 0.5399,  ..., 0.4923, 0.4773, 0.5171],
          [0.6555, 0.5359, 0.5106,  ..., 0.4077, 0.4707, 0.5303]]],


        [[[0.4571, 0.3667, 0.4175,  ..., 0.5037, 0.5712, 0.4410],
          [0.5907, 0.6059, 0.5293,  ..., 0.5316, 0.4715, 0.4175],
          [0.5271, 0.5264, 0.6850,  ..., 0.4324, 0.5142, 0.5079],
          [0.4957, 0.3808, 0.4380,  ..., 0.4729, 0.3898, 0.5896]],

         [[0.3993, 0.5458, 0.4213,  ..., 0.3637, 0.6890, 0.3887],
          [0.3612, 0.6026, 0.5174,  ..., 0.3664, 0.3953, 0.5711],
          [0.3110, 0.5213, 0.4313,  ..., 0.4922, 0.4514, 0.5284],
          [0.3792, 0.4424, 0.5100,  ..., 0.5094, 0.4439, 0.6175]],

         [[0.5940, 0.5547, 0.6320,  ..., 0.5279, 0.4649, 0.4739],
          [0.3974, 0.4699, 0.4161,  ..., 0.5578, 0.5586, 0.4805],
          [0.5488, 0.6407, 0.4891,  ..., 0.5711, 0.5239, 0.4521],
          [0.4662, 0.6416, 0.5148,  ..., 0.5554, 0.5302, 0.4614]],

         ...,

         [[0.6952, 0.4688, 0.4760,  ..., 0.5189, 0.4835, 0.5581],
          [0.5537, 0.4168, 0.4356,  ..., 0.5198, 0.4112, 0.6106],
          [0.4588, 0.3210, 0.5474,  ..., 0.4874, 0.6532, 0.5966],
          [0.5433, 0.4620, 0.5720,  ..., 0.3407, 0.4652, 0.4037]],

         [[0.6093, 0.3373, 0.6094,  ..., 0.5078, 0.4475, 0.5900],
          [0.4477, 0.5794, 0.4779,  ..., 0.4115, 0.4671, 0.4735],
          [0.5319, 0.4197, 0.4578,  ..., 0.4853, 0.5436, 0.5439],
          [0.4317, 0.4686, 0.4790,  ..., 0.5312, 0.3944, 0.3859]],

         [[0.4603, 0.5723, 0.3978,  ..., 0.4405, 0.4977, 0.4451],
          [0.3704, 0.5222, 0.5416,  ..., 0.4472, 0.3820, 0.4256],
          [0.5775, 0.4288, 0.4800,  ..., 0.5759, 0.4329, 0.6175],
          [0.4806, 0.4538, 0.6519,  ..., 0.4247, 0.6153, 0.5425]]]],
       device='cuda:0')
tensor([[[[0.4579, 0.5154, 0.4828,  ..., 0.4569, 0.3603, 0.4694],
          [0.5879, 0.5475, 0.5409,  ..., 0.3567, 0.5075, 0.6021],
          [0.4278, 0.4369, 0.5060,  ..., 0.5194, 0.5950, 0.5631],
          [0.4518, 0.6118, 0.4244,  ..., 0.4952, 0.5694, 0.6049]],

         [[0.5927, 0.5000, 0.3478,  ..., 0.4321, 0.4410, 0.4154],
          [0.5431, 0.5964, 0.3757,  ..., 0.4707, 0.3720, 0.4944],
          [0.4206, 0.5727, 0.4345,  ..., 0.3905, 0.4477, 0.6397],
          [0.6169, 0.4777, 0.4977,  ..., 0.4349, 0.5737, 0.5732]],

         [[0.4797, 0.5547, 0.3630,  ..., 0.4644, 0.6225, 0.4620],
          [0.5487, 0.3789, 0.4407,  ..., 0.4868, 0.4054, 0.4158],
          [0.4424, 0.4373, 0.5048,  ..., 0.4902, 0.4651, 0.5566],
          [0.3558, 0.5154, 0.4615,  ..., 0.7264, 0.4917, 0.5703]],

         ...,

         [[0.4656, 0.5465, 0.6689,  ..., 0.4547, 0.3191, 0.4946],
          [0.5969, 0.5516, 0.6951,  ..., 0.4936, 0.7387, 0.5545],
          [0.5537, 0.5047, 0.5312,  ..., 0.4434, 0.4220, 0.4960],
          [0.4695, 0.6680, 0.4581,  ..., 0.3849, 0.4763, 0.4402]],

         [[0.4908, 0.6442, 0.3877,  ..., 0.4369, 0.4737, 0.4069],
          [0.5004, 0.3794, 0.6723,  ..., 0.5770, 0.6007, 0.4540],
          [0.4976, 0.4956, 0.5818,  ..., 0.5898, 0.4441, 0.4955],
          [0.5784, 0.5780, 0.5870,  ..., 0.6049, 0.4852, 0.3844]],

         [[0.6044, 0.5927, 0.5117,  ..., 0.3844, 0.3585, 0.3372],
          [0.4788, 0.5655, 0.4988,  ..., 0.5076, 0.4268, 0.4359],
          [0.5542, 0.4836, 0.5349,  ..., 0.4853, 0.4683, 0.5221],
          [0.6645, 0.5349, 0.5056,  ..., 0.4007, 0.4617, 0.5353]]],


        [[[0.4661, 0.3657, 0.4125,  ..., 0.4967, 0.5622, 0.4460],
          [0.5997, 0.6049, 0.5243,  ..., 0.5246, 0.4625, 0.4225],
          [0.5361, 0.5254, 0.6800,  ..., 0.4254, 0.5052, 0.5129],
          [0.5047, 0.3798, 0.4330,  ..., 0.4659, 0.3808, 0.5946]],

         [[0.4083, 0.5448, 0.4163,  ..., 0.3567, 0.6800, 0.3937],
          [0.3702, 0.6016, 0.5124,  ..., 0.3594, 0.3863, 0.5761],
          [0.3200, 0.5203, 0.4263,  ..., 0.4852, 0.4424, 0.5334],
          [0.3882, 0.4414, 0.5050,  ..., 0.5024, 0.4349, 0.6225]],

         [[0.6030, 0.5537, 0.6270,  ..., 0.5209, 0.4559, 0.4789],
          [0.4064, 0.4689, 0.4111,  ..., 0.5508, 0.5496, 0.4855],
          [0.5578, 0.6397, 0.4841,  ..., 0.5641, 0.5149, 0.4571],
          [0.4752, 0.6406, 0.5098,  ..., 0.5484, 0.5212, 0.4664]],

         ...,

         [[0.7042, 0.4678, 0.4710,  ..., 0.5119, 0.4745, 0.5631],
          [0.5627, 0.4158, 0.4306,  ..., 0.5128, 0.4022, 0.6156],
          [0.4678, 0.3200, 0.5424,  ..., 0.4804, 0.6442, 0.6016],
          [0.5523, 0.4610, 0.5670,  ..., 0.3337, 0.4562, 0.4087]],

         [[0.6183, 0.3363, 0.6044,  ..., 0.5008, 0.4385, 0.5950],
          [0.4567, 0.5784, 0.4729,  ..., 0.4045, 0.4581, 0.4785],
          [0.5409, 0.4187, 0.4528,  ..., 0.4783, 0.5346, 0.5489],
          [0.4407, 0.4676, 0.4740,  ..., 0.5242, 0.3854, 0.3909]],

         [[0.4693, 0.5713, 0.3928,  ..., 0.4335, 0.4887, 0.4501],
          [0.3794, 0.5212, 0.5366,  ..., 0.4402, 0.3730, 0.4306],
          [0.5865, 0.4278, 0.4750,  ..., 0.5689, 0.4239, 0.6225],
          [0.4896, 0.4528, 0.6469,  ..., 0.4177, 0.6063, 0.5475]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0090,  0.0010,  0.0050,  0.0030,  0.0050,  0.0050, -0.0050,  0.0070,
         0.0090, -0.0050], device='cuda:0')
selected experts tensor([1703, 1664, 1614, 1643, 1578, 1608, 1640, 1660, 1651, 1623],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5626, 0.6722, 0.4208,  ..., 0.6027, 0.5028, 0.5171],
          [0.4897, 0.6372, 0.3835,  ..., 0.6230, 0.5520, 0.5496],
          [0.5032, 0.3497, 0.6196,  ..., 0.4824, 0.6320, 0.3199],
          [0.4371, 0.2946, 0.4886,  ..., 0.4836, 0.5406, 0.5653]],

         [[0.4850, 0.4718, 0.5243,  ..., 0.3771, 0.5124, 0.5135],
          [0.4227, 0.4714, 0.6168,  ..., 0.4521, 0.4976, 0.5255],
          [0.3344, 0.4785, 0.3871,  ..., 0.3962, 0.5382, 0.5147],
          [0.5834, 0.4884, 0.6173,  ..., 0.4776, 0.5013, 0.5839]],

         [[0.4697, 0.3587, 0.5248,  ..., 0.7128, 0.6634, 0.5580],
          [0.4641, 0.4916, 0.4180,  ..., 0.5381, 0.4275, 0.5839],
          [0.5041, 0.3956, 0.4841,  ..., 0.5989, 0.4561, 0.3835],
          [0.5343, 0.4523, 0.5440,  ..., 0.4635, 0.6617, 0.5115]],

         ...,

         [[0.3770, 0.4702, 0.4585,  ..., 0.3808, 0.5004, 0.6238],
          [0.5749, 0.4528, 0.4902,  ..., 0.3827, 0.5098, 0.4749],
          [0.4171, 0.5036, 0.6825,  ..., 0.4156, 0.4954, 0.4643],
          [0.5084, 0.4340, 0.4011,  ..., 0.6124, 0.5346, 0.4361]],

         [[0.4371, 0.4667, 0.5305,  ..., 0.5124, 0.4366, 0.6366],
          [0.4460, 0.4327, 0.5263,  ..., 0.4833, 0.4419, 0.5420],
          [0.6094, 0.6035, 0.4318,  ..., 0.5720, 0.4872, 0.5367],
          [0.4395, 0.4749, 0.5116,  ..., 0.3454, 0.4849, 0.4299]],

         [[0.4323, 0.6309, 0.5374,  ..., 0.4739, 0.4537, 0.4308],
          [0.5561, 0.3233, 0.5556,  ..., 0.4223, 0.3941, 0.4513],
          [0.3780, 0.4915, 0.5187,  ..., 0.3883, 0.4921, 0.6465],
          [0.4650, 0.5600, 0.5059,  ..., 0.4631, 0.5782, 0.4687]]],


        [[[0.4573, 0.5657, 0.5315,  ..., 0.5267, 0.3798, 0.5343],
          [0.5491, 0.4552, 0.3780,  ..., 0.5825, 0.2544, 0.5173],
          [0.4985, 0.4594, 0.5084,  ..., 0.4376, 0.4976, 0.4469],
          [0.5585, 0.6806, 0.5929,  ..., 0.5820, 0.5171, 0.6014]],

         [[0.5858, 0.4150, 0.6438,  ..., 0.5639, 0.3780, 0.4440],
          [0.4837, 0.4780, 0.5324,  ..., 0.6338, 0.3798, 0.4299],
          [0.5967, 0.5523, 0.5001,  ..., 0.5810, 0.4955, 0.5476],
          [0.5580, 0.5861, 0.5500,  ..., 0.4727, 0.4404, 0.3545]],

         [[0.5868, 0.5974, 0.4949,  ..., 0.3698, 0.5338, 0.4643],
          [0.3626, 0.4832, 0.7156,  ..., 0.4968, 0.6113, 0.5136],
          [0.4653, 0.4208, 0.6293,  ..., 0.6329, 0.5250, 0.5263],
          [0.5854, 0.6327, 0.5806,  ..., 0.5260, 0.5901, 0.4784]],

         ...,

         [[0.5148, 0.4492, 0.4571,  ..., 0.4939, 0.3945, 0.5027],
          [0.5777, 0.5738, 0.6773,  ..., 0.3855, 0.4332, 0.5190],
          [0.5471, 0.5472, 0.4894,  ..., 0.3981, 0.5040, 0.5178],
          [0.3379, 0.5218, 0.4455,  ..., 0.6943, 0.4646, 0.5352]],

         [[0.4048, 0.5941, 0.4838,  ..., 0.3580, 0.6155, 0.5437],
          [0.6206, 0.5861, 0.4746,  ..., 0.5668, 0.4672, 0.4409],
          [0.5660, 0.4630, 0.4062,  ..., 0.5345, 0.4119, 0.5043],
          [0.5374, 0.4098, 0.3224,  ..., 0.4863, 0.4476, 0.4876]],

         [[0.3734, 0.5662, 0.5854,  ..., 0.5277, 0.4342, 0.5033],
          [0.4366, 0.5676, 0.5224,  ..., 0.5565, 0.4810, 0.7275],
          [0.3466, 0.5366, 0.5782,  ..., 0.6119, 0.6501, 0.5729],
          [0.4571, 0.4337, 0.5686,  ..., 0.4066, 0.3807, 0.4933]]]],
       device='cuda:0')
tensor([[[[0.5576, 0.6792, 0.4158,  ..., 0.6077, 0.4978, 0.5121],
          [0.4847, 0.6442, 0.3785,  ..., 0.6280, 0.5470, 0.5446],
          [0.4982, 0.3567, 0.6146,  ..., 0.4874, 0.6270, 0.3149],
          [0.4321, 0.3016, 0.4836,  ..., 0.4886, 0.5356, 0.5603]],

         [[0.4800, 0.4788, 0.5193,  ..., 0.3821, 0.5074, 0.5085],
          [0.4177, 0.4784, 0.6118,  ..., 0.4571, 0.4926, 0.5205],
          [0.3294, 0.4855, 0.3821,  ..., 0.4012, 0.5332, 0.5097],
          [0.5784, 0.4954, 0.6123,  ..., 0.4826, 0.4963, 0.5789]],

         [[0.4647, 0.3657, 0.5198,  ..., 0.7178, 0.6584, 0.5530],
          [0.4591, 0.4986, 0.4130,  ..., 0.5431, 0.4225, 0.5789],
          [0.4991, 0.4026, 0.4791,  ..., 0.6039, 0.4511, 0.3785],
          [0.5293, 0.4593, 0.5390,  ..., 0.4685, 0.6567, 0.5065]],

         ...,

         [[0.3720, 0.4772, 0.4535,  ..., 0.3858, 0.4954, 0.6188],
          [0.5699, 0.4598, 0.4852,  ..., 0.3877, 0.5048, 0.4699],
          [0.4121, 0.5106, 0.6775,  ..., 0.4206, 0.4904, 0.4593],
          [0.5034, 0.4410, 0.3961,  ..., 0.6174, 0.5296, 0.4311]],

         [[0.4321, 0.4737, 0.5255,  ..., 0.5174, 0.4316, 0.6316],
          [0.4410, 0.4397, 0.5213,  ..., 0.4883, 0.4369, 0.5370],
          [0.6044, 0.6105, 0.4268,  ..., 0.5770, 0.4822, 0.5317],
          [0.4345, 0.4819, 0.5066,  ..., 0.3504, 0.4799, 0.4249]],

         [[0.4273, 0.6379, 0.5324,  ..., 0.4789, 0.4487, 0.4258],
          [0.5511, 0.3303, 0.5506,  ..., 0.4273, 0.3891, 0.4463],
          [0.3730, 0.4985, 0.5137,  ..., 0.3933, 0.4871, 0.6415],
          [0.4600, 0.5670, 0.5009,  ..., 0.4681, 0.5732, 0.4637]]],


        [[[0.4523, 0.5727, 0.5265,  ..., 0.5317, 0.3748, 0.5293],
          [0.5441, 0.4622, 0.3730,  ..., 0.5875, 0.2494, 0.5123],
          [0.4935, 0.4664, 0.5034,  ..., 0.4426, 0.4926, 0.4419],
          [0.5535, 0.6876, 0.5879,  ..., 0.5870, 0.5121, 0.5964]],

         [[0.5808, 0.4220, 0.6388,  ..., 0.5689, 0.3730, 0.4390],
          [0.4787, 0.4850, 0.5274,  ..., 0.6388, 0.3748, 0.4249],
          [0.5917, 0.5593, 0.4951,  ..., 0.5860, 0.4905, 0.5426],
          [0.5530, 0.5931, 0.5450,  ..., 0.4777, 0.4354, 0.3495]],

         [[0.5818, 0.6044, 0.4899,  ..., 0.3748, 0.5288, 0.4593],
          [0.3576, 0.4902, 0.7106,  ..., 0.5018, 0.6063, 0.5086],
          [0.4603, 0.4278, 0.6243,  ..., 0.6379, 0.5200, 0.5213],
          [0.5804, 0.6397, 0.5756,  ..., 0.5310, 0.5851, 0.4734]],

         ...,

         [[0.5098, 0.4562, 0.4521,  ..., 0.4989, 0.3895, 0.4977],
          [0.5727, 0.5808, 0.6723,  ..., 0.3905, 0.4282, 0.5140],
          [0.5421, 0.5542, 0.4844,  ..., 0.4031, 0.4990, 0.5128],
          [0.3329, 0.5288, 0.4405,  ..., 0.6993, 0.4596, 0.5302]],

         [[0.3998, 0.6011, 0.4788,  ..., 0.3630, 0.6105, 0.5387],
          [0.6156, 0.5931, 0.4696,  ..., 0.5718, 0.4622, 0.4359],
          [0.5610, 0.4700, 0.4012,  ..., 0.5395, 0.4069, 0.4993],
          [0.5324, 0.4168, 0.3174,  ..., 0.4913, 0.4426, 0.4826]],

         [[0.3684, 0.5732, 0.5804,  ..., 0.5327, 0.4292, 0.4983],
          [0.4316, 0.5746, 0.5174,  ..., 0.5615, 0.4760, 0.7225],
          [0.3416, 0.5436, 0.5732,  ..., 0.6169, 0.6451, 0.5679],
          [0.4521, 0.4407, 0.5636,  ..., 0.4116, 0.3757, 0.4883]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0050, -0.0070,  0.0050, -0.0010,  0.0010,  0.0050, -0.0070, -0.0050,
         0.0050,  0.0050], device='cuda:0')
selected experts tensor([1730, 1593, 1731, 1646, 1524, 1664, 1649, 1634, 1548, 1665],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4979, 0.4344, 0.4294,  ..., 0.4293, 0.4193, 0.5072],
          [0.4661, 0.4824, 0.4856,  ..., 0.5329, 0.5128, 0.3502],
          [0.5295, 0.5852, 0.6558,  ..., 0.4421, 0.5172, 0.4106],
          [0.5674, 0.5134, 0.4455,  ..., 0.5546, 0.4904, 0.5821]],

         [[0.5368, 0.5104, 0.5438,  ..., 0.4007, 0.4321, 0.5589],
          [0.5063, 0.4926, 0.5542,  ..., 0.4714, 0.4001, 0.5290],
          [0.5605, 0.4681, 0.5167,  ..., 0.5370, 0.5186, 0.5369],
          [0.4489, 0.4231, 0.3738,  ..., 0.6102, 0.4640, 0.5850]],

         [[0.5530, 0.5347, 0.5020,  ..., 0.3522, 0.3931, 0.3734],
          [0.4414, 0.4208, 0.5247,  ..., 0.4801, 0.4544, 0.5394],
          [0.5034, 0.5843, 0.5633,  ..., 0.5699, 0.5069, 0.4096],
          [0.4869, 0.4390, 0.4622,  ..., 0.5671, 0.5119, 0.6260]],

         ...,

         [[0.5222, 0.4315, 0.6062,  ..., 0.5813, 0.4368, 0.5124],
          [0.6279, 0.5960, 0.5283,  ..., 0.5227, 0.5956, 0.6518],
          [0.4883, 0.3999, 0.5308,  ..., 0.3708, 0.4759, 0.4684],
          [0.5498, 0.3104, 0.5738,  ..., 0.5018, 0.5141, 0.6439]],

         [[0.5346, 0.4613, 0.5979,  ..., 0.5827, 0.4735, 0.6278],
          [0.6201, 0.4860, 0.5946,  ..., 0.5561, 0.5650, 0.6341],
          [0.5491, 0.4364, 0.5162,  ..., 0.3685, 0.4918, 0.5069],
          [0.6173, 0.4849, 0.6399,  ..., 0.4520, 0.4094, 0.5142]],

         [[0.4096, 0.5676, 0.5734,  ..., 0.4817, 0.4718, 0.4350],
          [0.6048, 0.6408, 0.5506,  ..., 0.5063, 0.4500, 0.4883],
          [0.4724, 0.4436, 0.6336,  ..., 0.4801, 0.4971, 0.4849],
          [0.4573, 0.4680, 0.4530,  ..., 0.5770, 0.4306, 0.6069]]],


        [[[0.4354, 0.4791, 0.5096,  ..., 0.4259, 0.4005, 0.5307],
          [0.5474, 0.4586, 0.3669,  ..., 0.5028, 0.4691, 0.2816],
          [0.4620, 0.4641, 0.6549,  ..., 0.4903, 0.5327, 0.4601],
          [0.4644, 0.4504, 0.2921,  ..., 0.5775, 0.4127, 0.5078]],

         [[0.3139, 0.4939, 0.4303,  ..., 0.5300, 0.3704, 0.5290],
          [0.6029, 0.4060, 0.5557,  ..., 0.4295, 0.5170, 0.6008],
          [0.5358, 0.5173, 0.5705,  ..., 0.3441, 0.4921, 0.4965],
          [0.3114, 0.4299, 0.5337,  ..., 0.6894, 0.4010, 0.5002]],

         [[0.4182, 0.4526, 0.4494,  ..., 0.4785, 0.4587, 0.4813],
          [0.4828, 0.5007, 0.5005,  ..., 0.5327, 0.4732, 0.4427],
          [0.4315, 0.6173, 0.6255,  ..., 0.4320, 0.5088, 0.3912],
          [0.3047, 0.3328, 0.4642,  ..., 0.5775, 0.4033, 0.5798]],

         ...,

         [[0.4296, 0.4325, 0.4330,  ..., 0.3676, 0.4445, 0.5058],
          [0.4877, 0.5349, 0.4160,  ..., 0.5870, 0.4052, 0.3767],
          [0.5472, 0.4174, 0.4552,  ..., 0.4992, 0.4421, 0.4377],
          [0.4741, 0.4364, 0.4993,  ..., 0.7120, 0.4278, 0.3827]],

         [[0.5067, 0.5724, 0.4842,  ..., 0.5035, 0.3052, 0.5374],
          [0.3979, 0.5724, 0.5431,  ..., 0.5414, 0.4202, 0.6350],
          [0.6196, 0.6002, 0.4494,  ..., 0.4054, 0.4057, 0.4091],
          [0.6113, 0.3719, 0.5135,  ..., 0.5991, 0.6270, 0.3725]],

         [[0.4736, 0.5266, 0.4550,  ..., 0.5111, 0.6747, 0.5423],
          [0.5040, 0.7100, 0.6155,  ..., 0.6171, 0.4335, 0.5560],
          [0.4796, 0.5005, 0.6048,  ..., 0.4428, 0.4517, 0.4694],
          [0.5175, 0.5705, 0.3812,  ..., 0.7032, 0.4602, 0.4745]]]],
       device='cuda:0')
tensor([[[[0.4989, 0.4414, 0.4364,  ..., 0.4383, 0.4083, 0.5182],
          [0.4671, 0.4894, 0.4926,  ..., 0.5419, 0.5018, 0.3612],
          [0.5305, 0.5922, 0.6628,  ..., 0.4511, 0.5062, 0.4216],
          [0.5684, 0.5204, 0.4525,  ..., 0.5636, 0.4794, 0.5931]],

         [[0.5378, 0.5174, 0.5508,  ..., 0.4097, 0.4211, 0.5699],
          [0.5073, 0.4996, 0.5612,  ..., 0.4804, 0.3891, 0.5400],
          [0.5615, 0.4751, 0.5237,  ..., 0.5460, 0.5076, 0.5479],
          [0.4499, 0.4301, 0.3808,  ..., 0.6192, 0.4530, 0.5960]],

         [[0.5540, 0.5417, 0.5090,  ..., 0.3612, 0.3821, 0.3844],
          [0.4424, 0.4278, 0.5317,  ..., 0.4891, 0.4434, 0.5504],
          [0.5044, 0.5913, 0.5703,  ..., 0.5789, 0.4959, 0.4206],
          [0.4879, 0.4460, 0.4692,  ..., 0.5761, 0.5009, 0.6370]],

         ...,

         [[0.5232, 0.4385, 0.6132,  ..., 0.5903, 0.4258, 0.5234],
          [0.6289, 0.6030, 0.5353,  ..., 0.5317, 0.5846, 0.6628],
          [0.4893, 0.4069, 0.5378,  ..., 0.3798, 0.4649, 0.4794],
          [0.5508, 0.3174, 0.5808,  ..., 0.5108, 0.5031, 0.6549]],

         [[0.5356, 0.4683, 0.6049,  ..., 0.5917, 0.4625, 0.6388],
          [0.6211, 0.4930, 0.6016,  ..., 0.5651, 0.5540, 0.6451],
          [0.5501, 0.4434, 0.5232,  ..., 0.3775, 0.4808, 0.5179],
          [0.6183, 0.4919, 0.6469,  ..., 0.4610, 0.3984, 0.5252]],

         [[0.4106, 0.5746, 0.5804,  ..., 0.4907, 0.4608, 0.4460],
          [0.6058, 0.6478, 0.5576,  ..., 0.5153, 0.4390, 0.4993],
          [0.4734, 0.4506, 0.6406,  ..., 0.4891, 0.4861, 0.4959],
          [0.4583, 0.4750, 0.4600,  ..., 0.5860, 0.4196, 0.6179]]],


        [[[0.4364, 0.4861, 0.5166,  ..., 0.4349, 0.3895, 0.5417],
          [0.5484, 0.4656, 0.3739,  ..., 0.5118, 0.4581, 0.2926],
          [0.4630, 0.4711, 0.6619,  ..., 0.4993, 0.5217, 0.4711],
          [0.4654, 0.4574, 0.2991,  ..., 0.5865, 0.4017, 0.5188]],

         [[0.3149, 0.5009, 0.4373,  ..., 0.5390, 0.3594, 0.5400],
          [0.6039, 0.4130, 0.5627,  ..., 0.4385, 0.5060, 0.6118],
          [0.5368, 0.5243, 0.5775,  ..., 0.3531, 0.4811, 0.5075],
          [0.3124, 0.4369, 0.5407,  ..., 0.6984, 0.3900, 0.5112]],

         [[0.4192, 0.4596, 0.4564,  ..., 0.4875, 0.4477, 0.4923],
          [0.4838, 0.5077, 0.5075,  ..., 0.5417, 0.4622, 0.4537],
          [0.4325, 0.6243, 0.6325,  ..., 0.4410, 0.4978, 0.4022],
          [0.3057, 0.3398, 0.4712,  ..., 0.5865, 0.3923, 0.5908]],

         ...,

         [[0.4306, 0.4395, 0.4400,  ..., 0.3766, 0.4335, 0.5168],
          [0.4887, 0.5419, 0.4230,  ..., 0.5960, 0.3942, 0.3877],
          [0.5482, 0.4244, 0.4622,  ..., 0.5082, 0.4311, 0.4487],
          [0.4751, 0.4434, 0.5063,  ..., 0.7210, 0.4168, 0.3937]],

         [[0.5077, 0.5794, 0.4912,  ..., 0.5125, 0.2942, 0.5484],
          [0.3989, 0.5794, 0.5501,  ..., 0.5504, 0.4092, 0.6460],
          [0.6206, 0.6072, 0.4564,  ..., 0.4144, 0.3947, 0.4201],
          [0.6123, 0.3789, 0.5205,  ..., 0.6081, 0.6160, 0.3835]],

         [[0.4746, 0.5336, 0.4620,  ..., 0.5201, 0.6637, 0.5533],
          [0.5050, 0.7170, 0.6225,  ..., 0.6261, 0.4225, 0.5670],
          [0.4806, 0.5075, 0.6118,  ..., 0.4518, 0.4407, 0.4804],
          [0.5185, 0.5775, 0.3882,  ..., 0.7122, 0.4492, 0.4855]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0010, -0.0070, -0.0070,  0.0050, -0.0030, -0.0010,  0.0010, -0.0090,
         0.0110, -0.0110], device='cuda:0')
selected experts tensor([1476, 1612, 1703, 1399, 1999, 1762, 1761, 1500,  948, 2224],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.5142, 0.5377, 0.5345,  ..., 0.5082, 0.4197, 0.5399],
          [0.4498, 0.3867, 0.4633,  ..., 0.3928, 0.5312, 0.4308],
          [0.5021, 0.3626, 0.5362,  ..., 0.4758, 0.4708, 0.4266],
          [0.6756, 0.4619, 0.3899,  ..., 0.5474, 0.4240, 0.6247]],

         [[0.5996, 0.5544, 0.5178,  ..., 0.5218, 0.5732, 0.5544],
          [0.4583, 0.4759, 0.5120,  ..., 0.4946, 0.6115, 0.6080],
          [0.4699, 0.4894, 0.4617,  ..., 0.3961, 0.5327, 0.5018],
          [0.5896, 0.4701, 0.3671,  ..., 0.5807, 0.6170, 0.5232]],

         [[0.5573, 0.5017, 0.7511,  ..., 0.5949, 0.4603, 0.5074],
          [0.4933, 0.4587, 0.4721,  ..., 0.4191, 0.5327, 0.4633],
          [0.5224, 0.4133, 0.4682,  ..., 0.5598, 0.6690, 0.5616],
          [0.6438, 0.5305, 0.5729,  ..., 0.5992, 0.3957, 0.5739]],

         ...,

         [[0.5050, 0.5050, 0.5050,  ..., 0.5070, 0.5010, 0.5050],
          [0.5050, 0.5050, 0.5050,  ..., 0.5070, 0.5010, 0.5050],
          [0.5050, 0.5050, 0.5050,  ..., 0.5070, 0.5010, 0.5050],
          [0.5050, 0.5050, 0.5050,  ..., 0.5070, 0.5010, 0.5050]],

         [[0.5050, 0.5050, 0.5050,  ..., 0.5070, 0.5010, 0.5050],
          [0.5050, 0.5050, 0.5050,  ..., 0.5070, 0.5010, 0.5050],
          [0.5050, 0.5050, 0.5050,  ..., 0.5070, 0.5010, 0.5050],
          [0.5050, 0.5050, 0.5050,  ..., 0.5070, 0.5010, 0.5050]],

         [[0.5050, 0.5050, 0.5050,  ..., 0.5070, 0.5010, 0.5050],
          [0.5050, 0.5050, 0.5050,  ..., 0.5070, 0.5010, 0.5050],
          [0.5050, 0.5050, 0.5050,  ..., 0.5070, 0.5010, 0.5050],
          [0.5050, 0.5050, 0.5050,  ..., 0.5070, 0.5010, 0.5050]]],


        [[[0.5283, 0.6302, 0.6103,  ..., 0.4300, 0.5311, 0.4665],
          [0.4578, 0.5058, 0.5447,  ..., 0.4542, 0.6087, 0.4633],
          [0.4443, 0.4665, 0.4304,  ..., 0.5423, 0.6541, 0.5017],
          [0.6572, 0.5953, 0.3258,  ..., 0.4434, 0.4606, 0.5739]],

         [[0.4280, 0.5162, 0.5350,  ..., 0.3565, 0.5109, 0.4237],
          [0.5906, 0.5758, 0.4663,  ..., 0.4205, 0.6308, 0.5144],
          [0.4709, 0.4773, 0.5403,  ..., 0.5624, 0.6903, 0.5382],
          [0.5686, 0.4433, 0.4218,  ..., 0.5404, 0.6101, 0.6474]],

         [[0.5016, 0.5000, 0.7164,  ..., 0.5070, 0.6105, 0.4266],
          [0.4731, 0.4491, 0.6739,  ..., 0.5149, 0.6733, 0.4844],
          [0.5640, 0.4457, 0.6238,  ..., 0.5513, 0.5937, 0.6103],
          [0.5825, 0.6159, 0.4746,  ..., 0.5297, 0.4620, 0.6173]],

         ...,

         [[0.5050, 0.5050, 0.5050,  ..., 0.5070, 0.5010, 0.5050],
          [0.5050, 0.5050, 0.5050,  ..., 0.5070, 0.5010, 0.5050],
          [0.5050, 0.5050, 0.5050,  ..., 0.5070, 0.5010, 0.5050],
          [0.5050, 0.5050, 0.5050,  ..., 0.5070, 0.5010, 0.5050]],

         [[0.5050, 0.5050, 0.5050,  ..., 0.5070, 0.5010, 0.5050],
          [0.5050, 0.5050, 0.5050,  ..., 0.5070, 0.5010, 0.5050],
          [0.5050, 0.5050, 0.5050,  ..., 0.5070, 0.5010, 0.5050],
          [0.5050, 0.5050, 0.5050,  ..., 0.5070, 0.5010, 0.5050]],

         [[0.5050, 0.5050, 0.5050,  ..., 0.5070, 0.5010, 0.5050],
          [0.5050, 0.5050, 0.5050,  ..., 0.5070, 0.5010, 0.5050],
          [0.5050, 0.5050, 0.5050,  ..., 0.5070, 0.5010, 0.5050],
          [0.5050, 0.5050, 0.5050,  ..., 0.5070, 0.5010, 0.5050]]]],
       device='cuda:0')
tensor([[[[0.5092, 0.5327, 0.5295,  ..., 0.5012, 0.4187, 0.5349],
          [0.4448, 0.3817, 0.4583,  ..., 0.3858, 0.5302, 0.4258],
          [0.4971, 0.3576, 0.5312,  ..., 0.4688, 0.4698, 0.4216],
          [0.6706, 0.4569, 0.3849,  ..., 0.5404, 0.4230, 0.6197]],

         [[0.5946, 0.5494, 0.5128,  ..., 0.5148, 0.5722, 0.5494],
          [0.4533, 0.4709, 0.5070,  ..., 0.4876, 0.6105, 0.6030],
          [0.4649, 0.4844, 0.4567,  ..., 0.3891, 0.5317, 0.4968],
          [0.5846, 0.4651, 0.3621,  ..., 0.5737, 0.6160, 0.5182]],

         [[0.5523, 0.4967, 0.7461,  ..., 0.5879, 0.4593, 0.5024],
          [0.4883, 0.4537, 0.4671,  ..., 0.4121, 0.5317, 0.4583],
          [0.5174, 0.4083, 0.4632,  ..., 0.5528, 0.6680, 0.5566],
          [0.6388, 0.5255, 0.5679,  ..., 0.5922, 0.3947, 0.5689]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5233, 0.6252, 0.6053,  ..., 0.4230, 0.5301, 0.4615],
          [0.4528, 0.5008, 0.5397,  ..., 0.4472, 0.6077, 0.4583],
          [0.4393, 0.4615, 0.4254,  ..., 0.5353, 0.6531, 0.4967],
          [0.6522, 0.5903, 0.3208,  ..., 0.4364, 0.4596, 0.5689]],

         [[0.4230, 0.5112, 0.5300,  ..., 0.3495, 0.5099, 0.4187],
          [0.5856, 0.5708, 0.4613,  ..., 0.4135, 0.6298, 0.5094],
          [0.4659, 0.4723, 0.5353,  ..., 0.5554, 0.6893, 0.5332],
          [0.5636, 0.4383, 0.4168,  ..., 0.5334, 0.6091, 0.6424]],

         [[0.4966, 0.4950, 0.7114,  ..., 0.5000, 0.6095, 0.4216],
          [0.4681, 0.4441, 0.6689,  ..., 0.5079, 0.6723, 0.4794],
          [0.5590, 0.4407, 0.6188,  ..., 0.5443, 0.5927, 0.6053],
          [0.5775, 0.6109, 0.4696,  ..., 0.5227, 0.4610, 0.6123]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0070, 0.0070, 0.0010,
        0.0050], device='cuda:0')
selected experts tensor([ 762,  661, 1146,  751, 1023, 1006, 2529, 1851, 1572,  987],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5167, 0.4946, 0.4390,  ..., 0.5840, 0.5581, 0.4628],
          [0.5478, 0.5161, 0.6183,  ..., 0.5996, 0.5306, 0.3804],
          [0.3973, 0.4707, 0.4798,  ..., 0.5464, 0.5617, 0.3888],
          [0.4068, 0.5617, 0.4318,  ..., 0.4951, 0.6423, 0.5180]],

         [[0.4488, 0.5149, 0.4887,  ..., 0.4629, 0.3692, 0.4659],
          [0.5779, 0.5472, 0.5469,  ..., 0.3627, 0.5155, 0.5981],
          [0.4178, 0.4369, 0.5121,  ..., 0.5255, 0.6030, 0.5587],
          [0.4418, 0.6118, 0.4299,  ..., 0.5011, 0.5774, 0.6009]],

         [[0.6431, 0.6179, 0.5725,  ..., 0.4086, 0.3470, 0.5435],
          [0.3931, 0.4769, 0.5024,  ..., 0.6294, 0.3846, 0.6203],
          [0.4858, 0.5913, 0.5304,  ..., 0.7667, 0.6040, 0.4621],
          [0.3413, 0.3956, 0.4990,  ..., 0.4405, 0.3837, 0.5223]],

         ...,

         [[0.5457, 0.5615, 0.6015,  ..., 0.5332, 0.3417, 0.5188],
          [0.5944, 0.5908, 0.6394,  ..., 0.5072, 0.3549, 0.5711],
          [0.4781, 0.5875, 0.4871,  ..., 0.5486, 0.4806, 0.4926],
          [0.5014, 0.5641, 0.6484,  ..., 0.3932, 0.4928, 0.4442]],

         [[0.5007, 0.4596, 0.5489,  ..., 0.4634, 0.5912, 0.5735],
          [0.4474, 0.5032, 0.4721,  ..., 0.4755, 0.4826, 0.3921],
          [0.3639, 0.6270, 0.4323,  ..., 0.6439, 0.5348, 0.4580],
          [0.4269, 0.4944, 0.5269,  ..., 0.5428, 0.3737, 0.6438]],

         [[0.4476, 0.5069, 0.4395,  ..., 0.5626, 0.4463, 0.3846],
          [0.7006, 0.3956, 0.5720,  ..., 0.4053, 0.4518, 0.4531],
          [0.3007, 0.4436, 0.3117,  ..., 0.6412, 0.5443, 0.3411],
          [0.5522, 0.4003, 0.4743,  ..., 0.4714, 0.4401, 0.6321]]],


        [[[0.5991, 0.3730, 0.5418,  ..., 0.4814, 0.3962, 0.5490],
          [0.5794, 0.5479, 0.4694,  ..., 0.3681, 0.5049, 0.3726],
          [0.5215, 0.5689, 0.5542,  ..., 0.6239, 0.5675, 0.4885],
          [0.4445, 0.5097, 0.4252,  ..., 0.5996, 0.5002, 0.4114]],

         [[0.2322, 0.6169, 0.5382,  ..., 0.4166, 0.4923, 0.5517],
          [0.5515, 0.4841, 0.6670,  ..., 0.4544, 0.4923, 0.5844],
          [0.3777, 0.3107, 0.4376,  ..., 0.5547, 0.6821, 0.4700],
          [0.5367, 0.5870, 0.5237,  ..., 0.6466, 0.4927, 0.5321]],

         [[0.6692, 0.6487, 0.5460,  ..., 0.5359, 0.4659, 0.5408],
          [0.5637, 0.4709, 0.4643,  ..., 0.3974, 0.4487, 0.5849],
          [0.4068, 0.6424, 0.4195,  ..., 0.5283, 0.5922, 0.3869],
          [0.5522, 0.7564, 0.5973,  ..., 0.6330, 0.3665, 0.4444]],

         ...,

         [[0.5622, 0.4634, 0.7142,  ..., 0.5081, 0.3452, 0.3635],
          [0.5760, 0.6270, 0.4568,  ..., 0.4242, 0.5922, 0.6411],
          [0.5972, 0.6951, 0.4860,  ..., 0.6043, 0.4224, 0.4386],
          [0.5107, 0.6077, 0.3285,  ..., 0.5003, 0.4852, 0.4374]],

         [[0.4498, 0.3032, 0.4682,  ..., 0.3974, 0.4267, 0.3690],
          [0.5541, 0.5946, 0.4371,  ..., 0.6118, 0.4401, 0.4529],
          [0.6901, 0.5327, 0.4086,  ..., 0.5152, 0.4795, 0.5759],
          [0.6198, 0.2598, 0.4527,  ..., 0.5208, 0.3478, 0.4880]],

         [[0.4283, 0.5385, 0.3928,  ..., 0.4044, 0.4888, 0.4927],
          [0.6005, 0.4050, 0.5897,  ..., 0.4256, 0.3055, 0.5234],
          [0.4963, 0.4617, 0.4200,  ..., 0.6688, 0.5472, 0.3708],
          [0.5384, 0.5324, 0.5423,  ..., 0.5754, 0.6923, 0.5074]]]],
       device='cuda:0')
tensor([[[[0.5267, 0.4946, 0.4330,  ..., 0.5780, 0.5501, 0.4668],
          [0.5578, 0.5161, 0.6123,  ..., 0.5936, 0.5226, 0.3844],
          [0.4073, 0.4707, 0.4738,  ..., 0.5404, 0.5537, 0.3928],
          [0.4168, 0.5617, 0.4258,  ..., 0.4891, 0.6343, 0.5220]],

         [[0.4588, 0.5149, 0.4827,  ..., 0.4569, 0.3612, 0.4699],
          [0.5879, 0.5472, 0.5409,  ..., 0.3567, 0.5075, 0.6021],
          [0.4278, 0.4369, 0.5061,  ..., 0.5195, 0.5950, 0.5627],
          [0.4518, 0.6118, 0.4239,  ..., 0.4951, 0.5694, 0.6049]],

         [[0.6531, 0.6179, 0.5665,  ..., 0.4026, 0.3390, 0.5475],
          [0.4031, 0.4769, 0.4964,  ..., 0.6234, 0.3766, 0.6243],
          [0.4958, 0.5913, 0.5244,  ..., 0.7607, 0.5960, 0.4661],
          [0.3513, 0.3956, 0.4930,  ..., 0.4345, 0.3757, 0.5263]],

         ...,

         [[0.5557, 0.5615, 0.5955,  ..., 0.5272, 0.3337, 0.5228],
          [0.6044, 0.5908, 0.6334,  ..., 0.5012, 0.3469, 0.5751],
          [0.4881, 0.5875, 0.4811,  ..., 0.5426, 0.4726, 0.4966],
          [0.5114, 0.5641, 0.6424,  ..., 0.3872, 0.4848, 0.4482]],

         [[0.5107, 0.4596, 0.5429,  ..., 0.4574, 0.5832, 0.5775],
          [0.4574, 0.5032, 0.4661,  ..., 0.4695, 0.4746, 0.3961],
          [0.3739, 0.6270, 0.4263,  ..., 0.6379, 0.5268, 0.4620],
          [0.4369, 0.4944, 0.5209,  ..., 0.5368, 0.3657, 0.6478]],

         [[0.4576, 0.5069, 0.4335,  ..., 0.5566, 0.4383, 0.3886],
          [0.7106, 0.3956, 0.5660,  ..., 0.3993, 0.4438, 0.4571],
          [0.3107, 0.4436, 0.3057,  ..., 0.6352, 0.5363, 0.3451],
          [0.5622, 0.4003, 0.4683,  ..., 0.4654, 0.4321, 0.6361]]],


        [[[0.6091, 0.3730, 0.5358,  ..., 0.4754, 0.3882, 0.5530],
          [0.5894, 0.5479, 0.4634,  ..., 0.3621, 0.4969, 0.3766],
          [0.5315, 0.5689, 0.5482,  ..., 0.6179, 0.5595, 0.4925],
          [0.4545, 0.5097, 0.4192,  ..., 0.5936, 0.4922, 0.4154]],

         [[0.2422, 0.6169, 0.5322,  ..., 0.4106, 0.4843, 0.5557],
          [0.5615, 0.4841, 0.6610,  ..., 0.4484, 0.4843, 0.5884],
          [0.3877, 0.3107, 0.4316,  ..., 0.5487, 0.6741, 0.4740],
          [0.5467, 0.5870, 0.5177,  ..., 0.6406, 0.4847, 0.5361]],

         [[0.6792, 0.6487, 0.5400,  ..., 0.5299, 0.4579, 0.5448],
          [0.5737, 0.4709, 0.4583,  ..., 0.3914, 0.4407, 0.5889],
          [0.4168, 0.6424, 0.4135,  ..., 0.5223, 0.5842, 0.3909],
          [0.5622, 0.7564, 0.5913,  ..., 0.6270, 0.3585, 0.4484]],

         ...,

         [[0.5722, 0.4634, 0.7082,  ..., 0.5021, 0.3372, 0.3675],
          [0.5860, 0.6270, 0.4508,  ..., 0.4182, 0.5842, 0.6451],
          [0.6072, 0.6951, 0.4800,  ..., 0.5983, 0.4144, 0.4426],
          [0.5207, 0.6077, 0.3225,  ..., 0.4943, 0.4772, 0.4414]],

         [[0.4598, 0.3032, 0.4622,  ..., 0.3914, 0.4187, 0.3730],
          [0.5641, 0.5946, 0.4311,  ..., 0.6058, 0.4321, 0.4569],
          [0.7001, 0.5327, 0.4026,  ..., 0.5092, 0.4715, 0.5799],
          [0.6298, 0.2598, 0.4467,  ..., 0.5148, 0.3398, 0.4920]],

         [[0.4383, 0.5385, 0.3868,  ..., 0.3984, 0.4808, 0.4967],
          [0.6105, 0.4050, 0.5837,  ..., 0.4196, 0.2975, 0.5274],
          [0.5063, 0.4617, 0.4140,  ..., 0.6628, 0.5392, 0.3748],
          [0.5484, 0.5324, 0.5363,  ..., 0.5694, 0.6843, 0.5114]]]],
       device='cuda:0', requires_grad=True)
tensor([-1.0000e-02,  2.3283e-10,  6.0000e-03,  2.0000e-03,  6.0000e-03,
         6.0000e-03, -6.0000e-03,  6.0000e-03,  8.0000e-03, -4.0000e-03],
       device='cuda:0')
selected experts tensor([1763, 1768, 1535, 1436, 1594, 1574, 1631, 1738, 1596, 1749],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4602, 0.3347, 0.4833,  ..., 0.4703, 0.6071, 0.5010],
          [0.5056, 0.5448, 0.3875,  ..., 0.5016, 0.4814, 0.4899],
          [0.4450, 0.6123, 0.4275,  ..., 0.5386, 0.5646, 0.5643],
          [0.4948, 0.6007, 0.5934,  ..., 0.4975, 0.5568, 0.4570]],

         [[0.5616, 0.6732, 0.4198,  ..., 0.6032, 0.5037, 0.5158],
          [0.4887, 0.6382, 0.3825,  ..., 0.6240, 0.5530, 0.5486],
          [0.5022, 0.3507, 0.6186,  ..., 0.4834, 0.6330, 0.3189],
          [0.4361, 0.2956, 0.4876,  ..., 0.4846, 0.5418, 0.5643]],

         [[0.4925, 0.4261, 0.4488,  ..., 0.4531, 0.5000, 0.5580],
          [0.6464, 0.4855, 0.4529,  ..., 0.5820, 0.6493, 0.5729],
          [0.5786, 0.3743, 0.6580,  ..., 0.5069, 0.5183, 0.4404],
          [0.4783, 0.3789, 0.5066,  ..., 0.5725, 0.5844, 0.6933]],

         ...,

         [[0.5189, 0.5257, 0.5829,  ..., 0.3219, 0.5045, 0.4633],
          [0.4203, 0.5521, 0.6419,  ..., 0.6845, 0.3914, 0.4858],
          [0.3742, 0.4132, 0.5367,  ..., 0.4314, 0.6740, 0.4984],
          [0.3779, 0.4208, 0.6223,  ..., 0.5284, 0.4907, 0.5729]],

         [[0.5255, 0.4879, 0.5408,  ..., 0.3786, 0.5421, 0.5355],
          [0.4791, 0.4729, 0.4437,  ..., 0.3084, 0.4419, 0.3697],
          [0.5191, 0.4540, 0.4701,  ..., 0.5232, 0.3762, 0.4745],
          [0.5078, 0.4899, 0.5057,  ..., 0.4875, 0.4299, 0.5025]],

         [[0.5389, 0.4117, 0.4978,  ..., 0.6509, 0.6466, 0.3996],
          [0.5676, 0.3938, 0.4824,  ..., 0.6666, 0.5421, 0.5976],
          [0.4846, 0.3845, 0.4464,  ..., 0.4350, 0.4304, 0.6196],
          [0.4327, 0.4198, 0.4956,  ..., 0.4892, 0.5462, 0.5135]]],


        [[[0.5844, 0.4819, 0.6473,  ..., 0.5490, 0.3432, 0.5934],
          [0.3500, 0.5419, 0.5160,  ..., 0.5468, 0.5535, 0.6223],
          [0.4156, 0.5834, 0.5938,  ..., 0.6429, 0.3895, 0.5219],
          [0.3395, 0.5658, 0.5635,  ..., 0.5587, 0.5093, 0.4926]],

         [[0.5381, 0.4523, 0.3871,  ..., 0.6321, 0.5108, 0.5519],
          [0.6320, 0.4122, 0.3580,  ..., 0.4853, 0.6043, 0.5618],
          [0.5621, 0.4977, 0.5114,  ..., 0.5820, 0.3672, 0.5138],
          [0.5149, 0.2538, 0.4512,  ..., 0.4531, 0.6169, 0.5962]],

         [[0.6000, 0.6192, 0.4711,  ..., 0.5625, 0.4986, 0.4628],
          [0.4971, 0.4636, 0.4318,  ..., 0.6321, 0.4309, 0.3829],
          [0.4565, 0.7046, 0.4423,  ..., 0.4827, 0.4506, 0.5705],
          [0.5762, 0.5195, 0.3871,  ..., 0.4740, 0.5506, 0.4322]],

         ...,

         [[0.4213, 0.5158, 0.5330,  ..., 0.4406, 0.4124, 0.5739],
          [0.4442, 0.3706, 0.5844,  ..., 0.4452, 0.4999, 0.5442],
          [0.5011, 0.6301, 0.5604,  ..., 0.5820, 0.5245, 0.4898],
          [0.5686, 0.5255, 0.6274,  ..., 0.4300, 0.6085, 0.5990]],

         [[0.5743, 0.4997, 0.4723,  ..., 0.4743, 0.3251, 0.4611],
          [0.6473, 0.3615, 0.5858,  ..., 0.5037, 0.5537, 0.5515],
          [0.4515, 0.6391, 0.4609,  ..., 0.4505, 0.6034, 0.4643],
          [0.5418, 0.4652, 0.4275,  ..., 0.3795, 0.4072, 0.7016]],

         [[0.4850, 0.5089, 0.4767,  ..., 0.3749, 0.4936, 0.3922],
          [0.5544, 0.5758, 0.5495,  ..., 0.3527, 0.4673, 0.4755],
          [0.4965, 0.3182, 0.4679,  ..., 0.6312, 0.4789, 0.3788],
          [0.4878, 0.6489, 0.6014,  ..., 0.4924, 0.3965, 0.5122]]]],
       device='cuda:0')
tensor([[[[0.4562, 0.3407, 0.4793,  ..., 0.4743, 0.6011, 0.4970],
          [0.5016, 0.5508, 0.3835,  ..., 0.5056, 0.4754, 0.4859],
          [0.4410, 0.6183, 0.4235,  ..., 0.5426, 0.5586, 0.5603],
          [0.4908, 0.6067, 0.5894,  ..., 0.5015, 0.5508, 0.4530]],

         [[0.5576, 0.6792, 0.4158,  ..., 0.6072, 0.4977, 0.5118],
          [0.4847, 0.6442, 0.3785,  ..., 0.6280, 0.5470, 0.5446],
          [0.4982, 0.3567, 0.6146,  ..., 0.4874, 0.6270, 0.3149],
          [0.4321, 0.3016, 0.4836,  ..., 0.4886, 0.5358, 0.5603]],

         [[0.4885, 0.4321, 0.4448,  ..., 0.4571, 0.4940, 0.5540],
          [0.6424, 0.4915, 0.4489,  ..., 0.5860, 0.6433, 0.5689],
          [0.5746, 0.3803, 0.6540,  ..., 0.5109, 0.5123, 0.4364],
          [0.4743, 0.3849, 0.5026,  ..., 0.5765, 0.5784, 0.6893]],

         ...,

         [[0.5149, 0.5317, 0.5789,  ..., 0.3259, 0.4985, 0.4593],
          [0.4163, 0.5581, 0.6379,  ..., 0.6885, 0.3854, 0.4818],
          [0.3702, 0.4192, 0.5327,  ..., 0.4354, 0.6680, 0.4944],
          [0.3739, 0.4268, 0.6183,  ..., 0.5324, 0.4847, 0.5689]],

         [[0.5215, 0.4939, 0.5368,  ..., 0.3826, 0.5361, 0.5315],
          [0.4751, 0.4789, 0.4397,  ..., 0.3124, 0.4359, 0.3657],
          [0.5151, 0.4600, 0.4661,  ..., 0.5272, 0.3702, 0.4705],
          [0.5038, 0.4959, 0.5017,  ..., 0.4915, 0.4239, 0.4985]],

         [[0.5349, 0.4177, 0.4938,  ..., 0.6549, 0.6406, 0.3956],
          [0.5636, 0.3998, 0.4784,  ..., 0.6706, 0.5361, 0.5936],
          [0.4806, 0.3905, 0.4424,  ..., 0.4390, 0.4244, 0.6156],
          [0.4287, 0.4258, 0.4916,  ..., 0.4932, 0.5402, 0.5095]]],


        [[[0.5804, 0.4879, 0.6433,  ..., 0.5530, 0.3372, 0.5894],
          [0.3460, 0.5479, 0.5120,  ..., 0.5508, 0.5475, 0.6183],
          [0.4116, 0.5894, 0.5898,  ..., 0.6469, 0.3835, 0.5179],
          [0.3355, 0.5718, 0.5595,  ..., 0.5627, 0.5033, 0.4886]],

         [[0.5341, 0.4583, 0.3831,  ..., 0.6361, 0.5048, 0.5479],
          [0.6280, 0.4182, 0.3540,  ..., 0.4893, 0.5983, 0.5578],
          [0.5581, 0.5037, 0.5074,  ..., 0.5860, 0.3612, 0.5098],
          [0.5109, 0.2598, 0.4472,  ..., 0.4571, 0.6109, 0.5922]],

         [[0.5960, 0.6252, 0.4671,  ..., 0.5665, 0.4926, 0.4588],
          [0.4931, 0.4696, 0.4278,  ..., 0.6361, 0.4249, 0.3789],
          [0.4525, 0.7106, 0.4383,  ..., 0.4867, 0.4446, 0.5665],
          [0.5722, 0.5255, 0.3831,  ..., 0.4780, 0.5446, 0.4282]],

         ...,

         [[0.4173, 0.5218, 0.5290,  ..., 0.4446, 0.4064, 0.5699],
          [0.4402, 0.3766, 0.5804,  ..., 0.4492, 0.4939, 0.5402],
          [0.4971, 0.6361, 0.5564,  ..., 0.5860, 0.5185, 0.4858],
          [0.5646, 0.5315, 0.6234,  ..., 0.4340, 0.6025, 0.5950]],

         [[0.5703, 0.5057, 0.4683,  ..., 0.4783, 0.3191, 0.4571],
          [0.6433, 0.3675, 0.5818,  ..., 0.5077, 0.5477, 0.5475],
          [0.4475, 0.6451, 0.4569,  ..., 0.4545, 0.5974, 0.4603],
          [0.5378, 0.4712, 0.4235,  ..., 0.3835, 0.4012, 0.6976]],

         [[0.4810, 0.5149, 0.4727,  ..., 0.3789, 0.4876, 0.3882],
          [0.5504, 0.5818, 0.5455,  ..., 0.3567, 0.4613, 0.4715],
          [0.4925, 0.3242, 0.4639,  ..., 0.6352, 0.4729, 0.3748],
          [0.4838, 0.6549, 0.5974,  ..., 0.4964, 0.3905, 0.5082]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0040, -0.0060,  0.0040, -0.0020,  0.0020,  0.0040, -0.0080, -0.0040,
         0.0060,  0.0040], device='cuda:0')
selected experts tensor([1617, 1736, 1591, 1533, 1675, 1620, 1634, 1701, 1690, 1587],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.3909, 0.4601, 0.5142,  ..., 0.4361, 0.4648, 0.4752],
          [0.5188, 0.5223, 0.4644,  ..., 0.4274, 0.5537, 0.5396],
          [0.6469, 0.4847, 0.3613,  ..., 0.5165, 0.5349, 0.5526],
          [0.5590, 0.4070, 0.3984,  ..., 0.4642, 0.5585, 0.6551]],

         [[0.5332, 0.4381, 0.4637,  ..., 0.4814, 0.4231, 0.5258],
          [0.5884, 0.4789, 0.4919,  ..., 0.5633, 0.4489, 0.4302],
          [0.4453, 0.6063, 0.6661,  ..., 0.4694, 0.4718, 0.3004],
          [0.5400, 0.4773, 0.4407,  ..., 0.6746, 0.4269, 0.5938]],

         [[0.5248, 0.3947, 0.5191,  ..., 0.3469, 0.3414, 0.4138],
          [0.5238, 0.5122, 0.5370,  ..., 0.5050, 0.4393, 0.3165],
          [0.4187, 0.4599, 0.6353,  ..., 0.4688, 0.4619, 0.5037],
          [0.4864, 0.4786, 0.3487,  ..., 0.6661, 0.3804, 0.4753]],

         ...,

         [[0.3863, 0.4800, 0.3951,  ..., 0.3927, 0.5372, 0.6482],
          [0.4730, 0.6698, 0.5103,  ..., 0.4320, 0.3440, 0.4287],
          [0.3693, 0.5143, 0.4633,  ..., 0.3559, 0.5080, 0.4292],
          [0.4661, 0.2523, 0.5681,  ..., 0.5685, 0.5099, 0.5598]],

         [[0.3730, 0.2972, 0.6539,  ..., 0.4069, 0.4085, 0.5437],
          [0.4980, 0.5545, 0.3686,  ..., 0.6272, 0.3696, 0.3878],
          [0.3808, 0.4963, 0.5776,  ..., 0.2609, 0.4935, 0.5068],
          [0.4961, 0.3114, 0.4709,  ..., 0.5118, 0.4810, 0.5131]],

         [[0.5025, 0.3715, 0.4763,  ..., 0.5163, 0.4146, 0.4558],
          [0.5751, 0.5173, 0.5153,  ..., 0.4193, 0.3615, 0.4158],
          [0.5074, 0.6031, 0.5818,  ..., 0.3755, 0.5081, 0.4926],
          [0.6741, 0.3579, 0.4443,  ..., 0.4494, 0.3909, 0.6295]]],


        [[[0.4211, 0.5557, 0.4910,  ..., 0.6154, 0.4006, 0.7034],
          [0.4896, 0.5027, 0.4742,  ..., 0.4361, 0.4735, 0.5688],
          [0.5789, 0.3507, 0.6245,  ..., 0.4642, 0.5428, 0.4263],
          [0.4325, 0.3624, 0.5441,  ..., 0.5194, 0.5732, 0.4751]],

         [[0.5201, 0.5815, 0.4506,  ..., 0.3613, 0.3974, 0.3724],
          [0.5375, 0.4398, 0.4949,  ..., 0.4799, 0.4189, 0.5873],
          [0.6016, 0.5313, 0.4857,  ..., 0.5851, 0.3750, 0.5103],
          [0.4345, 0.5162, 0.4107,  ..., 0.4956, 0.4505, 0.6150]],

         [[0.5993, 0.5782, 0.4857,  ..., 0.4392, 0.4015, 0.3358],
          [0.6270, 0.4579, 0.4603,  ..., 0.4809, 0.4672, 0.3555],
          [0.4815, 0.4758, 0.6539,  ..., 0.4958, 0.4914, 0.4439],
          [0.5784, 0.3243, 0.2798,  ..., 0.6172, 0.3388, 0.6132]],

         ...,

         [[0.5289, 0.4696, 0.3769,  ..., 0.5520, 0.5366, 0.4754],
          [0.6325, 0.4747, 0.5652,  ..., 0.4637, 0.3678, 0.4578],
          [0.5106, 0.6192, 0.5380,  ..., 0.4178, 0.3269, 0.5272],
          [0.6307, 0.4475, 0.5035,  ..., 0.4981, 0.3046, 0.5107]],

         [[0.5865, 0.3943, 0.4593,  ..., 0.3718, 0.2556, 0.4229],
          [0.4906, 0.5244, 0.4293,  ..., 0.5081, 0.5231, 0.5425],
          [0.4979, 0.5110, 0.4031,  ..., 0.3640, 0.3687, 0.5924],
          [0.4397, 0.4151, 0.6145,  ..., 0.3760, 0.4609, 0.5185]],

         [[0.4230, 0.4371, 0.4636,  ..., 0.5168, 0.5761, 0.5188],
          [0.4306, 0.4750, 0.5155,  ..., 0.3984, 0.3741, 0.4253],
          [0.5823, 0.5101, 0.6308,  ..., 0.3505, 0.5118, 0.4524],
          [0.4369, 0.3910, 0.4245,  ..., 0.6245, 0.5111, 0.4539]]]],
       device='cuda:0')
tensor([[[[0.3909, 0.4661, 0.5222,  ..., 0.4441, 0.4528, 0.4872],
          [0.5188, 0.5283, 0.4724,  ..., 0.4354, 0.5417, 0.5516],
          [0.6469, 0.4907, 0.3693,  ..., 0.5245, 0.5229, 0.5646],
          [0.5590, 0.4130, 0.4064,  ..., 0.4722, 0.5465, 0.6671]],

         [[0.5332, 0.4441, 0.4717,  ..., 0.4894, 0.4111, 0.5378],
          [0.5884, 0.4849, 0.4999,  ..., 0.5713, 0.4369, 0.4422],
          [0.4453, 0.6123, 0.6741,  ..., 0.4774, 0.4598, 0.3124],
          [0.5400, 0.4833, 0.4487,  ..., 0.6826, 0.4149, 0.6058]],

         [[0.5248, 0.4007, 0.5271,  ..., 0.3549, 0.3294, 0.4258],
          [0.5238, 0.5182, 0.5450,  ..., 0.5130, 0.4273, 0.3285],
          [0.4187, 0.4659, 0.6433,  ..., 0.4768, 0.4499, 0.5157],
          [0.4864, 0.4846, 0.3567,  ..., 0.6741, 0.3684, 0.4873]],

         ...,

         [[0.3863, 0.4860, 0.4031,  ..., 0.4007, 0.5252, 0.6602],
          [0.4730, 0.6758, 0.5183,  ..., 0.4400, 0.3320, 0.4407],
          [0.3693, 0.5203, 0.4713,  ..., 0.3639, 0.4960, 0.4412],
          [0.4661, 0.2583, 0.5761,  ..., 0.5765, 0.4979, 0.5718]],

         [[0.3730, 0.3032, 0.6619,  ..., 0.4149, 0.3965, 0.5557],
          [0.4980, 0.5605, 0.3766,  ..., 0.6352, 0.3576, 0.3998],
          [0.3808, 0.5023, 0.5856,  ..., 0.2689, 0.4815, 0.5188],
          [0.4961, 0.3174, 0.4789,  ..., 0.5198, 0.4690, 0.5251]],

         [[0.5025, 0.3775, 0.4843,  ..., 0.5243, 0.4026, 0.4678],
          [0.5751, 0.5233, 0.5233,  ..., 0.4273, 0.3495, 0.4278],
          [0.5074, 0.6091, 0.5898,  ..., 0.3835, 0.4961, 0.5046],
          [0.6741, 0.3639, 0.4523,  ..., 0.4574, 0.3789, 0.6415]]],


        [[[0.4211, 0.5617, 0.4990,  ..., 0.6234, 0.3886, 0.7154],
          [0.4896, 0.5087, 0.4822,  ..., 0.4441, 0.4615, 0.5808],
          [0.5789, 0.3567, 0.6325,  ..., 0.4722, 0.5308, 0.4383],
          [0.4325, 0.3684, 0.5521,  ..., 0.5274, 0.5612, 0.4871]],

         [[0.5201, 0.5875, 0.4586,  ..., 0.3693, 0.3854, 0.3844],
          [0.5375, 0.4458, 0.5029,  ..., 0.4879, 0.4069, 0.5993],
          [0.6016, 0.5373, 0.4937,  ..., 0.5931, 0.3630, 0.5223],
          [0.4345, 0.5222, 0.4187,  ..., 0.5036, 0.4385, 0.6270]],

         [[0.5993, 0.5842, 0.4937,  ..., 0.4472, 0.3895, 0.3478],
          [0.6270, 0.4639, 0.4683,  ..., 0.4889, 0.4552, 0.3675],
          [0.4815, 0.4818, 0.6619,  ..., 0.5038, 0.4794, 0.4559],
          [0.5784, 0.3303, 0.2878,  ..., 0.6252, 0.3268, 0.6252]],

         ...,

         [[0.5289, 0.4756, 0.3849,  ..., 0.5600, 0.5246, 0.4874],
          [0.6325, 0.4807, 0.5732,  ..., 0.4717, 0.3558, 0.4698],
          [0.5106, 0.6252, 0.5460,  ..., 0.4258, 0.3149, 0.5392],
          [0.6307, 0.4535, 0.5115,  ..., 0.5061, 0.2926, 0.5227]],

         [[0.5865, 0.4003, 0.4673,  ..., 0.3798, 0.2436, 0.4349],
          [0.4906, 0.5304, 0.4373,  ..., 0.5161, 0.5111, 0.5545],
          [0.4979, 0.5170, 0.4111,  ..., 0.3720, 0.3567, 0.6044],
          [0.4397, 0.4211, 0.6225,  ..., 0.3840, 0.4489, 0.5305]],

         [[0.4230, 0.4431, 0.4716,  ..., 0.5248, 0.5641, 0.5308],
          [0.4306, 0.4810, 0.5235,  ..., 0.4064, 0.3621, 0.4373],
          [0.5823, 0.5161, 0.6388,  ..., 0.3585, 0.4998, 0.4644],
          [0.4369, 0.3970, 0.4325,  ..., 0.6325, 0.4991, 0.4659]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([ 0.0000, -0.0060, -0.0080,  0.0060, -0.0040, -0.0020,  0.0000, -0.0080,
         0.0120, -0.0120], device='cuda:0')
selected experts tensor([1777, 1857, 1678, 1440, 1720, 1693, 1667, 1853,  673, 2026],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.6038, 0.5887, 0.6099,  ..., 0.4950, 0.4026, 0.5629],
          [0.6252, 0.5088, 0.3467,  ..., 0.3826, 0.5860, 0.4756],
          [0.4896, 0.5232, 0.4513,  ..., 0.4280, 0.6984, 0.5532],
          [0.5052, 0.4585, 0.6127,  ..., 0.6919, 0.4939, 0.5297]],

         [[0.5585, 0.5123, 0.6430,  ..., 0.5245, 0.4799, 0.6403],
          [0.4157, 0.4209, 0.5264,  ..., 0.3817, 0.4863, 0.4682],
          [0.5188, 0.3681, 0.4945,  ..., 0.5050, 0.6487, 0.4953],
          [0.7134, 0.4877, 0.4035,  ..., 0.5849, 0.4017, 0.6285]],

         [[0.5711, 0.5421, 0.6015,  ..., 0.4697, 0.5101, 0.4857],
          [0.3937, 0.5682, 0.5336,  ..., 0.5309, 0.5784, 0.4361],
          [0.4091, 0.4702, 0.5353,  ..., 0.4271, 0.5414, 0.6275],
          [0.6376, 0.4503, 0.4295,  ..., 0.6877, 0.7372, 0.5892]],

         ...,

         [[0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060],
          [0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060],
          [0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060],
          [0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060]],

         [[0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060],
          [0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060],
          [0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060],
          [0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060]],

         [[0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060],
          [0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060],
          [0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060],
          [0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060]]],


        [[[0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060],
          [0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060],
          [0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060],
          [0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060]],

         [[0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060],
          [0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060],
          [0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060],
          [0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060]],

         [[0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060],
          [0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060],
          [0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060],
          [0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060]],

         ...,

         [[0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060],
          [0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060],
          [0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060],
          [0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060]],

         [[0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060],
          [0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060],
          [0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060],
          [0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060]],

         [[0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060],
          [0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060],
          [0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060],
          [0.5060, 0.5060, 0.5060,  ..., 0.5060, 0.5000, 0.5060]]]],
       device='cuda:0')
tensor([[[[0.5978, 0.5827, 0.6039,  ..., 0.4890, 0.4026, 0.5569],
          [0.6192, 0.5028, 0.3407,  ..., 0.3766, 0.5860, 0.4696],
          [0.4836, 0.5172, 0.4453,  ..., 0.4220, 0.6984, 0.5472],
          [0.4992, 0.4525, 0.6067,  ..., 0.6859, 0.4939, 0.5237]],

         [[0.5525, 0.5063, 0.6370,  ..., 0.5185, 0.4799, 0.6343],
          [0.4097, 0.4149, 0.5204,  ..., 0.3757, 0.4863, 0.4622],
          [0.5128, 0.3621, 0.4885,  ..., 0.4990, 0.6487, 0.4893],
          [0.7074, 0.4817, 0.3975,  ..., 0.5789, 0.4017, 0.6225]],

         [[0.5651, 0.5361, 0.5955,  ..., 0.4637, 0.5101, 0.4797],
          [0.3877, 0.5622, 0.5276,  ..., 0.5249, 0.5784, 0.4301],
          [0.4031, 0.4642, 0.5293,  ..., 0.4211, 0.5414, 0.6215],
          [0.6316, 0.4443, 0.4235,  ..., 0.6817, 0.7372, 0.5832]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([ 6.0000e-03,  6.0000e-03,  6.0000e-03,  6.0000e-03,  6.0000e-03,
         6.0000e-03,  6.0000e-03,  6.0000e-03, -2.3283e-10,  6.0000e-03],
       device='cuda:0')
selected experts tensor([3990, 3962,  609,  260,  616,  589,  487,  282,  981,  512],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5386, 0.4962, 0.5725,  ..., 0.5934, 0.5022, 0.5308],
          [0.6587, 0.6146, 0.4492,  ..., 0.4237, 0.4377, 0.5844],
          [0.4163, 0.3683, 0.5122,  ..., 0.4924, 0.5741, 0.6926],
          [0.3014, 0.4049, 0.4448,  ..., 0.5035, 0.4802, 0.3598]],

         [[0.5476, 0.4115, 0.5230,  ..., 0.5026, 0.5446, 0.5342],
          [0.4899, 0.6396, 0.6226,  ..., 0.6224, 0.5642, 0.4417],
          [0.3610, 0.5264, 0.5307,  ..., 0.4765, 0.6692, 0.5286],
          [0.4963, 0.4515, 0.5242,  ..., 0.5125, 0.6424, 0.4456]],

         [[0.4469, 0.5093, 0.5192,  ..., 0.5710, 0.5288, 0.4847],
          [0.5236, 0.5158, 0.5959,  ..., 0.6238, 0.4018, 0.6165],
          [0.5191, 0.4898, 0.5286,  ..., 0.4930, 0.4519, 0.5025],
          [0.4283, 0.3388, 0.6404,  ..., 0.4308, 0.4135, 0.5211]],

         ...,

         [[0.4532, 0.5841, 0.5394,  ..., 0.3707, 0.5453, 0.4601],
          [0.5032, 0.4320, 0.6179,  ..., 0.4223, 0.6753, 0.4466],
          [0.4659, 0.4766, 0.3855,  ..., 0.5284, 0.5396, 0.6338],
          [0.3739, 0.5626, 0.5663,  ..., 0.5420, 0.6505, 0.5035]],

         [[0.3693, 0.5262, 0.5281,  ..., 0.3950, 0.6162, 0.6156],
          [0.5887, 0.4306, 0.3790,  ..., 0.4242, 0.5198, 0.5844],
          [0.3776, 0.4101, 0.5375,  ..., 0.4361, 0.3930, 0.4670],
          [0.3977, 0.6150, 0.4530,  ..., 0.5815, 0.6406, 0.5834]],

         [[0.4573, 0.4781, 0.4843,  ..., 0.6546, 0.4425, 0.6284],
          [0.4822, 0.4287, 0.2924,  ..., 0.5283, 0.5261, 0.5989],
          [0.6197, 0.4540, 0.4997,  ..., 0.3932, 0.4957, 0.4223],
          [0.5911, 0.5621, 0.5489,  ..., 0.4361, 0.5774, 0.4842]]],


        [[[0.2816, 0.5907, 0.6680,  ..., 0.6465, 0.3747, 0.4185],
          [0.5101, 0.5491, 0.4210,  ..., 0.5619, 0.5041, 0.3767],
          [0.5579, 0.3665, 0.6776,  ..., 0.5316, 0.4553, 0.5114],
          [0.4929, 0.5200, 0.5721,  ..., 0.5609, 0.5927, 0.3357]],

         [[0.4756, 0.4106, 0.4952,  ..., 0.7671, 0.5141, 0.5754],
          [0.5622, 0.2860, 0.3530,  ..., 0.4549, 0.5803, 0.5536],
          [0.3538, 0.6459, 0.4809,  ..., 0.6275, 0.5429, 0.4548],
          [0.5593, 0.4785, 0.4319,  ..., 0.5020, 0.6050, 0.3967]],

         [[0.4345, 0.5590, 0.4054,  ..., 0.4006, 0.5480, 0.5663],
          [0.5167, 0.5585, 0.4045,  ..., 0.6145, 0.5721, 0.5839],
          [0.3610, 0.5124, 0.6896,  ..., 0.4479, 0.4512, 0.4982],
          [0.4367, 0.3674, 0.5178,  ..., 0.5882, 0.5142, 0.5696]],

         ...,

         [[0.5015, 0.5650, 0.4049,  ..., 0.5365, 0.4964, 0.4621],
          [0.5435, 0.5023, 0.5102,  ..., 0.5734, 0.5717, 0.3499],
          [0.5367, 0.5269, 0.5973,  ..., 0.5362, 0.4763, 0.4132],
          [0.5190, 0.4629, 0.5247,  ..., 0.5250, 0.4258, 0.3976]],

         [[0.4689, 0.5221, 0.4886,  ..., 0.5739, 0.5451, 0.5994],
          [0.5526, 0.5086, 0.4480,  ..., 0.4699, 0.6514, 0.4606],
          [0.3601, 0.5959, 0.4504,  ..., 0.5696, 0.5017, 0.3517],
          [0.6570, 0.5698, 0.4687,  ..., 0.4520, 0.5582, 0.4461]],

         [[0.5526, 0.5219, 0.4748,  ..., 0.4503, 0.4582, 0.5301],
          [0.4153, 0.5298, 0.5293,  ..., 0.5177, 0.5913, 0.5591],
          [0.3813, 0.3638, 0.5491,  ..., 0.4787, 0.6241, 0.6604],
          [0.5304, 0.4938, 0.4110,  ..., 0.4366, 0.6415, 0.4674]]]],
       device='cuda:0')
tensor([[[[0.5496, 0.4972, 0.5655,  ..., 0.5884, 0.4932, 0.5358],
          [0.6697, 0.6156, 0.4422,  ..., 0.4187, 0.4287, 0.5894],
          [0.4273, 0.3693, 0.5052,  ..., 0.4874, 0.5651, 0.6976],
          [0.3124, 0.4059, 0.4378,  ..., 0.4985, 0.4712, 0.3648]],

         [[0.5586, 0.4125, 0.5160,  ..., 0.4976, 0.5356, 0.5392],
          [0.5009, 0.6406, 0.6156,  ..., 0.6174, 0.5552, 0.4467],
          [0.3720, 0.5274, 0.5237,  ..., 0.4715, 0.6602, 0.5336],
          [0.5073, 0.4525, 0.5172,  ..., 0.5075, 0.6334, 0.4506]],

         [[0.4579, 0.5103, 0.5122,  ..., 0.5660, 0.5198, 0.4897],
          [0.5346, 0.5168, 0.5889,  ..., 0.6188, 0.3928, 0.6215],
          [0.5301, 0.4908, 0.5216,  ..., 0.4880, 0.4429, 0.5075],
          [0.4393, 0.3398, 0.6334,  ..., 0.4258, 0.4045, 0.5261]],

         ...,

         [[0.4642, 0.5851, 0.5324,  ..., 0.3657, 0.5363, 0.4651],
          [0.5142, 0.4330, 0.6109,  ..., 0.4173, 0.6663, 0.4516],
          [0.4769, 0.4776, 0.3785,  ..., 0.5234, 0.5306, 0.6388],
          [0.3849, 0.5636, 0.5593,  ..., 0.5370, 0.6415, 0.5085]],

         [[0.3803, 0.5272, 0.5211,  ..., 0.3900, 0.6072, 0.6206],
          [0.5997, 0.4316, 0.3720,  ..., 0.4192, 0.5108, 0.5894],
          [0.3886, 0.4111, 0.5305,  ..., 0.4311, 0.3840, 0.4720],
          [0.4087, 0.6160, 0.4460,  ..., 0.5765, 0.6316, 0.5884]],

         [[0.4683, 0.4791, 0.4773,  ..., 0.6496, 0.4335, 0.6334],
          [0.4932, 0.4297, 0.2854,  ..., 0.5233, 0.5171, 0.6039],
          [0.6307, 0.4550, 0.4927,  ..., 0.3882, 0.4867, 0.4273],
          [0.6021, 0.5631, 0.5419,  ..., 0.4311, 0.5684, 0.4892]]],


        [[[0.2926, 0.5917, 0.6610,  ..., 0.6415, 0.3657, 0.4235],
          [0.5211, 0.5501, 0.4140,  ..., 0.5569, 0.4951, 0.3817],
          [0.5689, 0.3675, 0.6706,  ..., 0.5266, 0.4463, 0.5164],
          [0.5039, 0.5210, 0.5651,  ..., 0.5559, 0.5837, 0.3407]],

         [[0.4866, 0.4116, 0.4882,  ..., 0.7621, 0.5051, 0.5804],
          [0.5732, 0.2870, 0.3460,  ..., 0.4499, 0.5713, 0.5586],
          [0.3648, 0.6469, 0.4739,  ..., 0.6225, 0.5339, 0.4598],
          [0.5703, 0.4795, 0.4249,  ..., 0.4970, 0.5960, 0.4017]],

         [[0.4455, 0.5600, 0.3984,  ..., 0.3956, 0.5390, 0.5713],
          [0.5277, 0.5595, 0.3975,  ..., 0.6095, 0.5631, 0.5889],
          [0.3720, 0.5134, 0.6826,  ..., 0.4429, 0.4422, 0.5032],
          [0.4477, 0.3684, 0.5108,  ..., 0.5832, 0.5052, 0.5746]],

         ...,

         [[0.5125, 0.5660, 0.3979,  ..., 0.5315, 0.4874, 0.4671],
          [0.5545, 0.5033, 0.5032,  ..., 0.5684, 0.5627, 0.3549],
          [0.5477, 0.5279, 0.5903,  ..., 0.5312, 0.4673, 0.4182],
          [0.5300, 0.4639, 0.5177,  ..., 0.5200, 0.4168, 0.4026]],

         [[0.4799, 0.5231, 0.4816,  ..., 0.5689, 0.5361, 0.6044],
          [0.5636, 0.5096, 0.4410,  ..., 0.4649, 0.6424, 0.4656],
          [0.3711, 0.5969, 0.4434,  ..., 0.5646, 0.4927, 0.3567],
          [0.6680, 0.5708, 0.4617,  ..., 0.4470, 0.5492, 0.4511]],

         [[0.5636, 0.5229, 0.4678,  ..., 0.4453, 0.4492, 0.5351],
          [0.4263, 0.5308, 0.5223,  ..., 0.5127, 0.5823, 0.5641],
          [0.3923, 0.3648, 0.5421,  ..., 0.4737, 0.6151, 0.6654],
          [0.5414, 0.4948, 0.4040,  ..., 0.4316, 0.6325, 0.4724]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0110, -0.0010,  0.0070,  0.0030,  0.0070,  0.0070, -0.0050,  0.0050,
         0.0090, -0.0050], device='cuda:0')
selected experts tensor([1673, 1702, 1602, 1609, 1627, 1663, 1641, 1586, 1659, 1622],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4048, 0.4540, 0.5028,  ..., 0.3082, 0.4893, 0.5229],
          [0.6066, 0.4845, 0.4114,  ..., 0.4555, 0.5796, 0.3224],
          [0.3789, 0.3980, 0.4988,  ..., 0.4526, 0.4697, 0.5310],
          [0.3913, 0.3807, 0.6456,  ..., 0.4592, 0.3807, 0.5268]],

         [[0.6252, 0.4294, 0.6173,  ..., 0.3689, 0.6256, 0.4356],
          [0.5435, 0.6601, 0.5527,  ..., 0.5062, 0.4491, 0.4830],
          [0.5233, 0.4340, 0.4256,  ..., 0.5957, 0.6816, 0.4011],
          [0.4414, 0.5520, 0.4527,  ..., 0.4897, 0.4590, 0.4351]],

         [[0.4270, 0.3900, 0.3680,  ..., 0.4180, 0.3908, 0.6196],
          [0.5887, 0.3551, 0.5568,  ..., 0.5581, 0.3752, 0.5117],
          [0.4847, 0.4265, 0.5030,  ..., 0.6525, 0.4607, 0.4015],
          [0.5488, 0.6497, 0.5508,  ..., 0.5639, 0.5534, 0.4371]],

         ...,

         [[0.4643, 0.6470, 0.5583,  ..., 0.4444, 0.4342, 0.4147],
          [0.5554, 0.5373, 0.5645,  ..., 0.3698, 0.4034, 0.3789],
          [0.5104, 0.3895, 0.5159,  ..., 0.6311, 0.5235, 0.6608],
          [0.5868, 0.6696, 0.4385,  ..., 0.5692, 0.4779, 0.4137]],

         [[0.5925, 0.5571, 0.4342,  ..., 0.5201, 0.4571, 0.5242],
          [0.5445, 0.6104, 0.5423,  ..., 0.6013, 0.3431, 0.3457],
          [0.4983, 0.4691, 0.4952,  ..., 0.2908, 0.4590, 0.3969],
          [0.6145, 0.3267, 0.5710,  ..., 0.3589, 0.5602, 0.4385]],

         [[0.5977, 0.4862, 0.4846,  ..., 0.5605, 0.6284, 0.5517],
          [0.4496, 0.5130, 0.5454,  ..., 0.3366, 0.3528, 0.5471],
          [0.5306, 0.4886, 0.4682,  ..., 0.5961, 0.3431, 0.6660],
          [0.4414, 0.4926, 0.4839,  ..., 0.4801, 0.5981, 0.4996]]],


        [[[0.4261, 0.4489, 0.6293,  ..., 0.5408, 0.5447, 0.3816],
          [0.5248, 0.5111, 0.4227,  ..., 0.4857, 0.4090, 0.6311],
          [0.6256, 0.4606, 0.3780,  ..., 0.3897, 0.4973, 0.5072],
          [0.5725, 0.5313, 0.5715,  ..., 0.5668, 0.7124, 0.5025]],

         [[0.4883, 0.5908, 0.3413,  ..., 0.5410, 0.5991, 0.5425],
          [0.5111, 0.4986, 0.5117,  ..., 0.3192, 0.6302, 0.6375],
          [0.6168, 0.4746, 0.5043,  ..., 0.5555, 0.6252, 0.4879],
          [0.4626, 0.4678, 0.5190,  ..., 0.3580, 0.4067, 0.5344]],

         [[0.6993, 0.4749, 0.6678,  ..., 0.5410, 0.5585, 0.5372],
          [0.4053, 0.5026, 0.3992,  ..., 0.4592, 0.5239, 0.3780],
          [0.3545, 0.3909, 0.4900,  ..., 0.6036, 0.4479, 0.4404],
          [0.4299, 0.4657, 0.5057,  ..., 0.6239, 0.4714, 0.6850]],

         ...,

         [[0.4697, 0.4695, 0.6909,  ..., 0.5562, 0.4906, 0.5758],
          [0.5782, 0.5120, 0.5045,  ..., 0.5555, 0.5246, 0.5986],
          [0.5101, 0.6150, 0.5344,  ..., 0.3410, 0.3770, 0.5332],
          [0.5131, 0.4470, 0.5109,  ..., 0.6401, 0.4414, 0.4947]],

         [[0.5114, 0.4577, 0.4756,  ..., 0.5787, 0.4532, 0.5681],
          [0.4194, 0.4615, 0.5372,  ..., 0.5639, 0.5464, 0.5083],
          [0.5177, 0.4702, 0.4438,  ..., 0.5187, 0.6000, 0.4299],
          [0.5532, 0.4688, 0.5953,  ..., 0.4427, 0.4208, 0.5215]],

         [[0.4646, 0.4581, 0.3973,  ..., 0.5179, 0.3057, 0.4261],
          [0.5266, 0.4212, 0.5066,  ..., 0.5376, 0.4950, 0.5686],
          [0.6348, 0.5652, 0.4156,  ..., 0.4536, 0.5221, 0.5260],
          [0.6108, 0.5388, 0.4646,  ..., 0.3771, 0.6465, 0.4484]]]],
       device='cuda:0')
tensor([[[[0.3998, 0.4610, 0.4978,  ..., 0.3132, 0.4843, 0.5179],
          [0.6016, 0.4915, 0.4064,  ..., 0.4605, 0.5746, 0.3174],
          [0.3739, 0.4050, 0.4938,  ..., 0.4576, 0.4647, 0.5260],
          [0.3863, 0.3877, 0.6406,  ..., 0.4642, 0.3757, 0.5218]],

         [[0.6202, 0.4364, 0.6123,  ..., 0.3739, 0.6206, 0.4306],
          [0.5385, 0.6671, 0.5477,  ..., 0.5112, 0.4441, 0.4780],
          [0.5183, 0.4410, 0.4206,  ..., 0.6007, 0.6766, 0.3961],
          [0.4364, 0.5590, 0.4477,  ..., 0.4947, 0.4540, 0.4301]],

         [[0.4220, 0.3970, 0.3630,  ..., 0.4230, 0.3858, 0.6146],
          [0.5837, 0.3621, 0.5518,  ..., 0.5631, 0.3702, 0.5067],
          [0.4797, 0.4335, 0.4980,  ..., 0.6575, 0.4557, 0.3965],
          [0.5438, 0.6567, 0.5458,  ..., 0.5689, 0.5484, 0.4321]],

         ...,

         [[0.4593, 0.6540, 0.5533,  ..., 0.4494, 0.4292, 0.4097],
          [0.5504, 0.5443, 0.5595,  ..., 0.3748, 0.3984, 0.3739],
          [0.5054, 0.3965, 0.5109,  ..., 0.6361, 0.5185, 0.6558],
          [0.5818, 0.6766, 0.4335,  ..., 0.5742, 0.4729, 0.4087]],

         [[0.5875, 0.5641, 0.4292,  ..., 0.5251, 0.4521, 0.5192],
          [0.5395, 0.6174, 0.5373,  ..., 0.6063, 0.3381, 0.3407],
          [0.4933, 0.4761, 0.4902,  ..., 0.2958, 0.4540, 0.3919],
          [0.6095, 0.3337, 0.5660,  ..., 0.3639, 0.5552, 0.4335]],

         [[0.5927, 0.4932, 0.4796,  ..., 0.5655, 0.6234, 0.5467],
          [0.4446, 0.5200, 0.5404,  ..., 0.3416, 0.3478, 0.5421],
          [0.5256, 0.4956, 0.4632,  ..., 0.6011, 0.3381, 0.6610],
          [0.4364, 0.4996, 0.4789,  ..., 0.4851, 0.5931, 0.4946]]],


        [[[0.4211, 0.4559, 0.6243,  ..., 0.5458, 0.5397, 0.3766],
          [0.5198, 0.5181, 0.4177,  ..., 0.4907, 0.4040, 0.6261],
          [0.6206, 0.4676, 0.3730,  ..., 0.3947, 0.4923, 0.5022],
          [0.5675, 0.5383, 0.5665,  ..., 0.5718, 0.7074, 0.4975]],

         [[0.4833, 0.5978, 0.3363,  ..., 0.5460, 0.5941, 0.5375],
          [0.5061, 0.5056, 0.5067,  ..., 0.3242, 0.6252, 0.6325],
          [0.6118, 0.4816, 0.4993,  ..., 0.5605, 0.6202, 0.4829],
          [0.4576, 0.4748, 0.5140,  ..., 0.3630, 0.4017, 0.5294]],

         [[0.6943, 0.4819, 0.6628,  ..., 0.5460, 0.5535, 0.5322],
          [0.4003, 0.5096, 0.3942,  ..., 0.4642, 0.5189, 0.3730],
          [0.3495, 0.3979, 0.4850,  ..., 0.6086, 0.4429, 0.4354],
          [0.4249, 0.4727, 0.5007,  ..., 0.6289, 0.4664, 0.6800]],

         ...,

         [[0.4647, 0.4765, 0.6859,  ..., 0.5612, 0.4856, 0.5708],
          [0.5732, 0.5190, 0.4995,  ..., 0.5605, 0.5196, 0.5936],
          [0.5051, 0.6220, 0.5294,  ..., 0.3460, 0.3720, 0.5282],
          [0.5081, 0.4540, 0.5059,  ..., 0.6451, 0.4364, 0.4897]],

         [[0.5064, 0.4647, 0.4706,  ..., 0.5837, 0.4482, 0.5631],
          [0.4144, 0.4685, 0.5322,  ..., 0.5689, 0.5414, 0.5033],
          [0.5127, 0.4772, 0.4388,  ..., 0.5237, 0.5950, 0.4249],
          [0.5482, 0.4758, 0.5903,  ..., 0.4477, 0.4158, 0.5165]],

         [[0.4596, 0.4651, 0.3923,  ..., 0.5229, 0.3007, 0.4211],
          [0.5216, 0.4282, 0.5016,  ..., 0.5426, 0.4900, 0.5636],
          [0.6298, 0.5722, 0.4106,  ..., 0.4586, 0.5171, 0.5210],
          [0.6058, 0.5458, 0.4596,  ..., 0.3821, 0.6415, 0.4434]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0050, -0.0070,  0.0050, -0.0010,  0.0010,  0.0050, -0.0070, -0.0050,
         0.0050,  0.0050], device='cuda:0')
selected experts tensor([1702, 1680, 1644, 1694, 1489, 1658, 1600, 1555, 1672, 1690],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5813, 0.4776, 0.4832,  ..., 0.3974, 0.3910, 0.4702],
          [0.4902, 0.5254, 0.4569,  ..., 0.3829, 0.3928, 0.3845],
          [0.4671, 0.5955, 0.4583,  ..., 0.4307, 0.4667, 0.4920],
          [0.5665, 0.3687, 0.4154,  ..., 0.6098, 0.3796, 0.5091]],

         [[0.3937, 0.5266, 0.5019,  ..., 0.5178, 0.5247, 0.3821],
          [0.4822, 0.5201, 0.6180,  ..., 0.4329, 0.4568, 0.5867],
          [0.6146, 0.6336, 0.4967,  ..., 0.6056, 0.4631, 0.5040],
          [0.5903, 0.5356, 0.4202,  ..., 0.5031, 0.6066, 0.5257]],

         [[0.4540, 0.4509, 0.5080,  ..., 0.3718, 0.5852, 0.5071],
          [0.5540, 0.5434, 0.5565,  ..., 0.5300, 0.4665, 0.4451],
          [0.5832, 0.4642, 0.5647,  ..., 0.4805, 0.4331, 0.4857],
          [0.3362, 0.4545, 0.5145,  ..., 0.4821, 0.5356, 0.4663]],

         ...,

         [[0.5317, 0.4904, 0.5959,  ..., 0.4402, 0.4556, 0.4215],
          [0.5317, 0.4402, 0.3213,  ..., 0.4339, 0.4067, 0.3868],
          [0.5535, 0.6118, 0.6235,  ..., 0.4927, 0.5347, 0.5933],
          [0.5457, 0.3497, 0.5752,  ..., 0.5024, 0.5522, 0.4536]],

         [[0.5448, 0.5913, 0.5052,  ..., 0.5893, 0.4764, 0.4014],
          [0.5240, 0.5843, 0.4409,  ..., 0.4235, 0.5430, 0.4862],
          [0.5194, 0.4441, 0.6112,  ..., 0.5011, 0.4518, 0.4306],
          [0.4445, 0.4455, 0.6503,  ..., 0.4857, 0.5600, 0.5998]],

         [[0.5270, 0.4940, 0.5808,  ..., 0.6116, 0.3760, 0.4526],
          [0.4841, 0.5576, 0.5445,  ..., 0.6761, 0.3787, 0.4357],
          [0.6477, 0.4870, 0.5604,  ..., 0.4712, 0.4417, 0.3437],
          [0.4819, 0.4402, 0.5680,  ..., 0.6052, 0.4938, 0.6294]]],


        [[[0.3629, 0.6408, 0.4822,  ..., 0.6217, 0.4105, 0.4277],
          [0.4320, 0.4686, 0.6244,  ..., 0.5921, 0.4431, 0.5197],
          [0.5233, 0.4184, 0.4982,  ..., 0.6520, 0.5343, 0.4304],
          [0.5496, 0.4721, 0.4154,  ..., 0.3932, 0.6090, 0.4502]],

         [[0.3765, 0.4467, 0.3838,  ..., 0.5963, 0.4189, 0.4342],
          [0.4513, 0.4859, 0.5799,  ..., 0.6280, 0.3814, 0.4888],
          [0.3181, 0.5643, 0.6033,  ..., 0.6370, 0.5471, 0.4613],
          [0.5665, 0.4708, 0.2390,  ..., 0.5167, 0.4503, 0.4988]],

         [[0.3397, 0.5633, 0.4958,  ..., 0.5879, 0.4331, 0.6838],
          [0.5234, 0.5643, 0.6253,  ..., 0.4838, 0.4293, 0.5506],
          [0.4925, 0.4041, 0.5204,  ..., 0.3875, 0.5646, 0.5456],
          [0.3756, 0.4823, 0.4510,  ..., 0.5525, 0.6165, 0.4579]],

         ...,

         [[0.4363, 0.3687, 0.5256,  ..., 0.4320, 0.4636, 0.3428],
          [0.3557, 0.5932, 0.4616,  ..., 0.4750, 0.3956, 0.4267],
          [0.5841, 0.4947, 0.6070,  ..., 0.5704, 0.4403, 0.5177],
          [0.5014, 0.4978, 0.5380,  ..., 0.5051, 0.5363, 0.6602]],

         [[0.5188, 0.6914, 0.4869,  ..., 0.4625, 0.5334, 0.5654],
          [0.3310, 0.5499, 0.5283,  ..., 0.5486, 0.4388, 0.5011],
          [0.3530, 0.4117, 0.5150,  ..., 0.4746, 0.5452, 0.4294],
          [0.7088, 0.4615, 0.4397,  ..., 0.5515, 0.3956, 0.5712]],

         [[0.5450, 0.5880, 0.4026,  ..., 0.4498, 0.3769, 0.3070],
          [0.4474, 0.5951, 0.3621,  ..., 0.4723, 0.4651, 0.3728],
          [0.4030, 0.5843, 0.5394,  ..., 0.4683, 0.4918, 0.4057],
          [0.5188, 0.3761, 0.4549,  ..., 0.6432, 0.4479, 0.4558]]]],
       device='cuda:0')
tensor([[[[0.5823, 0.4846, 0.4922,  ..., 0.4064, 0.3780, 0.4832],
          [0.4912, 0.5324, 0.4659,  ..., 0.3919, 0.3798, 0.3975],
          [0.4681, 0.6025, 0.4673,  ..., 0.4397, 0.4537, 0.5050],
          [0.5675, 0.3757, 0.4244,  ..., 0.6188, 0.3666, 0.5221]],

         [[0.3947, 0.5336, 0.5109,  ..., 0.5268, 0.5117, 0.3951],
          [0.4832, 0.5271, 0.6270,  ..., 0.4419, 0.4438, 0.5997],
          [0.6156, 0.6406, 0.5057,  ..., 0.6146, 0.4501, 0.5170],
          [0.5913, 0.5426, 0.4292,  ..., 0.5121, 0.5936, 0.5387]],

         [[0.4550, 0.4579, 0.5170,  ..., 0.3808, 0.5722, 0.5201],
          [0.5550, 0.5504, 0.5655,  ..., 0.5390, 0.4535, 0.4581],
          [0.5842, 0.4712, 0.5737,  ..., 0.4895, 0.4201, 0.4987],
          [0.3372, 0.4615, 0.5235,  ..., 0.4911, 0.5226, 0.4793]],

         ...,

         [[0.5327, 0.4974, 0.6049,  ..., 0.4492, 0.4426, 0.4345],
          [0.5327, 0.4472, 0.3303,  ..., 0.4429, 0.3937, 0.3998],
          [0.5545, 0.6188, 0.6325,  ..., 0.5017, 0.5217, 0.6063],
          [0.5467, 0.3567, 0.5842,  ..., 0.5114, 0.5392, 0.4666]],

         [[0.5458, 0.5983, 0.5142,  ..., 0.5983, 0.4634, 0.4144],
          [0.5250, 0.5913, 0.4499,  ..., 0.4325, 0.5300, 0.4992],
          [0.5204, 0.4511, 0.6202,  ..., 0.5101, 0.4388, 0.4436],
          [0.4455, 0.4525, 0.6593,  ..., 0.4947, 0.5470, 0.6128]],

         [[0.5280, 0.5010, 0.5898,  ..., 0.6206, 0.3630, 0.4656],
          [0.4851, 0.5646, 0.5535,  ..., 0.6851, 0.3657, 0.4487],
          [0.6487, 0.4940, 0.5694,  ..., 0.4802, 0.4287, 0.3567],
          [0.4829, 0.4472, 0.5770,  ..., 0.6142, 0.4808, 0.6424]]],


        [[[0.3639, 0.6478, 0.4912,  ..., 0.6307, 0.3975, 0.4407],
          [0.4330, 0.4756, 0.6334,  ..., 0.6011, 0.4301, 0.5327],
          [0.5243, 0.4254, 0.5072,  ..., 0.6610, 0.5213, 0.4434],
          [0.5506, 0.4791, 0.4244,  ..., 0.4022, 0.5960, 0.4632]],

         [[0.3775, 0.4537, 0.3928,  ..., 0.6053, 0.4059, 0.4472],
          [0.4523, 0.4929, 0.5889,  ..., 0.6370, 0.3684, 0.5018],
          [0.3191, 0.5713, 0.6123,  ..., 0.6460, 0.5341, 0.4743],
          [0.5675, 0.4778, 0.2480,  ..., 0.5257, 0.4373, 0.5118]],

         [[0.3407, 0.5703, 0.5048,  ..., 0.5969, 0.4201, 0.6968],
          [0.5244, 0.5713, 0.6343,  ..., 0.4928, 0.4163, 0.5636],
          [0.4935, 0.4111, 0.5294,  ..., 0.3965, 0.5516, 0.5586],
          [0.3766, 0.4893, 0.4600,  ..., 0.5615, 0.6035, 0.4709]],

         ...,

         [[0.4373, 0.3757, 0.5346,  ..., 0.4410, 0.4506, 0.3558],
          [0.3567, 0.6002, 0.4706,  ..., 0.4840, 0.3826, 0.4397],
          [0.5851, 0.5017, 0.6160,  ..., 0.5794, 0.4273, 0.5307],
          [0.5024, 0.5048, 0.5470,  ..., 0.5141, 0.5233, 0.6732]],

         [[0.5198, 0.6984, 0.4959,  ..., 0.4715, 0.5204, 0.5784],
          [0.3320, 0.5569, 0.5373,  ..., 0.5576, 0.4258, 0.5141],
          [0.3540, 0.4187, 0.5240,  ..., 0.4836, 0.5322, 0.4424],
          [0.7098, 0.4685, 0.4487,  ..., 0.5605, 0.3826, 0.5842]],

         [[0.5460, 0.5950, 0.4116,  ..., 0.4588, 0.3639, 0.3200],
          [0.4484, 0.6021, 0.3711,  ..., 0.4813, 0.4521, 0.3858],
          [0.4040, 0.5913, 0.5484,  ..., 0.4773, 0.4788, 0.4187],
          [0.5198, 0.3831, 0.4639,  ..., 0.6522, 0.4349, 0.4688]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0010, -0.0070, -0.0090,  0.0070, -0.0050, -0.0030, -0.0010, -0.0090,
         0.0130, -0.0130], device='cuda:0')
selected experts tensor([1233, 1683, 1729, 1377, 1812, 1994, 1650, 2023, 1120, 1763],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.5948, 0.5190, 0.5060,  ..., 0.3736, 0.6040, 0.3914],
          [0.5768, 0.5958, 0.4622,  ..., 0.3864, 0.4652, 0.4096],
          [0.4571, 0.4940, 0.4559,  ..., 0.5387, 0.6017, 0.5749],
          [0.5428, 0.6071, 0.4214,  ..., 0.5482, 0.5904, 0.5845]],

         [[0.5352, 0.5253, 0.6053,  ..., 0.4973, 0.4326, 0.5773],
          [0.4460, 0.3554, 0.4247,  ..., 0.4228, 0.5095, 0.5181],
          [0.4347, 0.4280, 0.4367,  ..., 0.4871, 0.4768, 0.4434],
          [0.6366, 0.4242, 0.4153,  ..., 0.6262, 0.3595, 0.5562]],

         [[0.6043, 0.5437, 0.6494,  ..., 0.5203, 0.5974, 0.4593],
          [0.6187, 0.4633, 0.4525,  ..., 0.4200, 0.4797, 0.5764],
          [0.5090, 0.4716, 0.5414,  ..., 0.4257, 0.4297, 0.5987],
          [0.5045, 0.5403, 0.4892,  ..., 0.5593, 0.5809, 0.4991]],

         ...,

         [[0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070],
          [0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070],
          [0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070],
          [0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070]],

         [[0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070],
          [0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070],
          [0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070],
          [0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070]],

         [[0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070],
          [0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070],
          [0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070],
          [0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070]]],


        [[[0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070],
          [0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070],
          [0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070],
          [0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070]],

         [[0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070],
          [0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070],
          [0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070],
          [0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070]],

         [[0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070],
          [0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070],
          [0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070],
          [0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070]],

         ...,

         [[0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070],
          [0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070],
          [0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070],
          [0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070]],

         [[0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070],
          [0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070],
          [0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070],
          [0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070]],

         [[0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070],
          [0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070],
          [0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070],
          [0.5050, 0.5050, 0.5070,  ..., 0.5070, 0.5010, 0.5070]]]],
       device='cuda:0')/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([[[[0.5898, 0.5140, 0.4990,  ..., 0.3666, 0.6030, 0.3844],
          [0.5718, 0.5908, 0.4552,  ..., 0.3794, 0.4642, 0.4026],
          [0.4521, 0.4890, 0.4489,  ..., 0.5317, 0.6007, 0.5679],
          [0.5378, 0.6021, 0.4144,  ..., 0.5412, 0.5894, 0.5775]],

         [[0.5302, 0.5203, 0.5983,  ..., 0.4903, 0.4316, 0.5703],
          [0.4410, 0.3504, 0.4177,  ..., 0.4158, 0.5085, 0.5111],
          [0.4297, 0.4230, 0.4297,  ..., 0.4801, 0.4758, 0.4364],
          [0.6316, 0.4192, 0.4083,  ..., 0.6192, 0.3585, 0.5492]],

         [[0.5993, 0.5387, 0.6424,  ..., 0.5133, 0.5964, 0.4523],
          [0.6137, 0.4583, 0.4455,  ..., 0.4130, 0.4787, 0.5694],
          [0.5040, 0.4666, 0.5344,  ..., 0.4187, 0.4287, 0.5917],
          [0.4995, 0.5353, 0.4822,  ..., 0.5523, 0.5799, 0.4921]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0050, 0.0050, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0010,
        0.0070], device='cuda:0')
selected experts tensor([ 414,  430, 4278, 4131,  522,  552,  554,  211,  688,  508],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4129, 0.5428, 0.5567,  ..., 0.6635, 0.5239, 0.4573],
          [0.4434, 0.4368, 0.5426,  ..., 0.5205, 0.4036, 0.4706],
          [0.4321, 0.5764, 0.3934,  ..., 0.5496, 0.6369, 0.4870],
          [0.5079, 0.5264, 0.6350,  ..., 0.4520, 0.5598, 0.5943]],

         [[0.4937, 0.4996, 0.4637,  ..., 0.5331, 0.4229, 0.3117],
          [0.4688, 0.6117, 0.5940,  ..., 0.4947, 0.6300, 0.5199],
          [0.5439, 0.5631, 0.4705,  ..., 0.5213, 0.3934, 0.5806],
          [0.4842, 0.6651, 0.5564,  ..., 0.5209, 0.6708, 0.7538]],

         [[0.4647, 0.4291, 0.5470,  ..., 0.3600, 0.5026, 0.5204],
          [0.5046, 0.6089, 0.4296,  ..., 0.6312, 0.3496, 0.4993],
          [0.5253, 0.5150, 0.4702,  ..., 0.3285, 0.6115, 0.6294],
          [0.5660, 0.4629, 0.4634,  ..., 0.5498, 0.3096, 0.6148]],

         ...,

         [[0.4656, 0.5433, 0.3878,  ..., 0.4986, 0.3487, 0.3763],
          [0.3789, 0.6547, 0.6450,  ..., 0.5062, 0.5256, 0.5896],
          [0.5574, 0.5968, 0.5584,  ..., 0.5610, 0.4593, 0.4464],
          [0.3762, 0.3529, 0.4310,  ..., 0.4242, 0.6124, 0.5971]],

         [[0.5712, 0.4096, 0.3549,  ..., 0.6280, 0.4605, 0.5055],
          [0.6214, 0.3908, 0.4702,  ..., 0.5320, 0.6058, 0.4452],
          [0.3934, 0.4700, 0.5368,  ..., 0.3717, 0.5993, 0.5330],
          [0.6304, 0.5484, 0.4954,  ..., 0.5944, 0.4845, 0.3117]],

         [[0.5021, 0.4742, 0.5745,  ..., 0.4588, 0.4690, 0.4432],
          [0.5113, 0.5659, 0.4538,  ..., 0.5138, 0.4859, 0.4723],
          [0.5511, 0.4457, 0.5774,  ..., 0.5561, 0.5683, 0.5270],
          [0.5209, 0.5722, 0.3593,  ..., 0.4271, 0.6166, 0.4319]]],


        [[[0.5877, 0.6191, 0.5034,  ..., 0.6627, 0.5416, 0.5335],
          [0.6141, 0.4636, 0.5328,  ..., 0.3285, 0.6558, 0.5101],
          [0.3619, 0.4426, 0.5463,  ..., 0.5958, 0.3800, 0.4166],
          [0.4043, 0.4957, 0.5465,  ..., 0.5406, 0.4615, 0.3671]],

         [[0.5158, 0.6117, 0.4651,  ..., 0.5025, 0.5295, 0.2297],
          [0.5216, 0.5474, 0.5079,  ..., 0.4821, 0.5190, 0.4345],
          [0.5550, 0.5278, 0.4835,  ..., 0.5518, 0.5040, 0.5142],
          [0.4704, 0.2947, 0.4470,  ..., 0.4588, 0.5252, 0.4803]],

         [[0.5260, 0.4503, 0.5049,  ..., 0.4058, 0.5262, 0.4563],
          [0.4500, 0.5817, 0.4396,  ..., 0.4338, 0.4717, 0.5716],
          [0.4784, 0.6136, 0.3810,  ..., 0.4357, 0.4521, 0.5735],
          [0.4422, 0.5722, 0.6332,  ..., 0.4361, 0.3575, 0.5452]],

         ...,

         [[0.6856, 0.3964, 0.5357,  ..., 0.4871, 0.5836, 0.5272],
          [0.4734, 0.4191, 0.5404,  ..., 0.5095, 0.5945, 0.5085],
          [0.3305, 0.4583, 0.6341,  ..., 0.4704, 0.6360, 0.2782],
          [0.4038, 0.4892, 0.3611,  ..., 0.5844, 0.4305, 0.4490]],

         [[0.5626, 0.4663, 0.4003,  ..., 0.4800, 0.3280, 0.3280],
          [0.5340, 0.5726, 0.4348,  ..., 0.5076, 0.4463, 0.5114],
          [0.4813, 0.5592, 0.6459,  ..., 0.4338, 0.6180, 0.6447],
          [0.6077, 0.4930, 0.4377,  ..., 0.4453, 0.5121, 0.5079]],

         [[0.3619, 0.6029, 0.5670,  ..., 0.5844, 0.3915, 0.4602],
          [0.5396, 0.4234, 0.5208,  ..., 0.5484, 0.6016, 0.4809],
          [0.4430, 0.3866, 0.3911,  ..., 0.5617, 0.5183, 0.4524],
          [0.5669, 0.5380, 0.5455,  ..., 0.6146, 0.3924, 0.4815]]]],
       device='cuda:0')
tensor([[[[0.4249, 0.5448, 0.5487,  ..., 0.6575, 0.5159, 0.4613],
          [0.4554, 0.4388, 0.5346,  ..., 0.5145, 0.3956, 0.4746],
          [0.4441, 0.5784, 0.3854,  ..., 0.5436, 0.6289, 0.4910],
          [0.5199, 0.5284, 0.6270,  ..., 0.4460, 0.5518, 0.5983]],

         [[0.5057, 0.5016, 0.4557,  ..., 0.5271, 0.4149, 0.3157],
          [0.4808, 0.6137, 0.5860,  ..., 0.4887, 0.6220, 0.5239],
          [0.5559, 0.5651, 0.4625,  ..., 0.5153, 0.3854, 0.5846],
          [0.4962, 0.6671, 0.5484,  ..., 0.5149, 0.6628, 0.7578]],

         [[0.4767, 0.4311, 0.5390,  ..., 0.3540, 0.4946, 0.5244],
          [0.5166, 0.6109, 0.4216,  ..., 0.6252, 0.3416, 0.5033],
          [0.5373, 0.5170, 0.4622,  ..., 0.3225, 0.6035, 0.6334],
          [0.5780, 0.4649, 0.4554,  ..., 0.5438, 0.3016, 0.6188]],

         ...,

         [[0.4776, 0.5453, 0.3798,  ..., 0.4926, 0.3407, 0.3803],
          [0.3909, 0.6567, 0.6370,  ..., 0.5002, 0.5176, 0.5936],
          [0.5694, 0.5988, 0.5504,  ..., 0.5550, 0.4513, 0.4504],
          [0.3882, 0.3549, 0.4230,  ..., 0.4182, 0.6044, 0.6011]],

         [[0.5832, 0.4116, 0.3469,  ..., 0.6220, 0.4525, 0.5095],
          [0.6334, 0.3928, 0.4622,  ..., 0.5260, 0.5978, 0.4492],
          [0.4054, 0.4720, 0.5288,  ..., 0.3657, 0.5913, 0.5370],
          [0.6424, 0.5504, 0.4874,  ..., 0.5884, 0.4765, 0.3157]],

         [[0.5141, 0.4762, 0.5665,  ..., 0.4528, 0.4610, 0.4472],
          [0.5233, 0.5679, 0.4458,  ..., 0.5078, 0.4779, 0.4763],
          [0.5631, 0.4477, 0.5694,  ..., 0.5501, 0.5603, 0.5310],
          [0.5329, 0.5742, 0.3513,  ..., 0.4211, 0.6086, 0.4359]]],


        [[[0.5997, 0.6211, 0.4954,  ..., 0.6567, 0.5336, 0.5375],
          [0.6261, 0.4656, 0.5248,  ..., 0.3225, 0.6478, 0.5141],
          [0.3739, 0.4446, 0.5383,  ..., 0.5898, 0.3720, 0.4206],
          [0.4163, 0.4977, 0.5385,  ..., 0.5346, 0.4535, 0.3711]],

         [[0.5278, 0.6137, 0.4571,  ..., 0.4965, 0.5215, 0.2337],
          [0.5336, 0.5494, 0.4999,  ..., 0.4761, 0.5110, 0.4385],
          [0.5670, 0.5297, 0.4755,  ..., 0.5458, 0.4960, 0.5182],
          [0.4824, 0.2967, 0.4390,  ..., 0.4528, 0.5172, 0.4843]],

         [[0.5380, 0.4523, 0.4969,  ..., 0.3998, 0.5182, 0.4603],
          [0.4620, 0.5837, 0.4316,  ..., 0.4278, 0.4637, 0.5756],
          [0.4904, 0.6156, 0.3730,  ..., 0.4297, 0.4441, 0.5775],
          [0.4542, 0.5742, 0.6252,  ..., 0.4301, 0.3495, 0.5492]],

         ...,

         [[0.6976, 0.3984, 0.5277,  ..., 0.4811, 0.5756, 0.5312],
          [0.4854, 0.4211, 0.5324,  ..., 0.5035, 0.5865, 0.5125],
          [0.3425, 0.4603, 0.6261,  ..., 0.4644, 0.6280, 0.2822],
          [0.4158, 0.4912, 0.3531,  ..., 0.5784, 0.4225, 0.4530]],

         [[0.5746, 0.4683, 0.3923,  ..., 0.4740, 0.3200, 0.3320],
          [0.5460, 0.5746, 0.4268,  ..., 0.5016, 0.4383, 0.5154],
          [0.4933, 0.5612, 0.6379,  ..., 0.4278, 0.6100, 0.6487],
          [0.6197, 0.4950, 0.4297,  ..., 0.4393, 0.5041, 0.5119]],

         [[0.3739, 0.6049, 0.5590,  ..., 0.5784, 0.3835, 0.4642],
          [0.5516, 0.4254, 0.5128,  ..., 0.5424, 0.5936, 0.4849],
          [0.4550, 0.3886, 0.3831,  ..., 0.5557, 0.5103, 0.4564],
          [0.5789, 0.5400, 0.5375,  ..., 0.6086, 0.3844, 0.4855]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0120, -0.0020,  0.0080,  0.0040,  0.0080,  0.0060, -0.0060,  0.0060,
         0.0080, -0.0040], device='cuda:0')
selected experts tensor([1524, 1613, 1788, 1693, 1689, 1729, 1529, 1624, 1565, 1630],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5919, 0.5214, 0.4936,  ..., 0.3297, 0.4289, 0.3931],
          [0.4227, 0.4676, 0.7024,  ..., 0.4727, 0.5953, 0.5191],
          [0.4052, 0.5027, 0.3991,  ..., 0.7034, 0.5527, 0.5695],
          [0.5362, 0.5856, 0.5943,  ..., 0.5811, 0.5527, 0.3553]],

         [[0.5249, 0.3086, 0.4660,  ..., 0.3855, 0.5469, 0.4884],
          [0.4708, 0.5604, 0.6562,  ..., 0.5787, 0.4175, 0.4127],
          [0.5406, 0.5987, 0.7106,  ..., 0.5090, 0.4883, 0.4076],
          [0.4457, 0.4518, 0.4981,  ..., 0.4852, 0.4784, 0.5260]],

         [[0.5558, 0.5733, 0.6677,  ..., 0.5170, 0.6033, 0.5801],
          [0.4741, 0.6006, 0.5691,  ..., 0.5990, 0.5604, 0.4933],
          [0.3954, 0.5757, 0.4180,  ..., 0.3177, 0.4483, 0.5839],
          [0.4995, 0.6071, 0.6338,  ..., 0.5012, 0.6283, 0.4993]],

         ...,

         [[0.3625, 0.5931, 0.6009,  ..., 0.5091, 0.4851, 0.6009],
          [0.5330, 0.4872, 0.4947,  ..., 0.5555, 0.3139, 0.5710],
          [0.3544, 0.4858, 0.4413,  ..., 0.3228, 0.6383, 0.4052],
          [0.5015, 0.7192, 0.5681,  ..., 0.5095, 0.5336, 0.6158]],

         [[0.4251, 0.4193, 0.5093,  ..., 0.3977, 0.4256, 0.5981],
          [0.7106, 0.5207, 0.5045,  ..., 0.4709, 0.6464, 0.4132],
          [0.4889, 0.4289, 0.3949,  ..., 0.4156, 0.4886, 0.3825],
          [0.4430, 0.6263, 0.6246,  ..., 0.4793, 0.4318, 0.3274]],

         [[0.3589, 0.4638, 0.7098,  ..., 0.5787, 0.4880, 0.5347],
          [0.4740, 0.4943, 0.5536,  ..., 0.5345, 0.5178, 0.4515],
          [0.6126, 0.5143, 0.5630,  ..., 0.4767, 0.5643, 0.4829],
          [0.4689, 0.5332, 0.4313,  ..., 0.4857, 0.4971, 0.3931]]],


        [[[0.4903, 0.3876, 0.5929,  ..., 0.4466, 0.4418, 0.5279],
          [0.5910, 0.4941, 0.4409,  ..., 0.4631, 0.3880, 0.5473],
          [0.5924, 0.5147, 0.5522,  ..., 0.5536, 0.4198, 0.4609],
          [0.4762, 0.4542, 0.6668,  ..., 0.6491, 0.2982, 0.5306]],

         [[0.5189, 0.4814, 0.6214,  ..., 0.4614, 0.5384, 0.4876],
          [0.3491, 0.4633, 0.5548,  ..., 0.5466, 0.5729, 0.5457],
          [0.3697, 0.4850, 0.6383,  ..., 0.3758, 0.5174, 0.6028],
          [0.5645, 0.4939, 0.7186,  ..., 0.5202, 0.4756, 0.6018]],

         [[0.5626, 0.5076, 0.5953,  ..., 0.4671, 0.5376, 0.4180],
          [0.5652, 0.5959, 0.3917,  ..., 0.6375, 0.5073, 0.5180],
          [0.4341, 0.5158, 0.3377,  ..., 0.6366, 0.6033, 0.5418],
          [0.5393, 0.5424, 0.6650,  ..., 0.4290, 0.5990, 0.6392]],

         ...,

         [[0.4483, 0.4482, 0.4594,  ..., 0.4912, 0.6562, 0.5210],
          [0.4447, 0.5356, 0.4568,  ..., 0.5488, 0.4768, 0.5038],
          [0.5657, 0.4245, 0.5197,  ..., 0.4753, 0.4837, 0.4208],
          [0.5762, 0.5448, 0.6037,  ..., 0.4085, 0.4733, 0.5425]],

         [[0.3465, 0.5414, 0.3770,  ..., 0.4300, 0.5376, 0.4546],
          [0.5391, 0.4368, 0.6677,  ..., 0.4686, 0.5150, 0.5119],
          [0.4539, 0.5695, 0.5515,  ..., 0.4156, 0.4984, 0.5493],
          [0.7457, 0.5884, 0.3482,  ..., 0.3786, 0.5217, 0.5277]],

         [[0.4694, 0.2903, 0.6037,  ..., 0.4343, 0.6482, 0.5815],
          [0.6383, 0.5209, 0.6428,  ..., 0.4495, 0.5430, 0.5719],
          [0.5359, 0.5395, 0.4553,  ..., 0.5579, 0.5314, 0.6182],
          [0.4701, 0.4330, 0.4887,  ..., 0.3842, 0.5671, 0.4227]]]],
       device='cuda:0')
tensor([[[[0.5879, 0.5294, 0.4896,  ..., 0.3337, 0.4249, 0.3891],
          [0.4187, 0.4756, 0.6984,  ..., 0.4767, 0.5913, 0.5151],
          [0.4012, 0.5107, 0.3951,  ..., 0.7074, 0.5487, 0.5655],
          [0.5322, 0.5936, 0.5903,  ..., 0.5851, 0.5487, 0.3513]],

         [[0.5209, 0.3166, 0.4620,  ..., 0.3895, 0.5429, 0.4844],
          [0.4668, 0.5684, 0.6522,  ..., 0.5827, 0.4135, 0.4087],
          [0.5366, 0.6067, 0.7066,  ..., 0.5130, 0.4843, 0.4036],
          [0.4417, 0.4598, 0.4941,  ..., 0.4892, 0.4744, 0.5220]],

         [[0.5518, 0.5813, 0.6637,  ..., 0.5210, 0.5993, 0.5761],
          [0.4701, 0.6086, 0.5651,  ..., 0.6030, 0.5564, 0.4893],
          [0.3914, 0.5837, 0.4140,  ..., 0.3217, 0.4443, 0.5799],
          [0.4955, 0.6151, 0.6298,  ..., 0.5052, 0.6243, 0.4953]],

         ...,

         [[0.3585, 0.6011, 0.5969,  ..., 0.5131, 0.4811, 0.5969],
          [0.5290, 0.4952, 0.4907,  ..., 0.5595, 0.3099, 0.5670],
          [0.3504, 0.4938, 0.4373,  ..., 0.3268, 0.6343, 0.4012],
          [0.4975, 0.7272, 0.5641,  ..., 0.5135, 0.5296, 0.6118]],

         [[0.4211, 0.4273, 0.5053,  ..., 0.4017, 0.4216, 0.5941],
          [0.7066, 0.5287, 0.5005,  ..., 0.4749, 0.6424, 0.4092],
          [0.4849, 0.4369, 0.3909,  ..., 0.4196, 0.4846, 0.3785],
          [0.4390, 0.6343, 0.6206,  ..., 0.4833, 0.4278, 0.3234]],

         [[0.3549, 0.4718, 0.7058,  ..., 0.5827, 0.4840, 0.5307],
          [0.4700, 0.5023, 0.5496,  ..., 0.5385, 0.5138, 0.4475],
          [0.6086, 0.5223, 0.5590,  ..., 0.4807, 0.5603, 0.4789],
          [0.4649, 0.5412, 0.4273,  ..., 0.4897, 0.4931, 0.3891]]],


        [[[0.4863, 0.3956, 0.5889,  ..., 0.4506, 0.4378, 0.5239],
          [0.5870, 0.5021, 0.4369,  ..., 0.4671, 0.3840, 0.5433],
          [0.5884, 0.5227, 0.5482,  ..., 0.5576, 0.4158, 0.4569],
          [0.4722, 0.4622, 0.6628,  ..., 0.6531, 0.2942, 0.5266]],

         [[0.5149, 0.4894, 0.6174,  ..., 0.4654, 0.5344, 0.4836],
          [0.3451, 0.4713, 0.5508,  ..., 0.5506, 0.5689, 0.5417],
          [0.3657, 0.4930, 0.6343,  ..., 0.3798, 0.5134, 0.5988],
          [0.5605, 0.5019, 0.7146,  ..., 0.5242, 0.4716, 0.5978]],

         [[0.5586, 0.5156, 0.5913,  ..., 0.4711, 0.5336, 0.4140],
          [0.5612, 0.6039, 0.3877,  ..., 0.6415, 0.5033, 0.5140],
          [0.4301, 0.5238, 0.3337,  ..., 0.6406, 0.5993, 0.5378],
          [0.5353, 0.5504, 0.6610,  ..., 0.4330, 0.5950, 0.6352]],

         ...,

         [[0.4443, 0.4562, 0.4554,  ..., 0.4952, 0.6522, 0.5170],
          [0.4407, 0.5436, 0.4528,  ..., 0.5528, 0.4728, 0.4998],
          [0.5617, 0.4325, 0.5157,  ..., 0.4793, 0.4797, 0.4168],
          [0.5722, 0.5528, 0.5997,  ..., 0.4125, 0.4693, 0.5385]],

         [[0.3425, 0.5494, 0.3730,  ..., 0.4340, 0.5336, 0.4506],
          [0.5351, 0.4448, 0.6637,  ..., 0.4726, 0.5110, 0.5079],
          [0.4499, 0.5775, 0.5475,  ..., 0.4196, 0.4944, 0.5453],
          [0.7417, 0.5964, 0.3442,  ..., 0.3826, 0.5177, 0.5237]],

         [[0.4654, 0.2983, 0.5997,  ..., 0.4383, 0.6442, 0.5775],
          [0.6343, 0.5289, 0.6388,  ..., 0.4535, 0.5390, 0.5679],
          [0.5319, 0.5475, 0.4513,  ..., 0.5619, 0.5274, 0.6142],
          [0.4661, 0.4410, 0.4847,  ..., 0.3882, 0.5631, 0.4187]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0040, -0.0080,  0.0040, -0.0020,  0.0020,  0.0040, -0.0060, -0.0040,
         0.0040,  0.0040], device='cuda:0')
selected experts tensor([1685, 1577, 1634, 1661, 1689, 1563, 1599, 1626, 1700, 1650],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.3909, 0.4754, 0.4077,  ..., 0.5256, 0.3680, 0.5616],
          [0.5075, 0.5547, 0.5680,  ..., 0.4581, 0.4494, 0.4270],
          [0.4489, 0.7215, 0.5760,  ..., 0.4704, 0.5083, 0.4427],
          [0.3504, 0.4508, 0.4842,  ..., 0.5227, 0.5668, 0.7093]],

         [[0.4735, 0.5000, 0.3954,  ..., 0.4527, 0.4991, 0.4598],
          [0.4718, 0.4416, 0.4611,  ..., 0.4695, 0.4176, 0.5095],
          [0.7114, 0.4738, 0.5618,  ..., 0.4807, 0.4989, 0.5048],
          [0.3984, 0.5460, 0.4493,  ..., 0.5483, 0.5221, 0.6320]],

         [[0.4163, 0.6190, 0.5249,  ..., 0.4576, 0.3139, 0.4729],
          [0.5304, 0.4174, 0.4322,  ..., 0.4365, 0.4317, 0.3797],
          [0.3693, 0.5762, 0.5967,  ..., 0.5779, 0.5518, 0.3418],
          [0.5846, 0.3318, 0.4264,  ..., 0.7141, 0.5058, 0.5390]],

         ...,

         [[0.5460, 0.2822, 0.5062,  ..., 0.6152, 0.5886, 0.5175],
          [0.4092, 0.5978, 0.4747,  ..., 0.4708, 0.4610, 0.5563],
          [0.5013, 0.5073, 0.5874,  ..., 0.2834, 0.5535, 0.5621],
          [0.6197, 0.4327, 0.3936,  ..., 0.5831, 0.4499, 0.5664]],

         [[0.6179, 0.3993, 0.4363,  ..., 0.4216, 0.6314, 0.3391],
          [0.4610, 0.7066, 0.4423,  ..., 0.5608, 0.4375, 0.4933],
          [0.5246, 0.4731, 0.3861,  ..., 0.3530, 0.4375, 0.4499],
          [0.5665, 0.4688, 0.5469,  ..., 0.3100, 0.5379, 0.5373]],

         [[0.5244, 0.5213, 0.5392,  ..., 0.4926, 0.4858, 0.5881],
          [0.4196, 0.5936, 0.5457,  ..., 0.6270, 0.4480, 0.2698],
          [0.7122, 0.3853, 0.5481,  ..., 0.3431, 0.5872, 0.3617],
          [0.5974, 0.4966, 0.4707,  ..., 0.4911, 0.5414, 0.3793]]],


        [[[0.4654, 0.5752, 0.4440,  ..., 0.2365, 0.4022, 0.6194],
          [0.5562, 0.5179, 0.4986,  ..., 0.4425, 0.3538, 0.5520],
          [0.3919, 0.3714, 0.4874,  ..., 0.3856, 0.3980, 0.5094],
          [0.4863, 0.3257, 0.4641,  ..., 0.4893, 0.5750, 0.5257]],

         [[0.3425, 0.6371, 0.5751,  ..., 0.3809, 0.4581, 0.6497],
          [0.4111, 0.4786, 0.4801,  ..., 0.5040, 0.4157, 0.5061],
          [0.5593, 0.3433, 0.6360,  ..., 0.4660, 0.5665, 0.4578],
          [0.4388, 0.4698, 0.3889,  ..., 0.5190, 0.5905, 0.5563]],

         [[0.2918, 0.6181, 0.5536,  ..., 0.5326, 0.4794, 0.6320],
          [0.5615, 0.4169, 0.4722,  ..., 0.3763, 0.4757, 0.6694],
          [0.4873, 0.4501, 0.4030,  ..., 0.5092, 0.5075, 0.4109],
          [0.4149, 0.3523, 0.3557,  ..., 0.5584, 0.5423, 0.5347]],

         ...,

         [[0.5689, 0.4226, 0.5022,  ..., 0.4273, 0.4494, 0.4118],
          [0.3363, 0.6103, 0.4044,  ..., 0.5154, 0.4190, 0.3137],
          [0.4083, 0.3345, 0.5365,  ..., 0.5071, 0.4437, 0.4439],
          [0.5610, 0.5087, 0.5088,  ..., 0.5675, 0.5338, 0.4775]],

         [[0.4436, 0.5575, 0.4645,  ..., 0.4310, 0.6147, 0.4564],
          [0.3074, 0.4579, 0.5459,  ..., 0.5275, 0.4533, 0.3654],
          [0.6406, 0.4241, 0.6342,  ..., 0.4949, 0.4646, 0.4649],
          [0.4709, 0.4260, 0.5039,  ..., 0.5224, 0.6015, 0.6575]],

         [[0.4518, 0.3310, 0.4858,  ..., 0.5211, 0.3340, 0.4688],
          [0.3594, 0.6652, 0.4578,  ..., 0.5670, 0.4451, 0.3590],
          [0.4359, 0.5690, 0.7226,  ..., 0.4532, 0.5726, 0.4429],
          [0.5142, 0.2936, 0.6014,  ..., 0.5775, 0.5752, 0.5453]]]],
       device='cuda:0')
tensor([[[[0.3909, 0.4834, 0.4177,  ..., 0.5356, 0.3540, 0.5756],
          [0.5075, 0.5627, 0.5780,  ..., 0.4681, 0.4354, 0.4410],
          [0.4489, 0.7295, 0.5860,  ..., 0.4804, 0.4943, 0.4567],
          [0.3504, 0.4588, 0.4942,  ..., 0.5327, 0.5528, 0.7233]],

         [[0.4735, 0.5080, 0.4054,  ..., 0.4627, 0.4851, 0.4738],
          [0.4718, 0.4496, 0.4711,  ..., 0.4795, 0.4036, 0.5235],
          [0.7114, 0.4818, 0.5718,  ..., 0.4907, 0.4849, 0.5188],
          [0.3984, 0.5540, 0.4593,  ..., 0.5583, 0.5081, 0.6460]],

         [[0.4163, 0.6270, 0.5349,  ..., 0.4676, 0.2999, 0.4869],
          [0.5304, 0.4254, 0.4422,  ..., 0.4465, 0.4177, 0.3937],
          [0.3693, 0.5842, 0.6067,  ..., 0.5879, 0.5378, 0.3558],
          [0.5846, 0.3398, 0.4364,  ..., 0.7241, 0.4918, 0.5530]],

         ...,

         [[0.5460, 0.2902, 0.5162,  ..., 0.6252, 0.5746, 0.5315],
          [0.4092, 0.6058, 0.4847,  ..., 0.4808, 0.4470, 0.5703],
          [0.5013, 0.5153, 0.5974,  ..., 0.2934, 0.5395, 0.5761],
          [0.6197, 0.4407, 0.4036,  ..., 0.5931, 0.4359, 0.5804]],

         [[0.6179, 0.4073, 0.4463,  ..., 0.4316, 0.6174, 0.3531],
          [0.4610, 0.7146, 0.4523,  ..., 0.5708, 0.4235, 0.5073],
          [0.5246, 0.4811, 0.3961,  ..., 0.3630, 0.4235, 0.4639],
          [0.5665, 0.4768, 0.5569,  ..., 0.3200, 0.5239, 0.5513]],

         [[0.5244, 0.5293, 0.5492,  ..., 0.5026, 0.4718, 0.6021],
          [0.4196, 0.6016, 0.5557,  ..., 0.6370, 0.4340, 0.2838],
          [0.7122, 0.3933, 0.5581,  ..., 0.3531, 0.5732, 0.3757],
          [0.5974, 0.5046, 0.4807,  ..., 0.5011, 0.5274, 0.3933]]],


        [[[0.4654, 0.5832, 0.4540,  ..., 0.2465, 0.3882, 0.6334],
          [0.5562, 0.5259, 0.5086,  ..., 0.4525, 0.3398, 0.5660],
          [0.3919, 0.3794, 0.4974,  ..., 0.3956, 0.3840, 0.5234],
          [0.4863, 0.3337, 0.4741,  ..., 0.4993, 0.5610, 0.5397]],

         [[0.3425, 0.6451, 0.5851,  ..., 0.3909, 0.4441, 0.6637],
          [0.4111, 0.4866, 0.4901,  ..., 0.5140, 0.4017, 0.5201],
          [0.5593, 0.3513, 0.6460,  ..., 0.4760, 0.5525, 0.4718],
          [0.4388, 0.4778, 0.3989,  ..., 0.5290, 0.5765, 0.5703]],

         [[0.2918, 0.6261, 0.5636,  ..., 0.5426, 0.4654, 0.6460],
          [0.5615, 0.4249, 0.4822,  ..., 0.3863, 0.4617, 0.6834],
          [0.4873, 0.4581, 0.4130,  ..., 0.5192, 0.4935, 0.4249],
          [0.4149, 0.3603, 0.3657,  ..., 0.5684, 0.5283, 0.5487]],

         ...,

         [[0.5689, 0.4306, 0.5122,  ..., 0.4373, 0.4354, 0.4258],
          [0.3363, 0.6183, 0.4144,  ..., 0.5254, 0.4050, 0.3277],
          [0.4083, 0.3425, 0.5465,  ..., 0.5171, 0.4297, 0.4579],
          [0.5610, 0.5167, 0.5188,  ..., 0.5775, 0.5198, 0.4915]],

         [[0.4436, 0.5655, 0.4745,  ..., 0.4410, 0.6007, 0.4704],
          [0.3074, 0.4659, 0.5559,  ..., 0.5375, 0.4393, 0.3794],
          [0.6406, 0.4321, 0.6442,  ..., 0.5049, 0.4506, 0.4789],
          [0.4709, 0.4340, 0.5139,  ..., 0.5324, 0.5875, 0.6715]],

         [[0.4518, 0.3390, 0.4958,  ..., 0.5311, 0.3200, 0.4828],
          [0.3594, 0.6732, 0.4678,  ..., 0.5770, 0.4311, 0.3730],
          [0.4359, 0.5770, 0.7326,  ..., 0.4632, 0.5586, 0.4569],
          [0.5142, 0.3016, 0.6114,  ..., 0.5875, 0.5612, 0.5593]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0000, -0.0080, -0.0100,  0.0080, -0.0060, -0.0040, -0.0020, -0.0100,
         0.0140, -0.0140], device='cuda:0')
selected experts tensor([1310, 1497, 1598, 2001, 1617, 2100, 1495, 1974,  946, 1846],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.5303, 0.6385, 0.5477,  ..., 0.4845, 0.4948, 0.5472],
          [0.5033, 0.4731, 0.5763,  ..., 0.4502, 0.5630, 0.5362],
          [0.4648, 0.4535, 0.6048,  ..., 0.5523, 0.4599, 0.4656],
          [0.6502, 0.4989, 0.4590,  ..., 0.6007, 0.5246, 0.6240]],

         [[0.6294, 0.5887, 0.6252,  ..., 0.4381, 0.5383, 0.6082],
          [0.4498, 0.4474, 0.4091,  ..., 0.5166, 0.5097, 0.5926],
          [0.3854, 0.4921, 0.3849,  ..., 0.4420, 0.4961, 0.4808],
          [0.5450, 0.5226, 0.4491,  ..., 0.6198, 0.5417, 0.6189]],

         [[0.4520, 0.5854, 0.6574,  ..., 0.3957, 0.5373, 0.4262],
          [0.3645, 0.4441, 0.4091,  ..., 0.4714, 0.6263, 0.5328],
          [0.4119, 0.5916, 0.4256,  ..., 0.4377, 0.5719, 0.4444],
          [0.5806, 0.5725, 0.3389,  ..., 0.6414, 0.5116, 0.5950]],

         ...,

         [[0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080],
          [0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080],
          [0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080],
          [0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080]],

         [[0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080],
          [0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080],
          [0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080],
          [0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080]],

         [[0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080],
          [0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080],
          [0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080],
          [0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080]]],


        [[[0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080],
          [0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080],
          [0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080],
          [0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080]],

         [[0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080],
          [0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080],
          [0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080],
          [0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080]],

         [[0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080],
          [0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080],
          [0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080],
          [0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080]],

         ...,

         [[0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080],
          [0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080],
          [0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080],
          [0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080]],

         [[0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080],
          [0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080],
          [0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080],
          [0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080]],

         [[0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080],
          [0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080],
          [0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080],
          [0.5060, 0.5060, 0.5060,  ..., 0.5080, 0.5020, 0.5080]]]],
       device='cuda:0')
tensor([[[[0.5243, 0.6325, 0.5417,  ..., 0.4765, 0.4928, 0.5392],
          [0.4973, 0.4671, 0.5703,  ..., 0.4422, 0.5610, 0.5282],
          [0.4588, 0.4475, 0.5988,  ..., 0.5443, 0.4579, 0.4576],
          [0.6442, 0.4929, 0.4530,  ..., 0.5927, 0.5226, 0.6160]],

         [[0.6234, 0.5827, 0.6192,  ..., 0.4301, 0.5363, 0.6002],
          [0.4438, 0.4414, 0.4031,  ..., 0.5086, 0.5077, 0.5846],
          [0.3794, 0.4861, 0.3789,  ..., 0.4340, 0.4941, 0.4728],
          [0.5390, 0.5166, 0.4431,  ..., 0.6118, 0.5397, 0.6109]],

         [[0.4460, 0.5794, 0.6514,  ..., 0.3877, 0.5353, 0.4182],
          [0.3585, 0.4381, 0.4031,  ..., 0.4634, 0.6243, 0.5248],
          [0.4059, 0.5856, 0.4196,  ..., 0.4297, 0.5699, 0.4364],
          [0.5746, 0.5665, 0.3329,  ..., 0.6334, 0.5096, 0.5870]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([0.0060, 0.0060, 0.0060, 0.0060, 0.0080, 0.0080, 0.0080, 0.0080, 0.0020,
        0.0080], device='cuda:0')
selected experts tensor([ 392,  501,  526,  416, 4091, 3989,  631,  319,  867,  556],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.3940, 0.5182, 0.3855,  ..., 0.4124, 0.5717, 0.5319],
          [0.3403, 0.5568, 0.5382,  ..., 0.5389, 0.3256, 0.5253],
          [0.5485, 0.4658, 0.5639,  ..., 0.4704, 0.5161, 0.5257],
          [0.4537, 0.3881, 0.4673,  ..., 0.5552, 0.5037, 0.6023]],

         [[0.4115, 0.5431, 0.5100,  ..., 0.5721, 0.5100, 0.3094],
          [0.5574, 0.5978, 0.5749,  ..., 0.4586, 0.4121, 0.4363],
          [0.5164, 0.5760, 0.3855,  ..., 0.7287, 0.4116, 0.4081],
          [0.6699, 0.5305, 0.4545,  ..., 0.5206, 0.5085, 0.4930]],

         [[0.4913, 0.3825, 0.5912,  ..., 0.4319, 0.5460, 0.5319],
          [0.4784, 0.5015, 0.4257,  ..., 0.5336, 0.5190, 0.4746],
          [0.4350, 0.3647, 0.4724,  ..., 0.3700, 0.4032, 0.3898],
          [0.4030, 0.5964, 0.4780,  ..., 0.5926, 0.7220, 0.4152]],

         ...,

         [[0.6707, 0.3275, 0.3790,  ..., 0.5600, 0.4206, 0.4305],
          [0.6395, 0.5269, 0.4675,  ..., 0.4903, 0.4018, 0.5716],
          [0.4857, 0.4965, 0.4219,  ..., 0.5593, 0.4315, 0.5068],
          [0.7076, 0.4586, 0.5263,  ..., 0.4139, 0.3981, 0.3247]],

         [[0.6707, 0.3275, 0.3790,  ..., 0.5600, 0.4206, 0.4305],
          [0.6395, 0.5269, 0.4675,  ..., 0.4903, 0.4018, 0.5716],
          [0.4857, 0.4965, 0.4219,  ..., 0.5593, 0.4315, 0.5068],
          [0.7076, 0.4586, 0.5263,  ..., 0.4139, 0.3981, 0.3247]],

         [[0.3944, 0.5175, 0.2924,  ..., 0.4419, 0.5407, 0.4713],
          [0.5041, 0.5356, 0.4894,  ..., 0.5474, 0.4363, 0.5052],
          [0.4440, 0.4354, 0.4661,  ..., 0.6119, 0.4497, 0.3161],
          [0.5164, 0.4728, 0.6133,  ..., 0.5511, 0.5013, 0.3819]]],


        [[[0.3574, 0.5394, 0.5821,  ..., 0.6221, 0.4594, 0.6231],
          [0.4788, 0.6224, 0.5549,  ..., 0.6175, 0.7477, 0.6519],
          [0.6749, 0.6178, 0.5725,  ..., 0.5682, 0.5812, 0.3838],
          [0.3940, 0.5211, 0.4914,  ..., 0.4533, 0.5855, 0.4100]],

         [[0.6707, 0.3275, 0.3790,  ..., 0.5600, 0.4206, 0.4305],
          [0.6395, 0.5269, 0.4675,  ..., 0.4903, 0.4018, 0.5716],
          [0.4857, 0.4965, 0.4219,  ..., 0.5593, 0.4315, 0.5068],
          [0.7076, 0.4586, 0.5263,  ..., 0.4139, 0.3981, 0.3247]],

         [[0.5442, 0.5096, 0.5821,  ..., 0.6230, 0.6397, 0.4844],
          [0.3511, 0.5368, 0.3855,  ..., 0.5428, 0.5793, 0.5362],
          [0.3359, 0.6405, 0.6750,  ..., 0.4922, 0.4912, 0.4869],
          [0.4532, 0.4354, 0.3970,  ..., 0.5697, 0.5908, 0.4764]],

         ...,

         [[0.5608, 0.5751, 0.4702,  ..., 0.4598, 0.5717, 0.7211],
          [0.3039, 0.3319, 0.4680,  ..., 0.4828, 0.5385, 0.5553],
          [0.3547, 0.4612, 0.6422,  ..., 0.4115, 0.5969, 0.5707],
          [0.3860, 0.4424, 0.5954,  ..., 0.5029, 0.4794, 0.5285]],

         [[0.4623, 0.5609, 0.4610,  ..., 0.3270, 0.4615, 0.5568],
          [0.5078, 0.4206, 0.4157,  ..., 0.4835, 0.5207, 0.5977],
          [0.5953, 0.5564, 0.5394,  ..., 0.4603, 0.5550, 0.5242],
          [0.4101, 0.4825, 0.5380,  ..., 0.4276, 0.5422, 0.4920]],

         [[0.4933, 0.4712, 0.5138,  ..., 0.5897, 0.6129, 0.4020],
          [0.4302, 0.4007, 0.4233,  ..., 0.4736, 0.4925, 0.4305],
          [0.4537, 0.4893, 0.6610,  ..., 0.5595, 0.5453, 0.3847],
          [0.6305, 0.4590, 0.4719,  ..., 0.5479, 0.4874, 0.4941]]]],
       device='cuda:0')
tensor([[[[0.4050, 0.5192, 0.3785,  ..., 0.4054, 0.5627, 0.5349],
          [0.3513, 0.5578, 0.5312,  ..., 0.5319, 0.3166, 0.5283],
          [0.5595, 0.4668, 0.5569,  ..., 0.4634, 0.5071, 0.5287],
          [0.4647, 0.3891, 0.4603,  ..., 0.5482, 0.4947, 0.6053]],

         [[0.4225, 0.5441, 0.5030,  ..., 0.5651, 0.5010, 0.3124],
          [0.5684, 0.5988, 0.5679,  ..., 0.4516, 0.4031, 0.4393],
          [0.5274, 0.5770, 0.3785,  ..., 0.7217, 0.4026, 0.4111],
          [0.6809, 0.5315, 0.4475,  ..., 0.5136, 0.4995, 0.4960]],

         [[0.5023, 0.3835, 0.5842,  ..., 0.4249, 0.5370, 0.5349],
          [0.4894, 0.5025, 0.4187,  ..., 0.5266, 0.5100, 0.4776],
          [0.4460, 0.3657, 0.4654,  ..., 0.3630, 0.3942, 0.3928],
          [0.4140, 0.5974, 0.4710,  ..., 0.5856, 0.7130, 0.4182]],

         ...,

         [[0.6817, 0.3285, 0.3720,  ..., 0.5530, 0.4116, 0.4335],
          [0.6505, 0.5279, 0.4605,  ..., 0.4833, 0.3928, 0.5746],
          [0.4967, 0.4975, 0.4149,  ..., 0.5523, 0.4225, 0.5098],
          [0.7186, 0.4596, 0.5193,  ..., 0.4069, 0.3891, 0.3277]],

         [[0.6817, 0.3285, 0.3720,  ..., 0.5530, 0.4116, 0.4335],
          [0.6505, 0.5279, 0.4605,  ..., 0.4833, 0.3928, 0.5746],
          [0.4967, 0.4975, 0.4149,  ..., 0.5523, 0.4225, 0.5098],
          [0.7186, 0.4596, 0.5193,  ..., 0.4069, 0.3891, 0.3277]],

         [[0.4054, 0.5185, 0.2854,  ..., 0.4349, 0.5317, 0.4743],
          [0.5151, 0.5366, 0.4824,  ..., 0.5404, 0.4273, 0.5082],
          [0.4550, 0.4364, 0.4591,  ..., 0.6049, 0.4407, 0.3191],
          [0.5274, 0.4738, 0.6063,  ..., 0.5441, 0.4923, 0.3849]]],


        [[[0.3684, 0.5404, 0.5751,  ..., 0.6151, 0.4504, 0.6261],
          [0.4898, 0.6234, 0.5479,  ..., 0.6105, 0.7387, 0.6549],
          [0.6859, 0.6188, 0.5655,  ..., 0.5612, 0.5722, 0.3868],
          [0.4050, 0.5221, 0.4844,  ..., 0.4463, 0.5765, 0.4130]],

         [[0.6817, 0.3285, 0.3720,  ..., 0.5530, 0.4116, 0.4335],
          [0.6505, 0.5279, 0.4605,  ..., 0.4833, 0.3928, 0.5746],
          [0.4967, 0.4975, 0.4149,  ..., 0.5523, 0.4225, 0.5098],
          [0.7186, 0.4596, 0.5193,  ..., 0.4069, 0.3891, 0.3277]],

         [[0.5552, 0.5106, 0.5751,  ..., 0.6160, 0.6307, 0.4874],
          [0.3621, 0.5378, 0.3785,  ..., 0.5358, 0.5703, 0.5392],
          [0.3469, 0.6415, 0.6680,  ..., 0.4852, 0.4822, 0.4899],
          [0.4642, 0.4364, 0.3900,  ..., 0.5627, 0.5818, 0.4794]],

         ...,

         [[0.5718, 0.5761, 0.4632,  ..., 0.4528, 0.5627, 0.7241],
          [0.3149, 0.3329, 0.4610,  ..., 0.4758, 0.5295, 0.5583],
          [0.3657, 0.4622, 0.6352,  ..., 0.4045, 0.5879, 0.5737],
          [0.3970, 0.4434, 0.5884,  ..., 0.4959, 0.4704, 0.5315]],

         [[0.4733, 0.5619, 0.4540,  ..., 0.3200, 0.4525, 0.5598],
          [0.5188, 0.4216, 0.4087,  ..., 0.4765, 0.5117, 0.6007],
          [0.6063, 0.5574, 0.5324,  ..., 0.4533, 0.5460, 0.5272],
          [0.4211, 0.4835, 0.5310,  ..., 0.4206, 0.5332, 0.4950]],

         [[0.5043, 0.4722, 0.5068,  ..., 0.5827, 0.6039, 0.4050],
          [0.4412, 0.4017, 0.4163,  ..., 0.4666, 0.4835, 0.4335],
          [0.4647, 0.4903, 0.6540,  ..., 0.5525, 0.5363, 0.3877],
          [0.6415, 0.4600, 0.4649,  ..., 0.5409, 0.4784, 0.4971]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0110, -0.0010,  0.0070,  0.0030,  0.0070,  0.0050, -0.0050,  0.0070,
         0.0090, -0.0030], device='cuda:0')
selected experts tensor([1575, 1587, 1667, 1386, 1778, 1638, 1787, 1538, 1736, 1692],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6508, 0.5412, 0.6420,  ..., 0.4138, 0.6579, 0.4468],
          [0.4322, 0.6072, 0.5587,  ..., 0.5750, 0.6102, 0.4621],
          [0.5314, 0.5795, 0.4761,  ..., 0.4233, 0.3525, 0.5000],
          [0.4713, 0.4208, 0.4515,  ..., 0.4947, 0.3986, 0.5565]],

         [[0.4825, 0.5259, 0.5413,  ..., 0.4524, 0.4939, 0.6083],
          [0.4217, 0.4647, 0.5948,  ..., 0.3403, 0.6675, 0.4758],
          [0.4960, 0.6756, 0.6302,  ..., 0.4186, 0.4606, 0.4478],
          [0.3129, 0.4308, 0.5050,  ..., 0.4377, 0.4089, 0.5183]],

         [[0.5990, 0.4364, 0.4992,  ..., 0.4704, 0.5536, 0.4476],
          [0.4493, 0.6090, 0.4443,  ..., 0.4788, 0.5383, 0.5408],
          [0.6139, 0.5429, 0.4882,  ..., 0.4505, 0.5285, 0.3669],
          [0.4534, 0.6354, 0.4587,  ..., 0.6047, 0.5661, 0.4558]],

         ...,

         [[0.4454, 0.5691, 0.5288,  ..., 0.4039, 0.6998, 0.4341],
          [0.4640, 0.4103, 0.3752,  ..., 0.2494, 0.5558, 0.4587],
          [0.4691, 0.4985, 0.4894,  ..., 0.4143, 0.4141, 0.5080],
          [0.4793, 0.4414, 0.3955,  ..., 0.6172, 0.5070, 0.4355]],

         [[0.4454, 0.5691, 0.5288,  ..., 0.4039, 0.6998, 0.4341],
          [0.4640, 0.4103, 0.3752,  ..., 0.2494, 0.5558, 0.4587],
          [0.4691, 0.4985, 0.4894,  ..., 0.4143, 0.4141, 0.5080],
          [0.4793, 0.4414, 0.3955,  ..., 0.6172, 0.5070, 0.4355]],

         [[0.4360, 0.3914, 0.5131,  ..., 0.3879, 0.5118, 0.4764],
          [0.5536, 0.4987, 0.6000,  ..., 0.5745, 0.4935, 0.5105],
          [0.3376, 0.4041, 0.5401,  ..., 0.5123, 0.4592, 0.5400],
          [0.4635, 0.3807, 0.5305,  ..., 0.4568, 0.4934, 0.5681]]],


        [[[0.6552, 0.5077, 0.4513,  ..., 0.3852, 0.4558, 0.4420],
          [0.4151, 0.4458, 0.5925,  ..., 0.3968, 0.5463, 0.6830],
          [0.5068, 0.4184, 0.5137,  ..., 0.5478, 0.5507, 0.6355],
          [0.4664, 0.6488, 0.4612,  ..., 0.4181, 0.5084, 0.4797]],

         [[0.4454, 0.5691, 0.5288,  ..., 0.4039, 0.6998, 0.4341],
          [0.4640, 0.4103, 0.3752,  ..., 0.2494, 0.5558, 0.4587],
          [0.4691, 0.4985, 0.4894,  ..., 0.4143, 0.4141, 0.5080],
          [0.4793, 0.4414, 0.3955,  ..., 0.6172, 0.5070, 0.4355]],

         [[0.4923, 0.5078, 0.4931,  ..., 0.5565, 0.4999, 0.5312],
          [0.4066, 0.5652, 0.5249,  ..., 0.4638, 0.4165, 0.4519],
          [0.4459, 0.4643, 0.5710,  ..., 0.4705, 0.5359, 0.4996],
          [0.5604, 0.5681, 0.4684,  ..., 0.6231, 0.5488, 0.5134]],

         ...,

         [[0.3935, 0.5066, 0.4351,  ..., 0.5645, 0.5957, 0.6614],
          [0.4284, 0.3285, 0.5043,  ..., 0.5488, 0.5150, 0.4488],
          [0.6605, 0.3738, 0.4638,  ..., 0.4128, 0.3778, 0.4824],
          [0.5398, 0.6815, 0.3716,  ..., 0.5015, 0.3898, 0.4858]],

         [[0.4606, 0.5298, 0.4399,  ..., 0.5333, 0.3187, 0.4322],
          [0.5012, 0.5843, 0.5195,  ..., 0.3342, 0.4524, 0.6364],
          [0.6310, 0.5215, 0.5365,  ..., 0.6466, 0.4108, 0.4667],
          [0.3884, 0.5482, 0.3918,  ..., 0.5953, 0.3893, 0.6499]],

         [[0.4792, 0.3994, 0.4100,  ..., 0.4726, 0.5376, 0.4558],
          [0.3981, 0.5786, 0.4011,  ..., 0.4295, 0.3930, 0.4806],
          [0.5099, 0.3320, 0.5374,  ..., 0.4010, 0.3760, 0.6008],
          [0.5290, 0.4497, 0.5510,  ..., 0.5630, 0.4767, 0.5805]]]],
       device='cuda:0')
tensor([[[[0.6478, 0.5482, 0.6370,  ..., 0.4168, 0.6549, 0.4438],
          [0.4292, 0.6142, 0.5537,  ..., 0.5780, 0.6072, 0.4591],
          [0.5284, 0.5865, 0.4711,  ..., 0.4263, 0.3495, 0.4970],
          [0.4683, 0.4278, 0.4465,  ..., 0.4977, 0.3956, 0.5535]],

         [[0.4795, 0.5329, 0.5363,  ..., 0.4554, 0.4909, 0.6053],
          [0.4187, 0.4717, 0.5898,  ..., 0.3433, 0.6645, 0.4728],
          [0.4930, 0.6826, 0.6252,  ..., 0.4216, 0.4576, 0.4448],
          [0.3099, 0.4378, 0.5000,  ..., 0.4407, 0.4059, 0.5153]],

         [[0.5960, 0.4434, 0.4942,  ..., 0.4734, 0.5506, 0.4446],
          [0.4463, 0.6160, 0.4393,  ..., 0.4818, 0.5353, 0.5378],
          [0.6109, 0.5499, 0.4832,  ..., 0.4535, 0.5255, 0.3639],
          [0.4504, 0.6424, 0.4537,  ..., 0.6077, 0.5631, 0.4528]],

         ...,

         [[0.4424, 0.5761, 0.5238,  ..., 0.4069, 0.6968, 0.4311],
          [0.4610, 0.4173, 0.3702,  ..., 0.2524, 0.5528, 0.4557],
          [0.4661, 0.5055, 0.4844,  ..., 0.4173, 0.4111, 0.5050],
          [0.4763, 0.4484, 0.3905,  ..., 0.6202, 0.5040, 0.4325]],

         [[0.4424, 0.5761, 0.5238,  ..., 0.4069, 0.6968, 0.4311],
          [0.4610, 0.4173, 0.3702,  ..., 0.2524, 0.5528, 0.4557],
          [0.4661, 0.5055, 0.4844,  ..., 0.4173, 0.4111, 0.5050],
          [0.4763, 0.4484, 0.3905,  ..., 0.6202, 0.5040, 0.4325]],

         [[0.4330, 0.3984, 0.5081,  ..., 0.3909, 0.5088, 0.4734],
          [0.5506, 0.5057, 0.5950,  ..., 0.5775, 0.4905, 0.5075],
          [0.3346, 0.4111, 0.5351,  ..., 0.5153, 0.4562, 0.5370],
          [0.4605, 0.3877, 0.5255,  ..., 0.4598, 0.4904, 0.5651]]],


        [[[0.6522, 0.5147, 0.4463,  ..., 0.3882, 0.4528, 0.4390],
          [0.4121, 0.4528, 0.5875,  ..., 0.3998, 0.5433, 0.6800],
          [0.5038, 0.4254, 0.5087,  ..., 0.5508, 0.5477, 0.6325],
          [0.4634, 0.6558, 0.4562,  ..., 0.4211, 0.5054, 0.4767]],

         [[0.4424, 0.5761, 0.5238,  ..., 0.4069, 0.6968, 0.4311],
          [0.4610, 0.4173, 0.3702,  ..., 0.2524, 0.5528, 0.4557],
          [0.4661, 0.5055, 0.4844,  ..., 0.4173, 0.4111, 0.5050],
          [0.4763, 0.4484, 0.3905,  ..., 0.6202, 0.5040, 0.4325]],

         [[0.4893, 0.5148, 0.4881,  ..., 0.5595, 0.4969, 0.5282],
          [0.4036, 0.5722, 0.5199,  ..., 0.4668, 0.4135, 0.4489],
          [0.4429, 0.4713, 0.5660,  ..., 0.4735, 0.5329, 0.4966],
          [0.5574, 0.5751, 0.4634,  ..., 0.6261, 0.5458, 0.5104]],

         ...,

         [[0.3905, 0.5136, 0.4301,  ..., 0.5675, 0.5927, 0.6584],
          [0.4254, 0.3355, 0.4993,  ..., 0.5518, 0.5120, 0.4458],
          [0.6575, 0.3808, 0.4588,  ..., 0.4158, 0.3748, 0.4794],
          [0.5368, 0.6885, 0.3666,  ..., 0.5045, 0.3868, 0.4828]],

         [[0.4576, 0.5368, 0.4349,  ..., 0.5363, 0.3157, 0.4292],
          [0.4982, 0.5913, 0.5145,  ..., 0.3372, 0.4494, 0.6334],
          [0.6280, 0.5285, 0.5315,  ..., 0.6496, 0.4078, 0.4637],
          [0.3854, 0.5552, 0.3868,  ..., 0.5983, 0.3863, 0.6469]],

         [[0.4762, 0.4064, 0.4050,  ..., 0.4756, 0.5346, 0.4528],
          [0.3951, 0.5856, 0.3961,  ..., 0.4325, 0.3900, 0.4776],
          [0.5069, 0.3390, 0.5324,  ..., 0.4040, 0.3730, 0.5978],
          [0.5260, 0.4567, 0.5460,  ..., 0.5660, 0.4737, 0.5775]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0030, -0.0070,  0.0050, -0.0030,  0.0010,  0.0050, -0.0050, -0.0030,
         0.0030,  0.0030], device='cuda:0')
selected experts tensor([1517, 1620, 1800, 1562, 1506, 1795, 1648, 1826, 1554, 1556],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5670, 0.4414, 0.4797,  ..., 0.5887, 0.3935, 0.4195],
          [0.4268, 0.5516, 0.5047,  ..., 0.4144, 0.4872, 0.4472],
          [0.4874, 0.5633, 0.4746,  ..., 0.6570, 0.4564, 0.5796],
          [0.6344, 0.4402, 0.3768,  ..., 0.4991, 0.3735, 0.5639]],

         [[0.4644, 0.4275, 0.5268,  ..., 0.5041, 0.4313, 0.5568],
          [0.5383, 0.5993, 0.5435,  ..., 0.4610, 0.5738, 0.4281],
          [0.4649, 0.4511, 0.6098,  ..., 0.5331, 0.4545, 0.5240],
          [0.5045, 0.3650, 0.4710,  ..., 0.6160, 0.5275, 0.4639]],

         [[0.4379, 0.4972, 0.4404,  ..., 0.5627, 0.4466, 0.4839],
          [0.5499, 0.6086, 0.6992,  ..., 0.2889, 0.4983, 0.5899],
          [0.3868, 0.5067, 0.5019,  ..., 0.5779, 0.4055, 0.3797],
          [0.5017, 0.4203, 0.4064,  ..., 0.6665, 0.4867, 0.5390]],

         ...,

         [[0.4511, 0.6282, 0.4779,  ..., 0.5355, 0.4214, 0.5191],
          [0.5077, 0.5376, 0.5661,  ..., 0.3968, 0.4743, 0.5852],
          [0.4705, 0.4685, 0.3676,  ..., 0.3520, 0.4821, 0.4935],
          [0.5325, 0.4371, 0.5234,  ..., 0.4651, 0.4471, 0.5090]],

         [[0.4226, 0.4873, 0.4111,  ..., 0.5331, 0.4394, 0.5668],
          [0.4901, 0.5042, 0.6753,  ..., 0.3893, 0.3780, 0.6859],
          [0.5171, 0.5356, 0.3927,  ..., 0.3412, 0.5415, 0.5334],
          [0.5913, 0.4518, 0.5532,  ..., 0.4196, 0.4986, 0.5093]],

         [[0.4742, 0.4963, 0.5273,  ..., 0.3996, 0.4064, 0.4407],
          [0.4107, 0.5385, 0.6352,  ..., 0.5200, 0.3592, 0.5016],
          [0.4664, 0.4308, 0.6116,  ..., 0.5231, 0.5642, 0.4721],
          [0.4851, 0.4409, 0.4854,  ..., 0.5292, 0.5496, 0.4895]]],


        [[[0.3985, 0.4417, 0.5317,  ..., 0.4852, 0.5401, 0.5040],
          [0.5259, 0.5908, 0.4739,  ..., 0.4901, 0.5479, 0.5715],
          [0.5167, 0.5885, 0.4126,  ..., 0.5102, 0.4356, 0.4666],
          [0.3989, 0.4203, 0.4621,  ..., 0.4546, 0.5663, 0.6148]],

         [[0.5012, 0.5113, 0.4392,  ..., 0.4382, 0.3953, 0.4199],
          [0.5552, 0.5373, 0.4898,  ..., 0.6110, 0.3505, 0.5620],
          [0.3595, 0.6417, 0.5237,  ..., 0.6115, 0.5920, 0.4711],
          [0.5593, 0.3793, 0.3135,  ..., 0.5560, 0.5214, 0.5383]],

         [[0.5119, 0.5904, 0.4728,  ..., 0.4077, 0.4004, 0.3895],
          [0.3795, 0.5356, 0.5888,  ..., 0.3702, 0.4064, 0.4622],
          [0.5574, 0.6470, 0.5609,  ..., 0.5248, 0.6020, 0.5132],
          [0.5138, 0.3596, 0.5370,  ..., 0.5646, 0.6393, 0.5539]],

         ...,

         [[0.5164, 0.4098, 0.6125,  ..., 0.5207, 0.5107, 0.6495],
          [0.4620, 0.5274, 0.4283,  ..., 0.5401, 0.3925, 0.5016],
          [0.3604, 0.5041, 0.6352,  ..., 0.6233, 0.5469, 0.5218],
          [0.5988, 0.2913, 0.6061,  ..., 0.5500, 0.3496, 0.4147]],

         [[0.4755, 0.3821, 0.3603,  ..., 0.4623, 0.4523, 0.6310],
          [0.5535, 0.5686, 0.6098,  ..., 0.3753, 0.3628, 0.4694],
          [0.4917, 0.4709, 0.5057,  ..., 0.4168, 0.5707, 0.4757],
          [0.4465, 0.4356, 0.5365,  ..., 0.3799, 0.5805, 0.5387]],

         [[0.4036, 0.4611, 0.6153,  ..., 0.5641, 0.4901, 0.5165],
          [0.3850, 0.5511, 0.4672,  ..., 0.5224, 0.4022, 0.4489],
          [0.5799, 0.4795, 0.5397,  ..., 0.5093, 0.5598, 0.3408],
          [0.4987, 0.3259, 0.5143,  ..., 0.4954, 0.6384, 0.3755]]]],
       device='cuda:0')
tensor([[[[0.5660, 0.4484, 0.4887,  ..., 0.5997, 0.3785, 0.4345],
          [0.4258, 0.5586, 0.5137,  ..., 0.4254, 0.4722, 0.4622],
          [0.4864, 0.5703, 0.4836,  ..., 0.6680, 0.4414, 0.5946],
          [0.6334, 0.4472, 0.3858,  ..., 0.5101, 0.3585, 0.5789]],

         [[0.4634, 0.4345, 0.5358,  ..., 0.5151, 0.4163, 0.5718],
          [0.5373, 0.6063, 0.5525,  ..., 0.4720, 0.5588, 0.4431],
          [0.4639, 0.4581, 0.6188,  ..., 0.5441, 0.4395, 0.5390],
          [0.5035, 0.3720, 0.4800,  ..., 0.6270, 0.5125, 0.4789]],

         [[0.4369, 0.5042, 0.4494,  ..., 0.5737, 0.4316, 0.4989],
          [0.5489, 0.6156, 0.7082,  ..., 0.2999, 0.4833, 0.6049],
          [0.3858, 0.5137, 0.5109,  ..., 0.5889, 0.3905, 0.3947],
          [0.5007, 0.4273, 0.4154,  ..., 0.6775, 0.4717, 0.5540]],

         ...,

         [[0.4501, 0.6352, 0.4869,  ..., 0.5465, 0.4064, 0.5341],
          [0.5067, 0.5446, 0.5751,  ..., 0.4078, 0.4593, 0.6002],
          [0.4695, 0.4755, 0.3766,  ..., 0.3630, 0.4671, 0.5085],
          [0.5315, 0.4441, 0.5324,  ..., 0.4761, 0.4321, 0.5240]],

         [[0.4216, 0.4943, 0.4201,  ..., 0.5441, 0.4244, 0.5818],
          [0.4891, 0.5112, 0.6843,  ..., 0.4003, 0.3630, 0.7009],
          [0.5161, 0.5426, 0.4017,  ..., 0.3522, 0.5265, 0.5484],
          [0.5903, 0.4588, 0.5622,  ..., 0.4306, 0.4836, 0.5243]],

         [[0.4732, 0.5033, 0.5363,  ..., 0.4106, 0.3914, 0.4557],
          [0.4097, 0.5455, 0.6442,  ..., 0.5310, 0.3442, 0.5166],
          [0.4654, 0.4378, 0.6206,  ..., 0.5341, 0.5492, 0.4871],
          [0.4841, 0.4479, 0.4944,  ..., 0.5402, 0.5346, 0.5045]]],


        [[[0.3975, 0.4487, 0.5407,  ..., 0.4962, 0.5251, 0.5190],
          [0.5249, 0.5978, 0.4829,  ..., 0.5011, 0.5329, 0.5865],
          [0.5157, 0.5955, 0.4216,  ..., 0.5212, 0.4206, 0.4816],
          [0.3979, 0.4273, 0.4711,  ..., 0.4656, 0.5513, 0.6298]],

         [[0.5002, 0.5183, 0.4482,  ..., 0.4492, 0.3803, 0.4349],
          [0.5542, 0.5443, 0.4988,  ..., 0.6220, 0.3355, 0.5770],
          [0.3585, 0.6487, 0.5327,  ..., 0.6225, 0.5770, 0.4861],
          [0.5583, 0.3863, 0.3225,  ..., 0.5670, 0.5064, 0.5533]],

         [[0.5109, 0.5974, 0.4818,  ..., 0.4187, 0.3854, 0.4045],
          [0.3785, 0.5426, 0.5978,  ..., 0.3812, 0.3914, 0.4772],
          [0.5564, 0.6540, 0.5699,  ..., 0.5358, 0.5870, 0.5282],
          [0.5128, 0.3666, 0.5460,  ..., 0.5756, 0.6243, 0.5689]],

         ...,

         [[0.5154, 0.4168, 0.6215,  ..., 0.5317, 0.4957, 0.6645],
          [0.4610, 0.5344, 0.4373,  ..., 0.5511, 0.3775, 0.5166],
          [0.3594, 0.5111, 0.6442,  ..., 0.6343, 0.5319, 0.5368],
          [0.5978, 0.2983, 0.6151,  ..., 0.5610, 0.3346, 0.4297]],

         [[0.4745, 0.3891, 0.3693,  ..., 0.4733, 0.4373, 0.6460],
          [0.5525, 0.5756, 0.6188,  ..., 0.3863, 0.3478, 0.4844],
          [0.4907, 0.4779, 0.5147,  ..., 0.4278, 0.5557, 0.4907],
          [0.4455, 0.4426, 0.5455,  ..., 0.3909, 0.5655, 0.5537]],

         [[0.4026, 0.4681, 0.6243,  ..., 0.5751, 0.4751, 0.5315],
          [0.3840, 0.5581, 0.4762,  ..., 0.5334, 0.3872, 0.4639],
          [0.5789, 0.4865, 0.5487,  ..., 0.5203, 0.5448, 0.3558],
          [0.4977, 0.3329, 0.5233,  ..., 0.5064, 0.6234, 0.3905]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([ 0.0010, -0.0070, -0.0090,  0.0070, -0.0050, -0.0050, -0.0010, -0.0110,
         0.0150, -0.0150], device='cuda:0')
selected experts tensor([1525, 1424, 1875, 1489, 1780, 2043, 1466, 2016,  810, 1956],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.4035, 0.6313, 0.6016,  ..., 0.5328, 0.6250, 0.5412],
          [0.5073, 0.4905, 0.5344,  ..., 0.5855, 0.6069, 0.5652],
          [0.4799, 0.4656, 0.5826,  ..., 0.3711, 0.4878, 0.6958],
          [0.4247, 0.5656, 0.3956,  ..., 0.5085, 0.4787, 0.5463]],

         [[0.5305, 0.5053, 0.6592,  ..., 0.4286, 0.5391, 0.5603],
          [0.5419, 0.5533, 0.3924,  ..., 0.5132, 0.6813, 0.5185],
          [0.3984, 0.5316, 0.3836,  ..., 0.3870, 0.4780, 0.5545],
          [0.4615, 0.6322, 0.4319,  ..., 0.6064, 0.5011, 0.5358]],

         [[0.5240, 0.5450, 0.6295,  ..., 0.3829, 0.3552, 0.5034],
          [0.4887, 0.4257, 0.4765,  ..., 0.5536, 0.6400, 0.4693],
          [0.4576, 0.4480, 0.4511,  ..., 0.3024, 0.5117, 0.4130],
          [0.5959, 0.4115, 0.4617,  ..., 0.5384, 0.4341, 0.6315]],

         ...,

         [[0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090]],

         [[0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090]],

         [[0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090]]],


        [[[0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090]],

         [[0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090]],

         [[0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090]],

         ...,

         [[0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090]],

         [[0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090]],

         [[0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5070, 0.5070,  ..., 0.5090, 0.5030, 0.5090]]]],
       device='cuda:0')
tensor([[[[0.3965, 0.6243, 0.5946,  ..., 0.5238, 0.6220, 0.5322],
          [0.5003, 0.4835, 0.5274,  ..., 0.5765, 0.6039, 0.5562],
          [0.4729, 0.4586, 0.5756,  ..., 0.3621, 0.4848, 0.6868],
          [0.4177, 0.5586, 0.3886,  ..., 0.4995, 0.4757, 0.5373]],

         [[0.5235, 0.4983, 0.6522,  ..., 0.4196, 0.5361, 0.5513],
          [0.5349, 0.5463, 0.3854,  ..., 0.5042, 0.6783, 0.5095],
          [0.3914, 0.5246, 0.3766,  ..., 0.3780, 0.4750, 0.5455],
          [0.4545, 0.6252, 0.4249,  ..., 0.5974, 0.4981, 0.5268]],

         [[0.5170, 0.5380, 0.6225,  ..., 0.3739, 0.3522, 0.4944],
          [0.4817, 0.4187, 0.4695,  ..., 0.5446, 0.6370, 0.4603],
          [0.4506, 0.4410, 0.4441,  ..., 0.2934, 0.5087, 0.4040],
          [0.5889, 0.4045, 0.4547,  ..., 0.5294, 0.4311, 0.6225]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0090, 0.0090, 0.0030,
        0.0090], device='cuda:0')
selected experts tensor([ 378,  506,  698,  411,  452,  464, 4082, 4037,  716,  544],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4651, 0.6058, 0.6015,  ..., 0.4634, 0.4647, 0.5256],
          [0.4307, 0.4886, 0.5072,  ..., 0.4753, 0.4477, 0.5778],
          [0.5372, 0.4499, 0.4959,  ..., 0.4596, 0.5627, 0.4176],
          [0.4612, 0.4102, 0.3849,  ..., 0.4955, 0.4480, 0.3851]],

         [[0.6658, 0.4596, 0.4549,  ..., 0.4514, 0.5214, 0.4754],
          [0.5897, 0.5021, 0.4200,  ..., 0.4487, 0.3874, 0.5920],
          [0.3675, 0.5062, 0.5906,  ..., 0.4835, 0.5521, 0.6411],
          [0.5433, 0.3416, 0.6057,  ..., 0.6647, 0.4139, 0.6194]],

         [[0.4977, 0.4429, 0.5696,  ..., 0.4485, 0.5080, 0.2396],
          [0.5241, 0.4364, 0.6001,  ..., 0.6300, 0.7210, 0.3671],
          [0.5723, 0.5913, 0.7053,  ..., 0.5579, 0.4163, 0.4960],
          [0.5270, 0.5903, 0.5327,  ..., 0.4073, 0.5950, 0.5464]],

         ...,

         [[0.4715, 0.5846, 0.5954,  ..., 0.5964, 0.3855, 0.2984],
          [0.4872, 0.3891, 0.4990,  ..., 0.5656, 0.5755, 0.4401],
          [0.6270, 0.5530, 0.4820,  ..., 0.6198, 0.4087, 0.6032],
          [0.6414, 0.4816, 0.5479,  ..., 0.5373, 0.4315, 0.4247]],

         [[0.5314, 0.5063, 0.6271,  ..., 0.5793, 0.3883, 0.4750],
          [0.6396, 0.5610, 0.5527,  ..., 0.4509, 0.4348, 0.5066],
          [0.3828, 0.4206, 0.4714,  ..., 0.5173, 0.4511, 0.6321],
          [0.4445, 0.4989, 0.5968,  ..., 0.5707, 0.5455, 0.3429]],

         [[0.6484, 0.3711, 0.6749,  ..., 0.4434, 0.6432, 0.4066],
          [0.3557, 0.3919, 0.6547,  ..., 0.4753, 0.3638, 0.4991],
          [0.4387, 0.4637, 0.5377,  ..., 0.6166, 0.5721, 0.3263],
          [0.4290, 0.4325, 0.5677,  ..., 0.5352, 0.4310, 0.6438]]],


        [[[0.6692, 0.6514, 0.5460,  ..., 0.5362, 0.4659, 0.5420],
          [0.5642, 0.4709, 0.4646,  ..., 0.3994, 0.4490, 0.5849],
          [0.4073, 0.6424, 0.4195,  ..., 0.5301, 0.5917, 0.3874],
          [0.5522, 0.7564, 0.5973,  ..., 0.6350, 0.3665, 0.4442]],

         [[0.5935, 0.4154, 0.3826,  ..., 0.4891, 0.6110, 0.5555],
          [0.6088, 0.4862, 0.3142,  ..., 0.4559, 0.4013, 0.4728],
          [0.4829, 0.5183, 0.4597,  ..., 0.4630, 0.5807, 0.4723],
          [0.5723, 0.4182, 0.5243,  ..., 0.3426, 0.5596, 0.5725]],

         [[0.5297, 0.3794, 0.4810,  ..., 0.5325, 0.4795, 0.5461],
          [0.5003, 0.4467, 0.5265,  ..., 0.5212, 0.5646, 0.4341],
          [0.3969, 0.5351, 0.5655,  ..., 0.6965, 0.3656, 0.5575],
          [0.4273, 0.5383, 0.5792,  ..., 0.5421, 0.5059, 0.4014]],

         ...,

         [[0.3703, 0.5272, 0.5269,  ..., 0.3980, 0.6157, 0.6171],
          [0.5897, 0.4316, 0.3780,  ..., 0.4276, 0.5188, 0.5854],
          [0.3786, 0.4111, 0.5366,  ..., 0.4391, 0.3920, 0.4678],
          [0.3987, 0.6160, 0.4518,  ..., 0.5845, 0.6396, 0.5839]],

         [[0.3584, 0.5404, 0.5816,  ..., 0.6231, 0.4591, 0.6221],
          [0.4799, 0.6234, 0.5537,  ..., 0.6185, 0.7467, 0.6509],
          [0.6759, 0.6188, 0.5715,  ..., 0.5692, 0.5802, 0.3823],
          [0.3950, 0.5221, 0.4903,  ..., 0.4540, 0.5845, 0.4090]],

         [[0.6115, 0.4620, 0.4361,  ..., 0.4812, 0.5184, 0.5130],
          [0.5367, 0.5703, 0.5549,  ..., 0.5029, 0.4610, 0.3563],
          [0.5270, 0.6137, 0.4856,  ..., 0.5286, 0.6129, 0.5221],
          [0.4707, 0.4706, 0.4789,  ..., 0.4876, 0.4310, 0.4633]]]],
       device='cuda:0')
tensor([[[[0.4751, 0.6058, 0.5955,  ..., 0.4554, 0.4567, 0.5296],
          [0.4407, 0.4886, 0.5012,  ..., 0.4673, 0.4397, 0.5818],
          [0.5472, 0.4499, 0.4899,  ..., 0.4516, 0.5547, 0.4216],
          [0.4712, 0.4102, 0.3789,  ..., 0.4875, 0.4400, 0.3891]],

         [[0.6758, 0.4596, 0.4489,  ..., 0.4434, 0.5134, 0.4794],
          [0.5997, 0.5021, 0.4140,  ..., 0.4407, 0.3794, 0.5960],
          [0.3775, 0.5062, 0.5846,  ..., 0.4755, 0.5441, 0.6451],
          [0.5533, 0.3416, 0.5997,  ..., 0.6567, 0.4059, 0.6234]],

         [[0.5077, 0.4429, 0.5636,  ..., 0.4405, 0.5000, 0.2436],
          [0.5341, 0.4364, 0.5941,  ..., 0.6220, 0.7130, 0.3711],
          [0.5823, 0.5913, 0.6993,  ..., 0.5499, 0.4083, 0.5000],
          [0.5370, 0.5903, 0.5267,  ..., 0.3993, 0.5870, 0.5504]],

         ...,

         [[0.4815, 0.5846, 0.5894,  ..., 0.5884, 0.3775, 0.3024],
          [0.4972, 0.3891, 0.4930,  ..., 0.5576, 0.5675, 0.4441],
          [0.6370, 0.5530, 0.4760,  ..., 0.6118, 0.4007, 0.6072],
          [0.6514, 0.4816, 0.5419,  ..., 0.5293, 0.4235, 0.4287]],

         [[0.5414, 0.5063, 0.6211,  ..., 0.5713, 0.3803, 0.4790],
          [0.6496, 0.5610, 0.5467,  ..., 0.4429, 0.4268, 0.5106],
          [0.3928, 0.4206, 0.4654,  ..., 0.5093, 0.4431, 0.6361],
          [0.4545, 0.4989, 0.5908,  ..., 0.5627, 0.5375, 0.3469]],

         [[0.6584, 0.3711, 0.6689,  ..., 0.4354, 0.6352, 0.4106],
          [0.3657, 0.3919, 0.6487,  ..., 0.4673, 0.3558, 0.5031],
          [0.4487, 0.4637, 0.5317,  ..., 0.6086, 0.5641, 0.3303],
          [0.4390, 0.4325, 0.5617,  ..., 0.5272, 0.4230, 0.6478]]],


        [[[0.6792, 0.6514, 0.5400,  ..., 0.5282, 0.4579, 0.5460],
          [0.5742, 0.4709, 0.4586,  ..., 0.3914, 0.4410, 0.5889],
          [0.4173, 0.6424, 0.4135,  ..., 0.5221, 0.5837, 0.3914],
          [0.5622, 0.7564, 0.5913,  ..., 0.6270, 0.3585, 0.4482]],

         [[0.6035, 0.4154, 0.3766,  ..., 0.4811, 0.6030, 0.5595],
          [0.6188, 0.4862, 0.3082,  ..., 0.4479, 0.3933, 0.4768],
          [0.4929, 0.5183, 0.4537,  ..., 0.4550, 0.5727, 0.4763],
          [0.5823, 0.4182, 0.5183,  ..., 0.3346, 0.5516, 0.5765]],

         [[0.5397, 0.3794, 0.4750,  ..., 0.5245, 0.4715, 0.5501],
          [0.5103, 0.4467, 0.5205,  ..., 0.5132, 0.5566, 0.4381],
          [0.4069, 0.5351, 0.5595,  ..., 0.6885, 0.3576, 0.5615],
          [0.4373, 0.5383, 0.5732,  ..., 0.5341, 0.4979, 0.4054]],

         ...,

         [[0.3803, 0.5272, 0.5209,  ..., 0.3900, 0.6077, 0.6211],
          [0.5997, 0.4316, 0.3720,  ..., 0.4196, 0.5108, 0.5894],
          [0.3886, 0.4111, 0.5306,  ..., 0.4311, 0.3840, 0.4718],
          [0.4087, 0.6160, 0.4458,  ..., 0.5765, 0.6316, 0.5879]],

         [[0.3684, 0.5404, 0.5756,  ..., 0.6151, 0.4511, 0.6261],
          [0.4899, 0.6234, 0.5477,  ..., 0.6105, 0.7387, 0.6549],
          [0.6859, 0.6188, 0.5655,  ..., 0.5612, 0.5722, 0.3863],
          [0.4050, 0.5221, 0.4843,  ..., 0.4460, 0.5765, 0.4130]],

         [[0.6215, 0.4620, 0.4301,  ..., 0.4732, 0.5104, 0.5170],
          [0.5467, 0.5703, 0.5489,  ..., 0.4949, 0.4530, 0.3603],
          [0.5370, 0.6137, 0.4796,  ..., 0.5206, 0.6049, 0.5261],
          [0.4807, 0.4706, 0.4729,  ..., 0.4796, 0.4230, 0.4673]]]],
       device='cuda:0', requires_grad=True)
tensor([-1.0000e-02,  2.3283e-10,  6.0000e-03,  4.0000e-03,  6.0000e-03,
         6.0000e-03, -6.0000e-03,  8.0000e-03,  8.0000e-03, -4.0000e-03],
       device='cuda:0')
selected experts tensor([1640, 1651, 1634, 1695, 1631, 1555, 1541, 1694, 1673, 1670],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4760, 0.4793, 0.5058,  ..., 0.5128, 0.7057, 0.6659],
          [0.5357, 0.4543, 0.3562,  ..., 0.4514, 0.4113, 0.6874],
          [0.4829, 0.4485, 0.4380,  ..., 0.4667, 0.4939, 0.5957],
          [0.5076, 0.5538, 0.5877,  ..., 0.3420, 0.3959, 0.3770]],

         [[0.5891, 0.3330, 0.5393,  ..., 0.4512, 0.4057, 0.4462],
          [0.5180, 0.5240, 0.4380,  ..., 0.5816, 0.5457, 0.4270],
          [0.4897, 0.4528, 0.4099,  ..., 0.4524, 0.4033, 0.5616],
          [0.4964, 0.4574, 0.5078,  ..., 0.5673, 0.5232, 0.5318]],

         [[0.4109, 0.4299, 0.5028,  ..., 0.5182, 0.5590, 0.7731],
          [0.5196, 0.4376, 0.5995,  ..., 0.4218, 0.5490, 0.4790],
          [0.5510, 0.3356, 0.4955,  ..., 0.5663, 0.4638, 0.3518],
          [0.4665, 0.6109, 0.4222,  ..., 0.4261, 0.6874, 0.4682]],

         ...,

         [[0.4256, 0.4919, 0.4631,  ..., 0.6125, 0.4875, 0.6107],
          [0.5019, 0.3642, 0.6301,  ..., 0.2649, 0.6265, 0.3097],
          [0.4189, 0.3962, 0.4670,  ..., 0.5437, 0.5748, 0.3072],
          [0.4866, 0.6364, 0.5122,  ..., 0.4104, 0.6145, 0.6214]],

         [[0.3779, 0.6908, 0.3518,  ..., 0.4090, 0.5196, 0.5863],
          [0.4241, 0.4275, 0.5500,  ..., 0.6579, 0.6428, 0.5995],
          [0.4495, 0.4850, 0.5418,  ..., 0.5985, 0.6214, 0.5372],
          [0.5604, 0.4961, 0.4708,  ..., 0.3958, 0.3922, 0.4076]],

         [[0.4794, 0.4853, 0.5815,  ..., 0.4867, 0.4825, 0.4963],
          [0.2870, 0.5595, 0.5333,  ..., 0.3893, 0.4806, 0.4517],
          [0.4696, 0.5441, 0.6242,  ..., 0.4100, 0.4294, 0.5391],
          [0.3456, 0.3752, 0.5348,  ..., 0.4137, 0.4409, 0.5532]]],


        [[[0.5986, 0.6220, 0.4713,  ..., 0.5635, 0.4979, 0.4631],
          [0.4971, 0.4635, 0.4318,  ..., 0.6321, 0.4289, 0.3829],
          [0.4568, 0.7046, 0.4425,  ..., 0.4825, 0.4488, 0.5710],
          [0.5762, 0.5192, 0.3866,  ..., 0.4739, 0.5486, 0.4322]],

         [[0.6410, 0.3579, 0.4964,  ..., 0.6321, 0.5524, 0.5243],
          [0.4260, 0.4213, 0.5019,  ..., 0.4348, 0.5301, 0.5026],
          [0.4409, 0.4256, 0.4957,  ..., 0.4133, 0.5120, 0.6255],
          [0.4839, 0.5327, 0.4577,  ..., 0.6037, 0.5561, 0.3616]],

         [[0.3369, 0.3579, 0.5180,  ..., 0.4983, 0.5556, 0.5626],
          [0.5461, 0.3597, 0.6437,  ..., 0.5538, 0.6383, 0.5246],
          [0.5577, 0.5065, 0.5910,  ..., 0.3680, 0.5090, 0.4758],
          [0.5279, 0.4330, 0.3403,  ..., 0.4238, 0.4896, 0.5891]],

         ...,

         [[0.5924, 0.5576, 0.4322,  ..., 0.5212, 0.4563, 0.5233],
          [0.5435, 0.6114, 0.5413,  ..., 0.6018, 0.3421, 0.3447],
          [0.4973, 0.4700, 0.4941,  ..., 0.2918, 0.4580, 0.3954],
          [0.6135, 0.3277, 0.5700,  ..., 0.3599, 0.5592, 0.4375]],

         [[0.6562, 0.5086, 0.4500,  ..., 0.3837, 0.4570, 0.4428],
          [0.4165, 0.4465, 0.5915,  ..., 0.3958, 0.5473, 0.6840],
          [0.5080, 0.4189, 0.5127,  ..., 0.5468, 0.5517, 0.6374],
          [0.4672, 0.6498, 0.4602,  ..., 0.4171, 0.5095, 0.4806]],

         [[0.4770, 0.4673, 0.5036,  ..., 0.4761, 0.5195, 0.4227],
          [0.5100, 0.4683, 0.6589,  ..., 0.3545, 0.5585, 0.3857],
          [0.5083, 0.4907, 0.5210,  ..., 0.4548, 0.4869, 0.5317],
          [0.5461, 0.5188, 0.5447,  ..., 0.5104, 0.6607, 0.6177]]]],
       device='cuda:0')
tensor([[[[0.4720, 0.4853, 0.5018,  ..., 0.5168, 0.7017, 0.6619],
          [0.5317, 0.4603, 0.3522,  ..., 0.4554, 0.4073, 0.6834],
          [0.4789, 0.4545, 0.4340,  ..., 0.4707, 0.4899, 0.5917],
          [0.5036, 0.5598, 0.5837,  ..., 0.3460, 0.3919, 0.3730]],

         [[0.5851, 0.3390, 0.5353,  ..., 0.4552, 0.4017, 0.4422],
          [0.5140, 0.5300, 0.4340,  ..., 0.5856, 0.5417, 0.4230],
          [0.4857, 0.4588, 0.4059,  ..., 0.4564, 0.3993, 0.5576],
          [0.4924, 0.4634, 0.5038,  ..., 0.5713, 0.5192, 0.5278]],

         [[0.4069, 0.4359, 0.4988,  ..., 0.5222, 0.5550, 0.7691],
          [0.5156, 0.4436, 0.5955,  ..., 0.4258, 0.5450, 0.4750],
          [0.5470, 0.3416, 0.4915,  ..., 0.5703, 0.4598, 0.3478],
          [0.4625, 0.6169, 0.4182,  ..., 0.4301, 0.6834, 0.4642]],

         ...,

         [[0.4216, 0.4979, 0.4591,  ..., 0.6165, 0.4835, 0.6067],
          [0.4979, 0.3702, 0.6261,  ..., 0.2689, 0.6225, 0.3057],
          [0.4149, 0.4022, 0.4630,  ..., 0.5477, 0.5708, 0.3032],
          [0.4826, 0.6424, 0.5082,  ..., 0.4144, 0.6105, 0.6174]],

         [[0.3739, 0.6968, 0.3478,  ..., 0.4130, 0.5156, 0.5823],
          [0.4201, 0.4335, 0.5460,  ..., 0.6619, 0.6388, 0.5955],
          [0.4455, 0.4910, 0.5378,  ..., 0.6025, 0.6174, 0.5332],
          [0.5564, 0.5021, 0.4668,  ..., 0.3998, 0.3882, 0.4036]],

         [[0.4754, 0.4913, 0.5775,  ..., 0.4907, 0.4785, 0.4923],
          [0.2830, 0.5655, 0.5293,  ..., 0.3933, 0.4766, 0.4477],
          [0.4656, 0.5501, 0.6202,  ..., 0.4140, 0.4254, 0.5351],
          [0.3416, 0.3812, 0.5308,  ..., 0.4177, 0.4369, 0.5492]]],


        [[[0.5946, 0.6280, 0.4673,  ..., 0.5675, 0.4939, 0.4591],
          [0.4931, 0.4695, 0.4278,  ..., 0.6361, 0.4249, 0.3789],
          [0.4528, 0.7106, 0.4385,  ..., 0.4865, 0.4448, 0.5670],
          [0.5722, 0.5252, 0.3826,  ..., 0.4779, 0.5446, 0.4282]],

         [[0.6370, 0.3639, 0.4924,  ..., 0.6361, 0.5484, 0.5203],
          [0.4220, 0.4273, 0.4979,  ..., 0.4388, 0.5261, 0.4986],
          [0.4369, 0.4316, 0.4917,  ..., 0.4173, 0.5080, 0.6215],
          [0.4799, 0.5387, 0.4537,  ..., 0.6077, 0.5521, 0.3576]],

         [[0.3329, 0.3639, 0.5140,  ..., 0.5023, 0.5516, 0.5586],
          [0.5421, 0.3657, 0.6397,  ..., 0.5578, 0.6343, 0.5206],
          [0.5537, 0.5125, 0.5870,  ..., 0.3720, 0.5050, 0.4718],
          [0.5239, 0.4390, 0.3363,  ..., 0.4278, 0.4856, 0.5851]],

         ...,

         [[0.5884, 0.5636, 0.4282,  ..., 0.5252, 0.4523, 0.5193],
          [0.5395, 0.6174, 0.5373,  ..., 0.6058, 0.3381, 0.3407],
          [0.4933, 0.4760, 0.4901,  ..., 0.2958, 0.4540, 0.3914],
          [0.6095, 0.3337, 0.5660,  ..., 0.3639, 0.5552, 0.4335]],

         [[0.6522, 0.5146, 0.4460,  ..., 0.3877, 0.4530, 0.4388],
          [0.4125, 0.4525, 0.5875,  ..., 0.3998, 0.5433, 0.6800],
          [0.5040, 0.4249, 0.5087,  ..., 0.5508, 0.5477, 0.6334],
          [0.4632, 0.6558, 0.4562,  ..., 0.4211, 0.5055, 0.4766]],

         [[0.4730, 0.4733, 0.4996,  ..., 0.4801, 0.5155, 0.4187],
          [0.5060, 0.4743, 0.6549,  ..., 0.3585, 0.5545, 0.3817],
          [0.5043, 0.4967, 0.5170,  ..., 0.4588, 0.4829, 0.5277],
          [0.5421, 0.5248, 0.5407,  ..., 0.5144, 0.6567, 0.6137]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0040, -0.0060,  0.0040, -0.0020,  0.0020,  0.0040, -0.0060, -0.0040,
         0.0040,  0.0040], device='cuda:0')
selected experts tensor([1692, 1657, 1617, 1613, 1609, 1665, 1616, 1647, 1641, 1627],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5039, 0.4453, 0.4554,  ..., 0.4057, 0.3977, 0.4361],
          [0.4103, 0.4816, 0.5972,  ..., 0.4432, 0.4804, 0.5134],
          [0.4905, 0.6210, 0.4769,  ..., 0.6114, 0.4177, 0.4312],
          [0.4432, 0.3994, 0.4476,  ..., 0.5887, 0.6106, 0.5734]],

         [[0.4826, 0.4712, 0.4355,  ..., 0.6577, 0.3727, 0.4588],
          [0.6300, 0.5734, 0.4949,  ..., 0.3771, 0.4792, 0.5586],
          [0.3401, 0.6174, 0.4697,  ..., 0.4015, 0.4964, 0.3843],
          [0.5193, 0.5306, 0.3521,  ..., 0.6402, 0.4608, 0.5610]],

         [[0.4885, 0.4667, 0.4932,  ..., 0.2593, 0.4555, 0.3880],
          [0.4774, 0.4165, 0.4476,  ..., 0.5891, 0.3835, 0.3852],
          [0.5862, 0.5344, 0.5512,  ..., 0.5013, 0.5494, 0.3965],
          [0.2764, 0.4436, 0.5813,  ..., 0.5778, 0.5404, 0.5653]],

         ...,

         [[0.4089, 0.5340, 0.4718,  ..., 0.5212, 0.4644, 0.5558],
          [0.3985, 0.5347, 0.5995,  ..., 0.4880, 0.4853, 0.5327],
          [0.3471, 0.5178, 0.6632,  ..., 0.5626, 0.4567, 0.3754],
          [0.5728, 0.4616, 0.5001,  ..., 0.4304, 0.4807, 0.3452]],

         [[0.5742, 0.4545, 0.5507,  ..., 0.5321, 0.4574, 0.3551],
          [0.5215, 0.5262, 0.6901,  ..., 0.4835, 0.4065, 0.5312],
          [0.5651, 0.6646, 0.6092,  ..., 0.5224, 0.4200, 0.4714],
          [0.6498, 0.3976, 0.5241,  ..., 0.6586, 0.4668, 0.4697]],

         [[0.5115, 0.4918, 0.3833,  ..., 0.4239, 0.4252, 0.3932],
          [0.6272, 0.6220, 0.5082,  ..., 0.4512, 0.3908, 0.4433],
          [0.3777, 0.4700, 0.5670,  ..., 0.4570, 0.5753, 0.4254],
          [0.7094, 0.4256, 0.5156,  ..., 0.5306, 0.3550, 0.4642]]],


        [[[0.5589, 0.4969, 0.4741,  ..., 0.4693, 0.4060, 0.3443],
          [0.5690, 0.4828, 0.4630,  ..., 0.4998, 0.3968, 0.3787],
          [0.4379, 0.4499, 0.6876,  ..., 0.5664, 0.6153, 0.4964],
          [0.5432, 0.3055, 0.2932,  ..., 0.5844, 0.3844, 0.6092]],

         [[0.4369, 0.5044, 0.4707,  ..., 0.5679, 0.3428, 0.4113],
          [0.4963, 0.5033, 0.5085,  ..., 0.3757, 0.5511, 0.5319],
          [0.5319, 0.6841, 0.5068,  ..., 0.4181, 0.4903, 0.4932],
          [0.4070, 0.4523, 0.5490,  ..., 0.5971, 0.4135, 0.5972]],

         [[0.5304, 0.4383, 0.5503,  ..., 0.3628, 0.4428, 0.4974],
          [0.3883, 0.5196, 0.5651,  ..., 0.5679, 0.4139, 0.3749],
          [0.6045, 0.4688, 0.5632,  ..., 0.4225, 0.5262, 0.4639],
          [0.5137, 0.5038, 0.4712,  ..., 0.6340, 0.5911, 0.5191]],

         ...,

         [[0.4825, 0.5634, 0.3134,  ..., 0.5078, 0.3317, 0.3918],
          [0.5666, 0.4948, 0.5594,  ..., 0.3982, 0.4606, 0.5591],
          [0.4126, 0.5352, 0.5358,  ..., 0.4299, 0.5327, 0.5524],
          [0.5036, 0.3831, 0.3884,  ..., 0.5413, 0.2553, 0.4872]],

         [[0.5102, 0.5378, 0.5179,  ..., 0.4340, 0.3981, 0.5186],
          [0.4197, 0.4562, 0.4716,  ..., 0.4886, 0.5023, 0.5481],
          [0.5514, 0.4796, 0.5967,  ..., 0.4432, 0.4916, 0.5686],
          [0.4023, 0.3915, 0.4346,  ..., 0.5830, 0.5020, 0.5700]],

         [[0.4013, 0.4417, 0.4476,  ..., 0.3808, 0.3709, 0.5912],
          [0.4374, 0.5682, 0.5127,  ..., 0.4034, 0.4337, 0.5117],
          [0.4824, 0.4203, 0.5083,  ..., 0.4967, 0.5210, 0.3353],
          [0.5473, 0.4027, 0.4938,  ..., 0.5168, 0.4945, 0.4977]]]],
       device='cuda:0')
tensor([[[[0.5019, 0.4513, 0.4654,  ..., 0.4177, 0.3817, 0.4521],
          [0.4083, 0.4876, 0.6072,  ..., 0.4552, 0.4644, 0.5294],
          [0.4885, 0.6270, 0.4869,  ..., 0.6234, 0.4017, 0.4472],
          [0.4412, 0.4054, 0.4576,  ..., 0.6007, 0.5946, 0.5894]],

         [[0.4806, 0.4772, 0.4455,  ..., 0.6697, 0.3567, 0.4748],
          [0.6280, 0.5794, 0.5049,  ..., 0.3891, 0.4632, 0.5746],
          [0.3381, 0.6234, 0.4797,  ..., 0.4135, 0.4804, 0.4003],
          [0.5173, 0.5366, 0.3621,  ..., 0.6522, 0.4448, 0.5770]],

         [[0.4865, 0.4727, 0.5032,  ..., 0.2713, 0.4395, 0.4040],
          [0.4754, 0.4225, 0.4576,  ..., 0.6011, 0.3675, 0.4012],
          [0.5842, 0.5404, 0.5612,  ..., 0.5133, 0.5334, 0.4125],
          [0.2744, 0.4496, 0.5913,  ..., 0.5898, 0.5244, 0.5813]],

         ...,

         [[0.4069, 0.5400, 0.4818,  ..., 0.5332, 0.4484, 0.5718],
          [0.3965, 0.5407, 0.6095,  ..., 0.5000, 0.4693, 0.5487],
          [0.3451, 0.5238, 0.6732,  ..., 0.5746, 0.4407, 0.3914],
          [0.5708, 0.4676, 0.5101,  ..., 0.4424, 0.4647, 0.3612]],

         [[0.5722, 0.4605, 0.5607,  ..., 0.5441, 0.4414, 0.3711],
          [0.5195, 0.5322, 0.7001,  ..., 0.4955, 0.3905, 0.5472],
          [0.5631, 0.6706, 0.6192,  ..., 0.5344, 0.4040, 0.4874],
          [0.6478, 0.4036, 0.5341,  ..., 0.6706, 0.4508, 0.4857]],

         [[0.5095, 0.4978, 0.3933,  ..., 0.4359, 0.4092, 0.4092],
          [0.6252, 0.6280, 0.5182,  ..., 0.4632, 0.3748, 0.4593],
          [0.3757, 0.4760, 0.5770,  ..., 0.4690, 0.5593, 0.4414],
          [0.7074, 0.4316, 0.5256,  ..., 0.5426, 0.3390, 0.4802]]],


        [[[0.5569, 0.5029, 0.4841,  ..., 0.4813, 0.3900, 0.3603],
          [0.5670, 0.4888, 0.4730,  ..., 0.5118, 0.3808, 0.3947],
          [0.4359, 0.4559, 0.6976,  ..., 0.5784, 0.5993, 0.5124],
          [0.5412, 0.3115, 0.3032,  ..., 0.5964, 0.3684, 0.6252]],

         [[0.4349, 0.5104, 0.4807,  ..., 0.5799, 0.3268, 0.4273],
          [0.4943, 0.5093, 0.5185,  ..., 0.3877, 0.5351, 0.5479],
          [0.5299, 0.6901, 0.5168,  ..., 0.4301, 0.4743, 0.5092],
          [0.4050, 0.4583, 0.5590,  ..., 0.6091, 0.3975, 0.6132]],

         [[0.5284, 0.4443, 0.5603,  ..., 0.3748, 0.4268, 0.5134],
          [0.3863, 0.5256, 0.5751,  ..., 0.5799, 0.3979, 0.3909],
          [0.6025, 0.4748, 0.5732,  ..., 0.4345, 0.5102, 0.4799],
          [0.5117, 0.5098, 0.4812,  ..., 0.6460, 0.5751, 0.5351]],

         ...,

         [[0.4805, 0.5694, 0.3234,  ..., 0.5198, 0.3157, 0.4078],
          [0.5646, 0.5008, 0.5694,  ..., 0.4102, 0.4446, 0.5751],
          [0.4106, 0.5412, 0.5458,  ..., 0.4419, 0.5167, 0.5684],
          [0.5016, 0.3891, 0.3984,  ..., 0.5533, 0.2393, 0.5032]],

         [[0.5082, 0.5438, 0.5279,  ..., 0.4460, 0.3821, 0.5346],
          [0.4177, 0.4622, 0.4816,  ..., 0.5006, 0.4863, 0.5641],
          [0.5494, 0.4856, 0.6067,  ..., 0.4552, 0.4756, 0.5846],
          [0.4003, 0.3975, 0.4446,  ..., 0.5950, 0.4860, 0.5860]],

         [[0.3993, 0.4477, 0.4576,  ..., 0.3928, 0.3549, 0.6072],
          [0.4354, 0.5742, 0.5227,  ..., 0.4154, 0.4177, 0.5277],
          [0.4804, 0.4263, 0.5183,  ..., 0.5087, 0.5050, 0.3513],
          [0.5453, 0.4087, 0.5038,  ..., 0.5288, 0.4785, 0.5137]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0020, -0.0060, -0.0100,  0.0080, -0.0060, -0.0060,  0.0000, -0.0120,
         0.0160, -0.0160], device='cuda:0')
selected experts tensor([1307, 1558, 1775, 1788, 1630, 2123, 1336, 1963, 1084, 1820],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.5978, 0.5755, 0.7056,  ..., 0.3957, 0.5801, 0.5037],
          [0.5783, 0.4984, 0.5142,  ..., 0.4781, 0.6037, 0.5432],
          [0.4210, 0.4300, 0.5402,  ..., 0.5303, 0.5758, 0.5837],
          [0.5397, 0.5605, 0.5707,  ..., 0.5745, 0.5967, 0.6551]],

         [[0.6189, 0.5147, 0.5745,  ..., 0.5030, 0.5364, 0.4582],
          [0.4856, 0.4661, 0.4936,  ..., 0.5769, 0.6401, 0.4698],
          [0.3297, 0.4697, 0.3374,  ..., 0.4300, 0.6283, 0.6288],
          [0.4832, 0.4516, 0.3620,  ..., 0.5526, 0.5143, 0.3631]],

         [[0.5845, 0.4753, 0.7701,  ..., 0.4210, 0.5264, 0.3551],
          [0.4724, 0.5011, 0.5678,  ..., 0.5232, 0.5948, 0.5555],
          [0.5087, 0.4243, 0.5134,  ..., 0.4116, 0.6589, 0.5553],
          [0.5711, 0.6002, 0.4281,  ..., 0.6016, 0.4785, 0.4637]],

         ...,

         [[0.5080, 0.5080, 0.5080,  ..., 0.5080, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5080, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5080, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5080, 0.5040, 0.5100]],

         [[0.5080, 0.5080, 0.5080,  ..., 0.5080, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5080, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5080, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5080, 0.5040, 0.5100]],

         [[0.5080, 0.5080, 0.5080,  ..., 0.5080, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5080, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5080, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5080, 0.5040, 0.5100]]],


        [[[0.4975, 0.4931, 0.6300,  ..., 0.3819, 0.4318, 0.3300],
          [0.4262, 0.5826, 0.4943,  ..., 0.3773, 0.4867, 0.5076],
          [0.5322, 0.5059, 0.4948,  ..., 0.4545, 0.7226, 0.4019],
          [0.6647, 0.6115, 0.3728,  ..., 0.3314, 0.5437, 0.6265]],

         [[0.4569, 0.3791, 0.6222,  ..., 0.3737, 0.5829, 0.4202],
          [0.3297, 0.4608, 0.4008,  ..., 0.6101, 0.5753, 0.6195],
          [0.3920, 0.4353, 0.4521,  ..., 0.4315, 0.4786, 0.4916],
          [0.5367, 0.5699, 0.3828,  ..., 0.5375, 0.5577, 0.6274]],

         [[0.4229, 0.4453, 0.6513,  ..., 0.3683, 0.6392, 0.4497],
          [0.3409, 0.5280, 0.5144,  ..., 0.5063, 0.5488, 0.6710],
          [0.4003, 0.4528, 0.5198,  ..., 0.5048, 0.6089, 0.4136],
          [0.6286, 0.6189, 0.4205,  ..., 0.4787, 0.4687, 0.6003]],

         ...,

         [[0.5080, 0.5080, 0.5080,  ..., 0.5080, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5080, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5080, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5080, 0.5040, 0.5100]],

         [[0.5080, 0.5080, 0.5080,  ..., 0.5080, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5080, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5080, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5080, 0.5040, 0.5100]],

         [[0.5080, 0.5080, 0.5080,  ..., 0.5080, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5080, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5080, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5080, 0.5040, 0.5100]]]],
       device='cuda:0')/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([[[[0.5898, 0.5675, 0.6976,  ..., 0.3877, 0.5761, 0.4937],
          [0.5703, 0.4904, 0.5062,  ..., 0.4701, 0.5997, 0.5332],
          [0.4130, 0.4220, 0.5322,  ..., 0.5223, 0.5718, 0.5737],
          [0.5317, 0.5525, 0.5627,  ..., 0.5665, 0.5927, 0.6451]],

         [[0.6109, 0.5067, 0.5665,  ..., 0.4950, 0.5324, 0.4482],
          [0.4776, 0.4581, 0.4856,  ..., 0.5689, 0.6361, 0.4598],
          [0.3217, 0.4617, 0.3294,  ..., 0.4220, 0.6243, 0.6188],
          [0.4752, 0.4436, 0.3540,  ..., 0.5446, 0.5103, 0.3531]],

         [[0.5765, 0.4673, 0.7621,  ..., 0.4130, 0.5224, 0.3451],
          [0.4644, 0.4931, 0.5598,  ..., 0.5152, 0.5908, 0.5455],
          [0.5007, 0.4163, 0.5054,  ..., 0.4036, 0.6549, 0.5453],
          [0.5631, 0.5922, 0.4201,  ..., 0.5936, 0.4745, 0.4537]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.4895, 0.4851, 0.6220,  ..., 0.3739, 0.4278, 0.3200],
          [0.4182, 0.5746, 0.4863,  ..., 0.3693, 0.4827, 0.4976],
          [0.5242, 0.4979, 0.4868,  ..., 0.4465, 0.7186, 0.3919],
          [0.6567, 0.6035, 0.3648,  ..., 0.3234, 0.5397, 0.6165]],

         [[0.4489, 0.3711, 0.6142,  ..., 0.3657, 0.5789, 0.4102],
          [0.3217, 0.4528, 0.3928,  ..., 0.6021, 0.5713, 0.6095],
          [0.3840, 0.4273, 0.4441,  ..., 0.4235, 0.4746, 0.4816],
          [0.5287, 0.5619, 0.3748,  ..., 0.5295, 0.5537, 0.6174]],

         [[0.4149, 0.4373, 0.6433,  ..., 0.3603, 0.6352, 0.4397],
          [0.3329, 0.5200, 0.5064,  ..., 0.4983, 0.5448, 0.6610],
          [0.3923, 0.4448, 0.5118,  ..., 0.4968, 0.6049, 0.4036],
          [0.6206, 0.6109, 0.4125,  ..., 0.4707, 0.4647, 0.5903]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0080, 0.0080, 0.0080, 0.0080, 0.0080, 0.0080, 0.0080, 0.0080, 0.0040,
        0.0100], device='cuda:0')
selected experts tensor([1488,  917, 1300,  719, 1277, 1314, 1250,  578, 1919, 1526],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4703, 0.5832, 0.5968,  ..., 0.5949, 0.3845, 0.2974],
          [0.4863, 0.3881, 0.4999,  ..., 0.5646, 0.5745, 0.4391],
          [0.6260, 0.5520, 0.4831,  ..., 0.6193, 0.4077, 0.6022],
          [0.6404, 0.4805, 0.5489,  ..., 0.5364, 0.4300, 0.4237]],

         [[0.5665, 0.4873, 0.6368,  ..., 0.4181, 0.4419, 0.4232],
          [0.5307, 0.5903, 0.5064,  ..., 0.4167, 0.4882, 0.4189],
          [0.5246, 0.5708, 0.5187,  ..., 0.5701, 0.5591, 0.3767],
          [0.5750, 0.5836, 0.6216,  ..., 0.3539, 0.3827, 0.4037]],

         [[0.3693, 0.5261, 0.5280,  ..., 0.3970, 0.6147, 0.6165],
          [0.5887, 0.4306, 0.3790,  ..., 0.4266, 0.5179, 0.5839],
          [0.3776, 0.4101, 0.5376,  ..., 0.4381, 0.3910, 0.4670],
          [0.3977, 0.6150, 0.4530,  ..., 0.5835, 0.6386, 0.5829]],

         ...,

         [[0.6707, 0.3275, 0.3790,  ..., 0.5598, 0.4186, 0.4285],
          [0.6395, 0.5269, 0.4673,  ..., 0.4902, 0.3998, 0.5696],
          [0.4858, 0.4963, 0.4219,  ..., 0.5593, 0.4300, 0.5045],
          [0.7076, 0.4586, 0.5260,  ..., 0.4139, 0.3961, 0.3227]],

         [[0.5041, 0.4148, 0.5491,  ..., 0.4448, 0.4787, 0.5444],
          [0.4091, 0.5233, 0.4467,  ..., 0.4451, 0.4886, 0.4752],
          [0.3968, 0.5817, 0.5680,  ..., 0.5968, 0.4937, 0.5362],
          [0.3912, 0.5284, 0.4405,  ..., 0.5680, 0.5807, 0.4319]],

         [[0.5008, 0.3198, 0.4554,  ..., 0.5371, 0.4928, 0.4886],
          [0.4759, 0.5249, 0.4309,  ..., 0.5940, 0.4736, 0.4997],
          [0.5521, 0.3756, 0.4007,  ..., 0.4484, 0.3202, 0.4768],
          [0.5999, 0.6015, 0.5945,  ..., 0.4566, 0.4463, 0.5848]]],


        [[[0.3323, 0.3539, 0.4617,  ..., 0.5711, 0.3583, 0.5139],
          [0.5531, 0.6933, 0.5552,  ..., 0.5949, 0.5326, 0.5413],
          [0.5032, 0.4433, 0.5973,  ..., 0.4554, 0.4007, 0.6508],
          [0.3601, 0.5712, 0.5209,  ..., 0.6216, 0.3790, 0.5543]],

         [[0.4529, 0.5120, 0.4899,  ..., 0.4637, 0.3691, 0.4694],
          [0.5765, 0.5462, 0.5479,  ..., 0.3637, 0.5146, 0.5971],
          [0.4168, 0.4363, 0.5132,  ..., 0.5265, 0.6020, 0.5577],
          [0.4411, 0.6108, 0.4309,  ..., 0.5021, 0.5759, 0.5999]],

         [[0.5622, 0.6941, 0.5821,  ..., 0.5253, 0.5080, 0.4890],
          [0.4048, 0.5669, 0.6340,  ..., 0.5029, 0.6449, 0.3517],
          [0.6092, 0.4549, 0.6929,  ..., 0.5013, 0.4049, 0.5663],
          [0.4408, 0.3848, 0.6715,  ..., 0.5612, 0.5402, 0.6992]],

         ...,

         [[0.3574, 0.5392, 0.5826,  ..., 0.6221, 0.4588, 0.6211],
          [0.4788, 0.6224, 0.5547,  ..., 0.6175, 0.7457, 0.6499],
          [0.6749, 0.6173, 0.5725,  ..., 0.5685, 0.5792, 0.3818],
          [0.3940, 0.5212, 0.4913,  ..., 0.4530, 0.5835, 0.4080]],

         [[0.6707, 0.3275, 0.3790,  ..., 0.5598, 0.4186, 0.4285],
          [0.6395, 0.5269, 0.4673,  ..., 0.4902, 0.3998, 0.5696],
          [0.4858, 0.4963, 0.4219,  ..., 0.5593, 0.4300, 0.5045],
          [0.7076, 0.4586, 0.5260,  ..., 0.4139, 0.3961, 0.3227]],

         [[0.6707, 0.3275, 0.3790,  ..., 0.5598, 0.4186, 0.4285],
          [0.6395, 0.5269, 0.4673,  ..., 0.4902, 0.3998, 0.5696],
          [0.4858, 0.4963, 0.4219,  ..., 0.5593, 0.4300, 0.5045],
          [0.7076, 0.4586, 0.5260,  ..., 0.4139, 0.3961, 0.3227]]]],
       device='cuda:0')
tensor([[[[0.4813, 0.5842, 0.5898,  ..., 0.5879, 0.3775, 0.3024],
          [0.4973, 0.3891, 0.4929,  ..., 0.5576, 0.5675, 0.4441],
          [0.6370, 0.5530, 0.4761,  ..., 0.6123, 0.4007, 0.6072],
          [0.6514, 0.4815, 0.5419,  ..., 0.5294, 0.4230, 0.4287]],

         [[0.5775, 0.4883, 0.6298,  ..., 0.4111, 0.4349, 0.4282],
          [0.5417, 0.5913, 0.4994,  ..., 0.4097, 0.4812, 0.4239],
          [0.5356, 0.5718, 0.5117,  ..., 0.5631, 0.5521, 0.3817],
          [0.5860, 0.5846, 0.6146,  ..., 0.3469, 0.3757, 0.4087]],

         [[0.3803, 0.5271, 0.5210,  ..., 0.3900, 0.6077, 0.6215],
          [0.5997, 0.4316, 0.3720,  ..., 0.4196, 0.5109, 0.5889],
          [0.3886, 0.4111, 0.5306,  ..., 0.4311, 0.3840, 0.4720],
          [0.4087, 0.6160, 0.4460,  ..., 0.5765, 0.6316, 0.5879]],

         ...,

         [[0.6817, 0.3285, 0.3720,  ..., 0.5528, 0.4116, 0.4335],
          [0.6505, 0.5279, 0.4603,  ..., 0.4832, 0.3928, 0.5746],
          [0.4968, 0.4973, 0.4149,  ..., 0.5523, 0.4230, 0.5095],
          [0.7186, 0.4596, 0.5190,  ..., 0.4069, 0.3891, 0.3277]],

         [[0.5151, 0.4158, 0.5421,  ..., 0.4378, 0.4717, 0.5494],
          [0.4201, 0.5243, 0.4397,  ..., 0.4381, 0.4816, 0.4802],
          [0.4078, 0.5827, 0.5610,  ..., 0.5898, 0.4867, 0.5412],
          [0.4022, 0.5294, 0.4335,  ..., 0.5610, 0.5737, 0.4369]],

         [[0.5118, 0.3208, 0.4484,  ..., 0.5301, 0.4858, 0.4936],
          [0.4869, 0.5259, 0.4239,  ..., 0.5870, 0.4666, 0.5047],
          [0.5631, 0.3766, 0.3937,  ..., 0.4414, 0.3132, 0.4818],
          [0.6109, 0.6025, 0.5875,  ..., 0.4496, 0.4393, 0.5898]]],


        [[[0.3433, 0.3549, 0.4547,  ..., 0.5641, 0.3513, 0.5189],
          [0.5641, 0.6943, 0.5482,  ..., 0.5879, 0.5256, 0.5463],
          [0.5142, 0.4443, 0.5903,  ..., 0.4484, 0.3937, 0.6558],
          [0.3711, 0.5722, 0.5139,  ..., 0.6146, 0.3720, 0.5593]],

         [[0.4639, 0.5130, 0.4829,  ..., 0.4567, 0.3621, 0.4744],
          [0.5875, 0.5472, 0.5409,  ..., 0.3567, 0.5076, 0.6021],
          [0.4278, 0.4373, 0.5062,  ..., 0.5195, 0.5950, 0.5627],
          [0.4521, 0.6118, 0.4239,  ..., 0.4951, 0.5689, 0.6049]],

         [[0.5732, 0.6951, 0.5751,  ..., 0.5183, 0.5010, 0.4940],
          [0.4158, 0.5679, 0.6270,  ..., 0.4959, 0.6379, 0.3567],
          [0.6202, 0.4559, 0.6859,  ..., 0.4943, 0.3979, 0.5713],
          [0.4518, 0.3858, 0.6645,  ..., 0.5542, 0.5332, 0.7042]],

         ...,

         [[0.3684, 0.5402, 0.5756,  ..., 0.6151, 0.4518, 0.6261],
          [0.4898, 0.6234, 0.5477,  ..., 0.6105, 0.7387, 0.6549],
          [0.6859, 0.6183, 0.5655,  ..., 0.5615, 0.5722, 0.3868],
          [0.4050, 0.5222, 0.4843,  ..., 0.4460, 0.5765, 0.4130]],

         [[0.6817, 0.3285, 0.3720,  ..., 0.5528, 0.4116, 0.4335],
          [0.6505, 0.5279, 0.4603,  ..., 0.4832, 0.3928, 0.5746],
          [0.4968, 0.4973, 0.4149,  ..., 0.5523, 0.4230, 0.5095],
          [0.7186, 0.4596, 0.5190,  ..., 0.4069, 0.3891, 0.3277]],

         [[0.6817, 0.3285, 0.3720,  ..., 0.5528, 0.4116, 0.4335],
          [0.6505, 0.5279, 0.4603,  ..., 0.4832, 0.3928, 0.5746],
          [0.4968, 0.4973, 0.4149,  ..., 0.5523, 0.4230, 0.5095],
          [0.7186, 0.4596, 0.5190,  ..., 0.4069, 0.3891, 0.3277]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0110, -0.0010,  0.0070,  0.0030,  0.0070,  0.0070, -0.0050,  0.0070,
         0.0070, -0.0050], device='cuda:0')
selected experts tensor([1701, 1664, 1709, 1559, 1629, 1686, 1675, 1652, 1584, 1525],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4246, 0.4913, 0.4646,  ..., 0.6115, 0.4868, 0.6117],
          [0.5009, 0.3632, 0.6311,  ..., 0.2639, 0.6255, 0.3107],
          [0.4179, 0.3952, 0.4677,  ..., 0.5425, 0.5738, 0.3082],
          [0.4857, 0.6354, 0.5131,  ..., 0.4090, 0.6135, 0.6224]],

         [[0.4892, 0.3071, 0.5772,  ..., 0.5758, 0.4555, 0.5062],
          [0.4312, 0.4690, 0.3344,  ..., 0.5313, 0.5171, 0.4617],
          [0.5546, 0.4840, 0.3848,  ..., 0.5383, 0.4113, 0.6108],
          [0.5042, 0.4676, 0.6293,  ..., 0.4868, 0.5563, 0.4585]],

         [[0.5919, 0.5566, 0.4332,  ..., 0.5205, 0.4555, 0.5243],
          [0.5425, 0.6104, 0.5423,  ..., 0.6013, 0.3411, 0.3457],
          [0.4963, 0.4688, 0.4951,  ..., 0.2908, 0.4572, 0.3969],
          [0.6125, 0.3267, 0.5710,  ..., 0.3589, 0.5580, 0.4385]],

         ...,

         [[0.4454, 0.5691, 0.5287,  ..., 0.4000, 0.7014, 0.4366],
          [0.4640, 0.4103, 0.3752,  ..., 0.2474, 0.5555, 0.4609],
          [0.4691, 0.4986, 0.4893,  ..., 0.4118, 0.4141, 0.5102],
          [0.4796, 0.4412, 0.3955,  ..., 0.6147, 0.5070, 0.4375]],

         [[0.3935, 0.5523, 0.5546,  ..., 0.5796, 0.4075, 0.4251],
          [0.5900, 0.3770, 0.5929,  ..., 0.4410, 0.4075, 0.6233],
          [0.4769, 0.5112, 0.5587,  ..., 0.5961, 0.4546, 0.5039],
          [0.4587, 0.3198, 0.4142,  ..., 0.5299, 0.5596, 0.4123]],

         [[0.5381, 0.4487, 0.6721,  ..., 0.4996, 0.4466, 0.4447],
          [0.5786, 0.5629, 0.4464,  ..., 0.5668, 0.4893, 0.5621],
          [0.4146, 0.5162, 0.4684,  ..., 0.5730, 0.4288, 0.5064],
          [0.4269, 0.4618, 0.5901,  ..., 0.7120, 0.6255, 0.5420]]],


        [[[0.5165, 0.5008, 0.5882,  ..., 0.6630, 0.4512, 0.5349],
          [0.4743, 0.4383, 0.5305,  ..., 0.5867, 0.6463, 0.3233],
          [0.4746, 0.4482, 0.6420,  ..., 0.4028, 0.5943, 0.5915],
          [0.5468, 0.4299, 0.5681,  ..., 0.4798, 0.4480, 0.3780]],

         [[0.5611, 0.6747, 0.4208,  ..., 0.6003, 0.4995, 0.5165],
          [0.4874, 0.6372, 0.3835,  ..., 0.6230, 0.5500, 0.5498],
          [0.5011, 0.3497, 0.6196,  ..., 0.4820, 0.6300, 0.3207],
          [0.4351, 0.2946, 0.4886,  ..., 0.4838, 0.5386, 0.5653]],

         [[0.5043, 0.4084, 0.6085,  ..., 0.5306, 0.5175, 0.5184],
          [0.3696, 0.4703, 0.6099,  ..., 0.5181, 0.5233, 0.3049],
          [0.3944, 0.4758, 0.5279,  ..., 0.6455, 0.5767, 0.5254],
          [0.4679, 0.5300, 0.5662,  ..., 0.4900, 0.4122, 0.5806]],

         ...,

         [[0.6552, 0.5077, 0.4513,  ..., 0.3832, 0.4560, 0.4433],
          [0.4155, 0.4455, 0.5925,  ..., 0.3943, 0.5461, 0.6850],
          [0.5071, 0.4179, 0.5137,  ..., 0.5458, 0.5507, 0.6384],
          [0.4662, 0.6488, 0.4612,  ..., 0.4161, 0.5085, 0.4816]],

         [[0.4454, 0.5691, 0.5287,  ..., 0.4000, 0.7014, 0.4366],
          [0.4640, 0.4103, 0.3752,  ..., 0.2474, 0.5555, 0.4609],
          [0.4691, 0.4986, 0.4893,  ..., 0.4118, 0.4141, 0.5102],
          [0.4796, 0.4412, 0.3955,  ..., 0.6147, 0.5070, 0.4375]],

         [[0.4454, 0.5691, 0.5287,  ..., 0.4000, 0.7014, 0.4366],
          [0.4640, 0.4103, 0.3752,  ..., 0.2474, 0.5555, 0.4609],
          [0.4691, 0.4986, 0.4893,  ..., 0.4118, 0.4141, 0.5102],
          [0.4796, 0.4412, 0.3955,  ..., 0.6147, 0.5070, 0.4375]]]],
       device='cuda:0')
tensor([[[[0.4216, 0.4983, 0.4596,  ..., 0.6165, 0.4838, 0.6067],
          [0.4979, 0.3702, 0.6261,  ..., 0.2689, 0.6225, 0.3057],
          [0.4149, 0.4022, 0.4627,  ..., 0.5475, 0.5708, 0.3032],
          [0.4827, 0.6424, 0.5081,  ..., 0.4140, 0.6105, 0.6174]],

         [[0.4862, 0.3141, 0.5722,  ..., 0.5808, 0.4525, 0.5012],
          [0.4282, 0.4760, 0.3294,  ..., 0.5363, 0.5141, 0.4567],
          [0.5516, 0.4910, 0.3798,  ..., 0.5433, 0.4083, 0.6058],
          [0.5012, 0.4746, 0.6243,  ..., 0.4918, 0.5533, 0.4535]],

         [[0.5889, 0.5636, 0.4282,  ..., 0.5255, 0.4525, 0.5193],
          [0.5395, 0.6174, 0.5373,  ..., 0.6063, 0.3381, 0.3407],
          [0.4933, 0.4758, 0.4901,  ..., 0.2958, 0.4542, 0.3919],
          [0.6095, 0.3337, 0.5660,  ..., 0.3639, 0.5550, 0.4335]],

         ...,

         [[0.4424, 0.5761, 0.5237,  ..., 0.4050, 0.6984, 0.4316],
          [0.4610, 0.4173, 0.3702,  ..., 0.2524, 0.5525, 0.4559],
          [0.4661, 0.5056, 0.4843,  ..., 0.4168, 0.4111, 0.5052],
          [0.4766, 0.4482, 0.3905,  ..., 0.6197, 0.5040, 0.4325]],

         [[0.3905, 0.5593, 0.5496,  ..., 0.5846, 0.4045, 0.4201],
          [0.5870, 0.3840, 0.5879,  ..., 0.4460, 0.4045, 0.6183],
          [0.4739, 0.5182, 0.5537,  ..., 0.6011, 0.4516, 0.4989],
          [0.4557, 0.3268, 0.4092,  ..., 0.5349, 0.5566, 0.4073]],

         [[0.5351, 0.4557, 0.6671,  ..., 0.5046, 0.4436, 0.4397],
          [0.5756, 0.5699, 0.4414,  ..., 0.5718, 0.4863, 0.5571],
          [0.4116, 0.5232, 0.4634,  ..., 0.5780, 0.4258, 0.5014],
          [0.4239, 0.4688, 0.5851,  ..., 0.7170, 0.6225, 0.5370]]],


        [[[0.5135, 0.5078, 0.5832,  ..., 0.6680, 0.4482, 0.5299],
          [0.4713, 0.4453, 0.5255,  ..., 0.5917, 0.6433, 0.3183],
          [0.4716, 0.4552, 0.6370,  ..., 0.4078, 0.5913, 0.5865],
          [0.5438, 0.4369, 0.5631,  ..., 0.4848, 0.4450, 0.3730]],

         [[0.5581, 0.6817, 0.4158,  ..., 0.6053, 0.4965, 0.5115],
          [0.4844, 0.6442, 0.3785,  ..., 0.6280, 0.5470, 0.5448],
          [0.4981, 0.3567, 0.6146,  ..., 0.4870, 0.6270, 0.3157],
          [0.4321, 0.3016, 0.4836,  ..., 0.4888, 0.5356, 0.5603]],

         [[0.5013, 0.4154, 0.6035,  ..., 0.5356, 0.5145, 0.5134],
          [0.3666, 0.4773, 0.6049,  ..., 0.5231, 0.5203, 0.2999],
          [0.3914, 0.4828, 0.5229,  ..., 0.6505, 0.5737, 0.5204],
          [0.4649, 0.5370, 0.5612,  ..., 0.4950, 0.4092, 0.5756]],

         ...,

         [[0.6522, 0.5147, 0.4463,  ..., 0.3882, 0.4530, 0.4383],
          [0.4125, 0.4525, 0.5875,  ..., 0.3993, 0.5431, 0.6800],
          [0.5041, 0.4249, 0.5087,  ..., 0.5508, 0.5477, 0.6334],
          [0.4632, 0.6558, 0.4562,  ..., 0.4211, 0.5055, 0.4766]],

         [[0.4424, 0.5761, 0.5237,  ..., 0.4050, 0.6984, 0.4316],
          [0.4610, 0.4173, 0.3702,  ..., 0.2524, 0.5525, 0.4559],
          [0.4661, 0.5056, 0.4843,  ..., 0.4168, 0.4111, 0.5052],
          [0.4766, 0.4482, 0.3905,  ..., 0.6197, 0.5040, 0.4325]],

         [[0.4424, 0.5761, 0.5237,  ..., 0.4050, 0.6984, 0.4316],
          [0.4610, 0.4173, 0.3702,  ..., 0.2524, 0.5525, 0.4559],
          [0.4661, 0.5056, 0.4843,  ..., 0.4168, 0.4111, 0.5052],
          [0.4766, 0.4482, 0.3905,  ..., 0.6197, 0.5040, 0.4325]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0030, -0.0070,  0.0050, -0.0010,  0.0030,  0.0030, -0.0050, -0.0050,
         0.0030,  0.0050], device='cuda:0')
selected experts tensor([1644, 1685, 1534, 1768, 1668, 1603, 1654, 1615, 1656, 1557],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4842, 0.4519, 0.3935,  ..., 0.4340, 0.4691, 0.4280],
          [0.5273, 0.4655, 0.4374,  ..., 0.3518, 0.4707, 0.4811],
          [0.3796, 0.5434, 0.5727,  ..., 0.3756, 0.4381, 0.5752],
          [0.5497, 0.4391, 0.4048,  ..., 0.6131, 0.3978, 0.5336]],

         [[0.4820, 0.4485, 0.5765,  ..., 0.4100, 0.3755, 0.4712],
          [0.4906, 0.4090, 0.3158,  ..., 0.4492, 0.3525, 0.4146],
          [0.3402, 0.4653, 0.4086,  ..., 0.4483, 0.5935, 0.4293],
          [0.3095, 0.4170, 0.4554,  ..., 0.7788, 0.5550, 0.4966]],

         [[0.4165, 0.5095, 0.6151,  ..., 0.4294, 0.3973, 0.4155],
          [0.4706, 0.4941, 0.4430,  ..., 0.3645, 0.5854, 0.5114],
          [0.3741, 0.6347, 0.6915,  ..., 0.4162, 0.5217, 0.4539],
          [0.3516, 0.4304, 0.4800,  ..., 0.7008, 0.5313, 0.5509]],

         ...,

         [[0.5890, 0.5400, 0.3930,  ..., 0.6030, 0.4939, 0.3624],
          [0.5671, 0.5417, 0.5128,  ..., 0.5013, 0.4505, 0.6379],
          [0.4370, 0.5238, 0.4726,  ..., 0.5678, 0.5476, 0.4567],
          [0.6116, 0.4004, 0.4044,  ..., 0.4571, 0.4780, 0.6064]],

         [[0.5909, 0.4717, 0.4191,  ..., 0.5417, 0.4996, 0.4443],
          [0.5685, 0.5739, 0.4512,  ..., 0.4619, 0.3936, 0.3246],
          [0.4418, 0.4666, 0.5545,  ..., 0.4891, 0.4154, 0.4377],
          [0.5810, 0.5408, 0.5627,  ..., 0.3251, 0.3909, 0.4763]],

         [[0.6037, 0.4468, 0.4729,  ..., 0.5674, 0.3499, 0.4421],
          [0.5252, 0.6338, 0.5883,  ..., 0.5013, 0.3404, 0.5127],
          [0.3315, 0.4319, 0.5943,  ..., 0.5316, 0.5175, 0.5334],
          [0.6355, 0.3270, 0.3629,  ..., 0.4571, 0.5170, 0.4996]]],


        [[[0.5335, 0.5262, 0.5703,  ..., 0.3986, 0.3936, 0.4283],
          [0.3939, 0.4531, 0.5859,  ..., 0.4565, 0.3412, 0.5076],
          [0.5881, 0.5260, 0.5817,  ..., 0.4191, 0.5907, 0.5276],
          [0.4941, 0.4061, 0.4959,  ..., 0.6140, 0.6422, 0.6527]],

         [[0.5061, 0.4208, 0.3879,  ..., 0.4798, 0.3836, 0.5227],
          [0.5601, 0.4721, 0.4735,  ..., 0.4608, 0.4229, 0.4864],
          [0.4042, 0.5834, 0.6018,  ..., 0.4725, 0.4807, 0.4084],
          [0.5649, 0.4902, 0.4030,  ..., 0.6507, 0.5005, 0.5686]],

         [[0.5905, 0.5629, 0.4425,  ..., 0.3981, 0.3639, 0.5278],
          [0.4265, 0.2804, 0.5444,  ..., 0.4449, 0.3525, 0.6790],
          [0.4327, 0.5471, 0.5574,  ..., 0.3991, 0.4625, 0.4050],
          [0.4370, 0.3616, 0.3739,  ..., 0.6072, 0.4749, 0.5981]],

         ...,

         [[0.5410, 0.3598, 0.4015,  ..., 0.5947, 0.5152, 0.5514],
          [0.4580, 0.5787, 0.6412,  ..., 0.3915, 0.5059, 0.6352],
          [0.5123, 0.6926, 0.5826,  ..., 0.4784, 0.4874, 0.4755],
          [0.4061, 0.4439, 0.4275,  ..., 0.3967, 0.4630, 0.5239]],

         [[0.5814, 0.6147, 0.5141,  ..., 0.4509, 0.3782, 0.4264],
          [0.5091, 0.5711, 0.4875,  ..., 0.5054, 0.5916, 0.3596],
          [0.4061, 0.4047, 0.4584,  ..., 0.4287, 0.4471, 0.4784],
          [0.5132, 0.4922, 0.4556,  ..., 0.5037, 0.4510, 0.5222]],

         [[0.5269, 0.5096, 0.5164,  ..., 0.5107, 0.4893, 0.4208],
          [0.4953, 0.6490, 0.5915,  ..., 0.6081, 0.4201, 0.3889],
          [0.4117, 0.5091, 0.5275,  ..., 0.4000, 0.4543, 0.4022],
          [0.5029, 0.3340, 0.3996,  ..., 0.5405, 0.4272, 0.6227]]]],
       device='cuda:0')
tensor([[[[0.4812, 0.4569, 0.4045,  ..., 0.4470, 0.4521, 0.4450],
          [0.5243, 0.4705, 0.4484,  ..., 0.3648, 0.4537, 0.4981],
          [0.3766, 0.5484, 0.5837,  ..., 0.3886, 0.4211, 0.5922],
          [0.5467, 0.4441, 0.4158,  ..., 0.6261, 0.3808, 0.5506]],

         [[0.4790, 0.4535, 0.5875,  ..., 0.4230, 0.3585, 0.4882],
          [0.4876, 0.4140, 0.3268,  ..., 0.4622, 0.3355, 0.4316],
          [0.3372, 0.4703, 0.4196,  ..., 0.4613, 0.5765, 0.4463],
          [0.3065, 0.4220, 0.4664,  ..., 0.7918, 0.5380, 0.5136]],

         [[0.4135, 0.5145, 0.6261,  ..., 0.4424, 0.3803, 0.4325],
          [0.4676, 0.4991, 0.4540,  ..., 0.3775, 0.5684, 0.5284],
          [0.3711, 0.6397, 0.7025,  ..., 0.4292, 0.5047, 0.4709],
          [0.3486, 0.4354, 0.4910,  ..., 0.7138, 0.5143, 0.5679]],

         ...,

         [[0.5860, 0.5450, 0.4040,  ..., 0.6160, 0.4769, 0.3794],
          [0.5641, 0.5467, 0.5238,  ..., 0.5143, 0.4335, 0.6549],
          [0.4340, 0.5288, 0.4836,  ..., 0.5808, 0.5306, 0.4737],
          [0.6086, 0.4054, 0.4154,  ..., 0.4701, 0.4610, 0.6234]],

         [[0.5879, 0.4767, 0.4301,  ..., 0.5547, 0.4826, 0.4613],
          [0.5655, 0.5789, 0.4622,  ..., 0.4749, 0.3766, 0.3416],
          [0.4388, 0.4716, 0.5655,  ..., 0.5021, 0.3984, 0.4547],
          [0.5780, 0.5458, 0.5737,  ..., 0.3381, 0.3739, 0.4933]],

         [[0.6007, 0.4518, 0.4839,  ..., 0.5804, 0.3329, 0.4591],
          [0.5222, 0.6388, 0.5993,  ..., 0.5143, 0.3234, 0.5297],
          [0.3285, 0.4369, 0.6053,  ..., 0.5446, 0.5005, 0.5504],
          [0.6325, 0.3320, 0.3739,  ..., 0.4701, 0.5000, 0.5166]]],


        [[[0.5305, 0.5312, 0.5813,  ..., 0.4116, 0.3766, 0.4453],
          [0.3909, 0.4581, 0.5969,  ..., 0.4695, 0.3242, 0.5246],
          [0.5851, 0.5310, 0.5927,  ..., 0.4321, 0.5737, 0.5446],
          [0.4911, 0.4111, 0.5069,  ..., 0.6270, 0.6252, 0.6697]],

         [[0.5031, 0.4258, 0.3989,  ..., 0.4928, 0.3666, 0.5397],
          [0.5571, 0.4771, 0.4845,  ..., 0.4738, 0.4059, 0.5034],
          [0.4012, 0.5884, 0.6128,  ..., 0.4855, 0.4637, 0.4254],
          [0.5619, 0.4952, 0.4140,  ..., 0.6637, 0.4835, 0.5856]],

         [[0.5875, 0.5679, 0.4535,  ..., 0.4111, 0.3469, 0.5448],
          [0.4235, 0.2854, 0.5554,  ..., 0.4579, 0.3355, 0.6960],
          [0.4297, 0.5521, 0.5684,  ..., 0.4121, 0.4455, 0.4220],
          [0.4340, 0.3666, 0.3849,  ..., 0.6202, 0.4579, 0.6151]],

         ...,

         [[0.5380, 0.3648, 0.4125,  ..., 0.6077, 0.4982, 0.5684],
          [0.4550, 0.5837, 0.6522,  ..., 0.4045, 0.4889, 0.6522],
          [0.5093, 0.6976, 0.5936,  ..., 0.4914, 0.4704, 0.4925],
          [0.4031, 0.4489, 0.4385,  ..., 0.4097, 0.4460, 0.5409]],

         [[0.5784, 0.6197, 0.5251,  ..., 0.4639, 0.3612, 0.4434],
          [0.5061, 0.5761, 0.4985,  ..., 0.5184, 0.5746, 0.3766],
          [0.4031, 0.4097, 0.4694,  ..., 0.4417, 0.4301, 0.4954],
          [0.5102, 0.4972, 0.4666,  ..., 0.5167, 0.4340, 0.5392]],

         [[0.5239, 0.5146, 0.5274,  ..., 0.5237, 0.4723, 0.4378],
          [0.4923, 0.6540, 0.6025,  ..., 0.6211, 0.4031, 0.4059],
          [0.4087, 0.5141, 0.5385,  ..., 0.4130, 0.4373, 0.4192],
          [0.4999, 0.3390, 0.4106,  ..., 0.5535, 0.4102, 0.6397]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0030, -0.0050, -0.0110,  0.0070, -0.0050, -0.0070,  0.0010, -0.0130,
         0.0170, -0.0170], device='cuda:0')
selected experts tensor([1466, 1583, 1524, 1727, 1629, 2150, 1389, 1825, 1101, 1990],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.4707, 0.5808, 0.5716,  ..., 0.5322, 0.5207, 0.4220],
          [0.5310, 0.5836, 0.6548,  ..., 0.3838, 0.6282, 0.5680],
          [0.4506, 0.3214, 0.4622,  ..., 0.4873, 0.6255, 0.4864],
          [0.6476, 0.5451, 0.4362,  ..., 0.3532, 0.2868, 0.5656]],

         [[0.5276, 0.5268, 0.6322,  ..., 0.4533, 0.6018, 0.5531],
          [0.4530, 0.5229, 0.4885,  ..., 0.4811, 0.5456, 0.4032],
          [0.5314, 0.3603, 0.3850,  ..., 0.3453, 0.6051, 0.3576],
          [0.5387, 0.6460, 0.4717,  ..., 0.5798, 0.3013, 0.6424]],

         [[0.4719, 0.5808, 0.5716,  ..., 0.5314, 0.5204, 0.4244],
          [0.5290, 0.5827, 0.6548,  ..., 0.3838, 0.6273, 0.5688],
          [0.4513, 0.3197, 0.4627,  ..., 0.4863, 0.6264, 0.4866],
          [0.6449, 0.5448, 0.4362,  ..., 0.3523, 0.2884, 0.5666]],

         ...,

         [[0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090]],

         [[0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090]],

         [[0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090]]],


        [[[0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090]],

         [[0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090]],

         [[0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090]],

         ...,

         [[0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090]],

         [[0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090]],

         [[0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090],
          [0.5070, 0.5090, 0.5070,  ..., 0.5090, 0.5030, 0.5090]]]],
       device='cuda:0')
tensor([[[[0.4637, 0.5718, 0.5646,  ..., 0.5232, 0.5177, 0.4130],
          [0.5240, 0.5746, 0.6478,  ..., 0.3748, 0.6252, 0.5590],
          [0.4436, 0.3124, 0.4552,  ..., 0.4783, 0.6225, 0.4774],
          [0.6406, 0.5361, 0.4292,  ..., 0.3442, 0.2838, 0.5566]],

         [[0.5206, 0.5178, 0.6252,  ..., 0.4443, 0.5988, 0.5441],
          [0.4460, 0.5139, 0.4815,  ..., 0.4721, 0.5426, 0.3942],
          [0.5244, 0.3513, 0.3780,  ..., 0.3363, 0.6021, 0.3486],
          [0.5317, 0.6370, 0.4647,  ..., 0.5708, 0.2983, 0.6334]],

         [[0.4649, 0.5718, 0.5646,  ..., 0.5224, 0.5174, 0.4154],
          [0.5220, 0.5737, 0.6478,  ..., 0.3748, 0.6243, 0.5598],
          [0.4443, 0.3107, 0.4557,  ..., 0.4773, 0.6234, 0.4776],
          [0.6379, 0.5358, 0.4292,  ..., 0.3433, 0.2854, 0.5576]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([0.0070, 0.0090, 0.0070, 0.0090, 0.0070, 0.0070, 0.0070, 0.0090, 0.0030,
        0.0090], device='cuda:0')
selected experts tensor([ 411, 3861,  611, 3801,  616,  691,  693,  227,  839,  538],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4798, 0.4086, 0.3458,  ..., 0.4831, 0.6341, 0.4142],
          [0.5516, 0.5864, 0.3849,  ..., 0.5462, 0.4401, 0.5004],
          [0.5788, 0.5053, 0.5258,  ..., 0.4204, 0.4630, 0.6649],
          [0.4333, 0.4096, 0.4629,  ..., 0.4527, 0.4523, 0.4427]],

         [[0.3911, 0.5241, 0.5231,  ..., 0.5023, 0.5603, 0.6276],
          [0.4343, 0.4653, 0.3573,  ..., 0.5954, 0.5978, 0.3996],
          [0.3564, 0.4377, 0.4982,  ..., 0.6062, 0.6973, 0.5678],
          [0.4480, 0.4920, 0.5939,  ..., 0.4736, 0.5346, 0.3545]],

         [[0.5030, 0.4325, 0.4323,  ..., 0.4981, 0.5047, 0.4558],
          [0.5028, 0.6154, 0.6109,  ..., 0.5773, 0.4717, 0.6726],
          [0.2463, 0.5602, 0.5199,  ..., 0.5345, 0.4832, 0.5259],
          [0.5195, 0.4835, 0.5030,  ..., 0.6109, 0.5974, 0.6553]],

         ...,

         [[0.3775, 0.4707, 0.4865,  ..., 0.5614, 0.5492, 0.4750],
          [0.3878, 0.6122, 0.4559,  ..., 0.5433, 0.6350, 0.6649],
          [0.6223, 0.5807, 0.3337,  ..., 0.4939, 0.5236, 0.5797],
          [0.4510, 0.6865, 0.5513,  ..., 0.5024, 0.5475, 0.4464]],

         [[0.5277, 0.3691, 0.3914,  ..., 0.5208, 0.3647, 0.4095],
          [0.5797, 0.4433, 0.5047,  ..., 0.5450, 0.5283, 0.4062],
          [0.4015, 0.4774, 0.4685,  ..., 0.5216, 0.4574, 0.5149],
          [0.6141, 0.4527, 0.4424,  ..., 0.5298, 0.3674, 0.5934]],

         [[0.4005, 0.4622, 0.5048,  ..., 0.5078, 0.4377, 0.3717],
          [0.4529, 0.4942, 0.5097,  ..., 0.7492, 0.4338, 0.3644],
          [0.4957, 0.4476, 0.6843,  ..., 0.5328, 0.4276, 0.5030],
          [0.4967, 0.5053, 0.4876,  ..., 0.6766, 0.5499, 0.5425]]],


        [[[0.5698, 0.5358, 0.6118,  ..., 0.5433, 0.4791, 0.5408],
          [0.5381, 0.5963, 0.4086,  ..., 0.4537, 0.4050, 0.4180],
          [0.3878, 0.6555, 0.4114,  ..., 0.4844, 0.4991, 0.3245],
          [0.4987, 0.5452, 0.5177,  ..., 0.4414, 0.3638, 0.4214]],

         [[0.5432, 0.5592, 0.6038,  ..., 0.5336, 0.3417, 0.5183],
          [0.5924, 0.5888, 0.6403,  ..., 0.5069, 0.3549, 0.5711],
          [0.4761, 0.5855, 0.4873,  ..., 0.5491, 0.4804, 0.4926],
          [0.4994, 0.5621, 0.6484,  ..., 0.3932, 0.4927, 0.4442]],

         [[0.6331, 0.4349, 0.3835,  ..., 0.5725, 0.5579, 0.5227],
          [0.3331, 0.6131, 0.5498,  ..., 0.4937, 0.5153, 0.4725],
          [0.3692, 0.4891, 0.6029,  ..., 0.6775, 0.4849, 0.5538],
          [0.6031, 0.3917, 0.4831,  ..., 0.4755, 0.4851, 0.6162]],

         ...,

         [[0.6672, 0.6511, 0.5460,  ..., 0.5332, 0.4663, 0.5425],
          [0.5622, 0.4687, 0.4646,  ..., 0.3974, 0.4490, 0.5849],
          [0.4053, 0.6404, 0.4195,  ..., 0.5281, 0.5922, 0.3874],
          [0.5502, 0.7544, 0.5973,  ..., 0.6330, 0.3665, 0.4442]],

         [[0.4333, 0.5578, 0.4039,  ..., 0.4011, 0.5467, 0.5706],
          [0.5154, 0.5575, 0.4035,  ..., 0.6160, 0.5711, 0.5849],
          [0.3600, 0.5116, 0.6886,  ..., 0.4486, 0.4499, 0.4993],
          [0.4352, 0.3664, 0.5170,  ..., 0.5892, 0.5132, 0.5711]],

         [[0.5364, 0.5042, 0.4119,  ..., 0.3993, 0.6314, 0.5678],
          [0.3963, 0.5474, 0.3951,  ..., 0.5203, 0.4875, 0.5104],
          [0.4333, 0.4368, 0.4895,  ..., 0.6484, 0.5216, 0.4733],
          [0.4838, 0.4258, 0.5763,  ..., 0.4114, 0.3819, 0.3837]]]],
       device='cuda:0')
tensor([[[[0.4918, 0.4106, 0.3398,  ..., 0.4771, 0.6261, 0.4182],
          [0.5636, 0.5884, 0.3789,  ..., 0.5402, 0.4321, 0.5044],
          [0.5908, 0.5073, 0.5198,  ..., 0.4144, 0.4550, 0.6689],
          [0.4453, 0.4116, 0.4569,  ..., 0.4467, 0.4443, 0.4467]],

         [[0.4031, 0.5261, 0.5171,  ..., 0.4963, 0.5523, 0.6316],
          [0.4463, 0.4673, 0.3513,  ..., 0.5894, 0.5898, 0.4036],
          [0.3684, 0.4397, 0.4922,  ..., 0.6002, 0.6893, 0.5718],
          [0.4600, 0.4940, 0.5879,  ..., 0.4676, 0.5266, 0.3585]],

         [[0.5150, 0.4345, 0.4263,  ..., 0.4921, 0.4967, 0.4598],
          [0.5148, 0.6174, 0.6049,  ..., 0.5713, 0.4637, 0.6766],
          [0.2583, 0.5622, 0.5139,  ..., 0.5285, 0.4752, 0.5299],
          [0.5315, 0.4855, 0.4970,  ..., 0.6049, 0.5894, 0.6593]],

         ...,

         [[0.3895, 0.4727, 0.4805,  ..., 0.5554, 0.5412, 0.4790],
          [0.3998, 0.6142, 0.4499,  ..., 0.5373, 0.6270, 0.6689],
          [0.6343, 0.5827, 0.3277,  ..., 0.4879, 0.5156, 0.5837],
          [0.4630, 0.6885, 0.5453,  ..., 0.4964, 0.5395, 0.4504]],

         [[0.5397, 0.3711, 0.3854,  ..., 0.5148, 0.3567, 0.4135],
          [0.5917, 0.4453, 0.4987,  ..., 0.5390, 0.5203, 0.4102],
          [0.4135, 0.4794, 0.4625,  ..., 0.5156, 0.4494, 0.5189],
          [0.6261, 0.4547, 0.4364,  ..., 0.5238, 0.3594, 0.5974]],

         [[0.4125, 0.4642, 0.4988,  ..., 0.5018, 0.4297, 0.3757],
          [0.4649, 0.4962, 0.5037,  ..., 0.7432, 0.4258, 0.3684],
          [0.5077, 0.4496, 0.6783,  ..., 0.5268, 0.4196, 0.5070],
          [0.5087, 0.5073, 0.4816,  ..., 0.6706, 0.5419, 0.5465]]],


        [[[0.5818, 0.5378, 0.6058,  ..., 0.5373, 0.4711, 0.5448],
          [0.5501, 0.5983, 0.4026,  ..., 0.4477, 0.3970, 0.4220],
          [0.3998, 0.6575, 0.4054,  ..., 0.4784, 0.4911, 0.3285],
          [0.5107, 0.5472, 0.5117,  ..., 0.4354, 0.3558, 0.4254]],

         [[0.5552, 0.5612, 0.5978,  ..., 0.5276, 0.3337, 0.5223],
          [0.6044, 0.5908, 0.6343,  ..., 0.5009, 0.3469, 0.5751],
          [0.4881, 0.5875, 0.4813,  ..., 0.5431, 0.4724, 0.4966],
          [0.5114, 0.5641, 0.6424,  ..., 0.3872, 0.4847, 0.4482]],

         [[0.6451, 0.4369, 0.3775,  ..., 0.5665, 0.5499, 0.5267],
          [0.3451, 0.6151, 0.5438,  ..., 0.4877, 0.5073, 0.4765],
          [0.3812, 0.4911, 0.5969,  ..., 0.6715, 0.4769, 0.5578],
          [0.6151, 0.3937, 0.4771,  ..., 0.4695, 0.4771, 0.6202]],

         ...,

         [[0.6792, 0.6531, 0.5400,  ..., 0.5272, 0.4583, 0.5465],
          [0.5742, 0.4707, 0.4586,  ..., 0.3914, 0.4410, 0.5889],
          [0.4173, 0.6424, 0.4135,  ..., 0.5221, 0.5842, 0.3914],
          [0.5622, 0.7564, 0.5913,  ..., 0.6270, 0.3585, 0.4482]],

         [[0.4453, 0.5598, 0.3979,  ..., 0.3951, 0.5387, 0.5746],
          [0.5274, 0.5595, 0.3975,  ..., 0.6100, 0.5631, 0.5889],
          [0.3720, 0.5136, 0.6826,  ..., 0.4426, 0.4419, 0.5033],
          [0.4472, 0.3684, 0.5110,  ..., 0.5832, 0.5052, 0.5751]],

         [[0.5484, 0.5062, 0.4059,  ..., 0.3933, 0.6234, 0.5718],
          [0.4083, 0.5494, 0.3891,  ..., 0.5143, 0.4795, 0.5144],
          [0.4453, 0.4388, 0.4835,  ..., 0.6424, 0.5136, 0.4773],
          [0.4958, 0.4278, 0.5703,  ..., 0.4054, 0.3739, 0.3877]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0120, -0.0020,  0.0060,  0.0040,  0.0080,  0.0060, -0.0060,  0.0060,
         0.0080, -0.0040], device='cuda:0')
selected experts tensor([1615, 1613, 1750, 1671, 1681, 1650, 1586, 1633, 1556, 1629],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4449, 0.5412, 0.5691,  ..., 0.5967, 0.4577, 0.3234],
          [0.4117, 0.4648, 0.4035,  ..., 0.4776, 0.3864, 0.5107],
          [0.6354, 0.3895, 0.5072,  ..., 0.5267, 0.5880, 0.3771],
          [0.5422, 0.4571, 0.5715,  ..., 0.6420, 0.5507, 0.3942]],

         [[0.6003, 0.3843, 0.3900,  ..., 0.6330, 0.4519, 0.3808],
          [0.5339, 0.4741, 0.4450,  ..., 0.6811, 0.5275, 0.4771],
          [0.5383, 0.5254, 0.4414,  ..., 0.5565, 0.6435, 0.4856],
          [0.4454, 0.5530, 0.4525,  ..., 0.5427, 0.5490, 0.5399]],

         [[0.5424, 0.4797, 0.5479,  ..., 0.4204, 0.3462, 0.4357],
          [0.5752, 0.4414, 0.5308,  ..., 0.7002, 0.3587, 0.5518],
          [0.5512, 0.5237, 0.5973,  ..., 0.5432, 0.4679, 0.5806],
          [0.5067, 0.6574, 0.4721,  ..., 0.4052, 0.4836, 0.5203]],

         ...,

         [[0.3177, 0.6308, 0.4366,  ..., 0.4683, 0.4817, 0.5135],
          [0.6587, 0.4613, 0.4775,  ..., 0.6777, 0.4155, 0.7301],
          [0.3375, 0.7018, 0.4978,  ..., 0.4109, 0.6017, 0.3762],
          [0.4541, 0.4746, 0.3965,  ..., 0.4338, 0.6045, 0.4409]],

         [[0.4174, 0.6015, 0.5925,  ..., 0.6106, 0.6263, 0.6132],
          [0.5456, 0.4045, 0.4067,  ..., 0.3608, 0.5914, 0.4523],
          [0.5215, 0.4207, 0.6412,  ..., 0.4660, 0.5785, 0.3988],
          [0.3990, 0.4971, 0.5068,  ..., 0.6597, 0.5451, 0.3780]],

         [[0.4861, 0.2342, 0.5527,  ..., 0.3644, 0.5824, 0.5377],
          [0.4778, 0.5523, 0.5715,  ..., 0.5234, 0.4345, 0.5194],
          [0.3902, 0.6154, 0.5792,  ..., 0.4514, 0.4712, 0.5122],
          [0.3786, 0.6317, 0.5638,  ..., 0.6701, 0.4432, 0.7246]]],


        [[[0.4679, 0.4370, 0.3904,  ..., 0.5692, 0.3869, 0.5115],
          [0.5110, 0.3709, 0.6723,  ..., 0.3832, 0.5690, 0.6109],
          [0.4216, 0.5945, 0.5906,  ..., 0.5432, 0.4942, 0.5506],
          [0.3768, 0.4159, 0.5384,  ..., 0.3572, 0.4360, 0.4361]],

         [[0.5166, 0.5239, 0.5859,  ..., 0.3219, 0.4998, 0.4656],
          [0.4183, 0.5498, 0.6439,  ..., 0.6836, 0.3878, 0.4884],
          [0.3713, 0.4112, 0.5384,  ..., 0.4309, 0.6700, 0.5004],
          [0.3768, 0.4188, 0.6239,  ..., 0.5284, 0.4866, 0.5749]],

         [[0.5824, 0.4947, 0.5314,  ..., 0.4955, 0.4623, 0.4314],
          [0.6240, 0.6281, 0.4129,  ..., 0.5256, 0.4454, 0.4694],
          [0.5154, 0.5695, 0.3780,  ..., 0.3617, 0.5096, 0.4554],
          [0.4886, 0.4441, 0.5291,  ..., 0.3986, 0.6166, 0.5730]],

         ...,

         [[0.5956, 0.6218, 0.4733,  ..., 0.5644, 0.4967, 0.4651],
          [0.4951, 0.4618, 0.4338,  ..., 0.6321, 0.4269, 0.3854],
          [0.4548, 0.7026, 0.4445,  ..., 0.4823, 0.4468, 0.5725],
          [0.5742, 0.5171, 0.3886,  ..., 0.4738, 0.5466, 0.4342]],

         [[0.6955, 0.4737, 0.6688,  ..., 0.5403, 0.5553, 0.5406],
          [0.4018, 0.5018, 0.4007,  ..., 0.4602, 0.5209, 0.3790],
          [0.3506, 0.3895, 0.4908,  ..., 0.6046, 0.4449, 0.4419],
          [0.4264, 0.4649, 0.5071,  ..., 0.6249, 0.4684, 0.6860]],

         [[0.3668, 0.5404, 0.4942,  ..., 0.4788, 0.4374, 0.5348],
          [0.6408, 0.4207, 0.4636,  ..., 0.3438, 0.6489, 0.5130],
          [0.4398, 0.6600, 0.5892,  ..., 0.5193, 0.5094, 0.5041],
          [0.3632, 0.3718, 0.4457,  ..., 0.5073, 0.4470, 0.6978]]]],
       device='cuda:0')
tensor([[[[0.4429, 0.5492, 0.5631,  ..., 0.6007, 0.4557, 0.3174],
          [0.4097, 0.4728, 0.3975,  ..., 0.4816, 0.3844, 0.5047],
          [0.6334, 0.3975, 0.5012,  ..., 0.5307, 0.5860, 0.3711],
          [0.5402, 0.4651, 0.5655,  ..., 0.6460, 0.5487, 0.3882]],

         [[0.5983, 0.3923, 0.3840,  ..., 0.6370, 0.4499, 0.3748],
          [0.5319, 0.4821, 0.4390,  ..., 0.6851, 0.5255, 0.4711],
          [0.5363, 0.5334, 0.4354,  ..., 0.5605, 0.6415, 0.4796],
          [0.4434, 0.5610, 0.4465,  ..., 0.5467, 0.5470, 0.5339]],

         [[0.5404, 0.4877, 0.5419,  ..., 0.4244, 0.3442, 0.4297],
          [0.5732, 0.4494, 0.5248,  ..., 0.7042, 0.3567, 0.5458],
          [0.5492, 0.5317, 0.5913,  ..., 0.5472, 0.4659, 0.5746],
          [0.5047, 0.6654, 0.4661,  ..., 0.4092, 0.4816, 0.5143]],

         ...,

         [[0.3157, 0.6388, 0.4306,  ..., 0.4723, 0.4797, 0.5075],
          [0.6567, 0.4693, 0.4715,  ..., 0.6817, 0.4135, 0.7241],
          [0.3355, 0.7098, 0.4918,  ..., 0.4149, 0.5997, 0.3702],
          [0.4521, 0.4826, 0.3905,  ..., 0.4378, 0.6025, 0.4349]],

         [[0.4154, 0.6095, 0.5865,  ..., 0.6146, 0.6243, 0.6072],
          [0.5436, 0.4125, 0.4007,  ..., 0.3648, 0.5894, 0.4463],
          [0.5195, 0.4287, 0.6352,  ..., 0.4700, 0.5765, 0.3928],
          [0.3970, 0.5051, 0.5008,  ..., 0.6637, 0.5431, 0.3720]],

         [[0.4841, 0.2422, 0.5467,  ..., 0.3684, 0.5804, 0.5317],
          [0.4758, 0.5603, 0.5655,  ..., 0.5274, 0.4325, 0.5134],
          [0.3882, 0.6234, 0.5732,  ..., 0.4554, 0.4692, 0.5062],
          [0.3766, 0.6397, 0.5578,  ..., 0.6741, 0.4412, 0.7186]]],


        [[[0.4659, 0.4450, 0.3844,  ..., 0.5732, 0.3849, 0.5055],
          [0.5090, 0.3789, 0.6663,  ..., 0.3872, 0.5670, 0.6049],
          [0.4196, 0.6025, 0.5846,  ..., 0.5472, 0.4922, 0.5446],
          [0.3748, 0.4239, 0.5324,  ..., 0.3612, 0.4340, 0.4301]],

         [[0.5146, 0.5319, 0.5799,  ..., 0.3259, 0.4978, 0.4596],
          [0.4163, 0.5578, 0.6379,  ..., 0.6876, 0.3858, 0.4824],
          [0.3693, 0.4192, 0.5324,  ..., 0.4349, 0.6680, 0.4944],
          [0.3748, 0.4268, 0.6179,  ..., 0.5324, 0.4846, 0.5689]],

         [[0.5804, 0.5027, 0.5254,  ..., 0.4995, 0.4603, 0.4254],
          [0.6220, 0.6361, 0.4069,  ..., 0.5296, 0.4434, 0.4634],
          [0.5134, 0.5775, 0.3720,  ..., 0.3657, 0.5076, 0.4494],
          [0.4866, 0.4521, 0.5231,  ..., 0.4026, 0.6146, 0.5670]],

         ...,

         [[0.5936, 0.6298, 0.4673,  ..., 0.5684, 0.4947, 0.4591],
          [0.4931, 0.4698, 0.4278,  ..., 0.6361, 0.4249, 0.3794],
          [0.4528, 0.7106, 0.4385,  ..., 0.4863, 0.4448, 0.5665],
          [0.5722, 0.5251, 0.3826,  ..., 0.4778, 0.5446, 0.4282]],

         [[0.6935, 0.4817, 0.6628,  ..., 0.5443, 0.5533, 0.5346],
          [0.3998, 0.5098, 0.3947,  ..., 0.4642, 0.5189, 0.3730],
          [0.3486, 0.3975, 0.4848,  ..., 0.6086, 0.4429, 0.4359],
          [0.4244, 0.4729, 0.5011,  ..., 0.6289, 0.4664, 0.6800]],

         [[0.3648, 0.5484, 0.4882,  ..., 0.4828, 0.4354, 0.5288],
          [0.6388, 0.4287, 0.4576,  ..., 0.3478, 0.6469, 0.5070],
          [0.4378, 0.6680, 0.5832,  ..., 0.5233, 0.5074, 0.4981],
          [0.3612, 0.3798, 0.4397,  ..., 0.5113, 0.4450, 0.6918]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0020, -0.0080,  0.0060, -0.0020,  0.0020,  0.0040, -0.0060, -0.0040,
         0.0020,  0.0060], device='cuda:0')
selected experts tensor([1525, 1540, 1889, 1611, 1612, 1635, 1648, 1625, 1555, 1744],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4941, 0.5437, 0.4970,  ..., 0.4056, 0.4057, 0.4328],
          [0.5657, 0.4541, 0.6351,  ..., 0.3544, 0.5121, 0.6298],
          [0.4062, 0.5194, 0.4542,  ..., 0.5475, 0.4759, 0.5038],
          [0.4926, 0.2766, 0.4832,  ..., 0.5419, 0.6003, 0.6145]],

         [[0.5011, 0.5260, 0.4987,  ..., 0.4650, 0.3050, 0.4552],
          [0.6571, 0.4884, 0.5493,  ..., 0.4706, 0.5153, 0.5652],
          [0.4529, 0.5277, 0.5249,  ..., 0.5597, 0.5301, 0.4850],
          [0.3509, 0.4104, 0.4476,  ..., 0.6418, 0.3640, 0.5609]],

         [[0.5006, 0.4161, 0.5168,  ..., 0.4147, 0.3873, 0.4088],
          [0.5524, 0.5999, 0.3731,  ..., 0.3877, 0.4291, 0.3441],
          [0.4898, 0.5239, 0.5341,  ..., 0.4942, 0.4630, 0.4991],
          [0.4265, 0.5155, 0.3307,  ..., 0.4699, 0.4713, 0.5873]],

         ...,

         [[0.5948, 0.4553, 0.4500,  ..., 0.3102, 0.4183, 0.5195],
          [0.5230, 0.4995, 0.5358,  ..., 0.3723, 0.4324, 0.4941],
          [0.5796, 0.3958, 0.4982,  ..., 0.4787, 0.4085, 0.4672],
          [0.5924, 0.4534, 0.4125,  ..., 0.4693, 0.2839, 0.6081]],

         [[0.5346, 0.6553, 0.3884,  ..., 0.5192, 0.4645, 0.3432],
          [0.5085, 0.6221, 0.5430,  ..., 0.6034, 0.4159, 0.4541],
          [0.4561, 0.4473, 0.5855,  ..., 0.5895, 0.5536, 0.3993],
          [0.5519, 0.4695, 0.2867,  ..., 0.4224, 0.5401, 0.6307]],

         [[0.5374, 0.3735, 0.6261,  ..., 0.5777, 0.3337, 0.4929],
          [0.3456, 0.5849, 0.5022,  ..., 0.4999, 0.4164, 0.5662],
          [0.3806, 0.5692, 0.5459,  ..., 0.4104, 0.5519, 0.5699],
          [0.5118, 0.3446, 0.5421,  ..., 0.5659, 0.5308, 0.5273]]],


        [[[0.5565, 0.5447, 0.5459,  ..., 0.6066, 0.4020, 0.3655],
          [0.5872, 0.4180, 0.5167,  ..., 0.4809, 0.4657, 0.6500],
          [0.4880, 0.5386, 0.6851,  ..., 0.5649, 0.4698, 0.5794],
          [0.4823, 0.3509, 0.4474,  ..., 0.6382, 0.4844, 0.6404]],

         [[0.4837, 0.5687, 0.3117,  ..., 0.5458, 0.4529, 0.4756],
          [0.4829, 0.4242, 0.5737,  ..., 0.4758, 0.3613, 0.5864],
          [0.5529, 0.3935, 0.5148,  ..., 0.4823, 0.5315, 0.4026],
          [0.4645, 0.3228, 0.4971,  ..., 0.4616, 0.6487, 0.5906]],

         [[0.5553, 0.5473, 0.4854,  ..., 0.4814, 0.2793, 0.3978],
          [0.5328, 0.5654, 0.4824,  ..., 0.3590, 0.4338, 0.5377],
          [0.2910, 0.5038, 0.5902,  ..., 0.5678, 0.4391, 0.5609],
          [0.5243, 0.3581, 0.4044,  ..., 0.5904, 0.5536, 0.5270]],

         ...,

         [[0.5580, 0.6366, 0.3768,  ..., 0.6203, 0.3587, 0.4467],
          [0.5214, 0.5144, 0.5037,  ..., 0.5011, 0.5153, 0.5461],
          [0.5905, 0.3735, 0.5040,  ..., 0.5506, 0.5385, 0.6235],
          [0.5938, 0.5186, 0.4530,  ..., 0.5339, 0.3810, 0.6190]],

         [[0.3884, 0.4913, 0.5627,  ..., 0.5829, 0.4024, 0.2969],
          [0.4019, 0.3939, 0.5365,  ..., 0.4339, 0.5096, 0.3123],
          [0.6301, 0.5473, 0.5079,  ..., 0.3839, 0.4015, 0.4792],
          [0.5896, 0.4309, 0.6279,  ..., 0.6284, 0.6059, 0.4232]],

         [[0.4845, 0.5219, 0.4168,  ..., 0.3553, 0.2733, 0.6145],
          [0.4425, 0.5560, 0.5440,  ..., 0.3797, 0.4946, 0.5423],
          [0.2998, 0.5284, 0.4869,  ..., 0.4980, 0.5628, 0.5657],
          [0.5101, 0.5191, 0.3413,  ..., 0.6540, 0.3371, 0.4331]]]],
       device='cuda:0')
tensor([[[[0.4901, 0.5477, 0.5070,  ..., 0.4196, 0.3877, 0.4508],
          [0.5617, 0.4581, 0.6451,  ..., 0.3684, 0.4941, 0.6478],
          [0.4022, 0.5234, 0.4642,  ..., 0.5615, 0.4579, 0.5218],
          [0.4886, 0.2806, 0.4932,  ..., 0.5559, 0.5823, 0.6325]],

         [[0.4971, 0.5300, 0.5087,  ..., 0.4790, 0.2870, 0.4732],
          [0.6531, 0.4924, 0.5593,  ..., 0.4846, 0.4973, 0.5832],
          [0.4489, 0.5317, 0.5349,  ..., 0.5737, 0.5121, 0.5030],
          [0.3469, 0.4144, 0.4576,  ..., 0.6558, 0.3460, 0.5789]],

         [[0.4966, 0.4201, 0.5268,  ..., 0.4287, 0.3693, 0.4268],
          [0.5484, 0.6039, 0.3831,  ..., 0.4017, 0.4111, 0.3621],
          [0.4858, 0.5279, 0.5441,  ..., 0.5082, 0.4450, 0.5171],
          [0.4225, 0.5195, 0.3407,  ..., 0.4839, 0.4533, 0.6053]],

         ...,

         [[0.5908, 0.4593, 0.4600,  ..., 0.3242, 0.4003, 0.5375],
          [0.5190, 0.5035, 0.5458,  ..., 0.3863, 0.4144, 0.5121],
          [0.5756, 0.3998, 0.5082,  ..., 0.4927, 0.3905, 0.4852],
          [0.5884, 0.4574, 0.4225,  ..., 0.4833, 0.2659, 0.6261]],

         [[0.5306, 0.6593, 0.3984,  ..., 0.5332, 0.4465, 0.3612],
          [0.5045, 0.6261, 0.5530,  ..., 0.6174, 0.3979, 0.4721],
          [0.4521, 0.4513, 0.5955,  ..., 0.6035, 0.5356, 0.4173],
          [0.5479, 0.4735, 0.2967,  ..., 0.4364, 0.5221, 0.6487]],

         [[0.5334, 0.3775, 0.6361,  ..., 0.5917, 0.3157, 0.5109],
          [0.3416, 0.5889, 0.5122,  ..., 0.5139, 0.3984, 0.5842],
          [0.3766, 0.5732, 0.5559,  ..., 0.4244, 0.5339, 0.5879],
          [0.5078, 0.3486, 0.5521,  ..., 0.5799, 0.5128, 0.5453]]],


        [[[0.5525, 0.5487, 0.5559,  ..., 0.6206, 0.3840, 0.3835],
          [0.5832, 0.4220, 0.5267,  ..., 0.4949, 0.4477, 0.6680],
          [0.4840, 0.5426, 0.6951,  ..., 0.5789, 0.4518, 0.5974],
          [0.4783, 0.3549, 0.4574,  ..., 0.6522, 0.4664, 0.6584]],

         [[0.4797, 0.5727, 0.3217,  ..., 0.5598, 0.4349, 0.4936],
          [0.4789, 0.4282, 0.5837,  ..., 0.4898, 0.3433, 0.6044],
          [0.5489, 0.3975, 0.5248,  ..., 0.4963, 0.5135, 0.4206],
          [0.4605, 0.3268, 0.5071,  ..., 0.4756, 0.6307, 0.6086]],

         [[0.5513, 0.5513, 0.4954,  ..., 0.4954, 0.2613, 0.4158],
          [0.5288, 0.5694, 0.4924,  ..., 0.3730, 0.4158, 0.5557],
          [0.2870, 0.5078, 0.6002,  ..., 0.5818, 0.4211, 0.5789],
          [0.5203, 0.3621, 0.4144,  ..., 0.6044, 0.5356, 0.5450]],

         ...,

         [[0.5540, 0.6406, 0.3868,  ..., 0.6343, 0.3407, 0.4647],
          [0.5174, 0.5184, 0.5137,  ..., 0.5151, 0.4973, 0.5641],
          [0.5865, 0.3775, 0.5140,  ..., 0.5646, 0.5205, 0.6415],
          [0.5898, 0.5226, 0.4630,  ..., 0.5479, 0.3630, 0.6370]],

         [[0.3844, 0.4953, 0.5727,  ..., 0.5969, 0.3844, 0.3149],
          [0.3979, 0.3979, 0.5465,  ..., 0.4479, 0.4916, 0.3303],
          [0.6261, 0.5513, 0.5179,  ..., 0.3979, 0.3835, 0.4972],
          [0.5856, 0.4349, 0.6379,  ..., 0.6424, 0.5879, 0.4412]],

         [[0.4805, 0.5259, 0.4268,  ..., 0.3693, 0.2553, 0.6325],
          [0.4385, 0.5600, 0.5540,  ..., 0.3937, 0.4766, 0.5603],
          [0.2958, 0.5324, 0.4969,  ..., 0.5120, 0.5448, 0.5837],
          [0.5061, 0.5231, 0.3513,  ..., 0.6680, 0.3191, 0.4511]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([ 0.0040, -0.0040, -0.0100,  0.0060, -0.0040, -0.0080,  0.0020, -0.0140,
         0.0180, -0.0180], device='cuda:0')
selected experts tensor([1554, 1715, 1511, 1517, 1560, 1998, 1530, 1969, 1133, 1897],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.4372, 0.5125, 0.6914,  ..., 0.4254, 0.5686, 0.5570],
          [0.5988, 0.5303, 0.3154,  ..., 0.5463, 0.6925, 0.5736],
          [0.4902, 0.5472, 0.3674,  ..., 0.4145, 0.5900, 0.5415],
          [0.5170, 0.4705, 0.5567,  ..., 0.6728, 0.5337, 0.5032]],

         [[0.5617, 0.4511, 0.6880,  ..., 0.4896, 0.5924, 0.5092],
          [0.3920, 0.4937, 0.4814,  ..., 0.5136, 0.4001, 0.4730],
          [0.4927, 0.5281, 0.4415,  ..., 0.4473, 0.5739, 0.3667],
          [0.5399, 0.6341, 0.3674,  ..., 0.5923, 0.4575, 0.6461]],

         [[0.7194, 0.6611, 0.6189,  ..., 0.4349, 0.6356, 0.4117],
          [0.4003, 0.4482, 0.5267,  ..., 0.5613, 0.5155, 0.3730],
          [0.4555, 0.5286, 0.5593,  ..., 0.3908, 0.6527, 0.5509],
          [0.6110, 0.5233, 0.3746,  ..., 0.6288, 0.4260, 0.5755]],

         ...,

         [[0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100]],

         [[0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100]],

         [[0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100]]],


        [[[0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100]],

         [[0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100]],

         [[0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100]],

         ...,

         [[0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100]],

         [[0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100]],

         [[0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100],
          [0.5080, 0.5080, 0.5080,  ..., 0.5100, 0.5040, 0.5100]]]],
       device='cuda:0')
tensor([[[[0.4292, 0.5045, 0.6834,  ..., 0.4154, 0.5646, 0.5470],
          [0.5908, 0.5223, 0.3074,  ..., 0.5363, 0.6885, 0.5636],
          [0.4822, 0.5392, 0.3594,  ..., 0.4045, 0.5860, 0.5315],
          [0.5090, 0.4625, 0.5487,  ..., 0.6628, 0.5297, 0.4932]],

         [[0.5537, 0.4431, 0.6800,  ..., 0.4796, 0.5884, 0.4992],
          [0.3840, 0.4857, 0.4734,  ..., 0.5036, 0.3961, 0.4630],
          [0.4847, 0.5201, 0.4335,  ..., 0.4373, 0.5699, 0.3567],
          [0.5319, 0.6261, 0.3594,  ..., 0.5823, 0.4535, 0.6361]],

         [[0.7114, 0.6531, 0.6109,  ..., 0.4249, 0.6316, 0.4017],
          [0.3923, 0.4402, 0.5187,  ..., 0.5513, 0.5115, 0.3630],
          [0.4475, 0.5206, 0.5513,  ..., 0.3808, 0.6487, 0.5409],
          [0.6030, 0.5153, 0.3666,  ..., 0.6188, 0.4220, 0.5655]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0080, 0.0080, 0.0080, 0.0080, 0.0080, 0.0080, 0.0080, 0.0100, 0.0040,
        0.0100], device='cuda:0')
selected experts tensor([ 403,  685,  561,  442,  617,  695,  871, 3533,  923, 3558],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5850, 0.4063, 0.6057,  ..., 0.5243, 0.6195, 0.4761],
          [0.5170, 0.5392, 0.4161,  ..., 0.3942, 0.6064, 0.5396],
          [0.5219, 0.4818, 0.4554,  ..., 0.5197, 0.5361, 0.4124],
          [0.3804, 0.5779, 0.4807,  ..., 0.6485, 0.4647, 0.5868]],

         [[0.6682, 0.6539, 0.5447,  ..., 0.5337, 0.4671, 0.5440],
          [0.5627, 0.4696, 0.4633,  ..., 0.3984, 0.4502, 0.5859],
          [0.4063, 0.6414, 0.4185,  ..., 0.5291, 0.5932, 0.3884],
          [0.5512, 0.7554, 0.5963,  ..., 0.6340, 0.3675, 0.4452]],

         [[0.4478, 0.6164, 0.4962,  ..., 0.4504, 0.4182, 0.4793],
          [0.3529, 0.5072, 0.4721,  ..., 0.5331, 0.6933, 0.4493],
          [0.4006, 0.4421, 0.4423,  ..., 0.6672, 0.4669, 0.5616],
          [0.4529, 0.4239, 0.6704,  ..., 0.6044, 0.2658, 0.4864]],

         ...,

         [[0.5584, 0.4248, 0.5301,  ..., 0.5735, 0.5153, 0.5187],
          [0.5836, 0.6958, 0.4849,  ..., 0.5330, 0.5721, 0.6222],
          [0.4815, 0.5107, 0.4532,  ..., 0.5082, 0.4810, 0.4522],
          [0.5442, 0.4479, 0.4385,  ..., 0.4933, 0.4647, 0.6676]],

         [[0.4706, 0.5832, 0.5958,  ..., 0.5935, 0.3870, 0.2994],
          [0.4863, 0.3885, 0.4979,  ..., 0.5648, 0.5765, 0.4413],
          [0.6260, 0.5518, 0.4810,  ..., 0.6188, 0.4097, 0.6042],
          [0.6404, 0.4805, 0.5469,  ..., 0.5366, 0.4320, 0.4257]],

         [[0.3698, 0.6108, 0.5045,  ..., 0.4982, 0.5195, 0.5304],
          [0.5746, 0.3683, 0.4957,  ..., 0.6067, 0.4943, 0.4595],
          [0.3869, 0.3441, 0.4104,  ..., 0.4892, 0.4596, 0.4949],
          [0.5189, 0.4816, 0.4062,  ..., 0.5545, 0.5014, 0.4928]]],


        [[[0.6046, 0.4515, 0.5520,  ..., 0.5297, 0.5736, 0.6268],
          [0.5205, 0.5665, 0.6339,  ..., 0.4148, 0.4211, 0.6019],
          [0.5106, 0.5455, 0.4828,  ..., 0.4191, 0.4678, 0.3537],
          [0.2972, 0.6095, 0.4873,  ..., 0.4871, 0.3765, 0.4600]],

         [[0.4529, 0.4569, 0.6215,  ..., 0.5000, 0.5414, 0.4872],
          [0.5612, 0.3512, 0.5145,  ..., 0.5382, 0.3870, 0.5826],
          [0.5314, 0.4836, 0.5796,  ..., 0.5119, 0.5649, 0.5003],
          [0.4379, 0.2603, 0.5021,  ..., 0.2609, 0.5063, 0.4503]],

         [[0.5593, 0.3899, 0.4852,  ..., 0.3592, 0.4941, 0.4466],
          [0.5517, 0.4915, 0.5113,  ..., 0.3827, 0.5623, 0.3921],
          [0.4652, 0.5978, 0.3932,  ..., 0.5304, 0.4598, 0.4810],
          [0.4115, 0.4335, 0.5488,  ..., 0.4012, 0.4382, 0.6905]],

         ...,

         [[0.4324, 0.4547, 0.4865,  ..., 0.5603, 0.5349, 0.4205],
          [0.4287, 0.5382, 0.5958,  ..., 0.5003, 0.5240, 0.4281],
          [0.4906, 0.5607, 0.5114,  ..., 0.6404, 0.4930, 0.5282],
          [0.3781, 0.6279, 0.3590,  ..., 0.5274, 0.6700, 0.4831]],

         [[0.4742, 0.4339, 0.6483,  ..., 0.5163, 0.5562, 0.6000],
          [0.4139, 0.5511, 0.5089,  ..., 0.5012, 0.4936, 0.4493],
          [0.4751, 0.3848, 0.4043,  ..., 0.6053, 0.4608, 0.4728],
          [0.5555, 0.5978, 0.5115,  ..., 0.4533, 0.4768, 0.3926]],

         [[0.5192, 0.4163, 0.4749,  ..., 0.6304, 0.6505, 0.5020],
          [0.5158, 0.4615, 0.3936,  ..., 0.4395, 0.6612, 0.4176],
          [0.3547, 0.5346, 0.5517,  ..., 0.5448, 0.3838, 0.5269],
          [0.4808, 0.5717, 0.5631,  ..., 0.5778, 0.5550, 0.5141]]]],
       device='cuda:0')
tensor([[[[0.5960, 0.4073, 0.6007,  ..., 0.5173, 0.6105, 0.4791],
          [0.5280, 0.5402, 0.4111,  ..., 0.3872, 0.5974, 0.5426],
          [0.5329, 0.4828, 0.4504,  ..., 0.5127, 0.5271, 0.4154],
          [0.3914, 0.5789, 0.4757,  ..., 0.6415, 0.4557, 0.5898]],

         [[0.6792, 0.6549, 0.5397,  ..., 0.5267, 0.4581, 0.5470],
          [0.5737, 0.4706, 0.4583,  ..., 0.3914, 0.4412, 0.5889],
          [0.4173, 0.6424, 0.4135,  ..., 0.5221, 0.5842, 0.3914],
          [0.5622, 0.7564, 0.5913,  ..., 0.6270, 0.3585, 0.4482]],

         [[0.4588, 0.6174, 0.4912,  ..., 0.4434, 0.4092, 0.4823],
          [0.3639, 0.5082, 0.4671,  ..., 0.5261, 0.6843, 0.4523],
          [0.4116, 0.4431, 0.4373,  ..., 0.6602, 0.4579, 0.5646],
          [0.4639, 0.4249, 0.6654,  ..., 0.5974, 0.2568, 0.4894]],

         ...,

         [[0.5694, 0.4258, 0.5251,  ..., 0.5665, 0.5063, 0.5217],
          [0.5946, 0.6968, 0.4799,  ..., 0.5260, 0.5631, 0.6252],
          [0.4925, 0.5117, 0.4482,  ..., 0.5012, 0.4720, 0.4552],
          [0.5552, 0.4489, 0.4335,  ..., 0.4863, 0.4557, 0.6706]],

         [[0.4816, 0.5842, 0.5908,  ..., 0.5865, 0.3780, 0.3024],
          [0.4973, 0.3895, 0.4929,  ..., 0.5578, 0.5675, 0.4443],
          [0.6370, 0.5528, 0.4760,  ..., 0.6118, 0.4007, 0.6072],
          [0.6514, 0.4815, 0.5419,  ..., 0.5296, 0.4230, 0.4287]],

         [[0.3808, 0.6118, 0.4995,  ..., 0.4912, 0.5105, 0.5334],
          [0.5856, 0.3693, 0.4907,  ..., 0.5997, 0.4853, 0.4625],
          [0.3979, 0.3451, 0.4054,  ..., 0.4822, 0.4506, 0.4979],
          [0.5299, 0.4826, 0.4012,  ..., 0.5475, 0.4924, 0.4958]]],


        [[[0.6156, 0.4525, 0.5470,  ..., 0.5227, 0.5646, 0.6298],
          [0.5315, 0.5675, 0.6289,  ..., 0.4078, 0.4121, 0.6049],
          [0.5216, 0.5465, 0.4778,  ..., 0.4121, 0.4588, 0.3567],
          [0.3082, 0.6105, 0.4823,  ..., 0.4801, 0.3675, 0.4630]],

         [[0.4639, 0.4579, 0.6165,  ..., 0.4930, 0.5324, 0.4902],
          [0.5722, 0.3522, 0.5095,  ..., 0.5312, 0.3780, 0.5856],
          [0.5424, 0.4846, 0.5746,  ..., 0.5049, 0.5559, 0.5033],
          [0.4489, 0.2613, 0.4971,  ..., 0.2539, 0.4973, 0.4533]],

         [[0.5703, 0.3909, 0.4802,  ..., 0.3522, 0.4851, 0.4496],
          [0.5627, 0.4925, 0.5063,  ..., 0.3757, 0.5533, 0.3951],
          [0.4762, 0.5988, 0.3882,  ..., 0.5234, 0.4508, 0.4840],
          [0.4225, 0.4345, 0.5438,  ..., 0.3942, 0.4292, 0.6935]],

         ...,

         [[0.4434, 0.4557, 0.4815,  ..., 0.5533, 0.5259, 0.4235],
          [0.4397, 0.5392, 0.5908,  ..., 0.4933, 0.5150, 0.4311],
          [0.5016, 0.5617, 0.5064,  ..., 0.6334, 0.4840, 0.5312],
          [0.3891, 0.6289, 0.3540,  ..., 0.5204, 0.6610, 0.4861]],

         [[0.4852, 0.4349, 0.6433,  ..., 0.5093, 0.5472, 0.6030],
          [0.4249, 0.5521, 0.5039,  ..., 0.4942, 0.4846, 0.4523],
          [0.4861, 0.3858, 0.3993,  ..., 0.5983, 0.4518, 0.4758],
          [0.5665, 0.5988, 0.5065,  ..., 0.4463, 0.4678, 0.3956]],

         [[0.5302, 0.4173, 0.4699,  ..., 0.6234, 0.6415, 0.5050],
          [0.5268, 0.4625, 0.3886,  ..., 0.4325, 0.6522, 0.4206],
          [0.3657, 0.5356, 0.5467,  ..., 0.5378, 0.3748, 0.5299],
          [0.4918, 0.5727, 0.5581,  ..., 0.5708, 0.5460, 0.5171]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0110, -0.0010,  0.0050,  0.0030,  0.0070,  0.0050, -0.0050,  0.0070,
         0.0090, -0.0030], device='cuda:0')
selected experts tensor([1658, 1703, 1496, 1561, 1665, 1641, 1593, 1605, 1682, 1780],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5417, 0.5108, 0.5182,  ..., 0.5845, 0.4863, 0.5457],
          [0.5268, 0.4112, 0.6791,  ..., 0.5198, 0.4113, 0.6438],
          [0.4541, 0.5499, 0.4990,  ..., 0.5897, 0.4203, 0.6182],
          [0.5254, 0.5540, 0.5655,  ..., 0.5967, 0.4582, 0.5221]],

         [[0.5957, 0.6237, 0.4721,  ..., 0.5659, 0.4978, 0.4638],
          [0.4961, 0.4629, 0.4323,  ..., 0.6331, 0.4284, 0.3844],
          [0.4558, 0.7036, 0.4435,  ..., 0.4833, 0.4478, 0.5720],
          [0.5752, 0.5180, 0.3876,  ..., 0.4749, 0.5476, 0.4332]],

         [[0.4741, 0.4356, 0.4208,  ..., 0.4437, 0.5867, 0.6150],
          [0.4934, 0.5857, 0.5571,  ..., 0.6204, 0.6675, 0.5566],
          [0.3884, 0.6318, 0.5681,  ..., 0.5570, 0.4604, 0.5433],
          [0.5488, 0.5020, 0.3770,  ..., 0.3977, 0.5019, 0.4933]],

         ...,

         [[0.5635, 0.4854, 0.6465,  ..., 0.5580, 0.4293, 0.5469],
          [0.5976, 0.4516, 0.4479,  ..., 0.5726, 0.5334, 0.3858],
          [0.4066, 0.4255, 0.3743,  ..., 0.3861, 0.4891, 0.5583],
          [0.3552, 0.3641, 0.4318,  ..., 0.4133, 0.5476, 0.5413]],

         [[0.4246, 0.4918, 0.4655,  ..., 0.6121, 0.4879, 0.6103],
          [0.5009, 0.3632, 0.6311,  ..., 0.2659, 0.6255, 0.3107],
          [0.4179, 0.3952, 0.4677,  ..., 0.5445, 0.5733, 0.3082],
          [0.4857, 0.6354, 0.5130,  ..., 0.4110, 0.6139, 0.6224]],

         [[0.5599, 0.4472, 0.5977,  ..., 0.5625, 0.5966, 0.5401],
          [0.5505, 0.4639, 0.4677,  ..., 0.4267, 0.4408, 0.4932],
          [0.6153, 0.5373, 0.6256,  ..., 0.4999, 0.5971, 0.5476],
          [0.5110, 0.4560, 0.4890,  ..., 0.4819, 0.4997, 0.3017]]],


        [[[0.5213, 0.4236, 0.5082,  ..., 0.4806, 0.4485, 0.3099],
          [0.4630, 0.5224, 0.5768,  ..., 0.5887, 0.5980, 0.2648],
          [0.4770, 0.4947, 0.5132,  ..., 0.5015, 0.5657, 0.4053],
          [0.5328, 0.4931, 0.5825,  ..., 0.3819, 0.3238, 0.5196]],

         [[0.4621, 0.5955, 0.5796,  ..., 0.4925, 0.5167, 0.5396],
          [0.4009, 0.4203, 0.5977,  ..., 0.4147, 0.3995, 0.3950],
          [0.5895, 0.4179, 0.3617,  ..., 0.4382, 0.5000, 0.4515],
          [0.4655, 0.5134, 0.5341,  ..., 0.4979, 0.5081, 0.6833]],

         [[0.5320, 0.5429, 0.5396,  ..., 0.7219, 0.4336, 0.6465],
          [0.4771, 0.4506, 0.5078,  ..., 0.5527, 0.4799, 0.5160],
          [0.4427, 0.3233, 0.6311,  ..., 0.4797, 0.5023, 0.4123],
          [0.4449, 0.4859, 0.5433,  ..., 0.5010, 0.4108, 0.6284]],

         ...,

         [[0.3499, 0.4817, 0.4175,  ..., 0.3759, 0.4814, 0.3789],
          [0.4009, 0.5714, 0.5669,  ..., 0.5343, 0.4604, 0.6233],
          [0.6121, 0.4653, 0.5648,  ..., 0.4034, 0.7006, 0.5513],
          [0.4694, 0.5438, 0.4123,  ..., 0.4609, 0.4375, 0.6599]],

         [[0.5943, 0.4731, 0.5223,  ..., 0.4086, 0.4741, 0.4638],
          [0.5834, 0.5438, 0.5796,  ..., 0.5423, 0.3606, 0.4304],
          [0.5994, 0.4122, 0.4242,  ..., 0.5483, 0.4980, 0.4856],
          [0.3750, 0.4965, 0.5201,  ..., 0.5930, 0.4070, 0.3997]],

         [[0.5471, 0.4477, 0.5014,  ..., 0.6075, 0.3516, 0.6215],
          [0.5153, 0.3650, 0.5418,  ..., 0.3052, 0.5909, 0.5691],
          [0.4970, 0.4528, 0.6411,  ..., 0.5394, 0.5219, 0.4972],
          [0.4851, 0.5079, 0.4818,  ..., 0.4319, 0.4751, 0.4868]]]],
       device='cuda:0')
tensor([[[[0.5387, 0.5178, 0.5132,  ..., 0.5875, 0.4833, 0.5407],
          [0.5238, 0.4182, 0.6741,  ..., 0.5228, 0.4083, 0.6388],
          [0.4511, 0.5569, 0.4940,  ..., 0.5927, 0.4173, 0.6132],
          [0.5224, 0.5610, 0.5605,  ..., 0.5997, 0.4552, 0.5171]],

         [[0.5927, 0.6307, 0.4671,  ..., 0.5689, 0.4948, 0.4588],
          [0.4931, 0.4699, 0.4273,  ..., 0.6361, 0.4254, 0.3794],
          [0.4528, 0.7106, 0.4385,  ..., 0.4863, 0.4448, 0.5670],
          [0.5722, 0.5250, 0.3826,  ..., 0.4779, 0.5446, 0.4282]],

         [[0.4711, 0.4426, 0.4158,  ..., 0.4467, 0.5837, 0.6100],
          [0.4904, 0.5927, 0.5521,  ..., 0.6234, 0.6645, 0.5516],
          [0.3854, 0.6388, 0.5631,  ..., 0.5600, 0.4574, 0.5383],
          [0.5458, 0.5090, 0.3720,  ..., 0.4007, 0.4989, 0.4883]],

         ...,

         [[0.5605, 0.4924, 0.6415,  ..., 0.5610, 0.4263, 0.5419],
          [0.5946, 0.4586, 0.4429,  ..., 0.5756, 0.5304, 0.3808],
          [0.4036, 0.4325, 0.3693,  ..., 0.3891, 0.4861, 0.5533],
          [0.3522, 0.3711, 0.4268,  ..., 0.4163, 0.5446, 0.5363]],

         [[0.4216, 0.4988, 0.4605,  ..., 0.6151, 0.4849, 0.6053],
          [0.4979, 0.3702, 0.6261,  ..., 0.2689, 0.6225, 0.3057],
          [0.4149, 0.4022, 0.4627,  ..., 0.5475, 0.5703, 0.3032],
          [0.4827, 0.6424, 0.5080,  ..., 0.4140, 0.6109, 0.6174]],

         [[0.5569, 0.4542, 0.5927,  ..., 0.5655, 0.5936, 0.5351],
          [0.5475, 0.4709, 0.4627,  ..., 0.4297, 0.4378, 0.4882],
          [0.6123, 0.5443, 0.6206,  ..., 0.5029, 0.5941, 0.5426],
          [0.5080, 0.4630, 0.4840,  ..., 0.4849, 0.4967, 0.2967]]],


        [[[0.5183, 0.4306, 0.5032,  ..., 0.4836, 0.4455, 0.3049],
          [0.4600, 0.5294, 0.5718,  ..., 0.5917, 0.5950, 0.2598],
          [0.4740, 0.5017, 0.5082,  ..., 0.5045, 0.5627, 0.4003],
          [0.5297, 0.5001, 0.5775,  ..., 0.3849, 0.3208, 0.5146]],

         [[0.4591, 0.6025, 0.5746,  ..., 0.4955, 0.5137, 0.5346],
          [0.3979, 0.4273, 0.5927,  ..., 0.4177, 0.3965, 0.3900],
          [0.5865, 0.4249, 0.3567,  ..., 0.4412, 0.4970, 0.4465],
          [0.4625, 0.5204, 0.5291,  ..., 0.5009, 0.5051, 0.6783]],

         [[0.5290, 0.5499, 0.5346,  ..., 0.7249, 0.4306, 0.6415],
          [0.4741, 0.4576, 0.5028,  ..., 0.5557, 0.4769, 0.5110],
          [0.4397, 0.3303, 0.6261,  ..., 0.4827, 0.4993, 0.4073],
          [0.4419, 0.4929, 0.5383,  ..., 0.5040, 0.4078, 0.6234]],

         ...,

         [[0.3469, 0.4887, 0.4125,  ..., 0.3789, 0.4784, 0.3739],
          [0.3979, 0.5784, 0.5619,  ..., 0.5373, 0.4574, 0.6183],
          [0.6091, 0.4723, 0.5598,  ..., 0.4064, 0.6976, 0.5463],
          [0.4664, 0.5508, 0.4073,  ..., 0.4639, 0.4345, 0.6549]],

         [[0.5913, 0.4801, 0.5173,  ..., 0.4116, 0.4711, 0.4588],
          [0.5804, 0.5508, 0.5746,  ..., 0.5453, 0.3576, 0.4254],
          [0.5964, 0.4192, 0.4192,  ..., 0.5513, 0.4950, 0.4806],
          [0.3720, 0.5035, 0.5151,  ..., 0.5960, 0.4040, 0.3947]],

         [[0.5441, 0.4547, 0.4964,  ..., 0.6105, 0.3486, 0.6165],
          [0.5123, 0.3720, 0.5368,  ..., 0.3082, 0.5879, 0.5641],
          [0.4940, 0.4598, 0.6361,  ..., 0.5424, 0.5189, 0.4922],
          [0.4821, 0.5149, 0.4768,  ..., 0.4349, 0.4721, 0.4818]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0030, -0.0070,  0.0050, -0.0010,  0.0030,  0.0050, -0.0070, -0.0030,
         0.0030,  0.0050], device='cuda:0')
selected experts tensor([1599, 1585, 1610, 1654, 1758, 1566, 1561, 1659, 1713, 1679],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4910, 0.3735, 0.5141,  ..., 0.4624, 0.3305, 0.3613],
          [0.4039, 0.5080, 0.4847,  ..., 0.4928, 0.4511, 0.5420],
          [0.5234, 0.6472, 0.5067,  ..., 0.5673, 0.5151, 0.4998],
          [0.4963, 0.4161, 0.4619,  ..., 0.6274, 0.4972, 0.6377]],

         [[0.5996, 0.5422, 0.4102,  ..., 0.4533, 0.3247, 0.3678],
          [0.6438, 0.4599, 0.4903,  ..., 0.4593, 0.3793, 0.4459],
          [0.4285, 0.5013, 0.6388,  ..., 0.5553, 0.5766, 0.4758],
          [0.5563, 0.3436, 0.3067,  ..., 0.6460, 0.3623, 0.5656]],

         [[0.4738, 0.4721, 0.3936,  ..., 0.5230, 0.3712, 0.4150],
          [0.5763, 0.6284, 0.5103,  ..., 0.4419, 0.4104, 0.2768],
          [0.4194, 0.5420, 0.6047,  ..., 0.5055, 0.4472, 0.5461],
          [0.5806, 0.4194, 0.3640,  ..., 0.6650, 0.3775, 0.4401]],

         ...,

         [[0.6028, 0.5228, 0.4264,  ..., 0.4663, 0.4769, 0.5984],
          [0.5257, 0.5425, 0.4609,  ..., 0.5496, 0.4090, 0.5480],
          [0.5140, 0.4422, 0.5069,  ..., 0.3722, 0.6853, 0.4150],
          [0.3387, 0.4384, 0.4324,  ..., 0.6102, 0.5115, 0.5071]],

         [[0.3876, 0.4648, 0.4486,  ..., 0.4711, 0.3748, 0.3599],
          [0.3671, 0.6193, 0.6811,  ..., 0.5184, 0.4487, 0.5064],
          [0.3908, 0.5773, 0.5775,  ..., 0.4730, 0.5821, 0.4311],
          [0.5839, 0.5013, 0.3857,  ..., 0.5100, 0.2941, 0.3165]],

         [[0.5176, 0.3976, 0.4566,  ..., 0.5144, 0.4429, 0.4616],
          [0.4256, 0.6918, 0.6153,  ..., 0.5293, 0.4320, 0.5263],
          [0.3431, 0.4391, 0.5580,  ..., 0.4308, 0.4320, 0.5173],
          [0.5616, 0.3184, 0.5714,  ..., 0.5625, 0.5711, 0.6002]]],


        [[[0.3761, 0.5165, 0.3513,  ..., 0.5298, 0.4406, 0.4527],
          [0.4959, 0.5768, 0.7182,  ..., 0.4701, 0.4358, 0.6108],
          [0.4756, 0.5357, 0.5254,  ..., 0.5133, 0.5302, 0.5393],
          [0.3581, 0.4028, 0.4068,  ..., 0.5959, 0.6187, 0.6035]],

         [[0.5815, 0.4056, 0.4858,  ..., 0.5040, 0.4410, 0.4978],
          [0.4256, 0.4507, 0.4976,  ..., 0.5262, 0.2879, 0.4577],
          [0.4232, 0.5296, 0.6130,  ..., 0.5824, 0.5752, 0.4813],
          [0.4578, 0.3244, 0.3630,  ..., 0.6328, 0.4179, 0.5309]],

         [[0.5454, 0.4667, 0.4368,  ..., 0.3498, 0.3998, 0.4102],
          [0.4043, 0.4199, 0.5589,  ..., 0.3453, 0.4030, 0.4155],
          [0.4498, 0.5422, 0.5917,  ..., 0.4696, 0.4016, 0.4654],
          [0.5021, 0.4085, 0.3265,  ..., 0.6980, 0.3892, 0.5680]],

         ...,

         [[0.6047, 0.4804, 0.5310,  ..., 0.5254, 0.3458, 0.4401],
          [0.4799, 0.5519, 0.3379,  ..., 0.6364, 0.4815, 0.4976],
          [0.4395, 0.5071, 0.5108,  ..., 0.6726, 0.4320, 0.4471],
          [0.6617, 0.4085, 0.4982,  ..., 0.5520, 0.5260, 0.4629]],

         [[0.5669, 0.3794, 0.5699,  ..., 0.4706, 0.3012, 0.4488],
          [0.5691, 0.4661, 0.5063,  ..., 0.5739, 0.5246, 0.4457],
          [0.4294, 0.6266, 0.5042,  ..., 0.4023, 0.4306, 0.5132],
          [0.6014, 0.4804, 0.3450,  ..., 0.6399, 0.5151, 0.4812]],

         [[0.5435, 0.4707, 0.4126,  ..., 0.3956, 0.3938, 0.4637],
          [0.5650, 0.4884, 0.5464,  ..., 0.5040, 0.3381, 0.3673],
          [0.5176, 0.5131, 0.4723,  ..., 0.5443, 0.4961, 0.4665],
          [0.5882, 0.3832, 0.5709,  ..., 0.5332, 0.4878, 0.6080]]]],
       device='cuda:0')
tensor([[[[0.4860, 0.3785, 0.5231,  ..., 0.4774, 0.3115, 0.3803],
          [0.3989, 0.5130, 0.4937,  ..., 0.5078, 0.4321, 0.5610],
          [0.5184, 0.6522, 0.5157,  ..., 0.5823, 0.4961, 0.5188],
          [0.4913, 0.4211, 0.4709,  ..., 0.6424, 0.4782, 0.6567]],

         [[0.5946, 0.5472, 0.4192,  ..., 0.4683, 0.3057, 0.3868],
          [0.6388, 0.4649, 0.4993,  ..., 0.4743, 0.3603, 0.4649],
          [0.4235, 0.5063, 0.6478,  ..., 0.5703, 0.5576, 0.4948],
          [0.5513, 0.3486, 0.3157,  ..., 0.6610, 0.3433, 0.5846]],

         [[0.4688, 0.4771, 0.4026,  ..., 0.5380, 0.3522, 0.4340],
          [0.5713, 0.6334, 0.5193,  ..., 0.4569, 0.3914, 0.2958],
          [0.4144, 0.5470, 0.6137,  ..., 0.5205, 0.4282, 0.5651],
          [0.5756, 0.4244, 0.3730,  ..., 0.6800, 0.3585, 0.4591]],

         ...,

         [[0.5978, 0.5278, 0.4354,  ..., 0.4813, 0.4579, 0.6174],
          [0.5207, 0.5475, 0.4699,  ..., 0.5646, 0.3900, 0.5670],
          [0.5090, 0.4472, 0.5159,  ..., 0.3872, 0.6663, 0.4340],
          [0.3337, 0.4434, 0.4414,  ..., 0.6252, 0.4925, 0.5261]],

         [[0.3826, 0.4698, 0.4576,  ..., 0.4861, 0.3558, 0.3789],
          [0.3621, 0.6243, 0.6901,  ..., 0.5334, 0.4297, 0.5254],
          [0.3858, 0.5823, 0.5865,  ..., 0.4880, 0.5631, 0.4501],
          [0.5789, 0.5063, 0.3947,  ..., 0.5250, 0.2751, 0.3355]],

         [[0.5126, 0.4026, 0.4656,  ..., 0.5294, 0.4239, 0.4806],
          [0.4206, 0.6968, 0.6243,  ..., 0.5443, 0.4130, 0.5453],
          [0.3381, 0.4441, 0.5670,  ..., 0.4458, 0.4130, 0.5363],
          [0.5566, 0.3234, 0.5804,  ..., 0.5775, 0.5521, 0.6192]]],


        [[[0.3711, 0.5215, 0.3603,  ..., 0.5448, 0.4216, 0.4717],
          [0.4909, 0.5818, 0.7272,  ..., 0.4851, 0.4168, 0.6298],
          [0.4706, 0.5407, 0.5344,  ..., 0.5283, 0.5112, 0.5583],
          [0.3531, 0.4078, 0.4158,  ..., 0.6109, 0.5997, 0.6225]],

         [[0.5765, 0.4106, 0.4948,  ..., 0.5190, 0.4220, 0.5168],
          [0.4206, 0.4557, 0.5066,  ..., 0.5412, 0.2689, 0.4767],
          [0.4182, 0.5346, 0.6220,  ..., 0.5974, 0.5562, 0.5003],
          [0.4528, 0.3294, 0.3720,  ..., 0.6478, 0.3989, 0.5499]],

         [[0.5404, 0.4717, 0.4458,  ..., 0.3648, 0.3808, 0.4292],
          [0.3993, 0.4249, 0.5679,  ..., 0.3603, 0.3840, 0.4345],
          [0.4448, 0.5472, 0.6007,  ..., 0.4846, 0.3826, 0.4844],
          [0.4971, 0.4135, 0.3355,  ..., 0.7130, 0.3702, 0.5870]],

         ...,

         [[0.5997, 0.4854, 0.5400,  ..., 0.5404, 0.3268, 0.4591],
          [0.4749, 0.5569, 0.3469,  ..., 0.6514, 0.4625, 0.5166],
          [0.4345, 0.5121, 0.5198,  ..., 0.6876, 0.4130, 0.4661],
          [0.6567, 0.4135, 0.5072,  ..., 0.5670, 0.5070, 0.4819]],

         [[0.5619, 0.3844, 0.5789,  ..., 0.4856, 0.2822, 0.4678],
          [0.5641, 0.4711, 0.5153,  ..., 0.5889, 0.5056, 0.4647],
          [0.4244, 0.6316, 0.5132,  ..., 0.4173, 0.4116, 0.5322],
          [0.5964, 0.4854, 0.3540,  ..., 0.6549, 0.4961, 0.5002]],

         [[0.5385, 0.4757, 0.4216,  ..., 0.4106, 0.3748, 0.4827],
          [0.5600, 0.4934, 0.5554,  ..., 0.5190, 0.3191, 0.3863],
          [0.5126, 0.5181, 0.4813,  ..., 0.5593, 0.4771, 0.4855],
          [0.5832, 0.3882, 0.5799,  ..., 0.5482, 0.4688, 0.6270]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0050, -0.0050, -0.0090,  0.0070, -0.0030, -0.0090,  0.0030, -0.0150,
         0.0190, -0.0190], device='cuda:0')
selected experts tensor([1669, 1681, 1488, 1829, 1586, 1963, 1440, 2128,  955, 1645],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.4615, 0.5632, 0.7416,  ..., 0.3999, 0.5650, 0.4594],
          [0.4683, 0.5419, 0.5774,  ..., 0.5611, 0.5882, 0.5207],
          [0.4582, 0.4771, 0.5006,  ..., 0.4420, 0.4932, 0.6559],
          [0.4911, 0.6744, 0.4615,  ..., 0.6882, 0.5525, 0.6115]],

         [[0.5283, 0.5246, 0.5870,  ..., 0.4248, 0.4617, 0.3765],
          [0.4962, 0.6305, 0.5146,  ..., 0.4339, 0.5110, 0.4196],
          [0.5760, 0.5528, 0.5112,  ..., 0.5323, 0.7657, 0.4149],
          [0.6709, 0.5741, 0.3612,  ..., 0.3197, 0.5210, 0.6559]],

         [[0.4839, 0.6292, 0.7767,  ..., 0.4703, 0.7026, 0.4637],
          [0.4051, 0.5044, 0.5832,  ..., 0.5177, 0.5401, 0.4507],
          [0.4681, 0.5507, 0.5553,  ..., 0.4306, 0.6528, 0.5731],
          [0.5218, 0.7307, 0.4468,  ..., 0.4606, 0.4266, 0.5988]],

         ...,

         [[0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090],
          [0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090],
          [0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090],
          [0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090]],

         [[0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090],
          [0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090],
          [0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090],
          [0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090]],

         [[0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090],
          [0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090],
          [0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090],
          [0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090]]],


        [[[0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090],
          [0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090],
          [0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090],
          [0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090]],

         [[0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090],
          [0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090],
          [0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090],
          [0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090]],

         [[0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090],
          [0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090],
          [0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090],
          [0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090]],

         ...,

         [[0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090],
          [0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090],
          [0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090],
          [0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090]],

         [[0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090],
          [0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090],
          [0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090],
          [0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090]],

         [[0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090],
          [0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090],
          [0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090],
          [0.5090, 0.5090, 0.5090,  ..., 0.5090, 0.5050, 0.5090]]]],
       device='cuda:0')/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
[batch=20/40]:
	 Train time/batch: 19
	 Train time/sample: 38
	 Train time/batch_in_epoch: 19
	 Train time/sample_in_epoch: 38
	 Train time/token: 38912
	 Train time/token_in_epoch: 38912
	 Train memory/current_allocated_mem: 1.1405
	 Train memory/current_active_mem: 1.1405
	 Train memory/current_inactive_mem: 0.3338
	 Train memory/current_reserved_mem: 3.3932
	 Train memory/peak_allocated_mem: 2.7514
	 Train memory/peak_active_mem: 2.7514
	 Train memory/peak_inactive_mem: 0.5823
	 Train memory/peak_reserved_mem: 3.3932
	 Train memory/alloc_retries: 0
	 Train trainer/device_train_microbatch_size: 10
	 Train loss/train/total: 0.0049
	 Train metrics/train/LanguageCrossEntropy: 10.1118
	 Train metrics/train/LanguagePerplexity: 24631.0938
	 Train metrics/train/TokenAccuracy: 0.1686
	 Train throughput/batches_per_sec: 0.2201
	 Train throughput/samples_per_sec: 0.4402
	 Train throughput/device/batches_per_sec: 0.2201
	 Train throughput/device/samples_per_sec: 0.4402
	 Train throughput/tokens_per_sec: 450.7994
	 Train throughput/device/tokens_per_sec: 450.7994
	 Train time/train: 0.0353
	 Train time/val: 0.0000
	 Train time/total: 0.0353
	 Train lr-DecoupledAdamW/group0: 0.0000
	 Train time/remaining_estimate: 0.0268
	 Train metrics/shannon_entropy: 10.6360
	 Train metrics/batch_shannon_entropy: <wandb.sdk.data_types.table.Table object at 0x7097b028cfb0>
	 Train metrics/seq_shannon_entropy: CustomChart(table=<wandb.sdk.data_types.table.Table object at 0x7097e08c36e0>, spec=CustomChartSpec(spec_name='wandb/lineseries/v0', fields={'step': 'step', 'lineKey': 'lineKey', 'lineVal': 'lineVal'}, string_fields={'title': 'Per-Layer Shannon Entropy', 'xname': 'Block Index'}, key='', panel_type='Vega2', split_table=False))
	 Train metrics/exit_entropy: 0.6641
	 Train metrics/batch_exit_entropy: <wandb.sdk.data_types.table.Table object at 0x70984213f1a0>
	 Train metrics/seq_exit_entropy: CustomChart(table=<wandb.sdk.data_types.table.Table object at 0x7097e21078c0>, spec=CustomChartSpec(spec_name='wandb/lineseries/v0', fields={'step': 'step', 'lineKey': 'lineKey', 'lineVal': 'lineVal'}, string_fields={'title': 'Per-Layer Exit Entropy', 'xname': 'Block Index'}, key='', panel_type='Vega2', split_table=False))
	 Train expert_selection/ffn_layer: <wandb.sdk.data_types.image.Image object at 0x7097de8ea0c0>
	 Train expert_selection/attn_o_layer: <wandb.sdk.data_types.image.Image object at 0x7097de488c80>
	 Train expert_selection/attn_v_layer: <wandb.sdk.data_types.image.Image object at 0x7097e3884800>
	 Train l2_norm/moment/model.transformer.router: 0.0000
	 Train l2_norm/param/model.transformer.router: 0.3635
	 Train l2_norm/update/model.transformer.router: 0.0004
	 Train l2_norm/grad/model.transformer.router: 0.0000
	 Train l2_norm/moment/model.transformer.tau: 0.0000
	 Train l2_norm/param/model.transformer.tau: 1.0000
	 Train l2_norm/update/model.transformer.tau: 0.0000
	 Train l2_norm/grad/model.transformer.tau: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.attention.v: 0.0006
	 Train l2_norm/param/model.transformer.layers.0.attention.v: 20.4715
	 Train l2_norm/update/model.transformer.layers.0.attention.v: 0.0141
	 Train l2_norm/grad/model.transformer.layers.0.attention.v: 0.0007
	 Train l2_norm/moment/model.transformer.layers.0.attention.o: 0.0006
	 Train l2_norm/param/model.transformer.layers.0.attention.o: 22.6948
	 Train l2_norm/update/model.transformer.layers.0.attention.o: 0.0152
	 Train l2_norm/grad/model.transformer.layers.0.attention.o: 0.0008
	 Train l2_norm/moment/model.transformer.layers.0.attention.sel_v: 0.0001
	 Train l2_norm/param/model.transformer.layers.0.attention.sel_v: 2.2340
	 Train l2_norm/update/model.transformer.layers.0.attention.sel_v: 0.0009
	 Train l2_norm/grad/model.transformer.layers.0.attention.sel_v: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.attention.sel_o: 0.0001
	 Train l2_norm/param/model.transformer.layers.0.attention.sel_o: 2.2493
	 Train l2_norm/update/model.transformer.layers.0.attention.sel_o: 0.0010
	 Train l2_norm/grad/model.transformer.layers.0.attention.sel_o: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.attention.q.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attention.q.weight: 6.4673
	 Train l2_norm/update/model.transformer.layers.0.attention.q.weight: 0.0046
	 Train l2_norm/grad/model.transformer.layers.0.attention.q.weight: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.attention.k.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attention.k.weight: 6.4733
	 Train l2_norm/update/model.transformer.layers.0.attention.k.weight: 0.0046
	 Train l2_norm/grad/model.transformer.layers.0.attention.k.weight: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.ffn.keys: 0.0005
	 Train l2_norm/param/model.transformer.layers.0.ffn.keys: 14.9963
	 Train l2_norm/update/model.transformer.layers.0.ffn.keys: 0.0123
	 Train l2_norm/grad/model.transformer.layers.0.ffn.keys: 0.0006
	 Train l2_norm/moment/model.transformer.layers.0.ffn.values: 0.0011
	 Train l2_norm/param/model.transformer.layers.0.ffn.values: 7.1908
	 Train l2_norm/update/model.transformer.layers.0.ffn.values: 0.0128
	 Train l2_norm/grad/model.transformer.layers.0.ffn.values: 0.0014
	 Train l2_norm/moment/model.transformer.layers.0.ffn.expert_sel: 0.0001
	 Train l2_norm/param/model.transformer.layers.0.ffn.expert_sel: 4.7509
	 Train l2_norm/update/model.transformer.layers.0.ffn.expert_sel: 0.0040
	 Train l2_norm/grad/model.transformer.layers.0.ffn.expert_sel: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.attn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attn_pre.weight: 20.2977
	 Train l2_norm/update/model.transformer.layers.0.attn_pre.weight: 0.0003
	 Train l2_norm/grad/model.transformer.layers.0.attn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.attn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attn_post.weight: 20.2977
	 Train l2_norm/update/model.transformer.layers.0.attn_post.weight: 0.0003
	 Train l2_norm/grad/model.transformer.layers.0.attn_post.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.ffn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.ffn_pre.weight: 20.2977
	 Train l2_norm/update/model.transformer.layers.0.ffn_pre.weight: 0.0003
	 Train l2_norm/grad/model.transformer.layers.0.ffn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.ffn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.ffn_post.weight: 20.2981
	 Train l2_norm/update/model.transformer.layers.0.ffn_post.weight: 0.0003
	 Train l2_norm/grad/model.transformer.layers.0.ffn_post.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.v: 0.0002
	 Train l2_norm/param/model.transformer.layers.1.attention.v: 20.4825
	 Train l2_norm/update/model.transformer.layers.1.attention.v: 0.0107
	 Train l2_norm/grad/model.transformer.layers.1.attention.v: 0.0004
	 Train l2_norm/moment/model.transformer.layers.1.attention.o: 0.0002
	 Train l2_norm/param/model.transformer.layers.1.attention.o: 22.6889
	 Train l2_norm/update/model.transformer.layers.1.attention.o: 0.0105
	 Train l2_norm/grad/model.transformer.layers.1.attention.o: 0.0003
	 Train l2_norm/moment/model.transformer.layers.1.attention.sel_v: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.sel_v: 2.2359
	 Train l2_norm/update/model.transformer.layers.1.attention.sel_v: 0.0006
	 Train l2_norm/grad/model.transformer.layers.1.attention.sel_v: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.sel_o: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.sel_o: 2.2254
	 Train l2_norm/update/model.transformer.layers.1.attention.sel_o: 0.0007
	 Train l2_norm/grad/model.transformer.layers.1.attention.sel_o: 0.0001
	 Train l2_norm/moment/model.transformer.layers.1.attention.q.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.q.weight: 6.4888
	 Train l2_norm/update/model.transformer.layers.1.attention.q.weight: 0.0022
	 Train l2_norm/grad/model.transformer.layers.1.attention.q.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.k.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.k.weight: 6.4777
	 Train l2_norm/update/model.transformer.layers.1.attention.k.weight: 0.0022
	 Train l2_norm/grad/model.transformer.layers.1.attention.k.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn.keys: 0.0002
	 Train l2_norm/param/model.transformer.layers.1.ffn.keys: 15.0065
	 Train l2_norm/update/model.transformer.layers.1.ffn.keys: 0.0094
	 Train l2_norm/grad/model.transformer.layers.1.ffn.keys: 0.0003
	 Train l2_norm/moment/model.transformer.layers.1.ffn.values: 0.0005
	 Train l2_norm/param/model.transformer.layers.1.ffn.values: 7.1644
	 Train l2_norm/update/model.transformer.layers.1.ffn.values: 0.0110
	 Train l2_norm/grad/model.transformer.layers.1.ffn.values: 0.0007
	 Train l2_norm/moment/model.transformer.layers.1.ffn.expert_sel: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.ffn.expert_sel: 4.7382
	 Train l2_norm/update/model.transformer.layers.1.ffn.expert_sel: 0.0030
	 Train l2_norm/grad/model.transformer.layers.1.ffn.expert_sel: 0.0001
	 Train l2_norm/moment/model.transformer.layers.1.attn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attn_pre.weight: 20.2977
	 Train l2_norm/update/model.transformer.layers.1.attn_pre.weight: 0.0002
	 Train l2_norm/grad/model.transformer.layers.1.attn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attn_post.weight: 20.2973
	 Train l2_norm/update/model.transformer.layers.1.attn_post.weight: 0.0002
	 Train l2_norm/grad/model.transformer.layers.1.attn_post.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.ffn_pre.weight: 20.2977
	 Train l2_norm/update/model.transformer.layers.1.ffn_pre.weight: 0.0002
	 Train l2_norm/grad/model.transformer.layers.1.ffn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.ffn_post.weight: 20.2978
	 Train l2_norm/update/model.transformer.layers.1.ffn_post.weight: 0.0003
	 Train l2_norm/grad/model.transformer.layers.1.ffn_post.weight: 0.0000
	 Train l2_norm/moment/model.embedding.weight: 0.0002
	 Train l2_norm/param/model.embedding.weight: 221.7554
	 Train l2_norm/update/model.embedding.weight: 0.0087
	 Train l2_norm/grad/model.embedding.weight: 0.0003
	 Train l2_norm/moment/model.lm_head.weight: 0.0006
	 Train l2_norm/param/model.lm_head.weight: 127.9860
	 Train l2_norm/update/model.lm_head.weight: 0.0300
	 Train l2_norm/grad/model.lm_head.weight: 0.0008
	 Train l2_norm/moment/model.lm_head.bias: 0.0000
	 Train l2_norm/param/model.lm_head.bias: 6.2885
	 Train l2_norm/update/model.lm_head.bias: 0.0025
	 Train l2_norm/grad/model.lm_head.bias: 0.0000
	 Train l2_norm/moment/model.out_norm.weight: 0.0000
	 Train l2_norm/param/model.out_norm.weight: 20.2974
	 Train l2_norm/update/model.out_norm.weight: 0.0003
	 Train l2_norm/grad/model.out_norm.weight: 0.0000
	 Train l2_norm/grad/global: 0.0022
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([[[[0.4525, 0.5542, 0.7326,  ..., 0.3909, 0.5600, 0.4504],
          [0.4593, 0.5329, 0.5684,  ..., 0.5521, 0.5832, 0.5117],
          [0.4492, 0.4681, 0.4916,  ..., 0.4330, 0.4882, 0.6469],
          [0.4821, 0.6654, 0.4525,  ..., 0.6792, 0.5475, 0.6025]],

         [[0.5193, 0.5156, 0.5780,  ..., 0.4158, 0.4567, 0.3675],
          [0.4872, 0.6215, 0.5056,  ..., 0.4249, 0.5060, 0.4106],
          [0.5670, 0.5438, 0.5022,  ..., 0.5233, 0.7607, 0.4059],
          [0.6619, 0.5651, 0.3522,  ..., 0.3107, 0.5160, 0.6469]],

         [[0.4749, 0.6202, 0.7677,  ..., 0.4613, 0.6976, 0.4547],
          [0.3961, 0.4954, 0.5742,  ..., 0.5087, 0.5351, 0.4417],
          [0.4591, 0.5417, 0.5463,  ..., 0.4216, 0.6478, 0.5641],
          [0.5128, 0.7217, 0.4378,  ..., 0.4516, 0.4216, 0.5898]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0090, 0.0090, 0.0090, 0.0090, 0.0090, 0.0090, 0.0090, 0.0090, 0.0050,
        0.0090], device='cuda:0')
selected experts tensor([4099, 4197,  503,  214,  520,  659,  505,  301,  802,  488],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4820, 0.5059, 0.3808,  ..., 0.5639, 0.6486, 0.5144],
          [0.4642, 0.5859, 0.6757,  ..., 0.4882, 0.4167, 0.3963],
          [0.6031, 0.6205, 0.6303,  ..., 0.4993, 0.4182, 0.3768],
          [0.5693, 0.3992, 0.5682,  ..., 0.2942, 0.6387, 0.4471]],

         [[0.4181, 0.4243, 0.5306,  ..., 0.4810, 0.5382, 0.3749],
          [0.3296, 0.6033, 0.4627,  ..., 0.5940, 0.6414, 0.5122],
          [0.5229, 0.5949, 0.6294,  ..., 0.6369, 0.6332, 0.3972],
          [0.5229, 0.4765, 0.7566,  ..., 0.5059, 0.3860, 0.6060]],

         [[0.5011, 0.3955, 0.5725,  ..., 0.4262, 0.5300, 0.4870],
          [0.6714, 0.5240, 0.5501,  ..., 0.3280, 0.4893, 0.6120],
          [0.4355, 0.6323, 0.5201,  ..., 0.4362, 0.6021, 0.4171],
          [0.3799, 0.4946, 0.4758,  ..., 0.4835, 0.4712, 0.4828]],

         ...,

         [[0.4762, 0.5264, 0.4525,  ..., 0.4893, 0.4502, 0.4204],
          [0.4328, 0.4767, 0.5527,  ..., 0.5448, 0.6249, 0.6051],
          [0.4671, 0.5496, 0.4361,  ..., 0.5128, 0.4045, 0.5282],
          [0.5124, 0.4966, 0.6197,  ..., 0.5089, 0.5074, 0.5439]],

         [[0.4519, 0.5175, 0.4573,  ..., 0.4027, 0.5497, 0.4975],
          [0.4473, 0.5474, 0.6662,  ..., 0.5658, 0.5112, 0.4619],
          [0.4186, 0.5421, 0.4077,  ..., 0.4848, 0.5467, 0.5025],
          [0.4364, 0.5930, 0.3167,  ..., 0.5793, 0.4885, 0.4726]],

         [[0.5090, 0.5631, 0.5749,  ..., 0.5291, 0.3901, 0.4507],
          [0.5333, 0.4668, 0.4429,  ..., 0.4792, 0.6477, 0.2976],
          [0.5811, 0.5888, 0.6600,  ..., 0.6369, 0.5783, 0.5971],
          [0.5012, 0.4873, 0.3699,  ..., 0.5318, 0.4714, 0.4700]]],


        [[[0.4563, 0.5100, 0.4888,  ..., 0.4644, 0.3719, 0.4739],
          [0.5755, 0.5452, 0.5469,  ..., 0.3647, 0.5158, 0.5981],
          [0.4158, 0.4353, 0.5121,  ..., 0.5275, 0.6026, 0.5587],
          [0.4401, 0.6098, 0.4299,  ..., 0.5031, 0.5769, 0.6009]],

         [[0.4225, 0.4617, 0.4429,  ..., 0.4790, 0.3339, 0.4233],
          [0.4249, 0.5048, 0.5170,  ..., 0.6314, 0.4013, 0.5783],
          [0.4442, 0.6024, 0.5095,  ..., 0.4936, 0.5326, 0.4884],
          [0.5167, 0.4547, 0.5122,  ..., 0.3602, 0.4836, 0.4019]],

         [[0.5197, 0.5597, 0.5987,  ..., 0.4224, 0.4710, 0.5396],
          [0.5947, 0.5219, 0.4007,  ..., 0.4276, 0.6208, 0.4648],
          [0.5352, 0.4607, 0.6085,  ..., 0.4485, 0.4581, 0.4471],
          [0.4771, 0.5963, 0.4448,  ..., 0.4943, 0.4973, 0.5046]],

         ...,

         [[0.5396, 0.4851, 0.5176,  ..., 0.4877, 0.6432, 0.5591],
          [0.4935, 0.4505, 0.6165,  ..., 0.6082, 0.6795, 0.7224],
          [0.3456, 0.3875, 0.5996,  ..., 0.4458, 0.4797, 0.2862],
          [0.4105, 0.5416, 0.6020,  ..., 0.4741, 0.4167, 0.3846]],

         [[0.5499, 0.5595, 0.5527,  ..., 0.6664, 0.4801, 0.4957],
          [0.3287, 0.3843, 0.5086,  ..., 0.5831, 0.5273, 0.6903],
          [0.4430, 0.3396, 0.4552,  ..., 0.5472, 0.3901, 0.4047],
          [0.5047, 0.4143, 0.4482,  ..., 0.5059, 0.3331, 0.5816]],

         [[0.4948, 0.6112, 0.2922,  ..., 0.2693, 0.5061, 0.3420],
          [0.5905, 0.5944, 0.5660,  ..., 0.6189, 0.5988, 0.5835],
          [0.4720, 0.4939, 0.3960,  ..., 0.5296, 0.3683, 0.4582],
          [0.4690, 0.4462, 0.6852,  ..., 0.4338, 0.5276, 0.5413]]]],
       device='cuda:0')
tensor([[[[0.4940, 0.5079, 0.3748,  ..., 0.5559, 0.6406, 0.5184],
          [0.4762, 0.5879, 0.6697,  ..., 0.4802, 0.4087, 0.4003],
          [0.6151, 0.6225, 0.6243,  ..., 0.4913, 0.4102, 0.3808],
          [0.5813, 0.4012, 0.5622,  ..., 0.2862, 0.6307, 0.4511]],

         [[0.4301, 0.4263, 0.5246,  ..., 0.4730, 0.5302, 0.3789],
          [0.3416, 0.6053, 0.4567,  ..., 0.5860, 0.6334, 0.5162],
          [0.5349, 0.5969, 0.6234,  ..., 0.6289, 0.6252, 0.4012],
          [0.5349, 0.4785, 0.7506,  ..., 0.4979, 0.3780, 0.6100]],

         [[0.5131, 0.3975, 0.5665,  ..., 0.4182, 0.5220, 0.4910],
          [0.6834, 0.5260, 0.5441,  ..., 0.3200, 0.4813, 0.6160],
          [0.4475, 0.6343, 0.5141,  ..., 0.4282, 0.5941, 0.4211],
          [0.3919, 0.4966, 0.4698,  ..., 0.4755, 0.4632, 0.4868]],

         ...,

         [[0.4882, 0.5284, 0.4465,  ..., 0.4813, 0.4422, 0.4244],
          [0.4448, 0.4787, 0.5467,  ..., 0.5368, 0.6169, 0.6091],
          [0.4791, 0.5516, 0.4301,  ..., 0.5048, 0.3965, 0.5322],
          [0.5244, 0.4986, 0.6137,  ..., 0.5009, 0.4994, 0.5479]],

         [[0.4639, 0.5195, 0.4513,  ..., 0.3947, 0.5417, 0.5015],
          [0.4593, 0.5494, 0.6602,  ..., 0.5578, 0.5032, 0.4659],
          [0.4306, 0.5441, 0.4017,  ..., 0.4768, 0.5387, 0.5065],
          [0.4484, 0.5950, 0.3107,  ..., 0.5713, 0.4805, 0.4766]],

         [[0.5210, 0.5651, 0.5689,  ..., 0.5211, 0.3821, 0.4547],
          [0.5453, 0.4688, 0.4369,  ..., 0.4712, 0.6397, 0.3016],
          [0.5931, 0.5908, 0.6540,  ..., 0.6289, 0.5703, 0.6011],
          [0.5132, 0.4893, 0.3639,  ..., 0.5238, 0.4634, 0.4740]]],


        [[[0.4683, 0.5120, 0.4828,  ..., 0.4564, 0.3639, 0.4779],
          [0.5875, 0.5472, 0.5409,  ..., 0.3567, 0.5078, 0.6021],
          [0.4278, 0.4373, 0.5061,  ..., 0.5195, 0.5946, 0.5627],
          [0.4521, 0.6118, 0.4239,  ..., 0.4951, 0.5689, 0.6049]],

         [[0.4345, 0.4637, 0.4369,  ..., 0.4710, 0.3259, 0.4273],
          [0.4369, 0.5068, 0.5110,  ..., 0.6234, 0.3933, 0.5823],
          [0.4562, 0.6044, 0.5035,  ..., 0.4856, 0.5246, 0.4924],
          [0.5287, 0.4567, 0.5062,  ..., 0.3522, 0.4756, 0.4059]],

         [[0.5317, 0.5617, 0.5927,  ..., 0.4144, 0.4630, 0.5436],
          [0.6067, 0.5239, 0.3947,  ..., 0.4196, 0.6128, 0.4688],
          [0.5472, 0.4627, 0.6025,  ..., 0.4405, 0.4501, 0.4511],
          [0.4891, 0.5983, 0.4388,  ..., 0.4863, 0.4893, 0.5086]],

         ...,

         [[0.5516, 0.4871, 0.5116,  ..., 0.4797, 0.6352, 0.5631],
          [0.5055, 0.4525, 0.6105,  ..., 0.6002, 0.6715, 0.7264],
          [0.3576, 0.3895, 0.5936,  ..., 0.4378, 0.4717, 0.2902],
          [0.4225, 0.5436, 0.5960,  ..., 0.4661, 0.4087, 0.3886]],

         [[0.5619, 0.5615, 0.5467,  ..., 0.6584, 0.4721, 0.4997],
          [0.3407, 0.3863, 0.5026,  ..., 0.5751, 0.5193, 0.6943],
          [0.4550, 0.3416, 0.4492,  ..., 0.5392, 0.3821, 0.4087],
          [0.5167, 0.4163, 0.4422,  ..., 0.4979, 0.3251, 0.5856]],

         [[0.5068, 0.6132, 0.2862,  ..., 0.2613, 0.4981, 0.3460],
          [0.6025, 0.5964, 0.5600,  ..., 0.6109, 0.5908, 0.5875],
          [0.4840, 0.4959, 0.3900,  ..., 0.5216, 0.3603, 0.4622],
          [0.4810, 0.4482, 0.6792,  ..., 0.4258, 0.5196, 0.5453]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0120, -0.0020,  0.0060,  0.0040,  0.0060,  0.0040, -0.0040,  0.0080,
         0.0080, -0.0040], device='cuda:0')
selected experts tensor([1652, 1600, 1616, 1686, 1582, 1622, 1651, 1695, 1619, 1661],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4954, 0.5152, 0.4862,  ..., 0.3341, 0.6212, 0.4818],
          [0.5495, 0.5133, 0.4523,  ..., 0.4228, 0.3911, 0.4898],
          [0.4440, 0.5667, 0.4309,  ..., 0.4717, 0.6595, 0.5623],
          [0.4170, 0.5605, 0.5423,  ..., 0.5091, 0.5361, 0.4232]],

         [[0.4029, 0.4227, 0.6146,  ..., 0.4892, 0.5548, 0.4365],
          [0.5976, 0.6274, 0.4716,  ..., 0.5721, 0.6045, 0.4071],
          [0.4796, 0.6489, 0.4817,  ..., 0.5401, 0.5108, 0.5435],
          [0.5782, 0.5369, 0.4419,  ..., 0.5854, 0.6087, 0.6615]],

         [[0.5444, 0.6210, 0.6151,  ..., 0.6097, 0.4060, 0.4788],
          [0.3544, 0.5672, 0.4965,  ..., 0.4977, 0.4516, 0.5318],
          [0.4752, 0.5398, 0.5906,  ..., 0.4934, 0.4596, 0.4389],
          [0.6000, 0.4156, 0.5892,  ..., 0.4372, 0.4283, 0.3351]],

         ...,

         [[0.5085, 0.5052, 0.5590,  ..., 0.5398, 0.4531, 0.5824],
          [0.5590, 0.3243, 0.5763,  ..., 0.5797, 0.6674, 0.4691],
          [0.3940, 0.5083, 0.5303,  ..., 0.5976, 0.6657, 0.4840],
          [0.4137, 0.6054, 0.6591,  ..., 0.6004, 0.5398, 0.6562]],

         [[0.4270, 0.4490, 0.5180,  ..., 0.4786, 0.4466, 0.4351],
          [0.4784, 0.4323, 0.5825,  ..., 0.4787, 0.7584, 0.5896],
          [0.3861, 0.5296, 0.4663,  ..., 0.5381, 0.5347, 0.4222],
          [0.5580, 0.4208, 0.3672,  ..., 0.6092, 0.5260, 0.5206]],

         [[0.5160, 0.5376, 0.6057,  ..., 0.5512, 0.5579, 0.5191],
          [0.4481, 0.3624, 0.4656,  ..., 0.4423, 0.5325, 0.3760],
          [0.4033, 0.5417, 0.4527,  ..., 0.4314, 0.3357, 0.6338],
          [0.4577, 0.5224, 0.5464,  ..., 0.3758, 0.4389, 0.4922]]],


        [[[0.5616, 0.6766, 0.4214,  ..., 0.5990, 0.4980, 0.5154],
          [0.4881, 0.6382, 0.3845,  ..., 0.6240, 0.5487, 0.5488],
          [0.5021, 0.3507, 0.6206,  ..., 0.4828, 0.6290, 0.3197],
          [0.4365, 0.2956, 0.4898,  ..., 0.4846, 0.5378, 0.5645]],

         [[0.6186, 0.5393, 0.4590,  ..., 0.4190, 0.4930, 0.5719],
          [0.5043, 0.5581, 0.5043,  ..., 0.5313, 0.5533, 0.5695],
          [0.5628, 0.3444, 0.4911,  ..., 0.4939, 0.5809, 0.4872],
          [0.3189, 0.5867, 0.6183,  ..., 0.6675, 0.6871, 0.4760]],

         [[0.5772, 0.4616, 0.4843,  ..., 0.5887, 0.4603, 0.4337],
          [0.4788, 0.4562, 0.5113,  ..., 0.5294, 0.4679, 0.5099],
          [0.5662, 0.5729, 0.5325,  ..., 0.4493, 0.4145, 0.5105],
          [0.5490, 0.4304, 0.4133,  ..., 0.5735, 0.5297, 0.4751]],

         ...,

         [[0.5210, 0.5653, 0.4658,  ..., 0.4912, 0.4497, 0.6392],
          [0.4931, 0.4977, 0.4690,  ..., 0.4907, 0.5074, 0.5205],
          [0.6200, 0.3808, 0.5996,  ..., 0.4777, 0.4951, 0.4298],
          [0.4790, 0.5677, 0.4992,  ..., 0.5420, 0.3383, 0.5976]],

         [[0.4845, 0.5289, 0.4876,  ..., 0.5173, 0.4541, 0.4227],
          [0.5271, 0.4884, 0.5493,  ..., 0.5170, 0.3828, 0.4858],
          [0.5782, 0.5667, 0.4185,  ..., 0.4927, 0.3220, 0.5207],
          [0.4318, 0.4013, 0.4105,  ..., 0.5102, 0.6498, 0.6356]],

         [[0.4570, 0.4831, 0.3251,  ..., 0.4343, 0.5951, 0.4558],
          [0.6950, 0.5753, 0.5749,  ..., 0.3572, 0.6087, 0.3386],
          [0.5705, 0.6247, 0.6243,  ..., 0.4624, 0.4688, 0.5251],
          [0.3257, 0.5048, 0.4884,  ..., 0.4587, 0.4283, 0.5473]]]],
       device='cuda:0')
tensor([[[[0.4914, 0.5212, 0.4802,  ..., 0.3381, 0.6192, 0.4778],
          [0.5455, 0.5193, 0.4463,  ..., 0.4268, 0.3891, 0.4858],
          [0.4400, 0.5727, 0.4249,  ..., 0.4757, 0.6575, 0.5583],
          [0.4130, 0.5665, 0.5363,  ..., 0.5131, 0.5341, 0.4192]],

         [[0.3989, 0.4287, 0.6086,  ..., 0.4932, 0.5528, 0.4325],
          [0.5936, 0.6334, 0.4656,  ..., 0.5761, 0.6025, 0.4031],
          [0.4756, 0.6549, 0.4757,  ..., 0.5441, 0.5088, 0.5395],
          [0.5742, 0.5429, 0.4359,  ..., 0.5894, 0.6067, 0.6575]],

         [[0.5404, 0.6270, 0.6091,  ..., 0.6137, 0.4040, 0.4748],
          [0.3504, 0.5732, 0.4905,  ..., 0.5017, 0.4496, 0.5278],
          [0.4712, 0.5458, 0.5846,  ..., 0.4974, 0.4576, 0.4349],
          [0.5960, 0.4216, 0.5832,  ..., 0.4412, 0.4263, 0.3311]],

         ...,

         [[0.5045, 0.5112, 0.5530,  ..., 0.5438, 0.4511, 0.5784],
          [0.5550, 0.3303, 0.5703,  ..., 0.5837, 0.6654, 0.4651],
          [0.3900, 0.5143, 0.5243,  ..., 0.6016, 0.6637, 0.4800],
          [0.4097, 0.6114, 0.6531,  ..., 0.6044, 0.5378, 0.6522]],

         [[0.4230, 0.4550, 0.5120,  ..., 0.4826, 0.4446, 0.4311],
          [0.4744, 0.4383, 0.5765,  ..., 0.4827, 0.7564, 0.5856],
          [0.3821, 0.5356, 0.4603,  ..., 0.5421, 0.5327, 0.4182],
          [0.5540, 0.4268, 0.3612,  ..., 0.6132, 0.5240, 0.5166]],

         [[0.5120, 0.5436, 0.5997,  ..., 0.5552, 0.5559, 0.5151],
          [0.4441, 0.3684, 0.4596,  ..., 0.4463, 0.5305, 0.3720],
          [0.3993, 0.5477, 0.4467,  ..., 0.4354, 0.3337, 0.6298],
          [0.4537, 0.5284, 0.5404,  ..., 0.3798, 0.4369, 0.4882]]],


        [[[0.5576, 0.6826, 0.4154,  ..., 0.6030, 0.4960, 0.5114],
          [0.4841, 0.6442, 0.3785,  ..., 0.6280, 0.5467, 0.5448],
          [0.4981, 0.3567, 0.6146,  ..., 0.4868, 0.6270, 0.3157],
          [0.4325, 0.3016, 0.4838,  ..., 0.4886, 0.5358, 0.5605]],

         [[0.6146, 0.5453, 0.4530,  ..., 0.4230, 0.4910, 0.5679],
          [0.5003, 0.5641, 0.4983,  ..., 0.5353, 0.5513, 0.5655],
          [0.5588, 0.3504, 0.4851,  ..., 0.4979, 0.5789, 0.4832],
          [0.3149, 0.5927, 0.6123,  ..., 0.6715, 0.6851, 0.4720]],

         [[0.5732, 0.4676, 0.4783,  ..., 0.5927, 0.4583, 0.4297],
          [0.4748, 0.4622, 0.5053,  ..., 0.5334, 0.4659, 0.5059],
          [0.5622, 0.5789, 0.5265,  ..., 0.4533, 0.4125, 0.5065],
          [0.5450, 0.4364, 0.4073,  ..., 0.5775, 0.5277, 0.4711]],

         ...,

         [[0.5170, 0.5713, 0.4598,  ..., 0.4952, 0.4477, 0.6352],
          [0.4891, 0.5037, 0.4630,  ..., 0.4947, 0.5054, 0.5165],
          [0.6160, 0.3868, 0.5936,  ..., 0.4817, 0.4931, 0.4258],
          [0.4750, 0.5737, 0.4932,  ..., 0.5460, 0.3363, 0.5936]],

         [[0.4805, 0.5349, 0.4816,  ..., 0.5213, 0.4521, 0.4187],
          [0.5231, 0.4944, 0.5433,  ..., 0.5210, 0.3808, 0.4818],
          [0.5742, 0.5727, 0.4125,  ..., 0.4967, 0.3200, 0.5167],
          [0.4278, 0.4073, 0.4045,  ..., 0.5142, 0.6478, 0.6316]],

         [[0.4530, 0.4891, 0.3191,  ..., 0.4383, 0.5931, 0.4518],
          [0.6910, 0.5813, 0.5689,  ..., 0.3612, 0.6067, 0.3346],
          [0.5665, 0.6307, 0.6183,  ..., 0.4664, 0.4668, 0.5211],
          [0.3217, 0.5108, 0.4824,  ..., 0.4627, 0.4263, 0.5433]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0040, -0.0060,  0.0060, -0.0020,  0.0020,  0.0060, -0.0060, -0.0040,
         0.0020,  0.0040], device='cuda:0')
selected experts tensor([1651, 1647, 1824, 1566, 1511, 1806, 1587, 1575, 1559, 1658],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4866, 0.5106, 0.3705,  ..., 0.5032, 0.4885, 0.4352],
          [0.5166, 0.5272, 0.4603,  ..., 0.5045, 0.4696, 0.4917],
          [0.4553, 0.6142, 0.5671,  ..., 0.6201, 0.5178, 0.5304],
          [0.5000, 0.3725, 0.3223,  ..., 0.4745, 0.5129, 0.5246]],

         [[0.4718, 0.4955, 0.4508,  ..., 0.5007, 0.3468, 0.4930],
          [0.5284, 0.5528, 0.4731,  ..., 0.4108, 0.4306, 0.5132],
          [0.5423, 0.5039, 0.4942,  ..., 0.4905, 0.4133, 0.5217],
          [0.5801, 0.5132, 0.4747,  ..., 0.5261, 0.5232, 0.5187]],

         [[0.5284, 0.6283, 0.5666,  ..., 0.4409, 0.4578, 0.5853],
          [0.5691, 0.5526, 0.5286,  ..., 0.4906, 0.3224, 0.4140],
          [0.4175, 0.4347, 0.5310,  ..., 0.3726, 0.5994, 0.3859],
          [0.4908, 0.3706, 0.4069,  ..., 0.6824, 0.4957, 0.5371]],

         ...,

         [[0.6009, 0.5162, 0.6530,  ..., 0.4700, 0.4588, 0.4437],
          [0.4736, 0.6310, 0.4721,  ..., 0.4952, 0.5266, 0.4101],
          [0.5638, 0.5155, 0.4760,  ..., 0.5186, 0.4985, 0.4308],
          [0.5104, 0.2786, 0.5931,  ..., 0.5907, 0.7043, 0.4639]],

         [[0.5633, 0.4117, 0.6442,  ..., 0.5079, 0.4521, 0.5395],
          [0.4761, 0.5800, 0.3677,  ..., 0.5562, 0.4430, 0.5217],
          [0.5101, 0.4098, 0.4878,  ..., 0.4907, 0.6211, 0.4154],
          [0.6374, 0.3251, 0.4935,  ..., 0.5336, 0.5294, 0.5613]],

         [[0.4992, 0.4170, 0.5628,  ..., 0.4278, 0.3383, 0.4992],
          [0.4996, 0.6301, 0.5329,  ..., 0.5724, 0.5216, 0.3621],
          [0.5758, 0.5567, 0.7276,  ..., 0.5099, 0.5484, 0.4837],
          [0.5981, 0.3165, 0.4520,  ..., 0.4960, 0.4377, 0.6393]]],


        [[[0.5522, 0.4340, 0.4525,  ..., 0.5010, 0.4156, 0.4694],
          [0.5386, 0.4584, 0.4045,  ..., 0.4689, 0.3776, 0.4121],
          [0.4236, 0.5497, 0.5443,  ..., 0.5114, 0.5032, 0.4574],
          [0.5729, 0.4570, 0.3899,  ..., 0.6485, 0.5889, 0.6116]],

         [[0.4660, 0.5002, 0.6076,  ..., 0.4672, 0.4525, 0.4766],
          [0.5379, 0.5005, 0.5185,  ..., 0.3876, 0.5152, 0.6331],
          [0.3164, 0.5763, 0.5719,  ..., 0.5409, 0.4184, 0.4413],
          [0.4952, 0.3688, 0.4586,  ..., 0.5090, 0.5377, 0.4936]],

         [[0.6320, 0.5540, 0.2564,  ..., 0.4460, 0.2844, 0.3817],
          [0.5626, 0.3382, 0.5121,  ..., 0.3318, 0.4316, 0.6471],
          [0.3949, 0.5043, 0.5049,  ..., 0.4233, 0.4283, 0.4282],
          [0.4604, 0.4251, 0.4767,  ..., 0.6291, 0.5650, 0.6784]],

         ...,

         [[0.5437, 0.4761, 0.4520,  ..., 0.5729, 0.4950, 0.4805],
          [0.4298, 0.6808, 0.5155,  ..., 0.4950, 0.2983, 0.4049],
          [0.5024, 0.4944, 0.5320,  ..., 0.5865, 0.4559, 0.4178],
          [0.6598, 0.4817, 0.4496,  ..., 0.5615, 0.5561, 0.4587]],

         [[0.3834, 0.4804, 0.4112,  ..., 0.3638, 0.5631, 0.4747],
          [0.4892, 0.4309, 0.6308,  ..., 0.4322, 0.4740, 0.3887],
          [0.4998, 0.5078, 0.5001,  ..., 0.4751, 0.5444, 0.4325],
          [0.4284, 0.4051, 0.4791,  ..., 0.5814, 0.4798, 0.5149]],

         [[0.4565, 0.3957, 0.5681,  ..., 0.5157, 0.5298, 0.4818],
          [0.4830, 0.5434, 0.6145,  ..., 0.3951, 0.4516, 0.5895],
          [0.3898, 0.5571, 0.6163,  ..., 0.4836, 0.5476, 0.5277],
          [0.4534, 0.4668, 0.5230,  ..., 0.6138, 0.4803, 0.3887]]]],
       device='cuda:0')
tensor([[[[0.4826, 0.5166, 0.3785,  ..., 0.5192, 0.4685, 0.4552],
          [0.5126, 0.5332, 0.4683,  ..., 0.5205, 0.4496, 0.5117],
          [0.4513, 0.6202, 0.5751,  ..., 0.6361, 0.4978, 0.5504],
          [0.4960, 0.3785, 0.3303,  ..., 0.4905, 0.4929, 0.5446]],

         [[0.4678, 0.5015, 0.4588,  ..., 0.5167, 0.3268, 0.5130],
          [0.5244, 0.5588, 0.4811,  ..., 0.4268, 0.4106, 0.5332],
          [0.5383, 0.5099, 0.5022,  ..., 0.5065, 0.3933, 0.5417],
          [0.5761, 0.5192, 0.4827,  ..., 0.5421, 0.5032, 0.5387]],

         [[0.5244, 0.6343, 0.5746,  ..., 0.4569, 0.4378, 0.6053],
          [0.5651, 0.5586, 0.5366,  ..., 0.5066, 0.3024, 0.4340],
          [0.4135, 0.4407, 0.5390,  ..., 0.3886, 0.5794, 0.4059],
          [0.4868, 0.3766, 0.4149,  ..., 0.6984, 0.4757, 0.5571]],

         ...,

         [[0.5969, 0.5222, 0.6610,  ..., 0.4860, 0.4388, 0.4637],
          [0.4696, 0.6370, 0.4801,  ..., 0.5112, 0.5066, 0.4301],
          [0.5598, 0.5215, 0.4840,  ..., 0.5346, 0.4785, 0.4508],
          [0.5064, 0.2846, 0.6011,  ..., 0.6067, 0.6843, 0.4839]],

         [[0.5593, 0.4177, 0.6522,  ..., 0.5239, 0.4321, 0.5595],
          [0.4721, 0.5860, 0.3757,  ..., 0.5722, 0.4230, 0.5417],
          [0.5061, 0.4158, 0.4958,  ..., 0.5067, 0.6011, 0.4354],
          [0.6334, 0.3311, 0.5015,  ..., 0.5496, 0.5094, 0.5813]],

         [[0.4952, 0.4230, 0.5708,  ..., 0.4438, 0.3183, 0.5192],
          [0.4956, 0.6361, 0.5409,  ..., 0.5884, 0.5016, 0.3821],
          [0.5718, 0.5627, 0.7356,  ..., 0.5259, 0.5284, 0.5037],
          [0.5941, 0.3225, 0.4600,  ..., 0.5120, 0.4177, 0.6593]]],


        [[[0.5482, 0.4400, 0.4605,  ..., 0.5170, 0.3956, 0.4894],
          [0.5346, 0.4644, 0.4125,  ..., 0.4849, 0.3576, 0.4321],
          [0.4196, 0.5557, 0.5523,  ..., 0.5274, 0.4832, 0.4774],
          [0.5689, 0.4630, 0.3979,  ..., 0.6645, 0.5689, 0.6316]],

         [[0.4620, 0.5062, 0.6156,  ..., 0.4832, 0.4325, 0.4966],
          [0.5339, 0.5065, 0.5265,  ..., 0.4036, 0.4952, 0.6531],
          [0.3124, 0.5823, 0.5799,  ..., 0.5569, 0.3984, 0.4613],
          [0.4912, 0.3748, 0.4666,  ..., 0.5250, 0.5177, 0.5136]],

         [[0.6280, 0.5600, 0.2644,  ..., 0.4620, 0.2644, 0.4017],
          [0.5586, 0.3442, 0.5201,  ..., 0.3478, 0.4116, 0.6671],
          [0.3909, 0.5103, 0.5129,  ..., 0.4393, 0.4083, 0.4482],
          [0.4564, 0.4311, 0.4847,  ..., 0.6451, 0.5450, 0.6984]],

         ...,

         [[0.5397, 0.4821, 0.4600,  ..., 0.5889, 0.4750, 0.5005],
          [0.4258, 0.6868, 0.5235,  ..., 0.5110, 0.2783, 0.4249],
          [0.4984, 0.5004, 0.5400,  ..., 0.6025, 0.4359, 0.4378],
          [0.6558, 0.4877, 0.4576,  ..., 0.5775, 0.5361, 0.4787]],

         [[0.3794, 0.4864, 0.4192,  ..., 0.3798, 0.5431, 0.4947],
          [0.4852, 0.4369, 0.6388,  ..., 0.4482, 0.4540, 0.4087],
          [0.4958, 0.5138, 0.5081,  ..., 0.4911, 0.5244, 0.4525],
          [0.4244, 0.4111, 0.4871,  ..., 0.5974, 0.4598, 0.5349]],

         [[0.4525, 0.4017, 0.5761,  ..., 0.5317, 0.5098, 0.5018],
          [0.4790, 0.5494, 0.6225,  ..., 0.4111, 0.4316, 0.6095],
          [0.3858, 0.5631, 0.6243,  ..., 0.4996, 0.5276, 0.5477],
          [0.4494, 0.4728, 0.5310,  ..., 0.6298, 0.4603, 0.4087]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0040, -0.0060, -0.0080,  0.0060, -0.0020, -0.0100,  0.0040, -0.0160,
         0.0200, -0.0200], device='cuda:0')
selected experts tensor([1703, 1526, 1426, 1797, 1676, 1599, 1551, 2001,  934, 2171],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100],
          [0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100],
          [0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100],
          [0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100]],

         [[0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100],
          [0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100],
          [0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100],
          [0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100]],

         [[0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100],
          [0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100],
          [0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100],
          [0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100]],

         ...,

         [[0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100],
          [0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100],
          [0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100],
          [0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100]],

         [[0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100],
          [0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100],
          [0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100],
          [0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100]],

         [[0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100],
          [0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100],
          [0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100],
          [0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100]]],


        [[[0.6208, 0.5328, 0.6605,  ..., 0.4773, 0.5060, 0.4662],
          [0.5029, 0.4995, 0.5036,  ..., 0.4900, 0.4828, 0.3481],
          [0.4710, 0.4324, 0.4671,  ..., 0.4555, 0.6911, 0.4131],
          [0.7178, 0.4588, 0.3766,  ..., 0.5466, 0.3690, 0.5770]],

         [[0.5538, 0.4683, 0.7262,  ..., 0.4296, 0.6582, 0.4263],
          [0.5279, 0.5215, 0.3507,  ..., 0.5415, 0.5370, 0.4710],
          [0.3237, 0.5049, 0.4221,  ..., 0.4512, 0.5176, 0.4897],
          [0.6350, 0.5555, 0.5305,  ..., 0.4596, 0.4668, 0.4889]],

         [[0.5387, 0.4793, 0.6407,  ..., 0.4974, 0.6243, 0.5380],
          [0.5421, 0.5581, 0.5164,  ..., 0.5396, 0.5973, 0.5946],
          [0.4907, 0.5022, 0.4235,  ..., 0.4473, 0.6961, 0.6144],
          [0.5254, 0.5936, 0.4183,  ..., 0.5635, 0.4467, 0.5441]],

         ...,

         [[0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100],
          [0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100],
          [0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100],
          [0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100]],

         [[0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100],
          [0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100],
          [0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100],
          [0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100]],

         [[0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100],
          [0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100],
          [0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100],
          [0.5080, 0.5080, 0.5100,  ..., 0.5100, 0.5060, 0.5100]]]],
       device='cuda:0')
tensor([[[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.6128, 0.5248, 0.6505,  ..., 0.4673, 0.5000, 0.4562],
          [0.4949, 0.4915, 0.4936,  ..., 0.4800, 0.4768, 0.3381],
          [0.4630, 0.4244, 0.4571,  ..., 0.4455, 0.6851, 0.4031],
          [0.7098, 0.4508, 0.3666,  ..., 0.5366, 0.3630, 0.5670]],

         [[0.5458, 0.4603, 0.7162,  ..., 0.4196, 0.6522, 0.4163],
          [0.5199, 0.5135, 0.3407,  ..., 0.5315, 0.5310, 0.4610],
          [0.3157, 0.4969, 0.4121,  ..., 0.4412, 0.5116, 0.4797],
          [0.6270, 0.5475, 0.5205,  ..., 0.4496, 0.4608, 0.4789]],

         [[0.5307, 0.4713, 0.6307,  ..., 0.4874, 0.6183, 0.5280],
          [0.5341, 0.5501, 0.5064,  ..., 0.5296, 0.5913, 0.5846],
          [0.4827, 0.4942, 0.4135,  ..., 0.4373, 0.6901, 0.6044],
          [0.5174, 0.5856, 0.4083,  ..., 0.5535, 0.4407, 0.5341]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([0.0080, 0.0080, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0060,
        0.0100], device='cuda:0')
selected experts tensor([ 506,  585, 3970, 3648,  703,  690,  731,  273,  800,  382],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5093, 0.5156, 0.5467,  ..., 0.6039, 0.4948, 0.5284],
          [0.4833, 0.4225, 0.5902,  ..., 0.5037, 0.6199, 0.5877],
          [0.5058, 0.4239, 0.5949,  ..., 0.5859, 0.5056, 0.3799],
          [0.4980, 0.4196, 0.5023,  ..., 0.4769, 0.3774, 0.4432]],

         [[0.4239, 0.6799, 0.6267,  ..., 0.4990, 0.5282, 0.4495],
          [0.4333, 0.5515, 0.4504,  ..., 0.5394, 0.5409, 0.5040],
          [0.4990, 0.5808, 0.4870,  ..., 0.5236, 0.5536, 0.6110],
          [0.4635, 0.5722, 0.5453,  ..., 0.5137, 0.4861, 0.5773]],

         [[0.5140, 0.4111, 0.6601,  ..., 0.6193, 0.4562, 0.6885],
          [0.6976, 0.4829, 0.4797,  ..., 0.5243, 0.4901, 0.5013],
          [0.5616, 0.5229, 0.4777,  ..., 0.4945, 0.4683, 0.4541],
          [0.6428, 0.5103, 0.5072,  ..., 0.4700, 0.3711, 0.4478]],

         ...,

         [[0.3995, 0.4177, 0.5912,  ..., 0.6304, 0.4377, 0.4688],
          [0.5064, 0.4486, 0.3827,  ..., 0.5474, 0.6559, 0.4635],
          [0.4109, 0.5940, 0.5632,  ..., 0.6741, 0.4732, 0.3526],
          [0.6392, 0.5017, 0.4850,  ..., 0.5773, 0.4852, 0.5316]],

         [[0.5124, 0.5540, 0.3938,  ..., 0.4395, 0.4478, 0.3781],
          [0.6222, 0.6790, 0.4944,  ..., 0.4815, 0.5965, 0.4672],
          [0.3821, 0.4537, 0.6048,  ..., 0.6216, 0.4348, 0.5234],
          [0.4860, 0.4390, 0.5364,  ..., 0.5251, 0.5128, 0.5495]],

         [[0.4086, 0.7293, 0.5508,  ..., 0.5176, 0.4942, 0.4271],
          [0.5182, 0.5224, 0.4622,  ..., 0.5706, 0.5779, 0.5427],
          [0.5049, 0.5593, 0.3827,  ..., 0.3855, 0.5827, 0.4750],
          [0.5146, 0.4465, 0.4554,  ..., 0.4362, 0.5197, 0.5422]]],


        [[[0.3682, 0.5269, 0.5343,  ..., 0.3933, 0.6007, 0.4175],
          [0.3934, 0.6104, 0.4542,  ..., 0.5874, 0.4727, 0.6868],
          [0.5303, 0.3965, 0.4769,  ..., 0.5372, 0.2688, 0.3906],
          [0.4823, 0.4791, 0.5343,  ..., 0.5419, 0.5088, 0.5644]],

         [[0.5386, 0.5009, 0.3530,  ..., 0.5270, 0.5642, 0.4531],
          [0.4954, 0.3665, 0.4124,  ..., 0.6440, 0.3506, 0.4175],
          [0.3682, 0.4620, 0.3718,  ..., 0.5578, 0.5033, 0.4052],
          [0.5569, 0.4671, 0.6304,  ..., 0.5095, 0.5261, 0.6455]],

         [[0.7150, 0.4206, 0.5540,  ..., 0.5930, 0.5126, 0.4009],
          [0.4849, 0.5281, 0.4191,  ..., 0.5874, 0.5127, 0.4504],
          [0.4512, 0.3692, 0.5864,  ..., 0.5338, 0.4401, 0.4904],
          [0.3896, 0.6141, 0.3924,  ..., 0.4395, 0.5745, 0.3962]],

         ...,

         [[0.5463, 0.3397, 0.6281,  ..., 0.5104, 0.4579, 0.4357],
          [0.4434, 0.5513, 0.5194,  ..., 0.4813, 0.4173, 0.6064],
          [0.4979, 0.5409, 0.6359,  ..., 0.5355, 0.5451, 0.5792],
          [0.5933, 0.5576, 0.4167,  ..., 0.4860, 0.5565, 0.4639]],

         [[0.6454, 0.6159, 0.4077,  ..., 0.2916, 0.5765, 0.5572],
          [0.5357, 0.4661, 0.5094,  ..., 0.5428, 0.4812, 0.4090],
          [0.5114, 0.5139, 0.5247,  ..., 0.4157, 0.4756, 0.4691],
          [0.3981, 0.5433, 0.4338,  ..., 0.5513, 0.4894, 0.5463]],

         [[0.6303, 0.5860, 0.3868,  ..., 0.5603, 0.5625, 0.3892],
          [0.6330, 0.4457, 0.6011,  ..., 0.4695, 0.4891, 0.3419],
          [0.4038, 0.5779, 0.5120,  ..., 0.6440, 0.5860, 0.4712],
          [0.4229, 0.5314, 0.4607,  ..., 0.4830, 0.4382, 0.3859]]]],
       device='cuda:0')
tensor([[[[0.5223, 0.5166, 0.5397,  ..., 0.5969, 0.4858, 0.5334],
          [0.4963, 0.4235, 0.5832,  ..., 0.4967, 0.6109, 0.5927],
          [0.5188, 0.4249, 0.5879,  ..., 0.5789, 0.4966, 0.3849],
          [0.5110, 0.4206, 0.4953,  ..., 0.4699, 0.3684, 0.4482]],

         [[0.4369, 0.6809, 0.6197,  ..., 0.4920, 0.5192, 0.4545],
          [0.4463, 0.5525, 0.4434,  ..., 0.5324, 0.5319, 0.5090],
          [0.5120, 0.5818, 0.4800,  ..., 0.5166, 0.5446, 0.6160],
          [0.4765, 0.5732, 0.5383,  ..., 0.5067, 0.4771, 0.5823]],

         [[0.5270, 0.4121, 0.6531,  ..., 0.6123, 0.4472, 0.6935],
          [0.7106, 0.4839, 0.4727,  ..., 0.5173, 0.4811, 0.5063],
          [0.5746, 0.5239, 0.4707,  ..., 0.4875, 0.4593, 0.4591],
          [0.6558, 0.5113, 0.5002,  ..., 0.4630, 0.3621, 0.4528]],

         ...,

         [[0.4125, 0.4187, 0.5842,  ..., 0.6234, 0.4287, 0.4738],
          [0.5194, 0.4496, 0.3757,  ..., 0.5404, 0.6469, 0.4685],
          [0.4239, 0.5950, 0.5562,  ..., 0.6671, 0.4642, 0.3576],
          [0.6522, 0.5027, 0.4780,  ..., 0.5703, 0.4762, 0.5366]],

         [[0.5254, 0.5550, 0.3868,  ..., 0.4325, 0.4388, 0.3831],
          [0.6352, 0.6800, 0.4874,  ..., 0.4745, 0.5875, 0.4722],
          [0.3951, 0.4547, 0.5978,  ..., 0.6146, 0.4258, 0.5284],
          [0.4990, 0.4400, 0.5294,  ..., 0.5181, 0.5038, 0.5545]],

         [[0.4216, 0.7303, 0.5438,  ..., 0.5106, 0.4852, 0.4321],
          [0.5312, 0.5234, 0.4552,  ..., 0.5636, 0.5689, 0.5477],
          [0.5179, 0.5603, 0.3757,  ..., 0.3785, 0.5737, 0.4800],
          [0.5276, 0.4475, 0.4484,  ..., 0.4292, 0.5107, 0.5472]]],


        [[[0.3812, 0.5279, 0.5273,  ..., 0.3863, 0.5917, 0.4225],
          [0.4064, 0.6114, 0.4472,  ..., 0.5804, 0.4637, 0.6918],
          [0.5433, 0.3975, 0.4699,  ..., 0.5302, 0.2598, 0.3956],
          [0.4953, 0.4801, 0.5273,  ..., 0.5349, 0.4998, 0.5694]],

         [[0.5516, 0.5019, 0.3460,  ..., 0.5200, 0.5552, 0.4581],
          [0.5084, 0.3675, 0.4054,  ..., 0.6370, 0.3416, 0.4225],
          [0.3812, 0.4630, 0.3648,  ..., 0.5508, 0.4943, 0.4102],
          [0.5699, 0.4681, 0.6234,  ..., 0.5025, 0.5171, 0.6505]],

         [[0.7280, 0.4216, 0.5470,  ..., 0.5860, 0.5036, 0.4059],
          [0.4979, 0.5291, 0.4121,  ..., 0.5804, 0.5037, 0.4554],
          [0.4642, 0.3702, 0.5794,  ..., 0.5268, 0.4311, 0.4954],
          [0.4026, 0.6151, 0.3854,  ..., 0.4325, 0.5655, 0.4012]],

         ...,

         [[0.5593, 0.3407, 0.6211,  ..., 0.5034, 0.4489, 0.4407],
          [0.4564, 0.5523, 0.5124,  ..., 0.4743, 0.4083, 0.6114],
          [0.5109, 0.5419, 0.6289,  ..., 0.5285, 0.5361, 0.5842],
          [0.6063, 0.5586, 0.4097,  ..., 0.4790, 0.5475, 0.4689]],

         [[0.6584, 0.6169, 0.4007,  ..., 0.2846, 0.5675, 0.5622],
          [0.5487, 0.4671, 0.5024,  ..., 0.5358, 0.4722, 0.4140],
          [0.5244, 0.5149, 0.5177,  ..., 0.4087, 0.4666, 0.4741],
          [0.4111, 0.5443, 0.4268,  ..., 0.5443, 0.4804, 0.5513]],

         [[0.6433, 0.5870, 0.3798,  ..., 0.5533, 0.5535, 0.3942],
          [0.6460, 0.4467, 0.5941,  ..., 0.4625, 0.4801, 0.3469],
          [0.4168, 0.5789, 0.5050,  ..., 0.6370, 0.5770, 0.4762],
          [0.4359, 0.5324, 0.4537,  ..., 0.4760, 0.4292, 0.3909]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0130, -0.0010,  0.0070,  0.0030,  0.0070,  0.0050, -0.0050,  0.0070,
         0.0090, -0.0050], device='cuda:0')
selected experts tensor([1657, 1765, 1595, 1675, 1658, 1554, 1636, 1673, 1538, 1633],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5276, 0.4624, 0.3941,  ..., 0.4286, 0.5900, 0.5795],
          [0.3696, 0.4103, 0.5052,  ..., 0.3870, 0.6391, 0.4735],
          [0.6088, 0.4653, 0.5179,  ..., 0.5141, 0.5169, 0.5119],
          [0.3916, 0.4279, 0.5561,  ..., 0.5345, 0.4928, 0.5200]],

         [[0.5121, 0.6435, 0.4395,  ..., 0.4675, 0.4548, 0.6649],
          [0.5434, 0.4289, 0.3853,  ..., 0.4694, 0.6965, 0.3796],
          [0.4246, 0.4315, 0.3396,  ..., 0.4315, 0.4890, 0.5514],
          [0.5105, 0.5031, 0.4999,  ..., 0.3187, 0.6116, 0.5207]],

         [[0.6135, 0.5438, 0.4001,  ..., 0.4563, 0.6195, 0.5546],
          [0.6632, 0.3989, 0.4261,  ..., 0.5707, 0.3196, 0.5671],
          [0.3819, 0.4844, 0.4549,  ..., 0.5459, 0.5158, 0.5468],
          [0.5538, 0.4279, 0.5300,  ..., 0.4698, 0.5623, 0.4745]],

         ...,

         [[0.4379, 0.5192, 0.4804,  ..., 0.5505, 0.5976, 0.5219],
          [0.4708, 0.5134, 0.3519,  ..., 0.6796, 0.5268, 0.5454],
          [0.5471, 0.4910, 0.4721,  ..., 0.4844, 0.5999, 0.5281],
          [0.4042, 0.6497, 0.5007,  ..., 0.4228, 0.6013, 0.4341]],

         [[0.4370, 0.3867, 0.4503,  ..., 0.4209, 0.4466, 0.3967],
          [0.4226, 0.7887, 0.6005,  ..., 0.6659, 0.5161, 0.6640],
          [0.4108, 0.3506, 0.5491,  ..., 0.4798, 0.5420, 0.4488],
          [0.5326, 0.5017, 0.5986,  ..., 0.6650, 0.3884, 0.4824]],

         [[0.7896, 0.6044, 0.5681,  ..., 0.6195, 0.4907, 0.5044],
          [0.5427, 0.5214, 0.5211,  ..., 0.4375, 0.4948, 0.4923],
          [0.3455, 0.4564, 0.6951,  ..., 0.5159, 0.4884, 0.5225],
          [0.4336, 0.4208, 0.3922,  ..., 0.5075, 0.5278, 0.4151]]],


        [[[0.5093, 0.5351, 0.4761,  ..., 0.3299, 0.6906, 0.5999],
          [0.3824, 0.5753, 0.4440,  ..., 0.4493, 0.5146, 0.5006],
          [0.4686, 0.4482, 0.4481,  ..., 0.3342, 0.5449, 0.4284],
          [0.4929, 0.4146, 0.3913,  ..., 0.4546, 0.6923, 0.4265]],

         [[0.6213, 0.3596, 0.4404,  ..., 0.5275, 0.5456, 0.4274],
          [0.4066, 0.6039, 0.3798,  ..., 0.4845, 0.6472, 0.4874],
          [0.4838, 0.5229, 0.5217,  ..., 0.3681, 0.6102, 0.4786],
          [0.4080, 0.6327, 0.5452,  ..., 0.4821, 0.5905, 0.5046]],

         [[0.4061, 0.4997, 0.4631,  ..., 0.5764, 0.5004, 0.7208],
          [0.3750, 0.6610, 0.5234,  ..., 0.5000, 0.3552, 0.6291],
          [0.5203, 0.5969, 0.6168,  ..., 0.5835, 0.4308, 0.6148],
          [0.4028, 0.5145, 0.4912,  ..., 0.4757, 0.3879, 0.3847]],

         ...,

         [[0.3861, 0.5231, 0.4804,  ..., 0.4360, 0.4103, 0.5371],
          [0.4719, 0.3919, 0.2559,  ..., 0.5338, 0.5606, 0.3838],
          [0.6088, 0.5757, 0.5486,  ..., 0.2912, 0.4626, 0.5886],
          [0.3490, 0.4227, 0.6293,  ..., 0.3917, 0.4848, 0.5437]],

         [[0.6931, 0.5150, 0.6483,  ..., 0.5105, 0.4269, 0.4943],
          [0.4888, 0.5165, 0.4986,  ..., 0.4765, 0.5063, 0.3769],
          [0.4170, 0.4710, 0.5537,  ..., 0.3917, 0.6018, 0.5454],
          [0.4630, 0.4315, 0.4621,  ..., 0.6014, 0.6181, 0.4355]],

         [[0.3750, 0.6264, 0.5299,  ..., 0.5321, 0.4884, 0.7023],
          [0.5608, 0.5833, 0.4452,  ..., 0.5323, 0.4500, 0.4155],
          [0.4322, 0.4907, 0.6010,  ..., 0.5520, 0.4971, 0.4447],
          [0.4812, 0.5786, 0.4924,  ..., 0.5191, 0.4998, 0.6008]]]],
       device='cuda:0')
tensor([[[[0.5246, 0.4694, 0.3891,  ..., 0.4316, 0.5870, 0.5765],
          [0.3666, 0.4173, 0.5002,  ..., 0.3900, 0.6361, 0.4705],
          [0.6058, 0.4723, 0.5129,  ..., 0.5171, 0.5139, 0.5089],
          [0.3886, 0.4349, 0.5511,  ..., 0.5375, 0.4898, 0.5170]],

         [[0.5091, 0.6505, 0.4345,  ..., 0.4705, 0.4518, 0.6619],
          [0.5404, 0.4359, 0.3803,  ..., 0.4724, 0.6935, 0.3766],
          [0.4216, 0.4385, 0.3346,  ..., 0.4345, 0.4860, 0.5484],
          [0.5075, 0.5101, 0.4949,  ..., 0.3217, 0.6086, 0.5177]],

         [[0.6105, 0.5508, 0.3951,  ..., 0.4593, 0.6165, 0.5516],
          [0.6602, 0.4059, 0.4211,  ..., 0.5737, 0.3166, 0.5641],
          [0.3789, 0.4914, 0.4499,  ..., 0.5489, 0.5128, 0.5438],
          [0.5508, 0.4349, 0.5250,  ..., 0.4728, 0.5593, 0.4715]],

         ...,

         [[0.4349, 0.5262, 0.4754,  ..., 0.5535, 0.5946, 0.5189],
          [0.4678, 0.5204, 0.3469,  ..., 0.6826, 0.5238, 0.5424],
          [0.5441, 0.4980, 0.4671,  ..., 0.4874, 0.5969, 0.5251],
          [0.4012, 0.6567, 0.4957,  ..., 0.4258, 0.5983, 0.4311]],

         [[0.4340, 0.3937, 0.4453,  ..., 0.4239, 0.4436, 0.3937],
          [0.4196, 0.7957, 0.5955,  ..., 0.6689, 0.5131, 0.6610],
          [0.4078, 0.3576, 0.5441,  ..., 0.4828, 0.5390, 0.4458],
          [0.5296, 0.5087, 0.5936,  ..., 0.6680, 0.3854, 0.4794]],

         [[0.7866, 0.6114, 0.5631,  ..., 0.6225, 0.4877, 0.5014],
          [0.5397, 0.5284, 0.5161,  ..., 0.4405, 0.4918, 0.4893],
          [0.3425, 0.4634, 0.6901,  ..., 0.5189, 0.4854, 0.5195],
          [0.4306, 0.4278, 0.3872,  ..., 0.5105, 0.5248, 0.4121]]],


        [[[0.5063, 0.5421, 0.4711,  ..., 0.3329, 0.6876, 0.5969],
          [0.3794, 0.5823, 0.4390,  ..., 0.4523, 0.5116, 0.4976],
          [0.4656, 0.4552, 0.4431,  ..., 0.3372, 0.5419, 0.4254],
          [0.4899, 0.4216, 0.3863,  ..., 0.4576, 0.6893, 0.4235]],

         [[0.6183, 0.3666, 0.4354,  ..., 0.5305, 0.5426, 0.4244],
          [0.4036, 0.6109, 0.3748,  ..., 0.4875, 0.6442, 0.4844],
          [0.4808, 0.5299, 0.5167,  ..., 0.3711, 0.6072, 0.4756],
          [0.4050, 0.6397, 0.5402,  ..., 0.4851, 0.5875, 0.5016]],

         [[0.4031, 0.5067, 0.4581,  ..., 0.5794, 0.4974, 0.7178],
          [0.3720, 0.6680, 0.5184,  ..., 0.5030, 0.3522, 0.6261],
          [0.5173, 0.6039, 0.6118,  ..., 0.5865, 0.4278, 0.6118],
          [0.3998, 0.5215, 0.4862,  ..., 0.4787, 0.3849, 0.3817]],

         ...,

         [[0.3831, 0.5301, 0.4754,  ..., 0.4390, 0.4073, 0.5341],
          [0.4689, 0.3989, 0.2509,  ..., 0.5368, 0.5576, 0.3808],
          [0.6058, 0.5827, 0.5436,  ..., 0.2942, 0.4596, 0.5856],
          [0.3460, 0.4297, 0.6243,  ..., 0.3947, 0.4818, 0.5407]],

         [[0.6901, 0.5220, 0.6433,  ..., 0.5135, 0.4239, 0.4913],
          [0.4858, 0.5235, 0.4936,  ..., 0.4795, 0.5033, 0.3739],
          [0.4140, 0.4780, 0.5487,  ..., 0.3947, 0.5988, 0.5424],
          [0.4600, 0.4385, 0.4571,  ..., 0.6044, 0.6151, 0.4325]],

         [[0.3720, 0.6334, 0.5249,  ..., 0.5351, 0.4854, 0.6993],
          [0.5578, 0.5903, 0.4402,  ..., 0.5353, 0.4470, 0.4125],
          [0.4292, 0.4977, 0.5960,  ..., 0.5550, 0.4941, 0.4417],
          [0.4782, 0.5856, 0.4874,  ..., 0.5221, 0.4968, 0.5978]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0030, -0.0070,  0.0050, -0.0010,  0.0030,  0.0050, -0.0050, -0.0030,
         0.0030,  0.0030], device='cuda:0')
selected experts tensor([1674, 1714, 1528, 1615, 1687, 1671, 1620, 1674, 1641, 1560],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4618, 0.4769, 0.4275,  ..., 0.4666, 0.4004, 0.4493],
          [0.5117, 0.4156, 0.6939,  ..., 0.5780, 0.3976, 0.4025],
          [0.4174, 0.4638, 0.6514,  ..., 0.5869, 0.5003, 0.5393],
          [0.4630, 0.5413, 0.4155,  ..., 0.4887, 0.5170, 0.5636]],

         [[0.4606, 0.3401, 0.4289,  ..., 0.5404, 0.3600, 0.4887],
          [0.5447, 0.5749, 0.3980,  ..., 0.5140, 0.4704, 0.4357],
          [0.3171, 0.5649, 0.5753,  ..., 0.5119, 0.5469, 0.4347],
          [0.5187, 0.4218, 0.3079,  ..., 0.6245, 0.3617, 0.4903]],

         [[0.5040, 0.4660, 0.5010,  ..., 0.4331, 0.2618, 0.4277],
          [0.5795, 0.4974, 0.4780,  ..., 0.4452, 0.4340, 0.5421],
          [0.3615, 0.5658, 0.3849,  ..., 0.5591, 0.4411, 0.4489],
          [0.5417, 0.4099, 0.4265,  ..., 0.5776, 0.4064, 0.4991]],

         ...,

         [[0.4953, 0.4090, 0.4299,  ..., 0.5643, 0.5501, 0.5712],
          [0.4995, 0.4891, 0.5899,  ..., 0.5346, 0.5583, 0.3877],
          [0.4689, 0.5115, 0.4270,  ..., 0.4141, 0.5733, 0.4944],
          [0.4524, 0.5792, 0.4624,  ..., 0.3451, 0.4440, 0.4944]],

         [[0.6667, 0.3652, 0.5480,  ..., 0.4858, 0.5157, 0.4301],
          [0.4662, 0.6311, 0.5303,  ..., 0.4329, 0.4147, 0.5173],
          [0.6167, 0.5725, 0.5828,  ..., 0.4438, 0.5275, 0.4983],
          [0.6535, 0.3990, 0.4857,  ..., 0.4012, 0.4803, 0.3584]],

         [[0.5138, 0.4704, 0.4393,  ..., 0.4923, 0.3912, 0.5385],
          [0.6400, 0.5696, 0.5691,  ..., 0.4844, 0.4878, 0.5318],
          [0.5335, 0.3116, 0.4942,  ..., 0.3193, 0.5176, 0.4683],
          [0.5291, 0.4323, 0.4146,  ..., 0.5040, 0.4689, 0.5289]]],


        [[[0.4944, 0.5543, 0.4982,  ..., 0.5548, 0.4279, 0.4163],
          [0.5681, 0.6059, 0.5371,  ..., 0.4737, 0.4124, 0.5825],
          [0.4360, 0.5257, 0.5178,  ..., 0.5714, 0.5046, 0.5736],
          [0.5413, 0.4410, 0.4055,  ..., 0.6622, 0.4859, 0.4800]],

         [[0.4056, 0.5425, 0.4659,  ..., 0.5921, 0.4213, 0.5074],
          [0.5056, 0.5178, 0.5899,  ..., 0.4151, 0.4750, 0.6241],
          [0.4108, 0.6003, 0.5159,  ..., 0.4912, 0.4938, 0.5871],
          [0.5148, 0.4391, 0.4251,  ..., 0.5581, 0.4827, 0.5589]],

         [[0.4957, 0.4266, 0.5547,  ..., 0.3932, 0.3903, 0.5294],
          [0.3525, 0.5173, 0.4275,  ..., 0.4215, 0.5250, 0.3110],
          [0.3393, 0.6152, 0.5762,  ..., 0.5084, 0.4714, 0.5192],
          [0.4859, 0.4180, 0.4251,  ..., 0.5883, 0.5303, 0.5834]],

         ...,

         [[0.5814, 0.4295, 0.5908,  ..., 0.5600, 0.4078, 0.4255],
          [0.4938, 0.5524, 0.3399,  ..., 0.4974, 0.3858, 0.3556],
          [0.5403, 0.5187, 0.5824,  ..., 0.3712, 0.5875, 0.4915],
          [0.6535, 0.3753, 0.5557,  ..., 0.3884, 0.4279, 0.3982]],

         [[0.5430, 0.4780, 0.6164,  ..., 0.4406, 0.4175, 0.4676],
          [0.4236, 0.5142, 0.4203,  ..., 0.4958, 0.4757, 0.5890],
          [0.5263, 0.5649, 0.6549,  ..., 0.5700, 0.5588, 0.4410],
          [0.3944, 0.2401, 0.4352,  ..., 0.5319, 0.4857, 0.4914]],

         [[0.5490, 0.2725, 0.5344,  ..., 0.4850, 0.3643, 0.5019],
          [0.4930, 0.5668, 0.4581,  ..., 0.5225, 0.3985, 0.4115],
          [0.4341, 0.4806, 0.6523,  ..., 0.3809, 0.4760, 0.3797],
          [0.4956, 0.3175, 0.6122,  ..., 0.6740, 0.5021, 0.4936]]]],
       device='cuda:0')
tensor([[[[0.4588, 0.4819, 0.4345,  ..., 0.4836, 0.3794, 0.4703],
          [0.5087, 0.4206, 0.7009,  ..., 0.5950, 0.3766, 0.4235],
          [0.4144, 0.4688, 0.6584,  ..., 0.6039, 0.4793, 0.5603],
          [0.4600, 0.5463, 0.4225,  ..., 0.5057, 0.4960, 0.5846]],

         [[0.4576, 0.3451, 0.4359,  ..., 0.5574, 0.3390, 0.5097],
          [0.5417, 0.5799, 0.4050,  ..., 0.5310, 0.4494, 0.4567],
          [0.3141, 0.5699, 0.5823,  ..., 0.5289, 0.5259, 0.4557],
          [0.5157, 0.4268, 0.3149,  ..., 0.6415, 0.3407, 0.5113]],

         [[0.5010, 0.4710, 0.5080,  ..., 0.4501, 0.2408, 0.4487],
          [0.5765, 0.5024, 0.4850,  ..., 0.4622, 0.4130, 0.5631],
          [0.3585, 0.5708, 0.3919,  ..., 0.5761, 0.4201, 0.4699],
          [0.5387, 0.4149, 0.4335,  ..., 0.5946, 0.3854, 0.5201]],

         ...,

         [[0.4923, 0.4140, 0.4369,  ..., 0.5813, 0.5291, 0.5922],
          [0.4965, 0.4941, 0.5969,  ..., 0.5516, 0.5373, 0.4087],
          [0.4659, 0.5165, 0.4340,  ..., 0.4311, 0.5523, 0.5154],
          [0.4494, 0.5842, 0.4694,  ..., 0.3621, 0.4230, 0.5154]],

         [[0.6637, 0.3702, 0.5550,  ..., 0.5028, 0.4947, 0.4511],
          [0.4632, 0.6361, 0.5373,  ..., 0.4499, 0.3937, 0.5383],
          [0.6137, 0.5775, 0.5898,  ..., 0.4608, 0.5065, 0.5193],
          [0.6505, 0.4040, 0.4927,  ..., 0.4182, 0.4593, 0.3794]],

         [[0.5108, 0.4754, 0.4463,  ..., 0.5093, 0.3702, 0.5595],
          [0.6370, 0.5746, 0.5761,  ..., 0.5014, 0.4668, 0.5528],
          [0.5305, 0.3166, 0.5012,  ..., 0.3363, 0.4966, 0.4893],
          [0.5261, 0.4373, 0.4216,  ..., 0.5210, 0.4479, 0.5499]]],


        [[[0.4914, 0.5593, 0.5052,  ..., 0.5718, 0.4069, 0.4373],
          [0.5651, 0.6109, 0.5441,  ..., 0.4907, 0.3914, 0.6035],
          [0.4330, 0.5307, 0.5248,  ..., 0.5884, 0.4836, 0.5946],
          [0.5383, 0.4460, 0.4125,  ..., 0.6792, 0.4649, 0.5010]],

         [[0.4026, 0.5475, 0.4729,  ..., 0.6091, 0.4003, 0.5284],
          [0.5026, 0.5228, 0.5969,  ..., 0.4321, 0.4540, 0.6451],
          [0.4078, 0.6053, 0.5229,  ..., 0.5082, 0.4728, 0.6081],
          [0.5118, 0.4441, 0.4321,  ..., 0.5751, 0.4617, 0.5799]],

         [[0.4927, 0.4316, 0.5617,  ..., 0.4102, 0.3693, 0.5504],
          [0.3495, 0.5223, 0.4345,  ..., 0.4385, 0.5040, 0.3320],
          [0.3363, 0.6202, 0.5832,  ..., 0.5254, 0.4504, 0.5402],
          [0.4829, 0.4230, 0.4321,  ..., 0.6053, 0.5093, 0.6044]],

         ...,

         [[0.5784, 0.4345, 0.5978,  ..., 0.5770, 0.3868, 0.4465],
          [0.4908, 0.5574, 0.3469,  ..., 0.5144, 0.3648, 0.3766],
          [0.5373, 0.5237, 0.5894,  ..., 0.3882, 0.5665, 0.5125],
          [0.6505, 0.3803, 0.5627,  ..., 0.4054, 0.4069, 0.4192]],

         [[0.5400, 0.4830, 0.6234,  ..., 0.4576, 0.3965, 0.4886],
          [0.4206, 0.5192, 0.4273,  ..., 0.5128, 0.4547, 0.6100],
          [0.5233, 0.5699, 0.6619,  ..., 0.5870, 0.5378, 0.4620],
          [0.3914, 0.2451, 0.4422,  ..., 0.5489, 0.4647, 0.5124]],

         [[0.5460, 0.2775, 0.5414,  ..., 0.5020, 0.3433, 0.5229],
          [0.4900, 0.5718, 0.4651,  ..., 0.5395, 0.3775, 0.4325],
          [0.4311, 0.4856, 0.6593,  ..., 0.3979, 0.4550, 0.4007],
          [0.4926, 0.3225, 0.6192,  ..., 0.6910, 0.4811, 0.5146]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([ 0.0030, -0.0050, -0.0070,  0.0050, -0.0030, -0.0090,  0.0050, -0.0170,
         0.0210, -0.0210], device='cuda:0')
selected experts tensor([1624, 1616, 1551, 1645, 1309, 2267, 1579, 2021,  987, 1785],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5094, 0.6856, 0.6148,  ..., 0.3596, 0.5292, 0.6426],
          [0.3829, 0.4192, 0.5137,  ..., 0.5104, 0.6230, 0.5956],
          [0.4126, 0.3958, 0.4046,  ..., 0.4842, 0.5036, 0.5170],
          [0.5769, 0.4839, 0.5649,  ..., 0.5575, 0.5680, 0.5952]],

         [[0.5419, 0.5702, 0.6259,  ..., 0.3945, 0.6239, 0.5342],
          [0.5642, 0.5779, 0.4528,  ..., 0.5109, 0.5025, 0.5069],
          [0.3180, 0.4844, 0.4387,  ..., 0.4909, 0.5210, 0.5159],
          [0.4797, 0.4867, 0.5387,  ..., 0.5434, 0.6359, 0.6266]],

         [[0.5627, 0.5028, 0.6040,  ..., 0.4672, 0.6091, 0.4723],
          [0.4471, 0.4729, 0.4306,  ..., 0.5454, 0.5199, 0.4402],
          [0.4201, 0.4487, 0.4377,  ..., 0.3959, 0.4530, 0.4517],
          [0.6370, 0.4729, 0.3702,  ..., 0.5765, 0.6485, 0.6228]],

         ...,

         [[0.5090, 0.5090, 0.5090,  ..., 0.5110, 0.5070, 0.5110],
          [0.5090, 0.5090, 0.5090,  ..., 0.5110, 0.5070, 0.5110],
          [0.5090, 0.5090, 0.5090,  ..., 0.5110, 0.5070, 0.5110],
          [0.5090, 0.5090, 0.5090,  ..., 0.5110, 0.5070, 0.5110]],

         [[0.5090, 0.5090, 0.5090,  ..., 0.5110, 0.5070, 0.5110],
          [0.5090, 0.5090, 0.5090,  ..., 0.5110, 0.5070, 0.5110],
          [0.5090, 0.5090, 0.5090,  ..., 0.5110, 0.5070, 0.5110],
          [0.5090, 0.5090, 0.5090,  ..., 0.5110, 0.5070, 0.5110]],

         [[0.5090, 0.5090, 0.5090,  ..., 0.5110, 0.5070, 0.5110],
          [0.5090, 0.5090, 0.5090,  ..., 0.5110, 0.5070, 0.5110],
          [0.5090, 0.5090, 0.5090,  ..., 0.5110, 0.5070, 0.5110],
          [0.5090, 0.5090, 0.5090,  ..., 0.5110, 0.5070, 0.5110]]],


        [[[0.6839, 0.5712, 0.5779,  ..., 0.4510, 0.5368, 0.4802],
          [0.5058, 0.4126, 0.4182,  ..., 0.6641, 0.4571, 0.5241],
          [0.4253, 0.5342, 0.3948,  ..., 0.4150, 0.4300, 0.6074],
          [0.4732, 0.5661, 0.4187,  ..., 0.5713, 0.5184, 0.4867]],

         [[0.5656, 0.6577, 0.6523,  ..., 0.5044, 0.5029, 0.4730],
          [0.5172, 0.6648, 0.5480,  ..., 0.5216, 0.5406, 0.4679],
          [0.4833, 0.4325, 0.5392,  ..., 0.4657, 0.6938, 0.4136],
          [0.6106, 0.5085, 0.4272,  ..., 0.4306, 0.3836, 0.4606]],

         [[0.5562, 0.6595, 0.7292,  ..., 0.4005, 0.4960, 0.5262],
          [0.4459, 0.5301, 0.5219,  ..., 0.5703, 0.3993, 0.4136],
          [0.5143, 0.5009, 0.4911,  ..., 0.3695, 0.5374, 0.3927],
          [0.6115, 0.6478, 0.4130,  ..., 0.6252, 0.5132, 0.6371]],

         ...,

         [[0.5090, 0.5090, 0.5090,  ..., 0.5110, 0.5070, 0.5110],
          [0.5090, 0.5090, 0.5090,  ..., 0.5110, 0.5070, 0.5110],
          [0.5090, 0.5090, 0.5090,  ..., 0.5110, 0.5070, 0.5110],
          [0.5090, 0.5090, 0.5090,  ..., 0.5110, 0.5070, 0.5110]],

         [[0.5090, 0.5090, 0.5090,  ..., 0.5110, 0.5070, 0.5110],
          [0.5090, 0.5090, 0.5090,  ..., 0.5110, 0.5070, 0.5110],
          [0.5090, 0.5090, 0.5090,  ..., 0.5110, 0.5070, 0.5110],
          [0.5090, 0.5090, 0.5090,  ..., 0.5110, 0.5070, 0.5110]],

         [[0.5090, 0.5090, 0.5090,  ..., 0.5110, 0.5070, 0.5110],
          [0.5090, 0.5090, 0.5090,  ..., 0.5110, 0.5070, 0.5110],
          [0.5090, 0.5090, 0.5090,  ..., 0.5110, 0.5070, 0.5110],
          [0.5090, 0.5090, 0.5090,  ..., 0.5110, 0.5070, 0.5110]]]],
       device='cuda:0')
tensor([[[[0.5004, 0.6766, 0.6058,  ..., 0.3486, 0.5222, 0.6316],
          [0.3739, 0.4102, 0.5047,  ..., 0.4994, 0.6160, 0.5846],
          [0.4036, 0.3868, 0.3956,  ..., 0.4732, 0.4966, 0.5060],
          [0.5679, 0.4749, 0.5559,  ..., 0.5465, 0.5610, 0.5842]],

         [[0.5329, 0.5612, 0.6169,  ..., 0.3835, 0.6169, 0.5232],
          [0.5552, 0.5689, 0.4438,  ..., 0.4999, 0.4955, 0.4959],
          [0.3090, 0.4754, 0.4297,  ..., 0.4799, 0.5140, 0.5049],
          [0.4707, 0.4777, 0.5297,  ..., 0.5324, 0.6289, 0.6156]],

         [[0.5537, 0.4938, 0.5950,  ..., 0.4562, 0.6021, 0.4613],
          [0.4381, 0.4639, 0.4216,  ..., 0.5344, 0.5129, 0.4292],
          [0.4111, 0.4397, 0.4287,  ..., 0.3849, 0.4460, 0.4407],
          [0.6280, 0.4639, 0.3612,  ..., 0.5655, 0.6415, 0.6118]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.6749, 0.5622, 0.5689,  ..., 0.4400, 0.5297, 0.4692],
          [0.4968, 0.4036, 0.4092,  ..., 0.6531, 0.4501, 0.5131],
          [0.4163, 0.5252, 0.3858,  ..., 0.4040, 0.4230, 0.5964],
          [0.4642, 0.5571, 0.4097,  ..., 0.5603, 0.5114, 0.4757]],

         [[0.5566, 0.6487, 0.6433,  ..., 0.4934, 0.4959, 0.4620],
          [0.5082, 0.6558, 0.5390,  ..., 0.5106, 0.5336, 0.4569],
          [0.4743, 0.4235, 0.5302,  ..., 0.4547, 0.6868, 0.4026],
          [0.6016, 0.4995, 0.4182,  ..., 0.4196, 0.3766, 0.4496]],

         [[0.5472, 0.6505, 0.7202,  ..., 0.3895, 0.4890, 0.5152],
          [0.4369, 0.5211, 0.5129,  ..., 0.5593, 0.3923, 0.4026],
          [0.5053, 0.4919, 0.4821,  ..., 0.3585, 0.5304, 0.3817],
          [0.6025, 0.6388, 0.4040,  ..., 0.6142, 0.5062, 0.6261]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0090, 0.0090, 0.0090, 0.0090, 0.0110, 0.0110, 0.0110, 0.0110, 0.0070,
        0.0110], device='cuda:0')
selected experts tensor([ 948, 1223, 1102,  548, 3688, 4249, 1343,  594, 1752,  937],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5414, 0.5595, 0.6073,  ..., 0.5337, 0.3437, 0.5184],
          [0.5904, 0.5888, 0.6423,  ..., 0.5070, 0.3569, 0.5711],
          [0.4740, 0.5850, 0.4892,  ..., 0.5491, 0.4824, 0.4928],
          [0.4974, 0.5621, 0.6504,  ..., 0.3937, 0.4947, 0.4444]],

         [[0.3981, 0.3673, 0.4987,  ..., 0.5525, 0.5014, 0.4855],
          [0.5088, 0.5635, 0.4751,  ..., 0.3467, 0.2668, 0.6152],
          [0.5535, 0.3880, 0.4736,  ..., 0.5602, 0.4815, 0.5702],
          [0.5735, 0.5418, 0.5399,  ..., 0.4530, 0.5149, 0.5287]],

         [[0.4573, 0.5096, 0.4908,  ..., 0.4627, 0.3757, 0.4765],
          [0.5735, 0.5452, 0.5489,  ..., 0.3627, 0.5177, 0.5985],
          [0.4138, 0.4353, 0.5142,  ..., 0.5255, 0.6046, 0.5582],
          [0.4383, 0.6098, 0.4319,  ..., 0.5011, 0.5784, 0.6009]],

         ...,

         [[0.4224, 0.4534, 0.5879,  ..., 0.5209, 0.4963, 0.4095],
          [0.5460, 0.4858, 0.5003,  ..., 0.5897, 0.6515, 0.4643],
          [0.4313, 0.4700, 0.6396,  ..., 0.6995, 0.4490, 0.5649],
          [0.4361, 0.4167, 0.6058,  ..., 0.5610, 0.5666, 0.4057]],

         [[0.6660, 0.6564, 0.5477,  ..., 0.5319, 0.4681, 0.5444],
          [0.5602, 0.4686, 0.4666,  ..., 0.3974, 0.4510, 0.5849],
          [0.4033, 0.6404, 0.4215,  ..., 0.5281, 0.5937, 0.3874],
          [0.5482, 0.7544, 0.5993,  ..., 0.6330, 0.3685, 0.4442]],

         [[0.5578, 0.6177, 0.5540,  ..., 0.6234, 0.5191, 0.5410],
          [0.5354, 0.5399, 0.5370,  ..., 0.6085, 0.7606, 0.6240],
          [0.6549, 0.5741, 0.4253,  ..., 0.6280, 0.4183, 0.4536],
          [0.2917, 0.3788, 0.6594,  ..., 0.4959, 0.3940, 0.4786]]],


        [[[0.5262, 0.5409, 0.5545,  ..., 0.5153, 0.6251, 0.4247],
          [0.4364, 0.3682, 0.5313,  ..., 0.5520, 0.5030, 0.6203],
          [0.4224, 0.4719, 0.4995,  ..., 0.5949, 0.6542, 0.3925],
          [0.5320, 0.4030, 0.5167,  ..., 0.5878, 0.6425, 0.5920]],

         [[0.4890, 0.3824, 0.5917,  ..., 0.4314, 0.5466, 0.5316],
          [0.4755, 0.5008, 0.4262,  ..., 0.5326, 0.5200, 0.4736],
          [0.4320, 0.3637, 0.4729,  ..., 0.3690, 0.4047, 0.3883],
          [0.4000, 0.5958, 0.4789,  ..., 0.5916, 0.7238, 0.4133]],

         [[0.5453, 0.3180, 0.4769,  ..., 0.3345, 0.4445, 0.3315],
          [0.5419, 0.5175, 0.5593,  ..., 0.5491, 0.5351, 0.4199],
          [0.3910, 0.5760, 0.4588,  ..., 0.5496, 0.6031, 0.4353],
          [0.6212, 0.4272, 0.4765,  ..., 0.4280, 0.5400, 0.6143]],

         ...,

         [[0.4514, 0.3769, 0.5521,  ..., 0.5245, 0.5219, 0.4670],
          [0.6347, 0.4985, 0.5988,  ..., 0.5106, 0.5010, 0.3800],
          [0.4301, 0.5095, 0.4377,  ..., 0.5802, 0.5923, 0.5565],
          [0.3241, 0.6368, 0.6760,  ..., 0.4916, 0.2994, 0.3662]],

         [[0.3285, 0.6467, 0.4439,  ..., 0.5143, 0.5861, 0.5721],
          [0.4289, 0.4020, 0.5346,  ..., 0.5071, 0.6228, 0.5538],
          [0.4511, 0.5413, 0.4425,  ..., 0.4510, 0.3766, 0.5654],
          [0.4436, 0.5189, 0.5567,  ..., 0.4758, 0.4572, 0.6152]],

         [[0.5164, 0.5592, 0.5637,  ..., 0.4724, 0.4567, 0.5678],
          [0.3094, 0.3710, 0.5156,  ..., 0.4323, 0.5563, 0.4619],
          [0.3517, 0.5331, 0.5450,  ..., 0.4814, 0.5560, 0.5294],
          [0.2142, 0.4782, 0.5931,  ..., 0.5423, 0.3694, 0.5437]]]],
       device='cuda:0')
tensor([[[[0.5554, 0.5615, 0.5993,  ..., 0.5277, 0.3337, 0.5224],
          [0.6044, 0.5908, 0.6343,  ..., 0.5010, 0.3469, 0.5751],
          [0.4880, 0.5870, 0.4812,  ..., 0.5431, 0.4724, 0.4968],
          [0.5114, 0.5641, 0.6424,  ..., 0.3877, 0.4847, 0.4484]],

         [[0.4121, 0.3693, 0.4907,  ..., 0.5465, 0.4914, 0.4895],
          [0.5228, 0.5655, 0.4671,  ..., 0.3407, 0.2568, 0.6192],
          [0.5675, 0.3900, 0.4656,  ..., 0.5542, 0.4715, 0.5742],
          [0.5875, 0.5438, 0.5319,  ..., 0.4470, 0.5049, 0.5327]],

         [[0.4713, 0.5116, 0.4828,  ..., 0.4567, 0.3657, 0.4805],
          [0.5875, 0.5472, 0.5409,  ..., 0.3567, 0.5077, 0.6025],
          [0.4278, 0.4373, 0.5062,  ..., 0.5195, 0.5946, 0.5622],
          [0.4523, 0.6118, 0.4239,  ..., 0.4951, 0.5684, 0.6049]],

         ...,

         [[0.4364, 0.4554, 0.5799,  ..., 0.5149, 0.4863, 0.4135],
          [0.5600, 0.4878, 0.4923,  ..., 0.5837, 0.6415, 0.4683],
          [0.4453, 0.4720, 0.6316,  ..., 0.6935, 0.4390, 0.5689],
          [0.4501, 0.4187, 0.5978,  ..., 0.5550, 0.5566, 0.4097]],

         [[0.6800, 0.6584, 0.5397,  ..., 0.5259, 0.4581, 0.5484],
          [0.5742, 0.4706, 0.4586,  ..., 0.3914, 0.4410, 0.5889],
          [0.4173, 0.6424, 0.4135,  ..., 0.5221, 0.5837, 0.3914],
          [0.5622, 0.7564, 0.5913,  ..., 0.6270, 0.3585, 0.4482]],

         [[0.5718, 0.6197, 0.5460,  ..., 0.6174, 0.5091, 0.5450],
          [0.5494, 0.5419, 0.5290,  ..., 0.6025, 0.7506, 0.6280],
          [0.6689, 0.5761, 0.4173,  ..., 0.6220, 0.4083, 0.4576],
          [0.3057, 0.3808, 0.6514,  ..., 0.4899, 0.3840, 0.4826]]],


        [[[0.5402, 0.5429, 0.5465,  ..., 0.5093, 0.6151, 0.4287],
          [0.4504, 0.3702, 0.5233,  ..., 0.5460, 0.4930, 0.6243],
          [0.4364, 0.4739, 0.4915,  ..., 0.5889, 0.6442, 0.3965],
          [0.5460, 0.4050, 0.5087,  ..., 0.5818, 0.6325, 0.5960]],

         [[0.5030, 0.3844, 0.5837,  ..., 0.4254, 0.5366, 0.5356],
          [0.4895, 0.5028, 0.4182,  ..., 0.5266, 0.5100, 0.4776],
          [0.4460, 0.3657, 0.4649,  ..., 0.3630, 0.3947, 0.3923],
          [0.4140, 0.5978, 0.4709,  ..., 0.5856, 0.7138, 0.4173]],

         [[0.5593, 0.3200, 0.4689,  ..., 0.3285, 0.4345, 0.3355],
          [0.5559, 0.5195, 0.5513,  ..., 0.5431, 0.5251, 0.4239],
          [0.4050, 0.5780, 0.4508,  ..., 0.5436, 0.5931, 0.4393],
          [0.6352, 0.4292, 0.4685,  ..., 0.4220, 0.5300, 0.6183]],

         ...,

         [[0.4654, 0.3789, 0.5441,  ..., 0.5185, 0.5119, 0.4710],
          [0.6487, 0.5005, 0.5908,  ..., 0.5046, 0.4910, 0.3840],
          [0.4441, 0.5115, 0.4297,  ..., 0.5742, 0.5823, 0.5605],
          [0.3381, 0.6388, 0.6680,  ..., 0.4856, 0.2894, 0.3702]],

         [[0.3425, 0.6487, 0.4359,  ..., 0.5083, 0.5761, 0.5761],
          [0.4429, 0.4040, 0.5266,  ..., 0.5011, 0.6128, 0.5578],
          [0.4651, 0.5433, 0.4345,  ..., 0.4450, 0.3666, 0.5694],
          [0.4576, 0.5209, 0.5487,  ..., 0.4698, 0.4472, 0.6192]],

         [[0.5304, 0.5612, 0.5557,  ..., 0.4664, 0.4467, 0.5718],
          [0.3234, 0.3730, 0.5076,  ..., 0.4263, 0.5463, 0.4659],
          [0.3657, 0.5351, 0.5370,  ..., 0.4754, 0.5460, 0.5334],
          [0.2282, 0.4802, 0.5851,  ..., 0.5363, 0.3594, 0.5477]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0140, -0.0020,  0.0080,  0.0020,  0.0060,  0.0060, -0.0040,  0.0060,
         0.0100, -0.0040], device='cuda:0')
selected experts tensor([1710, 1613, 1605, 1557, 1675, 1695, 1622, 1502, 1637, 1768],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5160, 0.5242, 0.5873,  ..., 0.3228, 0.4990, 0.4640],
          [0.4183, 0.5501, 0.6439,  ..., 0.6836, 0.3874, 0.4864],
          [0.3713, 0.4116, 0.5384,  ..., 0.4309, 0.6709, 0.4984],
          [0.3768, 0.4183, 0.6243,  ..., 0.5282, 0.4865, 0.5729]],

         [[0.4415, 0.3989, 0.5138,  ..., 0.3438, 0.5601, 0.5938],
          [0.4780, 0.5375, 0.6953,  ..., 0.4803, 0.4350, 0.6642],
          [0.4735, 0.5039, 0.4731,  ..., 0.4565, 0.4628, 0.5230],
          [0.4410, 0.4174, 0.5706,  ..., 0.4223, 0.4911, 0.4260]],

         [[0.5601, 0.6763, 0.4204,  ..., 0.5971, 0.4982, 0.5158],
          [0.4863, 0.6362, 0.3845,  ..., 0.6240, 0.5485, 0.5486],
          [0.5003, 0.3487, 0.6206,  ..., 0.4827, 0.6290, 0.3197],
          [0.4341, 0.2936, 0.4898,  ..., 0.4847, 0.5378, 0.5643]],

         ...,

         [[0.5296, 0.4965, 0.4247,  ..., 0.5577, 0.4221, 0.3105],
          [0.5984, 0.4436, 0.5130,  ..., 0.5178, 0.6761, 0.3308],
          [0.4811, 0.6451, 0.4663,  ..., 0.4305, 0.4983, 0.5336],
          [0.6097, 0.5709, 0.3909,  ..., 0.5711, 0.4836, 0.5396]],

         [[0.5928, 0.6254, 0.4731,  ..., 0.5659, 0.4986, 0.4628],
          [0.4950, 0.4621, 0.4333,  ..., 0.6321, 0.4269, 0.3834],
          [0.4548, 0.7026, 0.4443,  ..., 0.4823, 0.4468, 0.5710],
          [0.5742, 0.5168, 0.3886,  ..., 0.4740, 0.5463, 0.4322]],

         [[0.5028, 0.4630, 0.5028,  ..., 0.5454, 0.4884, 0.5454],
          [0.5316, 0.4859, 0.6303,  ..., 0.5882, 0.4070, 0.4706],
          [0.5410, 0.4479, 0.4039,  ..., 0.5459, 0.6938, 0.3652],
          [0.5601, 0.5363, 0.5682,  ..., 0.4360, 0.6879, 0.4956]]],


        [[[0.4408, 0.4501, 0.4082,  ..., 0.3341, 0.5223, 0.5645],
          [0.6055, 0.4554, 0.6860,  ..., 0.5292, 0.5499, 0.5767],
          [0.5781, 0.4036, 0.5423,  ..., 0.4142, 0.4065, 0.5473],
          [0.4221, 0.5392, 0.5617,  ..., 0.6579, 0.5453, 0.4512]],

         [[0.6027, 0.4390, 0.5002,  ..., 0.4694, 0.5526, 0.4490],
          [0.4478, 0.6080, 0.4453,  ..., 0.4782, 0.5373, 0.5420],
          [0.6129, 0.5421, 0.4887,  ..., 0.4493, 0.5281, 0.3679],
          [0.4524, 0.6344, 0.4602,  ..., 0.6037, 0.5656, 0.4565]],

         [[0.5176, 0.3686, 0.6529,  ..., 0.5854, 0.4298, 0.5623],
          [0.6905, 0.4245, 0.4837,  ..., 0.2862, 0.4808, 0.5192],
          [0.5006, 0.6020, 0.4484,  ..., 0.4726, 0.5369, 0.4236],
          [0.6231, 0.4984, 0.5987,  ..., 0.4925, 0.5536, 0.5264]],

         ...,

         [[0.4897, 0.4279, 0.5782,  ..., 0.6013, 0.4485, 0.5229],
          [0.5570, 0.4593, 0.5433,  ..., 0.4345, 0.5022, 0.5112],
          [0.5246, 0.5254, 0.4237,  ..., 0.4355, 0.4727, 0.5041],
          [0.3874, 0.3188, 0.4474,  ..., 0.4266, 0.4994, 0.5413]],

         [[0.5728, 0.5491, 0.5130,  ..., 0.3280, 0.3962, 0.5606],
          [0.5091, 0.5126, 0.6252,  ..., 0.5468, 0.5035, 0.4156],
          [0.4122, 0.5239, 0.5370,  ..., 0.5730, 0.3925, 0.4385],
          [0.5603, 0.6025, 0.5542,  ..., 0.4228, 0.5003, 0.4755]],

         [[0.5347, 0.5414, 0.4894,  ..., 0.6276, 0.6399, 0.6140],
          [0.6372, 0.4715, 0.4395,  ..., 0.5180, 0.4763, 0.3240],
          [0.5146, 0.5050, 0.4429,  ..., 0.5367, 0.5439, 0.4874],
          [0.6622, 0.6600, 0.6520,  ..., 0.5507, 0.4341, 0.7560]]]],
       device='cuda:0')
tensor([[[[0.5140, 0.5322, 0.5813,  ..., 0.3268, 0.4970, 0.4600],
          [0.4163, 0.5581, 0.6379,  ..., 0.6876, 0.3854, 0.4824],
          [0.3693, 0.4196, 0.5324,  ..., 0.4349, 0.6689, 0.4944],
          [0.3748, 0.4263, 0.6183,  ..., 0.5322, 0.4845, 0.5689]],

         [[0.4395, 0.4069, 0.5078,  ..., 0.3478, 0.5581, 0.5898],
          [0.4760, 0.5455, 0.6893,  ..., 0.4843, 0.4330, 0.6602],
          [0.4715, 0.5119, 0.4671,  ..., 0.4605, 0.4608, 0.5190],
          [0.4390, 0.4254, 0.5646,  ..., 0.4263, 0.4891, 0.4220]],

         [[0.5581, 0.6843, 0.4144,  ..., 0.6011, 0.4962, 0.5118],
          [0.4843, 0.6442, 0.3785,  ..., 0.6280, 0.5465, 0.5446],
          [0.4983, 0.3567, 0.6146,  ..., 0.4867, 0.6270, 0.3157],
          [0.4321, 0.3016, 0.4838,  ..., 0.4887, 0.5358, 0.5603]],

         ...,

         [[0.5276, 0.5045, 0.4187,  ..., 0.5617, 0.4201, 0.3065],
          [0.5964, 0.4516, 0.5070,  ..., 0.5218, 0.6741, 0.3268],
          [0.4791, 0.6531, 0.4603,  ..., 0.4345, 0.4963, 0.5296],
          [0.6077, 0.5789, 0.3849,  ..., 0.5751, 0.4816, 0.5356]],

         [[0.5908, 0.6334, 0.4671,  ..., 0.5699, 0.4966, 0.4588],
          [0.4930, 0.4701, 0.4273,  ..., 0.6361, 0.4249, 0.3794],
          [0.4528, 0.7106, 0.4383,  ..., 0.4863, 0.4448, 0.5670],
          [0.5722, 0.5248, 0.3826,  ..., 0.4780, 0.5443, 0.4282]],

         [[0.5008, 0.4710, 0.4968,  ..., 0.5494, 0.4864, 0.5414],
          [0.5296, 0.4939, 0.6243,  ..., 0.5922, 0.4050, 0.4666],
          [0.5390, 0.4559, 0.3979,  ..., 0.5499, 0.6918, 0.3612],
          [0.5581, 0.5443, 0.5622,  ..., 0.4400, 0.6859, 0.4916]]],


        [[[0.4388, 0.4581, 0.4022,  ..., 0.3381, 0.5203, 0.5605],
          [0.6035, 0.4634, 0.6800,  ..., 0.5332, 0.5479, 0.5727],
          [0.5761, 0.4116, 0.5363,  ..., 0.4182, 0.4045, 0.5433],
          [0.4201, 0.5472, 0.5557,  ..., 0.6619, 0.5433, 0.4472]],

         [[0.6007, 0.4470, 0.4942,  ..., 0.4734, 0.5506, 0.4450],
          [0.4458, 0.6160, 0.4393,  ..., 0.4822, 0.5353, 0.5380],
          [0.6109, 0.5501, 0.4827,  ..., 0.4533, 0.5261, 0.3639],
          [0.4504, 0.6424, 0.4542,  ..., 0.6077, 0.5636, 0.4525]],

         [[0.5156, 0.3766, 0.6469,  ..., 0.5894, 0.4278, 0.5583],
          [0.6885, 0.4325, 0.4777,  ..., 0.2902, 0.4788, 0.5152],
          [0.4986, 0.6100, 0.4424,  ..., 0.4766, 0.5349, 0.4196],
          [0.6211, 0.5064, 0.5927,  ..., 0.4965, 0.5516, 0.5224]],

         ...,

         [[0.4877, 0.4359, 0.5722,  ..., 0.6053, 0.4465, 0.5189],
          [0.5550, 0.4673, 0.5373,  ..., 0.4385, 0.5002, 0.5072],
          [0.5226, 0.5334, 0.4177,  ..., 0.4395, 0.4707, 0.5001],
          [0.3854, 0.3268, 0.4414,  ..., 0.4306, 0.4974, 0.5373]],

         [[0.5708, 0.5571, 0.5070,  ..., 0.3320, 0.3942, 0.5566],
          [0.5071, 0.5206, 0.6192,  ..., 0.5508, 0.5015, 0.4116],
          [0.4102, 0.5319, 0.5310,  ..., 0.5770, 0.3905, 0.4345],
          [0.5583, 0.6105, 0.5482,  ..., 0.4268, 0.4983, 0.4715]],

         [[0.5327, 0.5494, 0.4834,  ..., 0.6316, 0.6379, 0.6100],
          [0.6352, 0.4795, 0.4335,  ..., 0.5220, 0.4743, 0.3200],
          [0.5126, 0.5130, 0.4369,  ..., 0.5407, 0.5419, 0.4834],
          [0.6602, 0.6680, 0.6460,  ..., 0.5547, 0.4321, 0.7520]]]],
       device='cuda:0', requires_grad=True)
tensor([ 2.0000e-03, -8.0000e-03,  6.0000e-03,  2.3283e-10,  2.0000e-03,
         4.0000e-03, -4.0000e-03, -4.0000e-03,  2.0000e-03,  4.0000e-03],
       device='cuda:0')
selected experts tensor([1601, 1665, 1685, 1814, 1639, 1518, 1712, 1582, 1582, 1586],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.3806, 0.6125, 0.2882,  ..., 0.5142, 0.4661, 0.5056],
          [0.4689, 0.4389, 0.6391,  ..., 0.3912, 0.3114, 0.5320],
          [0.5117, 0.3367, 0.4876,  ..., 0.5280, 0.5267, 0.4709],
          [0.4466, 0.4377, 0.5492,  ..., 0.4714, 0.6463, 0.5926]],

         [[0.4532, 0.5122, 0.4670,  ..., 0.4336, 0.4120, 0.4308],
          [0.3838, 0.4057, 0.5490,  ..., 0.4833, 0.3260, 0.4857],
          [0.4222, 0.5531, 0.5753,  ..., 0.5200, 0.5358, 0.4414],
          [0.4418, 0.4418, 0.4599,  ..., 0.5533, 0.6795, 0.6123]],

         [[0.4850, 0.4142, 0.3887,  ..., 0.4977, 0.3671, 0.5310],
          [0.5853, 0.4999, 0.4855,  ..., 0.4799, 0.3986, 0.4397],
          [0.3661, 0.5825, 0.6265,  ..., 0.5169, 0.5075, 0.4281],
          [0.5758, 0.4844, 0.3660,  ..., 0.6595, 0.4083, 0.5631]],

         ...,

         [[0.4908, 0.4233, 0.4127,  ..., 0.4379, 0.3138, 0.5450],
          [0.5080, 0.5251, 0.5448,  ..., 0.4686, 0.4360, 0.4120],
          [0.4587, 0.4936, 0.6774,  ..., 0.4947, 0.5823, 0.5114],
          [0.4852, 0.4261, 0.4485,  ..., 0.5102, 0.5276, 0.5726]],

         [[0.4385, 0.4806, 0.4354,  ..., 0.5836, 0.3352, 0.5008],
          [0.4404, 0.5497, 0.5526,  ..., 0.4605, 0.5590, 0.4735],
          [0.4674, 0.6411, 0.5217,  ..., 0.6253, 0.5387, 0.3708],
          [0.5062, 0.3219, 0.4265,  ..., 0.4888, 0.5803, 0.5136]],

         [[0.6642, 0.4109, 0.4941,  ..., 0.5283, 0.4190, 0.5655],
          [0.4483, 0.6312, 0.4639,  ..., 0.4384, 0.4958, 0.4038],
          [0.4457, 0.4936, 0.5782,  ..., 0.4287, 0.5497, 0.4571],
          [0.5464, 0.3768, 0.4938,  ..., 0.5130, 0.4550, 0.5030]]],


        [[[0.5122, 0.6088, 0.4461,  ..., 0.4396, 0.4167, 0.4086],
          [0.5635, 0.4944, 0.5133,  ..., 0.4621, 0.3566, 0.5678],
          [0.6720, 0.4266, 0.5100,  ..., 0.5504, 0.5298, 0.5013],
          [0.4279, 0.5488, 0.3579,  ..., 0.6072, 0.4661, 0.6041]],

         [[0.4351, 0.6009, 0.4237,  ..., 0.4985, 0.3715, 0.4429],
          [0.5957, 0.4257, 0.6128,  ..., 0.3432, 0.4668, 0.6023],
          [0.2815, 0.5330, 0.5482,  ..., 0.4838, 0.4555, 0.4173],
          [0.3661, 0.3911, 0.4127,  ..., 0.5869, 0.5150, 0.5579]],

         [[0.5986, 0.5251, 0.4667,  ..., 0.5406, 0.2613, 0.4320],
          [0.6075, 0.4118, 0.5942,  ..., 0.3351, 0.3540, 0.6186],
          [0.4585, 0.4190, 0.5550,  ..., 0.4520, 0.4903, 0.5819],
          [0.4592, 0.4539, 0.5000,  ..., 0.6054, 0.4102, 0.5378]],

         ...,

         [[0.6009, 0.3763, 0.3854,  ..., 0.4343, 0.4459, 0.5735],
          [0.5200, 0.5318, 0.6542,  ..., 0.3702, 0.3294, 0.5531],
          [0.3147, 0.3341, 0.5410,  ..., 0.3306, 0.4601, 0.5683],
          [0.4587, 0.4449, 0.5509,  ..., 0.4994, 0.6071, 0.6546]],

         [[0.5107, 0.6562, 0.3706,  ..., 0.4691, 0.3950, 0.6495],
          [0.5700, 0.4799, 0.5819,  ..., 0.4498, 0.4464, 0.4163],
          [0.4866, 0.5117, 0.6247,  ..., 0.3441, 0.5147, 0.5133],
          [0.5490, 0.4945, 0.5559,  ..., 0.4944, 0.3877, 0.5829]],

         [[0.5910, 0.4714, 0.5123,  ..., 0.5680, 0.5156, 0.5363],
          [0.5202, 0.5863, 0.5653,  ..., 0.4710, 0.3778, 0.4243],
          [0.4936, 0.5596, 0.7372,  ..., 0.5671, 0.6795, 0.4371],
          [0.4546, 0.3972, 0.4792,  ..., 0.6022, 0.4649, 0.5875]]]],
       device='cuda:0')
tensor([[[[0.3766, 0.6165, 0.2942,  ..., 0.5322, 0.4441, 0.5276],
          [0.4649, 0.4429, 0.6451,  ..., 0.4092, 0.2894, 0.5540],
          [0.5077, 0.3407, 0.4936,  ..., 0.5460, 0.5047, 0.4929],
          [0.4426, 0.4417, 0.5552,  ..., 0.4894, 0.6243, 0.6146]],

         [[0.4492, 0.5162, 0.4730,  ..., 0.4516, 0.3900, 0.4528],
          [0.3798, 0.4097, 0.5550,  ..., 0.5013, 0.3040, 0.5077],
          [0.4182, 0.5571, 0.5813,  ..., 0.5380, 0.5138, 0.4634],
          [0.4378, 0.4458, 0.4659,  ..., 0.5713, 0.6575, 0.6343]],

         [[0.4810, 0.4182, 0.3947,  ..., 0.5157, 0.3451, 0.5530],
          [0.5813, 0.5039, 0.4915,  ..., 0.4979, 0.3766, 0.4617],
          [0.3621, 0.5865, 0.6325,  ..., 0.5349, 0.4855, 0.4501],
          [0.5718, 0.4884, 0.3720,  ..., 0.6775, 0.3863, 0.5851]],

         ...,

         [[0.4868, 0.4273, 0.4187,  ..., 0.4559, 0.2918, 0.5670],
          [0.5040, 0.5291, 0.5508,  ..., 0.4866, 0.4140, 0.4340],
          [0.4547, 0.4976, 0.6834,  ..., 0.5127, 0.5603, 0.5334],
          [0.4812, 0.4301, 0.4545,  ..., 0.5282, 0.5056, 0.5946]],

         [[0.4345, 0.4846, 0.4414,  ..., 0.6016, 0.3132, 0.5228],
          [0.4364, 0.5537, 0.5586,  ..., 0.4785, 0.5370, 0.4955],
          [0.4634, 0.6451, 0.5277,  ..., 0.6433, 0.5167, 0.3928],
          [0.5022, 0.3259, 0.4325,  ..., 0.5068, 0.5583, 0.5356]],

         [[0.6602, 0.4149, 0.5001,  ..., 0.5463, 0.3970, 0.5875],
          [0.4443, 0.6352, 0.4699,  ..., 0.4564, 0.4738, 0.4258],
          [0.4417, 0.4976, 0.5842,  ..., 0.4467, 0.5277, 0.4791],
          [0.5424, 0.3808, 0.4998,  ..., 0.5310, 0.4330, 0.5250]]],


        [[[0.5082, 0.6128, 0.4521,  ..., 0.4576, 0.3947, 0.4306],
          [0.5595, 0.4984, 0.5193,  ..., 0.4801, 0.3346, 0.5898],
          [0.6680, 0.4306, 0.5160,  ..., 0.5684, 0.5078, 0.5233],
          [0.4239, 0.5528, 0.3639,  ..., 0.6252, 0.4441, 0.6261]],

         [[0.4311, 0.6049, 0.4297,  ..., 0.5165, 0.3495, 0.4649],
          [0.5917, 0.4297, 0.6188,  ..., 0.3612, 0.4448, 0.6243],
          [0.2775, 0.5370, 0.5542,  ..., 0.5018, 0.4335, 0.4393],
          [0.3621, 0.3951, 0.4187,  ..., 0.6049, 0.4930, 0.5799]],

         [[0.5946, 0.5291, 0.4727,  ..., 0.5586, 0.2393, 0.4540],
          [0.6035, 0.4158, 0.6002,  ..., 0.3531, 0.3320, 0.6406],
          [0.4545, 0.4230, 0.5610,  ..., 0.4700, 0.4683, 0.6039],
          [0.4552, 0.4579, 0.5060,  ..., 0.6234, 0.3882, 0.5598]],

         ...,

         [[0.5969, 0.3803, 0.3914,  ..., 0.4523, 0.4239, 0.5955],
          [0.5160, 0.5358, 0.6602,  ..., 0.3882, 0.3074, 0.5751],
          [0.3107, 0.3381, 0.5470,  ..., 0.3486, 0.4381, 0.5903],
          [0.4547, 0.4489, 0.5569,  ..., 0.5174, 0.5851, 0.6766]],

         [[0.5067, 0.6602, 0.3766,  ..., 0.4871, 0.3730, 0.6715],
          [0.5660, 0.4839, 0.5879,  ..., 0.4678, 0.4244, 0.4383],
          [0.4826, 0.5157, 0.6307,  ..., 0.3621, 0.4927, 0.5353],
          [0.5450, 0.4985, 0.5619,  ..., 0.5124, 0.3657, 0.6049]],

         [[0.5870, 0.4754, 0.5183,  ..., 0.5860, 0.4936, 0.5583],
          [0.5162, 0.5903, 0.5713,  ..., 0.4890, 0.3558, 0.4463],
          [0.4896, 0.5636, 0.7432,  ..., 0.5851, 0.6575, 0.4591],
          [0.4506, 0.4012, 0.4852,  ..., 0.6202, 0.4429, 0.6095]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0040, -0.0040, -0.0060,  0.0040, -0.0020, -0.0100,  0.0060, -0.0180,
         0.0220, -0.0220], device='cuda:0')
selected experts tensor([1707, 1490, 1604, 1565, 1368, 2194, 1598, 2244,  775, 1839],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5837, 0.5439, 0.5784,  ..., 0.5691, 0.5151, 0.5162],
          [0.5894, 0.5295, 0.4705,  ..., 0.5520, 0.5406, 0.4597],
          [0.5337, 0.5322, 0.4988,  ..., 0.4648, 0.6843, 0.6979],
          [0.5775, 0.4633, 0.3880,  ..., 0.4985, 0.5440, 0.4939]],

         [[0.5690, 0.5994, 0.6181,  ..., 0.5133, 0.5286, 0.6678],
          [0.5553, 0.5884, 0.4939,  ..., 0.5097, 0.4496, 0.5828],
          [0.4901, 0.4103, 0.4240,  ..., 0.4359, 0.5233, 0.5814],
          [0.5536, 0.4852, 0.3775,  ..., 0.6215, 0.5011, 0.5823]],

         [[0.5509, 0.5707, 0.5956,  ..., 0.4203, 0.4820, 0.5823],
          [0.4625, 0.4536, 0.5861,  ..., 0.4460, 0.5377, 0.5199],
          [0.4363, 0.4776, 0.4292,  ..., 0.4837, 0.5782, 0.4441],
          [0.6658, 0.5095, 0.4411,  ..., 0.5147, 0.4704, 0.6340]],

         ...,

         [[0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120]],

         [[0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120]],

         [[0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120]]],


        [[[0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120]],

         [[0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120]],

         [[0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120]],

         ...,

         [[0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120]],

         [[0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120]],

         [[0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5100,  ..., 0.5120, 0.5060, 0.5120]]]],
       device='cuda:0')/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([[[[0.5737, 0.5339, 0.5684,  ..., 0.5571, 0.5091, 0.5042],
          [0.5794, 0.5195, 0.4605,  ..., 0.5400, 0.5346, 0.4477],
          [0.5237, 0.5222, 0.4888,  ..., 0.4528, 0.6783, 0.6859],
          [0.5675, 0.4533, 0.3780,  ..., 0.4865, 0.5380, 0.4819]],

         [[0.5590, 0.5894, 0.6081,  ..., 0.5013, 0.5226, 0.6558],
          [0.5453, 0.5784, 0.4839,  ..., 0.4977, 0.4436, 0.5708],
          [0.4801, 0.4003, 0.4140,  ..., 0.4239, 0.5173, 0.5694],
          [0.5436, 0.4752, 0.3675,  ..., 0.6095, 0.4951, 0.5703]],

         [[0.5409, 0.5607, 0.5856,  ..., 0.4083, 0.4760, 0.5703],
          [0.4525, 0.4436, 0.5761,  ..., 0.4340, 0.5317, 0.5079],
          [0.4263, 0.4676, 0.4192,  ..., 0.4717, 0.5722, 0.4321],
          [0.6558, 0.4995, 0.4311,  ..., 0.5027, 0.4644, 0.6220]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0120, 0.0120, 0.0060,
        0.0120], device='cuda:0')
selected experts tensor([ 428,  773,  574,  402,  517,  914, 5759, 5400,  924,  693],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.3534, 0.5585, 0.3953,  ..., 0.4600, 0.4245, 0.4599],
          [0.4866, 0.4834, 0.4906,  ..., 0.5869, 0.5587, 0.5020],
          [0.5327, 0.4282, 0.3349,  ..., 0.6539, 0.5676, 0.6008],
          [0.5101, 0.4368, 0.4693,  ..., 0.5831, 0.4193, 0.4790]],

         [[0.5121, 0.4263, 0.5685,  ..., 0.3855, 0.4640, 0.5291],
          [0.6319, 0.4990, 0.5303,  ..., 0.4943, 0.5856, 0.6106],
          [0.4842, 0.4373, 0.5702,  ..., 0.5677, 0.6677, 0.4123],
          [0.4385, 0.4891, 0.4846,  ..., 0.6785, 0.5007, 0.5218]],

         [[0.5772, 0.4905, 0.3594,  ..., 0.4782, 0.3927, 0.5881],
          [0.5051, 0.6113, 0.3990,  ..., 0.4148, 0.6163, 0.5161],
          [0.3755, 0.6210, 0.5555,  ..., 0.5916, 0.5770, 0.4531],
          [0.4506, 0.4571, 0.5339,  ..., 0.5535, 0.3978, 0.3827]],

         ...,

         [[0.4296, 0.5583, 0.4074,  ..., 0.4031, 0.5497, 0.5734],
          [0.5122, 0.5588, 0.4069,  ..., 0.6170, 0.5741, 0.5839],
          [0.3570, 0.5129, 0.6907,  ..., 0.4501, 0.4532, 0.4982],
          [0.4325, 0.3683, 0.5196,  ..., 0.5902, 0.5165, 0.5701]],

         [[0.5116, 0.6169, 0.6568,  ..., 0.6566, 0.3677, 0.3366],
          [0.3041, 0.6583, 0.5375,  ..., 0.5711, 0.5928, 0.5971],
          [0.5819, 0.5363, 0.6753,  ..., 0.4477, 0.4582, 0.5533],
          [0.4080, 0.4465, 0.5870,  ..., 0.5612, 0.5302, 0.4758]],

         [[0.5772, 0.3692, 0.5941,  ..., 0.6767, 0.4991, 0.5403],
          [0.3676, 0.5457, 0.5676,  ..., 0.5298, 0.3936, 0.5294],
          [0.4008, 0.4590, 0.4272,  ..., 0.6086, 0.4839, 0.4797],
          [0.5194, 0.5789, 0.7066,  ..., 0.2638, 0.4997, 0.5289]]],


        [[[0.5247, 0.3692, 0.3939,  ..., 0.5210, 0.3677, 0.4080],
          [0.5763, 0.4445, 0.5077,  ..., 0.5455, 0.5313, 0.4052],
          [0.3980, 0.4784, 0.4715,  ..., 0.5227, 0.4604, 0.5139],
          [0.6111, 0.4540, 0.4449,  ..., 0.5308, 0.3704, 0.5924]],

         [[0.4305, 0.4215, 0.6415,  ..., 0.4021, 0.3159, 0.5244],
          [0.5847, 0.5741, 0.6087,  ..., 0.4376, 0.4860, 0.5192],
          [0.4339, 0.4646, 0.4577,  ..., 0.5021, 0.4505, 0.4911],
          [0.5534, 0.4586, 0.4135,  ..., 0.5595, 0.6256, 0.4343]],

         [[0.4004, 0.5832, 0.5314,  ..., 0.6672, 0.4174, 0.4739],
          [0.4349, 0.5317, 0.5659,  ..., 0.3818, 0.4254, 0.6446],
          [0.5814, 0.5387, 0.5008,  ..., 0.4712, 0.5536, 0.4849],
          [0.4617, 0.5060, 0.4789,  ..., 0.5065, 0.4674, 0.4937]],

         ...,

         [[0.4577, 0.5104, 0.4918,  ..., 0.4634, 0.3776, 0.4769],
          [0.5725, 0.5462, 0.5497,  ..., 0.3637, 0.5187, 0.5975],
          [0.4123, 0.4363, 0.5153,  ..., 0.5264, 0.6056, 0.5572],
          [0.4371, 0.6108, 0.4325,  ..., 0.5019, 0.5799, 0.5999]],

         [[0.4885, 0.3521, 0.4211,  ..., 0.4333, 0.5302, 0.6401],
          [0.4185, 0.3485, 0.4538,  ..., 0.5179, 0.4071, 0.5533],
          [0.6202, 0.4225, 0.3612,  ..., 0.4928, 0.5090, 0.4290],
          [0.5748, 0.6099, 0.6269,  ..., 0.5511, 0.4435, 0.4758]],

         [[0.6084, 0.5755, 0.4671,  ..., 0.4157, 0.5543, 0.5328],
          [0.5587, 0.5122, 0.3630,  ..., 0.5049, 0.6859, 0.5437],
          [0.5501, 0.5375, 0.6083,  ..., 0.3425, 0.4942, 0.3463],
          [0.4387, 0.4239, 0.5998,  ..., 0.5523, 0.4493, 0.5463]]]],
       device='cuda:0')
tensor([[[[0.3684, 0.5595, 0.3863,  ..., 0.4530, 0.4135, 0.4649],
          [0.5016, 0.4844, 0.4816,  ..., 0.5799, 0.5477, 0.5070],
          [0.5477, 0.4292, 0.3259,  ..., 0.6469, 0.5566, 0.6058],
          [0.5251, 0.4378, 0.4603,  ..., 0.5761, 0.4083, 0.4840]],

         [[0.5271, 0.4273, 0.5595,  ..., 0.3785, 0.4530, 0.5341],
          [0.6469, 0.5000, 0.5213,  ..., 0.4873, 0.5746, 0.6156],
          [0.4992, 0.4383, 0.5612,  ..., 0.5607, 0.6567, 0.4173],
          [0.4535, 0.4901, 0.4756,  ..., 0.6715, 0.4897, 0.5268]],

         [[0.5922, 0.4915, 0.3504,  ..., 0.4712, 0.3817, 0.5931],
          [0.5201, 0.6123, 0.3900,  ..., 0.4078, 0.6053, 0.5211],
          [0.3905, 0.6220, 0.5465,  ..., 0.5846, 0.5660, 0.4581],
          [0.4656, 0.4581, 0.5249,  ..., 0.5465, 0.3868, 0.3877]],

         ...,

         [[0.4446, 0.5593, 0.3984,  ..., 0.3961, 0.5387, 0.5784],
          [0.5272, 0.5598, 0.3979,  ..., 0.6100, 0.5631, 0.5889],
          [0.3720, 0.5139, 0.6817,  ..., 0.4431, 0.4422, 0.5032],
          [0.4475, 0.3693, 0.5106,  ..., 0.5832, 0.5055, 0.5751]],

         [[0.5266, 0.6179, 0.6478,  ..., 0.6496, 0.3567, 0.3416],
          [0.3191, 0.6593, 0.5285,  ..., 0.5641, 0.5818, 0.6021],
          [0.5969, 0.5373, 0.6663,  ..., 0.4407, 0.4472, 0.5583],
          [0.4230, 0.4475, 0.5780,  ..., 0.5542, 0.5192, 0.4808]],

         [[0.5922, 0.3702, 0.5851,  ..., 0.6697, 0.4881, 0.5453],
          [0.3826, 0.5467, 0.5586,  ..., 0.5228, 0.3826, 0.5344],
          [0.4158, 0.4600, 0.4182,  ..., 0.6016, 0.4729, 0.4847],
          [0.5344, 0.5799, 0.6976,  ..., 0.2568, 0.4887, 0.5339]]],


        [[[0.5397, 0.3702, 0.3849,  ..., 0.5140, 0.3567, 0.4130],
          [0.5913, 0.4455, 0.4987,  ..., 0.5385, 0.5203, 0.4102],
          [0.4130, 0.4794, 0.4625,  ..., 0.5157, 0.4494, 0.5189],
          [0.6261, 0.4550, 0.4359,  ..., 0.5238, 0.3594, 0.5974]],

         [[0.4455, 0.4225, 0.6325,  ..., 0.3951, 0.3049, 0.5294],
          [0.5997, 0.5751, 0.5997,  ..., 0.4306, 0.4750, 0.5242],
          [0.4489, 0.4656, 0.4487,  ..., 0.4951, 0.4395, 0.4961],
          [0.5684, 0.4596, 0.4045,  ..., 0.5525, 0.6146, 0.4393]],

         [[0.4154, 0.5842, 0.5224,  ..., 0.6602, 0.4064, 0.4789],
          [0.4499, 0.5327, 0.5569,  ..., 0.3748, 0.4144, 0.6496],
          [0.5964, 0.5397, 0.4918,  ..., 0.4642, 0.5426, 0.4899],
          [0.4767, 0.5070, 0.4699,  ..., 0.4995, 0.4564, 0.4987]],

         ...,

         [[0.4727, 0.5114, 0.4828,  ..., 0.4564, 0.3666, 0.4819],
          [0.5875, 0.5472, 0.5407,  ..., 0.3567, 0.5077, 0.6025],
          [0.4273, 0.4373, 0.5063,  ..., 0.5194, 0.5946, 0.5622],
          [0.4521, 0.6118, 0.4235,  ..., 0.4949, 0.5689, 0.6049]],

         [[0.5035, 0.3531, 0.4121,  ..., 0.4263, 0.5192, 0.6451],
          [0.4335, 0.3495, 0.4448,  ..., 0.5109, 0.3961, 0.5583],
          [0.6352, 0.4235, 0.3522,  ..., 0.4858, 0.4980, 0.4340],
          [0.5898, 0.6109, 0.6179,  ..., 0.5441, 0.4325, 0.4808]],

         [[0.6234, 0.5765, 0.4581,  ..., 0.4087, 0.5433, 0.5378],
          [0.5737, 0.5132, 0.3540,  ..., 0.4979, 0.6749, 0.5487],
          [0.5651, 0.5385, 0.5993,  ..., 0.3355, 0.4832, 0.3513],
          [0.4537, 0.4249, 0.5908,  ..., 0.5453, 0.4383, 0.5513]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0150, -0.0010,  0.0090,  0.0030,  0.0050,  0.0050, -0.0030,  0.0070,
         0.0110, -0.0050], device='cuda:0')
selected experts tensor([1592, 1704, 1727, 1709, 1582, 1576, 1666, 1608, 1604, 1616],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4907, 0.4479, 0.4304,  ..., 0.5022, 0.5337, 0.4660],
          [0.5272, 0.5594, 0.6393,  ..., 0.4286, 0.5082, 0.5309],
          [0.6065, 0.7016, 0.7067,  ..., 0.4917, 0.3298, 0.6164],
          [0.5422, 0.4971, 0.3626,  ..., 0.5592, 0.4778, 0.4818]],

         [[0.3402, 0.5766, 0.5725,  ..., 0.5678, 0.4084, 0.5109],
          [0.3570, 0.6047, 0.4304,  ..., 0.4873, 0.5422, 0.4760],
          [0.5388, 0.4389, 0.5266,  ..., 0.5939, 0.4420, 0.3881],
          [0.5961, 0.3885, 0.4323,  ..., 0.2994, 0.5369, 0.6492]],

         [[0.5620, 0.4974, 0.5566,  ..., 0.3709, 0.4890, 0.5758],
          [0.6481, 0.5169, 0.6537,  ..., 0.4946, 0.4207, 0.3983],
          [0.5219, 0.4743, 0.4987,  ..., 0.3884, 0.5352, 0.4419],
          [0.5049, 0.6625, 0.4963,  ..., 0.5370, 0.5309, 0.4479]],

         ...,

         [[0.6948, 0.4727, 0.6669,  ..., 0.5394, 0.5567, 0.5433],
          [0.4028, 0.5008, 0.3997,  ..., 0.4612, 0.5220, 0.3780],
          [0.3516, 0.3885, 0.4899,  ..., 0.6056, 0.4459, 0.4409],
          [0.4279, 0.4640, 0.5058,  ..., 0.6268, 0.4696, 0.6850]],

         [[0.4609, 0.4928, 0.6311,  ..., 0.5481, 0.4303, 0.4185],
          [0.5177, 0.5176, 0.4119,  ..., 0.2961, 0.3446, 0.3716],
          [0.4802, 0.4154, 0.5650,  ..., 0.5287, 0.4884, 0.5471],
          [0.5167, 0.5307, 0.6061,  ..., 0.4658, 0.4222, 0.4778]],

         [[0.4722, 0.5271, 0.5298,  ..., 0.4925, 0.4500, 0.4204],
          [0.4609, 0.2877, 0.4493,  ..., 0.4910, 0.5582, 0.6968],
          [0.5341, 0.6334, 0.3881,  ..., 0.5220, 0.4613, 0.4716],
          [0.4103, 0.5421, 0.5073,  ..., 0.4488, 0.4606, 0.5768]]],


        [[[0.4179, 0.6005, 0.5910,  ..., 0.6121, 0.6264, 0.6122],
          [0.5466, 0.4040, 0.4062,  ..., 0.3618, 0.5924, 0.4513],
          [0.5229, 0.4197, 0.6402,  ..., 0.4666, 0.5795, 0.3983],
          [0.4005, 0.4961, 0.5058,  ..., 0.6607, 0.5463, 0.3770]],

         [[0.4303, 0.4959, 0.4908,  ..., 0.4658, 0.3705, 0.5037],
          [0.4212, 0.5019, 0.6224,  ..., 0.4072, 0.5555, 0.3770],
          [0.5148, 0.4202, 0.5626,  ..., 0.6139, 0.6282, 0.3707],
          [0.4900, 0.5000, 0.5009,  ..., 0.4238, 0.4575, 0.4561]],

         [[0.3865, 0.6432, 0.3626,  ..., 0.6930, 0.4616, 0.4505],
          [0.4711, 0.5069, 0.5001,  ..., 0.5953, 0.3944, 0.6229],
          [0.4241, 0.5756, 0.4680,  ..., 0.3973, 0.3916, 0.5551],
          [0.5417, 0.5042, 0.5003,  ..., 0.3870, 0.5829, 0.5044]],

         ...,

         [[0.5608, 0.6761, 0.4185,  ..., 0.5972, 0.4989, 0.5168],
          [0.4873, 0.6352, 0.3835,  ..., 0.6250, 0.5495, 0.5498],
          [0.5012, 0.3477, 0.6196,  ..., 0.4836, 0.6300, 0.3207],
          [0.4351, 0.2926, 0.4885,  ..., 0.4856, 0.5386, 0.5653]],

         [[0.4468, 0.4924, 0.3798,  ..., 0.4423, 0.4706, 0.5858],
          [0.4365, 0.4520, 0.4566,  ..., 0.5503, 0.3778, 0.6537],
          [0.5526, 0.4520, 0.3707,  ..., 0.4507, 0.5147, 0.5457],
          [0.3916, 0.6033, 0.4947,  ..., 0.3264, 0.5752, 0.4629]],

         [[0.5329, 0.5336, 0.4081,  ..., 0.7226, 0.5961, 0.5887],
          [0.6032, 0.6093, 0.3770,  ..., 0.5963, 0.3732, 0.5575],
          [0.4810, 0.5954, 0.4227,  ..., 0.4248, 0.3760, 0.4385],
          [0.3879, 0.6079, 0.4648,  ..., 0.5466, 0.5334, 0.5052]]]],
       device='cuda:0')
tensor([[[[0.4877, 0.4569, 0.4254,  ..., 0.5052, 0.5307, 0.4610],
          [0.5242, 0.5684, 0.6343,  ..., 0.4316, 0.5052, 0.5259],
          [0.6035, 0.7106, 0.7017,  ..., 0.4947, 0.3268, 0.6114],
          [0.5392, 0.5061, 0.3576,  ..., 0.5622, 0.4748, 0.4768]],

         [[0.3372, 0.5856, 0.5675,  ..., 0.5708, 0.4054, 0.5059],
          [0.3540, 0.6137, 0.4254,  ..., 0.4903, 0.5392, 0.4710],
          [0.5358, 0.4479, 0.5216,  ..., 0.5969, 0.4390, 0.3831],
          [0.5931, 0.3975, 0.4273,  ..., 0.3024, 0.5339, 0.6442]],

         [[0.5590, 0.5064, 0.5516,  ..., 0.3739, 0.4860, 0.5708],
          [0.6451, 0.5259, 0.6487,  ..., 0.4976, 0.4177, 0.3933],
          [0.5189, 0.4833, 0.4937,  ..., 0.3914, 0.5322, 0.4369],
          [0.5019, 0.6715, 0.4913,  ..., 0.5400, 0.5279, 0.4429]],

         ...,

         [[0.6918, 0.4817, 0.6619,  ..., 0.5424, 0.5537, 0.5383],
          [0.3998, 0.5098, 0.3947,  ..., 0.4642, 0.5190, 0.3730],
          [0.3486, 0.3975, 0.4849,  ..., 0.6086, 0.4429, 0.4359],
          [0.4249, 0.4730, 0.5008,  ..., 0.6298, 0.4666, 0.6800]],

         [[0.4579, 0.5018, 0.6261,  ..., 0.5511, 0.4273, 0.4135],
          [0.5147, 0.5266, 0.4069,  ..., 0.2991, 0.3416, 0.3666],
          [0.4772, 0.4244, 0.5600,  ..., 0.5317, 0.4854, 0.5421],
          [0.5137, 0.5397, 0.6011,  ..., 0.4688, 0.4192, 0.4728]],

         [[0.4692, 0.5361, 0.5248,  ..., 0.4955, 0.4470, 0.4154],
          [0.4579, 0.2967, 0.4443,  ..., 0.4940, 0.5552, 0.6918],
          [0.5311, 0.6424, 0.3831,  ..., 0.5250, 0.4583, 0.4666],
          [0.4073, 0.5511, 0.5023,  ..., 0.4518, 0.4576, 0.5718]]],


        [[[0.4149, 0.6095, 0.5860,  ..., 0.6151, 0.6234, 0.6072],
          [0.5436, 0.4130, 0.4012,  ..., 0.3648, 0.5894, 0.4463],
          [0.5199, 0.4287, 0.6352,  ..., 0.4696, 0.5765, 0.3933],
          [0.3975, 0.5051, 0.5008,  ..., 0.6637, 0.5433, 0.3720]],

         [[0.4273, 0.5049, 0.4858,  ..., 0.4688, 0.3675, 0.4987],
          [0.4182, 0.5109, 0.6174,  ..., 0.4102, 0.5525, 0.3720],
          [0.5118, 0.4292, 0.5576,  ..., 0.6169, 0.6252, 0.3657],
          [0.4870, 0.5090, 0.4959,  ..., 0.4268, 0.4545, 0.4511]],

         [[0.3835, 0.6522, 0.3576,  ..., 0.6960, 0.4586, 0.4455],
          [0.4681, 0.5159, 0.4951,  ..., 0.5983, 0.3914, 0.6179],
          [0.4211, 0.5846, 0.4630,  ..., 0.4003, 0.3886, 0.5501],
          [0.5387, 0.5132, 0.4953,  ..., 0.3900, 0.5799, 0.4994]],

         ...,

         [[0.5578, 0.6851, 0.4135,  ..., 0.6002, 0.4959, 0.5118],
          [0.4843, 0.6442, 0.3785,  ..., 0.6280, 0.5465, 0.5448],
          [0.4982, 0.3567, 0.6146,  ..., 0.4866, 0.6270, 0.3157],
          [0.4321, 0.3016, 0.4835,  ..., 0.4886, 0.5356, 0.5603]],

         [[0.4438, 0.5014, 0.3748,  ..., 0.4453, 0.4676, 0.5808],
          [0.4335, 0.4610, 0.4516,  ..., 0.5533, 0.3748, 0.6487],
          [0.5496, 0.4610, 0.3657,  ..., 0.4537, 0.5117, 0.5407],
          [0.3886, 0.6123, 0.4897,  ..., 0.3294, 0.5722, 0.4579]],

         [[0.5299, 0.5426, 0.4031,  ..., 0.7256, 0.5931, 0.5837],
          [0.6002, 0.6183, 0.3720,  ..., 0.5993, 0.3702, 0.5525],
          [0.4780, 0.6044, 0.4177,  ..., 0.4278, 0.3730, 0.4335],
          [0.3849, 0.6169, 0.4598,  ..., 0.5496, 0.5304, 0.5002]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0030, -0.0090,  0.0050, -0.0010,  0.0010,  0.0050, -0.0050, -0.0030,
         0.0030,  0.0050], device='cuda:0')
selected experts tensor([1595, 1705, 1578, 1626, 1551, 1770, 1657, 1631, 1510, 1761],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6004, 0.4471, 0.4827,  ..., 0.3738, 0.3262, 0.4462],
          [0.4028, 0.5425, 0.4682,  ..., 0.3799, 0.3770, 0.4716],
          [0.5415, 0.5326, 0.5677,  ..., 0.4693, 0.5423, 0.6203],
          [0.5886, 0.4281, 0.5524,  ..., 0.5970, 0.6717, 0.6924]],

         [[0.6436, 0.4612, 0.3689,  ..., 0.4016, 0.3567, 0.3500],
          [0.3687, 0.4343, 0.4655,  ..., 0.3061, 0.4177, 0.5329],
          [0.4570, 0.5807, 0.5610,  ..., 0.4384, 0.5017, 0.4353],
          [0.4490, 0.4209, 0.4369,  ..., 0.5979, 0.5456, 0.7217]],

         [[0.4502, 0.4310, 0.5286,  ..., 0.4752, 0.4741, 0.4742],
          [0.5642, 0.5977, 0.5886,  ..., 0.4198, 0.5128, 0.6149],
          [0.4399, 0.6130, 0.5335,  ..., 0.4706, 0.3797, 0.4990],
          [0.4113, 0.3510, 0.4090,  ..., 0.5114, 0.4412, 0.5191]],

         ...,

         [[0.4303, 0.4959, 0.5677,  ..., 0.4282, 0.3084, 0.4649],
          [0.5093, 0.5230, 0.5620,  ..., 0.4135, 0.4584, 0.4865],
          [0.5463, 0.6286, 0.6138,  ..., 0.6288, 0.4953, 0.4515],
          [0.5857, 0.4495, 0.4657,  ..., 0.5756, 0.6491, 0.4745]],

         [[0.3778, 0.5430, 0.6552,  ..., 0.4357, 0.3950, 0.5360],
          [0.5007, 0.4454, 0.3972,  ..., 0.4622, 0.4070, 0.6086],
          [0.6088, 0.4996, 0.5092,  ..., 0.5047, 0.6237, 0.5074],
          [0.5786, 0.4281, 0.3808,  ..., 0.5765, 0.6335, 0.4473]],

         [[0.4798, 0.4233, 0.4708,  ..., 0.4532, 0.3205, 0.3933],
          [0.4449, 0.4916, 0.6003,  ..., 0.5111, 0.4275, 0.4305],
          [0.5719, 0.6711, 0.6320,  ..., 0.5352, 0.5023, 0.5269],
          [0.5867, 0.4348, 0.3855,  ..., 0.4871, 0.4488, 0.5317]]],


        [[[0.5478, 0.5645, 0.4825,  ..., 0.4690, 0.4158, 0.4879],
          [0.4495, 0.6213, 0.6365,  ..., 0.4193, 0.2798, 0.4802],
          [0.5999, 0.4539, 0.4565,  ..., 0.4908, 0.6185, 0.5739],
          [0.6445, 0.5029, 0.5325,  ..., 0.5270, 0.5579, 0.5944]],

         [[0.4952, 0.4692, 0.3490,  ..., 0.5093, 0.4247, 0.4349],
          [0.4548, 0.4925, 0.5519,  ..., 0.5013, 0.3779, 0.6194],
          [0.4551, 0.4669, 0.4953,  ..., 0.4810, 0.4661, 0.5692],
          [0.4626, 0.3582, 0.4967,  ..., 0.4919, 0.6510, 0.6398]],

         [[0.5606, 0.5536, 0.5019,  ..., 0.4256, 0.3279, 0.4067],
          [0.4932, 0.4152, 0.4741,  ..., 0.5386, 0.4102, 0.5218],
          [0.4047, 0.5716, 0.6742,  ..., 0.6368, 0.5536, 0.5259],
          [0.4866, 0.4214, 0.3878,  ..., 0.6368, 0.3969, 0.5678]],

         ...,

         [[0.5966, 0.4401, 0.4949,  ..., 0.4263, 0.5157, 0.4549],
          [0.6508, 0.5101, 0.4704,  ..., 0.5151, 0.3941, 0.4940],
          [0.5056, 0.5991, 0.6017,  ..., 0.5059, 0.6330, 0.5002],
          [0.4560, 0.4478, 0.4533,  ..., 0.6270, 0.5562, 0.5473]],

         [[0.3916, 0.4590, 0.6193,  ..., 0.4678, 0.5552, 0.5225],
          [0.5121, 0.6051, 0.5038,  ..., 0.6117, 0.4797, 0.3337],
          [0.4575, 0.5099, 0.5117,  ..., 0.3836, 0.7263, 0.4608],
          [0.4616, 0.4924, 0.5749,  ..., 0.6261, 0.5040, 0.4315]],

         [[0.6195, 0.3819, 0.4480,  ..., 0.6108, 0.4787, 0.5668],
          [0.4669, 0.6322, 0.6410,  ..., 0.5014, 0.2403, 0.5047],
          [0.3963, 0.4814, 0.5318,  ..., 0.4860, 0.5267, 0.5612],
          [0.5194, 0.2760, 0.4946,  ..., 0.4333, 0.5588, 0.4898]]]],
       device='cuda:0')
tensor([[[[0.5974, 0.4501, 0.4877,  ..., 0.3928, 0.3032, 0.4692],
          [0.3998, 0.5455, 0.4732,  ..., 0.3989, 0.3540, 0.4946],
          [0.5385, 0.5356, 0.5727,  ..., 0.4883, 0.5193, 0.6433],
          [0.5856, 0.4311, 0.5574,  ..., 0.6160, 0.6487, 0.7154]],

         [[0.6406, 0.4642, 0.3739,  ..., 0.4206, 0.3337, 0.3730],
          [0.3657, 0.4373, 0.4705,  ..., 0.3251, 0.3947, 0.5559],
          [0.4540, 0.5837, 0.5660,  ..., 0.4574, 0.4787, 0.4583],
          [0.4460, 0.4239, 0.4419,  ..., 0.6169, 0.5226, 0.7447]],

         [[0.4472, 0.4340, 0.5336,  ..., 0.4942, 0.4511, 0.4972],
          [0.5612, 0.6007, 0.5936,  ..., 0.4388, 0.4898, 0.6379],
          [0.4369, 0.6160, 0.5385,  ..., 0.4896, 0.3567, 0.5220],
          [0.4083, 0.3540, 0.4140,  ..., 0.5304, 0.4182, 0.5421]],

         ...,

         [[0.4273, 0.4989, 0.5727,  ..., 0.4472, 0.2854, 0.4879],
          [0.5063, 0.5260, 0.5670,  ..., 0.4325, 0.4354, 0.5095],
          [0.5433, 0.6316, 0.6188,  ..., 0.6478, 0.4723, 0.4745],
          [0.5827, 0.4525, 0.4707,  ..., 0.5946, 0.6261, 0.4975]],

         [[0.3748, 0.5460, 0.6602,  ..., 0.4547, 0.3720, 0.5590],
          [0.4977, 0.4484, 0.4022,  ..., 0.4812, 0.3840, 0.6316],
          [0.6058, 0.5026, 0.5142,  ..., 0.5237, 0.6007, 0.5304],
          [0.5756, 0.4311, 0.3858,  ..., 0.5955, 0.6105, 0.4703]],

         [[0.4768, 0.4263, 0.4758,  ..., 0.4722, 0.2975, 0.4163],
          [0.4419, 0.4946, 0.6053,  ..., 0.5301, 0.4045, 0.4535],
          [0.5689, 0.6741, 0.6370,  ..., 0.5542, 0.4793, 0.5499],
          [0.5837, 0.4378, 0.3905,  ..., 0.5061, 0.4258, 0.5547]]],


        [[[0.5448, 0.5675, 0.4875,  ..., 0.4880, 0.3928, 0.5109],
          [0.4465, 0.6243, 0.6415,  ..., 0.4383, 0.2568, 0.5032],
          [0.5969, 0.4569, 0.4615,  ..., 0.5098, 0.5955, 0.5969],
          [0.6415, 0.5059, 0.5375,  ..., 0.5460, 0.5349, 0.6174]],

         [[0.4922, 0.4722, 0.3540,  ..., 0.5283, 0.4017, 0.4579],
          [0.4518, 0.4955, 0.5569,  ..., 0.5203, 0.3549, 0.6424],
          [0.4521, 0.4699, 0.5003,  ..., 0.5000, 0.4431, 0.5922],
          [0.4596, 0.3612, 0.5017,  ..., 0.5109, 0.6280, 0.6628]],

         [[0.5576, 0.5566, 0.5069,  ..., 0.4446, 0.3049, 0.4297],
          [0.4902, 0.4182, 0.4791,  ..., 0.5576, 0.3872, 0.5448],
          [0.4017, 0.5746, 0.6792,  ..., 0.6558, 0.5306, 0.5489],
          [0.4836, 0.4244, 0.3928,  ..., 0.6558, 0.3739, 0.5908]],

         ...,

         [[0.5936, 0.4431, 0.4999,  ..., 0.4453, 0.4927, 0.4779],
          [0.6478, 0.5131, 0.4754,  ..., 0.5341, 0.3711, 0.5170],
          [0.5026, 0.6021, 0.6067,  ..., 0.5249, 0.6100, 0.5232],
          [0.4530, 0.4508, 0.4583,  ..., 0.6460, 0.5332, 0.5703]],

         [[0.3886, 0.4620, 0.6243,  ..., 0.4868, 0.5322, 0.5455],
          [0.5091, 0.6081, 0.5088,  ..., 0.6307, 0.4567, 0.3567],
          [0.4545, 0.5129, 0.5167,  ..., 0.4026, 0.7033, 0.4838],
          [0.4586, 0.4954, 0.5799,  ..., 0.6451, 0.4810, 0.4545]],

         [[0.6165, 0.3849, 0.4530,  ..., 0.6298, 0.4557, 0.5898],
          [0.4639, 0.6352, 0.6460,  ..., 0.5204, 0.2173, 0.5277],
          [0.3933, 0.4844, 0.5368,  ..., 0.5050, 0.5037, 0.5842],
          [0.5164, 0.2790, 0.4996,  ..., 0.4523, 0.5358, 0.5128]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0030, -0.0030, -0.0050,  0.0050, -0.0010, -0.0110,  0.0070, -0.0190,
         0.0230, -0.0230], device='cuda:0')
selected experts tensor([1355, 1528, 1584, 1790, 1368, 2171, 1420, 2376,  828, 1964],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.5875, 0.5970, 0.6344,  ..., 0.4515, 0.6095, 0.5222],
          [0.5686, 0.4795, 0.4005,  ..., 0.6013, 0.6295, 0.4869],
          [0.4915, 0.5206, 0.6168,  ..., 0.4831, 0.5821, 0.5127],
          [0.5856, 0.5201, 0.4679,  ..., 0.6056, 0.4429, 0.6202]],

         [[0.6694, 0.5001, 0.6362,  ..., 0.4548, 0.4734, 0.4903],
          [0.5847, 0.5054, 0.4340,  ..., 0.5186, 0.6654, 0.4585],
          [0.4888, 0.4364, 0.4805,  ..., 0.4979, 0.6216, 0.6295],
          [0.5406, 0.5025, 0.4507,  ..., 0.4701, 0.5552, 0.5161]],

         [[0.5594, 0.5354, 0.6205,  ..., 0.4928, 0.6715, 0.4740],
          [0.5461, 0.5128, 0.4146,  ..., 0.5033, 0.3745, 0.4508],
          [0.4672, 0.5358, 0.4684,  ..., 0.4817, 0.5000, 0.4826],
          [0.4819, 0.5952, 0.4250,  ..., 0.6041, 0.4995, 0.5126]],

         ...,

         [[0.5110, 0.5110, 0.5110,  ..., 0.5110, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5110, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5110, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5110, 0.5070, 0.5130]],

         [[0.5110, 0.5110, 0.5110,  ..., 0.5110, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5110, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5110, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5110, 0.5070, 0.5130]],

         [[0.5110, 0.5110, 0.5110,  ..., 0.5110, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5110, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5110, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5110, 0.5070, 0.5130]]],


        [[[0.5785, 0.7152, 0.6325,  ..., 0.4958, 0.5269, 0.4743],
          [0.5425, 0.4019, 0.4882,  ..., 0.5267, 0.5381, 0.4890],
          [0.4993, 0.4411, 0.4655,  ..., 0.5284, 0.6887, 0.5447],
          [0.5828, 0.6859, 0.3284,  ..., 0.5543, 0.5149, 0.6391]],

         [[0.4388, 0.6677, 0.6842,  ..., 0.5446, 0.5749, 0.5311],
          [0.5717, 0.5543, 0.4585,  ..., 0.5339, 0.6244, 0.5629],
          [0.6498, 0.4207, 0.5476,  ..., 0.4715, 0.5840, 0.6090],
          [0.5770, 0.5594, 0.5238,  ..., 0.6677, 0.4333, 0.5481]],

         [[0.3785, 0.5429, 0.6893,  ..., 0.3978, 0.5318, 0.3706],
          [0.4150, 0.5582, 0.4959,  ..., 0.4749, 0.4781, 0.4523],
          [0.4146, 0.6023, 0.3812,  ..., 0.4136, 0.6485, 0.2866],
          [0.6117, 0.5244, 0.4010,  ..., 0.5414, 0.5533, 0.6801]],

         ...,

         [[0.5110, 0.5110, 0.5110,  ..., 0.5110, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5110, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5110, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5110, 0.5070, 0.5130]],

         [[0.5110, 0.5110, 0.5110,  ..., 0.5110, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5110, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5110, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5110, 0.5070, 0.5130]],

         [[0.5110, 0.5110, 0.5110,  ..., 0.5110, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5110, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5110, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5110, 0.5070, 0.5130]]]],
       device='cuda:0')
tensor([[[[0.5765, 0.5860, 0.6234,  ..., 0.4405, 0.6025, 0.5092],
          [0.5576, 0.4685, 0.3895,  ..., 0.5903, 0.6225, 0.4739],
          [0.4805, 0.5096, 0.6058,  ..., 0.4721, 0.5751, 0.4997],
          [0.5746, 0.5091, 0.4569,  ..., 0.5946, 0.4359, 0.6072]],

         [[0.6584, 0.4891, 0.6252,  ..., 0.4438, 0.4664, 0.4773],
          [0.5737, 0.4944, 0.4230,  ..., 0.5076, 0.6584, 0.4455],
          [0.4778, 0.4254, 0.4695,  ..., 0.4869, 0.6146, 0.6165],
          [0.5296, 0.4915, 0.4397,  ..., 0.4591, 0.5482, 0.5031]],

         [[0.5484, 0.5244, 0.6095,  ..., 0.4818, 0.6645, 0.4610],
          [0.5351, 0.5018, 0.4036,  ..., 0.4923, 0.3675, 0.4378],
          [0.4562, 0.5248, 0.4574,  ..., 0.4707, 0.4930, 0.4696],
          [0.4709, 0.5842, 0.4140,  ..., 0.5931, 0.4925, 0.4996]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5675, 0.7042, 0.6215,  ..., 0.4848, 0.5199, 0.4613],
          [0.5315, 0.3909, 0.4772,  ..., 0.5157, 0.5311, 0.4760],
          [0.4883, 0.4301, 0.4545,  ..., 0.5174, 0.6817, 0.5317],
          [0.5718, 0.6749, 0.3174,  ..., 0.5433, 0.5079, 0.6261]],

         [[0.4278, 0.6567, 0.6732,  ..., 0.5336, 0.5679, 0.5181],
          [0.5607, 0.5433, 0.4475,  ..., 0.5229, 0.6174, 0.5499],
          [0.6388, 0.4097, 0.5366,  ..., 0.4605, 0.5770, 0.5960],
          [0.5660, 0.5484, 0.5128,  ..., 0.6567, 0.4263, 0.5351]],

         [[0.3675, 0.5319, 0.6783,  ..., 0.3868, 0.5248, 0.3576],
          [0.4040, 0.5472, 0.4849,  ..., 0.4639, 0.4711, 0.4393],
          [0.4036, 0.5913, 0.3702,  ..., 0.4026, 0.6415, 0.2736],
          [0.6007, 0.5134, 0.3900,  ..., 0.5304, 0.5463, 0.6671]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0070,
        0.0130], device='cuda:0')
selected experts tensor([1665, 1271, 1073,  651,  947, 1755, 1318,  476, 1573, 1559],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.3320, 0.5094, 0.5429,  ..., 0.6396, 0.6713, 0.5251],
          [0.5383, 0.4823, 0.5576,  ..., 0.5212, 0.5587, 0.3608],
          [0.5001, 0.5845, 0.5850,  ..., 0.5201, 0.5583, 0.5204],
          [0.5960, 0.6686, 0.4780,  ..., 0.4632, 0.4747, 0.4977]],

         [[0.5021, 0.4162, 0.5201,  ..., 0.5584, 0.4350, 0.5010],
          [0.6728, 0.4148, 0.4567,  ..., 0.4617, 0.4701, 0.5205],
          [0.6048, 0.2755, 0.5983,  ..., 0.5494, 0.4786, 0.4697],
          [0.6002, 0.5527, 0.4073,  ..., 0.4329, 0.4378, 0.5579]],

         [[0.5132, 0.5150, 0.6002,  ..., 0.5576, 0.5890, 0.4870],
          [0.6505, 0.3737, 0.4954,  ..., 0.5199, 0.6571, 0.4653],
          [0.5739, 0.3431, 0.5150,  ..., 0.5265, 0.6553, 0.4341],
          [0.4912, 0.6806, 0.4449,  ..., 0.4739, 0.5016, 0.4493]],

         ...,

         [[0.6660, 0.6599, 0.5475,  ..., 0.5331, 0.4701, 0.5456],
          [0.5602, 0.4687, 0.4663,  ..., 0.3994, 0.4534, 0.5849],
          [0.4033, 0.6404, 0.4215,  ..., 0.5301, 0.5962, 0.3879],
          [0.5482, 0.7544, 0.5993,  ..., 0.6360, 0.3705, 0.4439]],

         [[0.3952, 0.5469, 0.7541,  ..., 0.5470, 0.5374, 0.3897],
          [0.5616, 0.5377, 0.5189,  ..., 0.4338, 0.4493, 0.4911],
          [0.4628, 0.6191, 0.4528,  ..., 0.3883, 0.6257, 0.2967],
          [0.3718, 0.4903, 0.6468,  ..., 0.6846, 0.6895, 0.3617]],

         [[0.5472, 0.4534, 0.4434,  ..., 0.4642, 0.4955, 0.6447],
          [0.5201, 0.4215, 0.3994,  ..., 0.5521, 0.4020, 0.5234],
          [0.6085, 0.4101, 0.6468,  ..., 0.5248, 0.6098, 0.5976],
          [0.3400, 0.5291, 0.4050,  ..., 0.4840, 0.5686, 0.5490]]],


        [[[0.3599, 0.4876, 0.5661,  ..., 0.5993, 0.5866, 0.4166],
          [0.4052, 0.4534, 0.5057,  ..., 0.5596, 0.4846, 0.5221],
          [0.6427, 0.6564, 0.4562,  ..., 0.5228, 0.4682, 0.2774],
          [0.5838, 0.3610, 0.6082,  ..., 0.4914, 0.5061, 0.5920]],

         [[0.4128, 0.5736, 0.4945,  ..., 0.4824, 0.3868, 0.6078],
          [0.3436, 0.5907, 0.5997,  ..., 0.5699, 0.5343, 0.5639],
          [0.4368, 0.5549, 0.4940,  ..., 0.5513, 0.5943, 0.6276],
          [0.3472, 0.3370, 0.5433,  ..., 0.5441, 0.5282, 0.2830]],

         [[0.5352, 0.4938, 0.5740,  ..., 0.5945, 0.5050, 0.5338],
          [0.6557, 0.6136, 0.4502,  ..., 0.4272, 0.4412, 0.5854],
          [0.4133, 0.3673, 0.5130,  ..., 0.4952, 0.5771, 0.6936],
          [0.2984, 0.4034, 0.4458,  ..., 0.5064, 0.4826, 0.3599]],

         ...,

         [[0.4604, 0.5090, 0.4912,  ..., 0.4642, 0.3795, 0.4796],
          [0.5735, 0.5452, 0.5489,  ..., 0.3647, 0.5196, 0.5985],
          [0.4138, 0.4353, 0.5141,  ..., 0.5273, 0.6066, 0.5582],
          [0.4381, 0.6098, 0.4315,  ..., 0.5029, 0.5809, 0.6009]],

         [[0.5071, 0.3778, 0.5661,  ..., 0.5226, 0.4864, 0.3925],
          [0.4018, 0.4025, 0.5016,  ..., 0.5143, 0.6695, 0.5550],
          [0.5342, 0.5264, 0.5574,  ..., 0.4470, 0.4813, 0.5692],
          [0.5189, 0.4696, 0.4353,  ..., 0.6829, 0.5362, 0.4836]],

         [[0.3714, 0.5326, 0.3957,  ..., 0.5482, 0.5466, 0.6465],
          [0.5000, 0.5869, 0.4434,  ..., 0.6558, 0.6946, 0.4350],
          [0.3051, 0.3484, 0.6629,  ..., 0.5368, 0.4861, 0.5079],
          [0.3985, 0.4646, 0.4036,  ..., 0.5651, 0.5587, 0.4872]]]],
       device='cuda:0')
tensor([[[[0.3460, 0.5114, 0.5349,  ..., 0.6316, 0.6593, 0.5291],
          [0.5523, 0.4843, 0.5496,  ..., 0.5132, 0.5467, 0.3648],
          [0.5141, 0.5865, 0.5770,  ..., 0.5121, 0.5463, 0.5244],
          [0.6100, 0.6706, 0.4700,  ..., 0.4552, 0.4627, 0.5017]],

         [[0.5161, 0.4182, 0.5121,  ..., 0.5504, 0.4230, 0.5050],
          [0.6868, 0.4168, 0.4487,  ..., 0.4537, 0.4581, 0.5245],
          [0.6188, 0.2775, 0.5903,  ..., 0.5414, 0.4666, 0.4737],
          [0.6142, 0.5547, 0.3993,  ..., 0.4249, 0.4258, 0.5619]],

         [[0.5272, 0.5170, 0.5922,  ..., 0.5496, 0.5770, 0.4910],
          [0.6645, 0.3757, 0.4874,  ..., 0.5119, 0.6451, 0.4693],
          [0.5879, 0.3451, 0.5070,  ..., 0.5185, 0.6433, 0.4381],
          [0.5052, 0.6826, 0.4369,  ..., 0.4659, 0.4896, 0.4533]],

         ...,

         [[0.6800, 0.6619, 0.5395,  ..., 0.5251, 0.4581, 0.5496],
          [0.5742, 0.4707, 0.4583,  ..., 0.3914, 0.4414, 0.5889],
          [0.4173, 0.6424, 0.4135,  ..., 0.5221, 0.5842, 0.3919],
          [0.5622, 0.7564, 0.5913,  ..., 0.6280, 0.3585, 0.4479]],

         [[0.4092, 0.5489, 0.7461,  ..., 0.5390, 0.5254, 0.3937],
          [0.5756, 0.5397, 0.5109,  ..., 0.4258, 0.4373, 0.4951],
          [0.4768, 0.6211, 0.4448,  ..., 0.3803, 0.6137, 0.3007],
          [0.3858, 0.4923, 0.6388,  ..., 0.6766, 0.6775, 0.3657]],

         [[0.5612, 0.4554, 0.4354,  ..., 0.4562, 0.4835, 0.6487],
          [0.5341, 0.4235, 0.3914,  ..., 0.5441, 0.3900, 0.5274],
          [0.6225, 0.4121, 0.6388,  ..., 0.5168, 0.5978, 0.6016],
          [0.3540, 0.5311, 0.3970,  ..., 0.4760, 0.5566, 0.5530]]],


        [[[0.3739, 0.4896, 0.5581,  ..., 0.5913, 0.5746, 0.4206],
          [0.4192, 0.4554, 0.4977,  ..., 0.5516, 0.4726, 0.5261],
          [0.6567, 0.6584, 0.4482,  ..., 0.5148, 0.4562, 0.2814],
          [0.5978, 0.3630, 0.6002,  ..., 0.4834, 0.4941, 0.5960]],

         [[0.4268, 0.5756, 0.4865,  ..., 0.4744, 0.3748, 0.6118],
          [0.3576, 0.5927, 0.5917,  ..., 0.5619, 0.5223, 0.5679],
          [0.4508, 0.5569, 0.4860,  ..., 0.5433, 0.5823, 0.6316],
          [0.3612, 0.3390, 0.5353,  ..., 0.5361, 0.5162, 0.2870]],

         [[0.5492, 0.4958, 0.5660,  ..., 0.5865, 0.4930, 0.5378],
          [0.6697, 0.6156, 0.4422,  ..., 0.4192, 0.4292, 0.5894],
          [0.4273, 0.3693, 0.5050,  ..., 0.4872, 0.5651, 0.6976],
          [0.3124, 0.4054, 0.4378,  ..., 0.4984, 0.4706, 0.3639]],

         ...,

         [[0.4744, 0.5110, 0.4832,  ..., 0.4562, 0.3675, 0.4836],
          [0.5875, 0.5472, 0.5409,  ..., 0.3567, 0.5076, 0.6025],
          [0.4278, 0.4373, 0.5061,  ..., 0.5193, 0.5946, 0.5622],
          [0.4521, 0.6118, 0.4235,  ..., 0.4949, 0.5689, 0.6049]],

         [[0.5211, 0.3798, 0.5581,  ..., 0.5146, 0.4744, 0.3965],
          [0.4158, 0.4045, 0.4936,  ..., 0.5063, 0.6575, 0.5590],
          [0.5482, 0.5284, 0.5494,  ..., 0.4390, 0.4693, 0.5732],
          [0.5329, 0.4716, 0.4273,  ..., 0.6749, 0.5242, 0.4876]],

         [[0.3854, 0.5346, 0.3877,  ..., 0.5402, 0.5346, 0.6505],
          [0.5140, 0.5889, 0.4354,  ..., 0.6478, 0.6826, 0.4390],
          [0.3191, 0.3504, 0.6549,  ..., 0.5288, 0.4741, 0.5119],
          [0.4125, 0.4666, 0.3956,  ..., 0.5571, 0.5467, 0.4912]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0140, -0.0020,  0.0080,  0.0020,  0.0060,  0.0060, -0.0040,  0.0080,
         0.0120, -0.0040], device='cuda:0')
selected experts tensor([1651, 1588, 1704, 1558, 1552, 1608, 1715, 1699, 1626, 1683],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4318, 0.4568, 0.3868,  ..., 0.5817, 0.4104, 0.4260],
          [0.5364, 0.3503, 0.4016,  ..., 0.4421, 0.4524, 0.2854],
          [0.4944, 0.5779, 0.5949,  ..., 0.4167, 0.4010, 0.5886],
          [0.4561, 0.5836, 0.3159,  ..., 0.2127, 0.5539, 0.4161]],

         [[0.5257, 0.4192, 0.4752,  ..., 0.6287, 0.6177, 0.6014],
          [0.4332, 0.6369, 0.4190,  ..., 0.3755, 0.4847, 0.6182],
          [0.6051, 0.5142, 0.4477,  ..., 0.5140, 0.4818, 0.5162],
          [0.7146, 0.5115, 0.7751,  ..., 0.5297, 0.5229, 0.4582]],

         [[0.4418, 0.5416, 0.5189,  ..., 0.5455, 0.3829, 0.4836],
          [0.6112, 0.2982, 0.3354,  ..., 0.5474, 0.4967, 0.4873],
          [0.7210, 0.4863, 0.4721,  ..., 0.4503, 0.4510, 0.6455],
          [0.4322, 0.5760, 0.5337,  ..., 0.5902, 0.6703, 0.4876]],

         ...,

         [[0.5929, 0.6252, 0.4728,  ..., 0.5683, 0.5015, 0.4628],
          [0.4969, 0.4601, 0.4333,  ..., 0.6341, 0.4289, 0.3838],
          [0.4565, 0.7006, 0.4445,  ..., 0.4842, 0.4490, 0.5710],
          [0.5767, 0.5145, 0.3886,  ..., 0.4758, 0.5483, 0.4322]],

         [[0.3334, 0.4168, 0.3762,  ..., 0.4091, 0.3760, 0.6140],
          [0.4677, 0.4544, 0.5481,  ..., 0.5100, 0.6014, 0.6028],
          [0.6126, 0.6125, 0.6015,  ..., 0.4831, 0.5710, 0.3815],
          [0.5573, 0.6207, 0.5854,  ..., 0.5544, 0.5753, 0.4385]],

         [[0.3848, 0.5440, 0.6430,  ..., 0.3774, 0.3447, 0.5829],
          [0.4341, 0.4125, 0.5213,  ..., 0.5014, 0.3871, 0.5222],
          [0.3369, 0.4855, 0.4414,  ..., 0.4566, 0.4284, 0.5459],
          [0.4919, 0.4314, 0.6635,  ..., 0.4571, 0.4905, 0.5551]]],


        [[[0.4980, 0.5536, 0.2540,  ..., 0.5247, 0.4964, 0.4728],
          [0.6018, 0.3211, 0.5622,  ..., 0.4810, 0.4507, 0.6392],
          [0.4968, 0.3861, 0.4096,  ..., 0.5836, 0.5715, 0.4071],
          [0.4104, 0.5855, 0.4285,  ..., 0.6205, 0.5929, 0.5269]],

         [[0.5059, 0.5855, 0.5778,  ..., 0.5450, 0.6562, 0.5490],
          [0.4985, 0.6571, 0.4946,  ..., 0.5445, 0.5158, 0.5929],
          [0.5123, 0.5157, 0.5920,  ..., 0.5607, 0.3580, 0.5372],
          [0.5398, 0.4187, 0.4077,  ..., 0.4658, 0.3806, 0.3829]],

         [[0.4047, 0.4505, 0.5039,  ..., 0.3104, 0.4878, 0.5234],
          [0.6047, 0.4810, 0.4124,  ..., 0.4593, 0.5782, 0.3223],
          [0.3788, 0.3950, 0.4993,  ..., 0.4554, 0.4684, 0.5301],
          [0.3903, 0.3772, 0.6466,  ..., 0.4622, 0.3797, 0.5255]],

         ...,

         [[0.5623, 0.6751, 0.4190,  ..., 0.5968, 0.4997, 0.5157],
          [0.4881, 0.6342, 0.3845,  ..., 0.6260, 0.5505, 0.5486],
          [0.5024, 0.3467, 0.6206,  ..., 0.4846, 0.6310, 0.3197],
          [0.4365, 0.2916, 0.4896,  ..., 0.4866, 0.5396, 0.5643]],

         [[0.4534, 0.4425, 0.4518,  ..., 0.4578, 0.4839, 0.4645],
          [0.5805, 0.5622, 0.5506,  ..., 0.3945, 0.5256, 0.5981],
          [0.5541, 0.3884, 0.5706,  ..., 0.5314, 0.6214, 0.4029],
          [0.5743, 0.5627, 0.5530,  ..., 0.4775, 0.4841, 0.5919]],

         [[0.5418, 0.3386, 0.4790,  ..., 0.4651, 0.5943, 0.3954],
          [0.5423, 0.3041, 0.5754,  ..., 0.5015, 0.4019, 0.5034],
          [0.4029, 0.6032, 0.4453,  ..., 0.3737, 0.4736, 0.5210],
          [0.5427, 0.5841, 0.5825,  ..., 0.5365, 0.4785, 0.5858]]]],
       device='cuda:0')
tensor([[[[0.4278, 0.4668, 0.3808,  ..., 0.5837, 0.4064, 0.4220],
          [0.5324, 0.3603, 0.3956,  ..., 0.4441, 0.4484, 0.2814],
          [0.4904, 0.5879, 0.5889,  ..., 0.4187, 0.3970, 0.5846],
          [0.4521, 0.5936, 0.3099,  ..., 0.2147, 0.5499, 0.4121]],

         [[0.5217, 0.4292, 0.4692,  ..., 0.6307, 0.6137, 0.5974],
          [0.4292, 0.6469, 0.4130,  ..., 0.3775, 0.4807, 0.6142],
          [0.6011, 0.5242, 0.4417,  ..., 0.5160, 0.4778, 0.5122],
          [0.7106, 0.5215, 0.7691,  ..., 0.5317, 0.5189, 0.4542]],

         [[0.4378, 0.5516, 0.5129,  ..., 0.5475, 0.3789, 0.4796],
          [0.6072, 0.3082, 0.3294,  ..., 0.5494, 0.4927, 0.4833],
          [0.7170, 0.4963, 0.4661,  ..., 0.4523, 0.4470, 0.6415],
          [0.4282, 0.5860, 0.5277,  ..., 0.5922, 0.6663, 0.4836]],

         ...,

         [[0.5889, 0.6352, 0.4668,  ..., 0.5703, 0.4975, 0.4588],
          [0.4929, 0.4701, 0.4273,  ..., 0.6361, 0.4249, 0.3798],
          [0.4525, 0.7106, 0.4385,  ..., 0.4862, 0.4450, 0.5670],
          [0.5727, 0.5245, 0.3826,  ..., 0.4778, 0.5443, 0.4282]],

         [[0.3294, 0.4268, 0.3702,  ..., 0.4111, 0.3720, 0.6100],
          [0.4637, 0.4644, 0.5421,  ..., 0.5120, 0.5974, 0.5988],
          [0.6086, 0.6225, 0.5955,  ..., 0.4851, 0.5670, 0.3775],
          [0.5533, 0.6307, 0.5794,  ..., 0.5564, 0.5713, 0.4345]],

         [[0.3808, 0.5540, 0.6370,  ..., 0.3794, 0.3407, 0.5789],
          [0.4301, 0.4225, 0.5153,  ..., 0.5034, 0.3831, 0.5182],
          [0.3329, 0.4955, 0.4354,  ..., 0.4586, 0.4244, 0.5419],
          [0.4879, 0.4414, 0.6575,  ..., 0.4591, 0.4865, 0.5511]]],


        [[[0.4940, 0.5636, 0.2480,  ..., 0.5267, 0.4924, 0.4688],
          [0.5978, 0.3311, 0.5562,  ..., 0.4830, 0.4467, 0.6352],
          [0.4928, 0.3961, 0.4036,  ..., 0.5856, 0.5675, 0.4031],
          [0.4064, 0.5955, 0.4225,  ..., 0.6225, 0.5889, 0.5229]],

         [[0.5019, 0.5955, 0.5718,  ..., 0.5470, 0.6522, 0.5450],
          [0.4945, 0.6671, 0.4886,  ..., 0.5465, 0.5118, 0.5889],
          [0.5083, 0.5257, 0.5860,  ..., 0.5627, 0.3540, 0.5332],
          [0.5358, 0.4287, 0.4017,  ..., 0.4678, 0.3766, 0.3789]],

         [[0.4007, 0.4605, 0.4979,  ..., 0.3124, 0.4838, 0.5194],
          [0.6007, 0.4910, 0.4064,  ..., 0.4613, 0.5742, 0.3183],
          [0.3748, 0.4050, 0.4933,  ..., 0.4574, 0.4644, 0.5261],
          [0.3863, 0.3872, 0.6406,  ..., 0.4642, 0.3757, 0.5215]],

         ...,

         [[0.5583, 0.6851, 0.4130,  ..., 0.5988, 0.4957, 0.5117],
          [0.4841, 0.6442, 0.3785,  ..., 0.6280, 0.5465, 0.5446],
          [0.4984, 0.3567, 0.6146,  ..., 0.4866, 0.6270, 0.3157],
          [0.4325, 0.3016, 0.4836,  ..., 0.4886, 0.5356, 0.5603]],

         [[0.4494, 0.4525, 0.4458,  ..., 0.4598, 0.4799, 0.4605],
          [0.5765, 0.5722, 0.5446,  ..., 0.3965, 0.5216, 0.5941],
          [0.5501, 0.3984, 0.5646,  ..., 0.5334, 0.6174, 0.3989],
          [0.5703, 0.5727, 0.5470,  ..., 0.4795, 0.4801, 0.5879]],

         [[0.5378, 0.3486, 0.4730,  ..., 0.4671, 0.5903, 0.3914],
          [0.5383, 0.3141, 0.5694,  ..., 0.5035, 0.3979, 0.4994],
          [0.3989, 0.6132, 0.4393,  ..., 0.3757, 0.4696, 0.5170],
          [0.5387, 0.5941, 0.5765,  ..., 0.5385, 0.4745, 0.5818]]]],
       device='cuda:0', requires_grad=True)
tensor([ 4.0000e-03, -1.0000e-02,  6.0000e-03,  2.3283e-10,  2.0000e-03,
         4.0000e-03, -6.0000e-03, -2.0000e-03,  4.0000e-03,  4.0000e-03],
       device='cuda:0')
selected experts tensor([1644, 1551, 1808, 1708, 1571, 1594, 1646, 1723, 1660, 1479],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4802, 0.4853, 0.4100,  ..., 0.3944, 0.3816, 0.4704],
          [0.4906, 0.4428, 0.4466,  ..., 0.4169, 0.4257, 0.4728],
          [0.5317, 0.5267, 0.6175,  ..., 0.5484, 0.5758, 0.5706],
          [0.4590, 0.4162, 0.4180,  ..., 0.6034, 0.4177, 0.6202]],

         [[0.4546, 0.4520, 0.5294,  ..., 0.4111, 0.3673, 0.5184],
          [0.5599, 0.3538, 0.6330,  ..., 0.2974, 0.5093, 0.5283],
          [0.3589, 0.5539, 0.5478,  ..., 0.4738, 0.4080, 0.5980],
          [0.3973, 0.2716, 0.4826,  ..., 0.7276, 0.5324, 0.6628]],

         [[0.6191, 0.5017, 0.5032,  ..., 0.4669, 0.3215, 0.4363],
          [0.6145, 0.4072, 0.5577,  ..., 0.4504, 0.4819, 0.4906],
          [0.3509, 0.4673, 0.5797,  ..., 0.5052, 0.5584, 0.4955],
          [0.5135, 0.4129, 0.3245,  ..., 0.4724, 0.4262, 0.5324]],

         ...,

         [[0.4047, 0.4631, 0.3958,  ..., 0.4978, 0.3551, 0.5762],
          [0.5221, 0.4720, 0.6152,  ..., 0.5760, 0.5327, 0.4572],
          [0.4198, 0.4210, 0.6171,  ..., 0.4840, 0.5640, 0.3399],
          [0.6365, 0.3792, 0.4733,  ..., 0.4981, 0.5525, 0.5411]],

         [[0.4194, 0.4224, 0.5906,  ..., 0.5040, 0.4346, 0.4645],
          [0.6079, 0.6341, 0.3245,  ..., 0.5350, 0.4580, 0.4733],
          [0.6075, 0.5274, 0.5502,  ..., 0.5684, 0.5131, 0.5208],
          [0.4901, 0.5968, 0.6570,  ..., 0.4622, 0.5458, 0.4071]],

         [[0.4609, 0.3769, 0.4964,  ..., 0.5627, 0.4122, 0.4915],
          [0.5359, 0.5179, 0.6004,  ..., 0.5932, 0.3314, 0.3971],
          [0.3779, 0.4745, 0.6106,  ..., 0.4837, 0.5250, 0.5317],
          [0.5333, 0.2677, 0.4739,  ..., 0.4753, 0.4271, 0.5420]]],


        [[[0.5283, 0.4784, 0.5102,  ..., 0.5168, 0.5195, 0.4531],
          [0.5676, 0.5476, 0.5406,  ..., 0.5694, 0.3718, 0.6220],
          [0.4418, 0.5231, 0.6078,  ..., 0.6419, 0.5794, 0.4411],
          [0.4587, 0.3829, 0.4047,  ..., 0.4819, 0.5739, 0.5425]],

         [[0.3973, 0.4597, 0.5001,  ..., 0.4333, 0.4112, 0.5473],
          [0.4711, 0.3866, 0.5236,  ..., 0.4907, 0.3834, 0.4145],
          [0.5495, 0.5774, 0.5981,  ..., 0.4383, 0.5777, 0.5635],
          [0.5568, 0.4949, 0.4377,  ..., 0.4506, 0.6529, 0.6300]],

         [[0.6329, 0.4612, 0.4281,  ..., 0.3878, 0.3735, 0.4850],
          [0.5263, 0.5635, 0.4100,  ..., 0.3484, 0.3726, 0.4150],
          [0.4445, 0.5803, 0.5211,  ..., 0.4804, 0.5080, 0.5401],
          [0.5667, 0.4172, 0.4633,  ..., 0.6743, 0.4580, 0.5174]],

         ...,

         [[0.5493, 0.5262, 0.5369,  ..., 0.4374, 0.5895, 0.5239],
          [0.5398, 0.4120, 0.4176,  ..., 0.4918, 0.5531, 0.6457],
          [0.5486, 0.3737, 0.5934,  ..., 0.5180, 0.4635, 0.4675],
          [0.3806, 0.4397, 0.5560,  ..., 0.4732, 0.5190, 0.5047]],

         [[0.5384, 0.5566, 0.4394,  ..., 0.4718, 0.4522, 0.3628],
          [0.4573, 0.4404, 0.6023,  ..., 0.4490, 0.4408, 0.5724],
          [0.3589, 0.3931, 0.5374,  ..., 0.4532, 0.3682, 0.6552],
          [0.5056, 0.3335, 0.4742,  ..., 0.5788, 0.5574, 0.4988]],

         [[0.5469, 0.5020, 0.6570,  ..., 0.6305, 0.3118, 0.4719],
          [0.5348, 0.5336, 0.3991,  ..., 0.5156, 0.3150, 0.5167],
          [0.5585, 0.5455, 0.5026,  ..., 0.4284, 0.4896, 0.4884],
          [0.5135, 0.3565, 0.5369,  ..., 0.4301, 0.6048, 0.5011]]]],
       device='cuda:0')
tensor([[[[0.4762, 0.4873, 0.4140,  ..., 0.4144, 0.3576, 0.4944],
          [0.4866, 0.4448, 0.4506,  ..., 0.4369, 0.4017, 0.4968],
          [0.5277, 0.5287, 0.6215,  ..., 0.5684, 0.5518, 0.5946],
          [0.4550, 0.4182, 0.4220,  ..., 0.6234, 0.3937, 0.6442]],

         [[0.4506, 0.4540, 0.5334,  ..., 0.4311, 0.3433, 0.5424],
          [0.5559, 0.3558, 0.6370,  ..., 0.3174, 0.4853, 0.5523],
          [0.3549, 0.5559, 0.5518,  ..., 0.4938, 0.3840, 0.6220],
          [0.3933, 0.2736, 0.4866,  ..., 0.7476, 0.5084, 0.6868]],

         [[0.6151, 0.5037, 0.5072,  ..., 0.4869, 0.2975, 0.4603],
          [0.6105, 0.4092, 0.5617,  ..., 0.4704, 0.4579, 0.5146],
          [0.3469, 0.4693, 0.5837,  ..., 0.5252, 0.5344, 0.5195],
          [0.5095, 0.4149, 0.3285,  ..., 0.4924, 0.4022, 0.5564]],

         ...,

         [[0.4007, 0.4651, 0.3998,  ..., 0.5178, 0.3311, 0.6002],
          [0.5181, 0.4740, 0.6192,  ..., 0.5960, 0.5087, 0.4812],
          [0.4158, 0.4230, 0.6211,  ..., 0.5040, 0.5400, 0.3639],
          [0.6325, 0.3812, 0.4773,  ..., 0.5181, 0.5285, 0.5651]],

         [[0.4154, 0.4244, 0.5946,  ..., 0.5240, 0.4106, 0.4885],
          [0.6039, 0.6361, 0.3285,  ..., 0.5550, 0.4340, 0.4973],
          [0.6035, 0.5294, 0.5542,  ..., 0.5884, 0.4891, 0.5448],
          [0.4861, 0.5988, 0.6610,  ..., 0.4822, 0.5218, 0.4311]],

         [[0.4569, 0.3789, 0.5004,  ..., 0.5827, 0.3882, 0.5155],
          [0.5319, 0.5199, 0.6044,  ..., 0.6132, 0.3074, 0.4211],
          [0.3739, 0.4765, 0.6146,  ..., 0.5037, 0.5010, 0.5557],
          [0.5293, 0.2697, 0.4779,  ..., 0.4953, 0.4031, 0.5660]]],


        [[[0.5243, 0.4804, 0.5142,  ..., 0.5368, 0.4955, 0.4771],
          [0.5636, 0.5496, 0.5446,  ..., 0.5894, 0.3478, 0.6460],
          [0.4378, 0.5251, 0.6118,  ..., 0.6619, 0.5554, 0.4651],
          [0.4547, 0.3849, 0.4087,  ..., 0.5019, 0.5499, 0.5665]],

         [[0.3933, 0.4617, 0.5041,  ..., 0.4533, 0.3872, 0.5713],
          [0.4671, 0.3886, 0.5276,  ..., 0.5107, 0.3594, 0.4385],
          [0.5455, 0.5794, 0.6021,  ..., 0.4583, 0.5537, 0.5875],
          [0.5528, 0.4969, 0.4417,  ..., 0.4706, 0.6289, 0.6540]],

         [[0.6289, 0.4632, 0.4321,  ..., 0.4078, 0.3495, 0.5090],
          [0.5223, 0.5655, 0.4140,  ..., 0.3684, 0.3486, 0.4390],
          [0.4405, 0.5823, 0.5251,  ..., 0.5004, 0.4840, 0.5641],
          [0.5627, 0.4192, 0.4673,  ..., 0.6943, 0.4340, 0.5414]],

         ...,

         [[0.5453, 0.5282, 0.5409,  ..., 0.4574, 0.5655, 0.5479],
          [0.5358, 0.4140, 0.4216,  ..., 0.5118, 0.5291, 0.6697],
          [0.5446, 0.3757, 0.5974,  ..., 0.5380, 0.4395, 0.4915],
          [0.3766, 0.4417, 0.5600,  ..., 0.4932, 0.4950, 0.5287]],

         [[0.5344, 0.5586, 0.4434,  ..., 0.4918, 0.4282, 0.3868],
          [0.4533, 0.4424, 0.6063,  ..., 0.4690, 0.4168, 0.5964],
          [0.3549, 0.3951, 0.5414,  ..., 0.4732, 0.3442, 0.6792],
          [0.5016, 0.3355, 0.4782,  ..., 0.5988, 0.5334, 0.5228]],

         [[0.5429, 0.5040, 0.6610,  ..., 0.6505, 0.2878, 0.4959],
          [0.5308, 0.5356, 0.4031,  ..., 0.5356, 0.2910, 0.5407],
          [0.5545, 0.5475, 0.5066,  ..., 0.4484, 0.4656, 0.5124],
          [0.5095, 0.3585, 0.5409,  ..., 0.4501, 0.5808, 0.5251]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([ 4.0000e-03, -2.0000e-03, -4.0000e-03,  4.0000e-03,  2.3283e-10,
        -1.2000e-02,  8.0000e-03, -2.0000e-02,  2.4000e-02, -2.4000e-02],
       device='cuda:0')
selected experts tensor([1755, 1275, 1934, 1327, 1625, 1976, 1587, 2224,  828, 1853],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120]],

         [[0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120]],

         [[0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120]],

         ...,

         [[0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120]],

         [[0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120]],

         [[0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120]]],


        [[[0.4708, 0.6370, 0.7415,  ..., 0.4537, 0.4871, 0.5251],
          [0.4558, 0.4282, 0.4269,  ..., 0.4587, 0.5076, 0.4937],
          [0.4330, 0.4720, 0.4670,  ..., 0.4813, 0.5973, 0.5162],
          [0.6352, 0.4868, 0.5030,  ..., 0.6381, 0.4984, 0.5626]],

         [[0.5461, 0.5659, 0.5447,  ..., 0.3562, 0.6312, 0.4757],
          [0.5705, 0.6361, 0.4011,  ..., 0.5174, 0.4525, 0.3696],
          [0.4268, 0.4918, 0.4388,  ..., 0.5532, 0.5925, 0.6183],
          [0.5288, 0.6569, 0.4336,  ..., 0.4913, 0.5825, 0.5333]],

         [[0.5487, 0.6265, 0.6971,  ..., 0.4179, 0.5230, 0.5522],
          [0.5813, 0.5842, 0.5503,  ..., 0.5387, 0.6809, 0.5624],
          [0.4122, 0.4633, 0.4165,  ..., 0.4179, 0.6234, 0.5999],
          [0.5370, 0.5053, 0.4599,  ..., 0.7186, 0.5778, 0.4530]],

         ...,

         [[0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120]],

         [[0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120]],

         [[0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120],
          [0.5100, 0.5100, 0.5120,  ..., 0.5120, 0.5060, 0.5120]]]],
       device='cuda:0')
tensor([[[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.4608, 0.6270, 0.7295,  ..., 0.4417, 0.4811, 0.5131],
          [0.4458, 0.4182, 0.4149,  ..., 0.4467, 0.5016, 0.4817],
          [0.4230, 0.4620, 0.4550,  ..., 0.4693, 0.5913, 0.5042],
          [0.6252, 0.4768, 0.4910,  ..., 0.6261, 0.4924, 0.5506]],

         [[0.5361, 0.5559, 0.5327,  ..., 0.3442, 0.6252, 0.4637],
          [0.5605, 0.6261, 0.3891,  ..., 0.5054, 0.4465, 0.3576],
          [0.4168, 0.4818, 0.4268,  ..., 0.5412, 0.5865, 0.6063],
          [0.5188, 0.6469, 0.4216,  ..., 0.4793, 0.5765, 0.5213]],

         [[0.5387, 0.6165, 0.6851,  ..., 0.4059, 0.5170, 0.5402],
          [0.5713, 0.5742, 0.5383,  ..., 0.5267, 0.6749, 0.5504],
          [0.4022, 0.4533, 0.4045,  ..., 0.4059, 0.6174, 0.5879],
          [0.5270, 0.4953, 0.4479,  ..., 0.7066, 0.5718, 0.4410]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0100, 0.0100, 0.0120, 0.0120, 0.0120, 0.0100, 0.0100, 0.0120, 0.0060,
        0.0120], device='cuda:0')
selected experts tensor([ 381,  605, 4041, 3849,  504,  562,  772,  334,  872,  368],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6328, 0.5423, 0.4172,  ..., 0.4985, 0.5245, 0.4978],
          [0.4696, 0.3459, 0.4869,  ..., 0.5304, 0.6108, 0.4631],
          [0.4147, 0.4654, 0.6105,  ..., 0.5249, 0.6104, 0.5143],
          [0.5796, 0.5213, 0.3772,  ..., 0.5121, 0.4767, 0.3124]],

         [[0.4199, 0.5171, 0.6016,  ..., 0.5482, 0.3381, 0.4507],
          [0.4883, 0.4890, 0.4661,  ..., 0.5117, 0.4317, 0.3790],
          [0.4305, 0.5097, 0.3574,  ..., 0.5759, 0.4128, 0.3878],
          [0.5861, 0.5200, 0.4489,  ..., 0.7038, 0.5910, 0.5162]],

         [[0.5899, 0.5583, 0.3574,  ..., 0.3718, 0.5150, 0.3670],
          [0.5045, 0.4016, 0.6619,  ..., 0.4542, 0.4730, 0.3049],
          [0.6337, 0.4893, 0.5484,  ..., 0.4910, 0.5554, 0.4319],
          [0.5838, 0.3811, 0.5540,  ..., 0.4641, 0.5938, 0.4754]],

         ...,

         [[0.5233, 0.5294, 0.4210,  ..., 0.3781, 0.4451, 0.5567],
          [0.4872, 0.4263, 0.6530,  ..., 0.5394, 0.5742, 0.5150],
          [0.5654, 0.4996, 0.7200,  ..., 0.5195, 0.4655, 0.5245],
          [0.4298, 0.6618, 0.3521,  ..., 0.5826, 0.6309, 0.3698]],

         [[0.4937, 0.5540, 0.5316,  ..., 0.4811, 0.4806, 0.4266],
          [0.5269, 0.5207, 0.6449,  ..., 0.6359, 0.4853, 0.3771],
          [0.3727, 0.4186, 0.5778,  ..., 0.4610, 0.5087, 0.6106],
          [0.3990, 0.5319, 0.5288,  ..., 0.6253, 0.6428, 0.4623]],

         [[0.4821, 0.5674, 0.5045,  ..., 0.5783, 0.4293, 0.5157],
          [0.6292, 0.4296, 0.5453,  ..., 0.5634, 0.5042, 0.5067],
          [0.3708, 0.3485, 0.5126,  ..., 0.5663, 0.5544, 0.4094],
          [0.4303, 0.4311, 0.5845,  ..., 0.4484, 0.5289, 0.5910]]],


        [[[0.5383, 0.5554, 0.5537,  ..., 0.5115, 0.5070, 0.3049],
          [0.6102, 0.4808, 0.6165,  ..., 0.3495, 0.4241, 0.4099],
          [0.3966, 0.3638, 0.4810,  ..., 0.4721, 0.4819, 0.6211],
          [0.5012, 0.5201, 0.4743,  ..., 0.6610, 0.4578, 0.5881]],

         [[0.6659, 0.6627, 0.5467,  ..., 0.5320, 0.4711, 0.5451],
          [0.5592, 0.4696, 0.4656,  ..., 0.3984, 0.4544, 0.5844],
          [0.4023, 0.6414, 0.4200,  ..., 0.5288, 0.5972, 0.3864],
          [0.5472, 0.7554, 0.5978,  ..., 0.6350, 0.3715, 0.4432]],

         [[0.4291, 0.4440, 0.5525,  ..., 0.4352, 0.5578, 0.6161],
          [0.4099, 0.2997, 0.3878,  ..., 0.4092, 0.4307, 0.3099],
          [0.5383, 0.6034, 0.5547,  ..., 0.5656, 0.5795, 0.3962],
          [0.3685, 0.4986, 0.4415,  ..., 0.3077, 0.5273, 0.3383]],

         ...,

         [[0.5606, 0.4059, 0.4726,  ..., 0.4300, 0.4217, 0.5801],
          [0.4650, 0.4443, 0.5764,  ..., 0.5421, 0.6141, 0.4170],
          [0.3834, 0.4375, 0.4186,  ..., 0.5423, 0.4494, 0.3716],
          [0.4446, 0.4928, 0.4914,  ..., 0.5335, 0.4508, 0.4762]],

         [[0.6328, 0.5365, 0.5106,  ..., 0.4721, 0.5953, 0.5555],
          [0.6547, 0.3047, 0.4176,  ..., 0.3619, 0.6391, 0.5260],
          [0.5303, 0.5416, 0.5583,  ..., 0.4501, 0.6705, 0.5773],
          [0.5337, 0.6933, 0.4992,  ..., 0.3442, 0.5208, 0.4170]],

         [[0.5982, 0.5808, 0.4620,  ..., 0.3745, 0.5438, 0.3366],
          [0.3952, 0.5693, 0.6212,  ..., 0.4386, 0.3170, 0.5162],
          [0.3787, 0.6025, 0.4717,  ..., 0.5935, 0.6350, 0.3481],
          [0.5491, 0.5822, 0.5864,  ..., 0.5591, 0.5924, 0.4970]]]],
       device='cuda:0')
tensor([[[[0.6478, 0.5433, 0.4102,  ..., 0.4915, 0.5115, 0.5028],
          [0.4846, 0.3469, 0.4799,  ..., 0.5234, 0.5978, 0.4681],
          [0.4297, 0.4664, 0.6035,  ..., 0.5179, 0.5974, 0.5193],
          [0.5946, 0.5223, 0.3702,  ..., 0.5051, 0.4637, 0.3174]],

         [[0.4349, 0.5181, 0.5946,  ..., 0.5412, 0.3251, 0.4557],
          [0.5033, 0.4900, 0.4591,  ..., 0.5047, 0.4187, 0.3840],
          [0.4455, 0.5107, 0.3504,  ..., 0.5689, 0.3998, 0.3928],
          [0.6011, 0.5210, 0.4419,  ..., 0.6968, 0.5780, 0.5212]],

         [[0.6049, 0.5593, 0.3504,  ..., 0.3648, 0.5020, 0.3720],
          [0.5195, 0.4026, 0.6549,  ..., 0.4472, 0.4600, 0.3099],
          [0.6487, 0.4903, 0.5414,  ..., 0.4840, 0.5424, 0.4369],
          [0.5988, 0.3821, 0.5470,  ..., 0.4571, 0.5808, 0.4804]],

         ...,

         [[0.5383, 0.5304, 0.4140,  ..., 0.3711, 0.4321, 0.5617],
          [0.5022, 0.4273, 0.6460,  ..., 0.5324, 0.5612, 0.5200],
          [0.5804, 0.5006, 0.7130,  ..., 0.5125, 0.4525, 0.5295],
          [0.4448, 0.6628, 0.3451,  ..., 0.5756, 0.6179, 0.3748]],

         [[0.5087, 0.5550, 0.5246,  ..., 0.4741, 0.4676, 0.4316],
          [0.5419, 0.5217, 0.6379,  ..., 0.6289, 0.4723, 0.3821],
          [0.3877, 0.4196, 0.5708,  ..., 0.4540, 0.4957, 0.6156],
          [0.4140, 0.5329, 0.5218,  ..., 0.6183, 0.6298, 0.4673]],

         [[0.4971, 0.5684, 0.4975,  ..., 0.5713, 0.4163, 0.5207],
          [0.6442, 0.4306, 0.5383,  ..., 0.5564, 0.4912, 0.5117],
          [0.3858, 0.3495, 0.5056,  ..., 0.5593, 0.5414, 0.4144],
          [0.4453, 0.4321, 0.5775,  ..., 0.4414, 0.5159, 0.5960]]],


        [[[0.5533, 0.5564, 0.5467,  ..., 0.5045, 0.4940, 0.3099],
          [0.6252, 0.4818, 0.6095,  ..., 0.3425, 0.4111, 0.4149],
          [0.4116, 0.3648, 0.4740,  ..., 0.4651, 0.4689, 0.6261],
          [0.5162, 0.5211, 0.4673,  ..., 0.6540, 0.4448, 0.5931]],

         [[0.6809, 0.6637, 0.5397,  ..., 0.5250, 0.4581, 0.5501],
          [0.5742, 0.4706, 0.4586,  ..., 0.3914, 0.4414, 0.5894],
          [0.4173, 0.6424, 0.4130,  ..., 0.5218, 0.5842, 0.3914],
          [0.5622, 0.7564, 0.5908,  ..., 0.6280, 0.3585, 0.4482]],

         [[0.4441, 0.4450, 0.5455,  ..., 0.4282, 0.5448, 0.6211],
          [0.4249, 0.3007, 0.3808,  ..., 0.4022, 0.4177, 0.3149],
          [0.5533, 0.6044, 0.5477,  ..., 0.5586, 0.5665, 0.4012],
          [0.3835, 0.4996, 0.4345,  ..., 0.3007, 0.5143, 0.3433]],

         ...,

         [[0.5756, 0.4069, 0.4656,  ..., 0.4230, 0.4087, 0.5851],
          [0.4800, 0.4453, 0.5694,  ..., 0.5351, 0.6011, 0.4220],
          [0.3984, 0.4385, 0.4116,  ..., 0.5353, 0.4364, 0.3766],
          [0.4596, 0.4938, 0.4844,  ..., 0.5265, 0.4378, 0.4812]],

         [[0.6478, 0.5375, 0.5036,  ..., 0.4651, 0.5823, 0.5605],
          [0.6697, 0.3057, 0.4106,  ..., 0.3549, 0.6261, 0.5310],
          [0.5453, 0.5426, 0.5513,  ..., 0.4431, 0.6575, 0.5823],
          [0.5487, 0.6943, 0.4922,  ..., 0.3372, 0.5078, 0.4220]],

         [[0.6132, 0.5818, 0.4550,  ..., 0.3675, 0.5308, 0.3416],
          [0.4102, 0.5703, 0.6142,  ..., 0.4316, 0.3040, 0.5212],
          [0.3937, 0.6035, 0.4647,  ..., 0.5865, 0.6220, 0.3531],
          [0.5641, 0.5832, 0.5794,  ..., 0.5521, 0.5794, 0.5020]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0150, -0.0010,  0.0070,  0.0030,  0.0070,  0.0070, -0.0050,  0.0070,
         0.0130, -0.0050], device='cuda:0')
selected experts tensor([1565, 1596, 1634, 1662, 1597, 1680, 1522, 1693, 1788, 1647],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5317, 0.5520, 0.5312,  ..., 0.4401, 0.6889, 0.4313],
          [0.4459, 0.6477, 0.6617,  ..., 0.3681, 0.3615, 0.4171],
          [0.5748, 0.4984, 0.3780,  ..., 0.6598, 0.5142, 0.4766],
          [0.4056, 0.4755, 0.7406,  ..., 0.5911, 0.5079, 0.5038]],

         [[0.4444, 0.4130, 0.5285,  ..., 0.4286, 0.4875, 0.4496],
          [0.4941, 0.4293, 0.5089,  ..., 0.4822, 0.4246, 0.4817],
          [0.5251, 0.5973, 0.7236,  ..., 0.4295, 0.4399, 0.3644],
          [0.4236, 0.3903, 0.5710,  ..., 0.4186, 0.4825, 0.3816]],

         [[0.5008, 0.3731, 0.5107,  ..., 0.5345, 0.5408, 0.3876],
          [0.3204, 0.6042, 0.4592,  ..., 0.3600, 0.6055, 0.3881],
          [0.5132, 0.5656, 0.3707,  ..., 0.4491, 0.6041, 0.5072],
          [0.4887, 0.4637, 0.5539,  ..., 0.4693, 0.3597, 0.4086]],

         ...,

         [[0.6060, 0.5358, 0.5566,  ..., 0.3127, 0.5147, 0.6168],
          [0.5534, 0.6253, 0.6000,  ..., 0.5498, 0.5182, 0.4409],
          [0.3874, 0.4341, 0.6581,  ..., 0.4491, 0.5724, 0.4770],
          [0.5092, 0.5661, 0.5701,  ..., 0.5563, 0.5135, 0.5184]],

         [[0.3925, 0.3239, 0.5614,  ..., 0.2800, 0.5020, 0.4629],
          [0.6364, 0.6280, 0.6456,  ..., 0.3672, 0.5971, 0.5290],
          [0.4298, 0.5443, 0.5097,  ..., 0.4699, 0.6032, 0.3680],
          [0.3732, 0.4926, 0.6187,  ..., 0.4948, 0.5463, 0.4527]],

         [[0.6337, 0.5121, 0.5428,  ..., 0.6598, 0.4824, 0.3335],
          [0.5098, 0.4603, 0.3216,  ..., 0.6181, 0.2868, 0.6420],
          [0.6291, 0.4801, 0.5033,  ..., 0.5015, 0.5814, 0.6713],
          [0.6597, 0.5515, 0.4072,  ..., 0.4532, 0.5349, 0.5777]]],


        [[[0.3769, 0.3941, 0.6238,  ..., 0.5712, 0.3851, 0.5806],
          [0.6856, 0.3477, 0.3258,  ..., 0.4995, 0.4279, 0.4903],
          [0.5786, 0.4202, 0.5214,  ..., 0.5210, 0.4990, 0.3644],
          [0.4957, 0.4554, 0.5500,  ..., 0.6014, 0.5267, 0.5131]],

         [[0.5909, 0.6280, 0.4721,  ..., 0.5673, 0.5010, 0.4638],
          [0.4957, 0.4611, 0.4323,  ..., 0.6331, 0.4279, 0.3848],
          [0.4558, 0.7016, 0.4438,  ..., 0.4832, 0.4480, 0.5720],
          [0.5757, 0.5155, 0.3876,  ..., 0.4747, 0.5473, 0.4332]],

         [[0.3939, 0.5666, 0.4810,  ..., 0.6598, 0.5070, 0.5238],
          [0.3981, 0.3468, 0.5640,  ..., 0.3238, 0.6079, 0.5111],
          [0.5909, 0.5789, 0.3752,  ..., 0.4365, 0.5459, 0.5271],
          [0.5928, 0.5718, 0.4624,  ..., 0.4416, 0.5886, 0.5834]],

         ...,

         [[0.5592, 0.5561, 0.4488,  ..., 0.2729, 0.5165, 0.6038],
          [0.5393, 0.3486, 0.5834,  ..., 0.4024, 0.6517, 0.4896],
          [0.5752, 0.4537, 0.4067,  ..., 0.4286, 0.4127, 0.5107],
          [0.4679, 0.5968, 0.4629,  ..., 0.4987, 0.5647, 0.6721]],

         [[0.5032, 0.5154, 0.2856,  ..., 0.4786, 0.4379, 0.6357],
          [0.4954, 0.4477, 0.5345,  ..., 0.6492, 0.4394, 0.4409],
          [0.5354, 0.4692, 0.4655,  ..., 0.5769, 0.4284, 0.5792],
          [0.4718, 0.5038, 0.4328,  ..., 0.4927, 0.4711, 0.4347]],

         [[0.5158, 0.3076, 0.5084,  ..., 0.2583, 0.3402, 0.4580],
          [0.4500, 0.4930, 0.5749,  ..., 0.4440, 0.4312, 0.6173],
          [0.4969, 0.4695, 0.4001,  ..., 0.5553, 0.4170, 0.6284],
          [0.4724, 0.3955, 0.6005,  ..., 0.3600, 0.5661, 0.5739]]]],
       device='cuda:0')
tensor([[[[0.5287, 0.5610, 0.5262,  ..., 0.4431, 0.6859, 0.4263],
          [0.4429, 0.6567, 0.6567,  ..., 0.3711, 0.3585, 0.4121],
          [0.5718, 0.5074, 0.3730,  ..., 0.6628, 0.5112, 0.4716],
          [0.4026, 0.4845, 0.7356,  ..., 0.5941, 0.5049, 0.4988]],

         [[0.4414, 0.4220, 0.5235,  ..., 0.4316, 0.4845, 0.4446],
          [0.4911, 0.4383, 0.5039,  ..., 0.4852, 0.4216, 0.4767],
          [0.5221, 0.6063, 0.7186,  ..., 0.4325, 0.4369, 0.3594],
          [0.4206, 0.3993, 0.5660,  ..., 0.4216, 0.4795, 0.3766]],

         [[0.4978, 0.3821, 0.5057,  ..., 0.5375, 0.5378, 0.3826],
          [0.3174, 0.6132, 0.4542,  ..., 0.3630, 0.6025, 0.3831],
          [0.5102, 0.5746, 0.3657,  ..., 0.4521, 0.6011, 0.5022],
          [0.4857, 0.4727, 0.5489,  ..., 0.4723, 0.3567, 0.4036]],

         ...,

         [[0.6030, 0.5448, 0.5516,  ..., 0.3157, 0.5117, 0.6118],
          [0.5504, 0.6343, 0.5950,  ..., 0.5528, 0.5152, 0.4359],
          [0.3844, 0.4431, 0.6531,  ..., 0.4521, 0.5694, 0.4720],
          [0.5062, 0.5751, 0.5651,  ..., 0.5593, 0.5105, 0.5134]],

         [[0.3895, 0.3329, 0.5564,  ..., 0.2830, 0.4990, 0.4579],
          [0.6334, 0.6370, 0.6406,  ..., 0.3702, 0.5941, 0.5240],
          [0.4268, 0.5533, 0.5047,  ..., 0.4729, 0.6002, 0.3630],
          [0.3702, 0.5016, 0.6137,  ..., 0.4978, 0.5433, 0.4477]],

         [[0.6307, 0.5211, 0.5378,  ..., 0.6628, 0.4794, 0.3285],
          [0.5068, 0.4693, 0.3166,  ..., 0.6211, 0.2838, 0.6370],
          [0.6261, 0.4891, 0.4983,  ..., 0.5045, 0.5784, 0.6663],
          [0.6567, 0.5605, 0.4022,  ..., 0.4562, 0.5319, 0.5727]]],


        [[[0.3739, 0.4031, 0.6188,  ..., 0.5742, 0.3821, 0.5756],
          [0.6826, 0.3567, 0.3208,  ..., 0.5025, 0.4249, 0.4853],
          [0.5756, 0.4292, 0.5164,  ..., 0.5240, 0.4960, 0.3594],
          [0.4927, 0.4644, 0.5450,  ..., 0.6044, 0.5237, 0.5081]],

         [[0.5879, 0.6370, 0.4671,  ..., 0.5703, 0.4980, 0.4588],
          [0.4927, 0.4701, 0.4273,  ..., 0.6361, 0.4249, 0.3798],
          [0.4528, 0.7106, 0.4388,  ..., 0.4862, 0.4450, 0.5670],
          [0.5727, 0.5245, 0.3826,  ..., 0.4777, 0.5443, 0.4282]],

         [[0.3909, 0.5756, 0.4760,  ..., 0.6628, 0.5040, 0.5188],
          [0.3951, 0.3558, 0.5590,  ..., 0.3268, 0.6049, 0.5061],
          [0.5879, 0.5879, 0.3702,  ..., 0.4395, 0.5429, 0.5221],
          [0.5898, 0.5808, 0.4574,  ..., 0.4446, 0.5856, 0.5784]],

         ...,

         [[0.5562, 0.5651, 0.4438,  ..., 0.2759, 0.5135, 0.5988],
          [0.5363, 0.3576, 0.5784,  ..., 0.4054, 0.6487, 0.4846],
          [0.5722, 0.4627, 0.4017,  ..., 0.4316, 0.4097, 0.5057],
          [0.4649, 0.6058, 0.4579,  ..., 0.5017, 0.5617, 0.6671]],

         [[0.5002, 0.5244, 0.2806,  ..., 0.4816, 0.4349, 0.6307],
          [0.4924, 0.4567, 0.5295,  ..., 0.6522, 0.4364, 0.4359],
          [0.5324, 0.4782, 0.4605,  ..., 0.5799, 0.4254, 0.5742],
          [0.4688, 0.5128, 0.4278,  ..., 0.4957, 0.4681, 0.4297]],

         [[0.5128, 0.3166, 0.5034,  ..., 0.2613, 0.3372, 0.4530],
          [0.4470, 0.5020, 0.5699,  ..., 0.4470, 0.4282, 0.6123],
          [0.4939, 0.4785, 0.3951,  ..., 0.5583, 0.4140, 0.6234],
          [0.4694, 0.4045, 0.5955,  ..., 0.3630, 0.5631, 0.5689]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/activation_monitor.py:211: UserWarning: Value passed to ActivationMonitor is not a torch.Tensor, skipping.
  warnings.warn(

tensor([ 0.0030, -0.0090,  0.0050, -0.0010,  0.0030,  0.0050, -0.0070, -0.0030,
         0.0030,  0.0050], device='cuda:0')
selected experts tensor([1663, 1549, 1635, 1637, 1700, 1588, 1584, 1652, 1697, 1679],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4870, 0.5964, 0.4606,  ..., 0.5522, 0.4220, 0.4285],
          [0.4871, 0.4268, 0.4599,  ..., 0.4053, 0.4108, 0.5217],
          [0.6121, 0.6071, 0.5209,  ..., 0.6357, 0.5266, 0.4746],
          [0.5976, 0.5022, 0.4381,  ..., 0.5532, 0.5558, 0.5345]],

         [[0.5685, 0.6159, 0.6239,  ..., 0.5450, 0.3989, 0.5154],
          [0.3898, 0.5469, 0.5166,  ..., 0.4373, 0.3168, 0.3980],
          [0.5239, 0.4807, 0.5272,  ..., 0.4728, 0.5476, 0.5743],
          [0.6158, 0.4311, 0.3776,  ..., 0.5579, 0.6317, 0.5794]],

         [[0.4604, 0.5129, 0.4166,  ..., 0.4257, 0.4797, 0.5217],
          [0.5235, 0.5097, 0.4628,  ..., 0.4547, 0.3989, 0.5377],
          [0.5594, 0.5289, 0.4865,  ..., 0.6001, 0.4551, 0.4939],
          [0.5800, 0.3530, 0.3410,  ..., 0.4702, 0.5295, 0.5496]],

         ...,

         [[0.5267, 0.3620, 0.5497,  ..., 0.4053, 0.5437, 0.3087],
          [0.4606, 0.4811, 0.5586,  ..., 0.2797, 0.2730, 0.4451],
          [0.4708, 0.4924, 0.5649,  ..., 0.4330, 0.5785, 0.5610],
          [0.3723, 0.4818, 0.5096,  ..., 0.5829, 0.5313, 0.3748]],

         [[0.3669, 0.3450, 0.4814,  ..., 0.3774, 0.4094, 0.4213],
          [0.5990, 0.4506, 0.4266,  ..., 0.3906, 0.4693, 0.4414],
          [0.5009, 0.5626, 0.5492,  ..., 0.4908, 0.5778, 0.4750],
          [0.4418, 0.4101, 0.5207,  ..., 0.4925, 0.3754, 0.4912]],

         [[0.5307, 0.4087, 0.5458,  ..., 0.3330, 0.4104, 0.4692],
          [0.5724, 0.5007, 0.4811,  ..., 0.4539, 0.3570, 0.3480],
          [0.4493, 0.4201, 0.5543,  ..., 0.4149, 0.4923, 0.4336],
          [0.3916, 0.4598, 0.5672,  ..., 0.6133, 0.4427, 0.5831]]],


        [[[0.5900, 0.4746, 0.4966,  ..., 0.3774, 0.4385, 0.4128],
          [0.4803, 0.5155, 0.4504,  ..., 0.3492, 0.3382, 0.4786],
          [0.4212, 0.4111, 0.4958,  ..., 0.4308, 0.4848, 0.4764],
          [0.5007, 0.4605, 0.5398,  ..., 0.6616, 0.4901, 0.4505]],

         [[0.5666, 0.5370, 0.3915,  ..., 0.4912, 0.4229, 0.3762],
          [0.6097, 0.4325, 0.5512,  ..., 0.4111, 0.3736, 0.4859],
          [0.4394, 0.4695, 0.5961,  ..., 0.5721, 0.6163, 0.4681],
          [0.5810, 0.4144, 0.2748,  ..., 0.6582, 0.3605, 0.5285]],

         [[0.4274, 0.4419, 0.3962,  ..., 0.5209, 0.3382, 0.4839],
          [0.5357, 0.3415, 0.6793,  ..., 0.4661, 0.5848, 0.4727],
          [0.4317, 0.6450, 0.6639,  ..., 0.5378, 0.5905, 0.5501],
          [0.4360, 0.4359, 0.3906,  ..., 0.5279, 0.4390, 0.5610]],

         ...,

         [[0.4858, 0.5907, 0.4251,  ..., 0.4665, 0.3701, 0.5001],
          [0.4136, 0.5440, 0.6428,  ..., 0.3963, 0.5217, 0.5013],
          [0.5275, 0.5617, 0.5677,  ..., 0.5041, 0.4225, 0.4535],
          [0.6364, 0.3969, 0.2647,  ..., 0.5721, 0.5260, 0.5353]],

         [[0.5671, 0.4153, 0.5258,  ..., 0.3520, 0.5001, 0.4474],
          [0.4914, 0.4656, 0.5749,  ..., 0.3930, 0.4155, 0.3590],
          [0.4888, 0.5879, 0.5853,  ..., 0.5134, 0.4352, 0.3245],
          [0.6310, 0.4979, 0.4127,  ..., 0.5153, 0.5773, 0.4438]],

         [[0.4394, 0.4632, 0.3883,  ..., 0.4631, 0.4742, 0.5892],
          [0.5538, 0.6324, 0.5758,  ..., 0.4639, 0.3898, 0.4935],
          [0.4207, 0.5898, 0.6050,  ..., 0.5707, 0.5857, 0.4995],
          [0.5087, 0.3638, 0.5035,  ..., 0.5436, 0.4961, 0.4300]]]],
       device='cuda:0')
tensor([[[[0.4840, 0.5974, 0.4656,  ..., 0.5732, 0.3970, 0.4535],
          [0.4841, 0.4278, 0.4649,  ..., 0.4263, 0.3858, 0.5467],
          [0.6091, 0.6081, 0.5259,  ..., 0.6567, 0.5016, 0.4996],
          [0.5946, 0.5032, 0.4431,  ..., 0.5742, 0.5308, 0.5595]],

         [[0.5655, 0.6169, 0.6289,  ..., 0.5660, 0.3739, 0.5404],
          [0.3868, 0.5479, 0.5216,  ..., 0.4583, 0.2918, 0.4230],
          [0.5209, 0.4817, 0.5322,  ..., 0.4938, 0.5226, 0.5993],
          [0.6128, 0.4321, 0.3826,  ..., 0.5789, 0.6067, 0.6044]],

         [[0.4574, 0.5139, 0.4216,  ..., 0.4467, 0.4547, 0.5467],
          [0.5205, 0.5107, 0.4678,  ..., 0.4757, 0.3739, 0.5627],
          [0.5564, 0.5299, 0.4915,  ..., 0.6211, 0.4301, 0.5189],
          [0.5770, 0.3540, 0.3460,  ..., 0.4912, 0.5045, 0.5746]],

         ...,

         [[0.5237, 0.3630, 0.5547,  ..., 0.4263, 0.5187, 0.3337],
          [0.4576, 0.4821, 0.5636,  ..., 0.3007, 0.2480, 0.4701],
          [0.4678, 0.4934, 0.5699,  ..., 0.4540, 0.5535, 0.5860],
          [0.3693, 0.4828, 0.5146,  ..., 0.6039, 0.5063, 0.3998]],

         [[0.3639, 0.3460, 0.4864,  ..., 0.3984, 0.3844, 0.4463],
          [0.5960, 0.4516, 0.4316,  ..., 0.4116, 0.4443, 0.4664],
          [0.4979, 0.5636, 0.5542,  ..., 0.5118, 0.5528, 0.5000],
          [0.4388, 0.4111, 0.5257,  ..., 0.5135, 0.3504, 0.5162]],

         [[0.5277, 0.4097, 0.5508,  ..., 0.3540, 0.3854, 0.4942],
          [0.5694, 0.5017, 0.4861,  ..., 0.4749, 0.3320, 0.3730],
          [0.4463, 0.4211, 0.5593,  ..., 0.4359, 0.4673, 0.4586],
          [0.3886, 0.4608, 0.5722,  ..., 0.6343, 0.4177, 0.6081]]],


        [[[0.5870, 0.4756, 0.5016,  ..., 0.3984, 0.4135, 0.4378],
          [0.4773, 0.5165, 0.4554,  ..., 0.3702, 0.3132, 0.5036],
          [0.4182, 0.4121, 0.5008,  ..., 0.4518, 0.4598, 0.5014],
          [0.4977, 0.4615, 0.5448,  ..., 0.6826, 0.4651, 0.4755]],

         [[0.5636, 0.5380, 0.3965,  ..., 0.5122, 0.3979, 0.4012],
          [0.6067, 0.4335, 0.5562,  ..., 0.4321, 0.3486, 0.5109],
          [0.4364, 0.4705, 0.6011,  ..., 0.5931, 0.5913, 0.4931],
          [0.5780, 0.4154, 0.2798,  ..., 0.6792, 0.3355, 0.5535]],

         [[0.4244, 0.4429, 0.4012,  ..., 0.5419, 0.3132, 0.5089],
          [0.5327, 0.3425, 0.6843,  ..., 0.4871, 0.5598, 0.4977],
          [0.4287, 0.6460, 0.6689,  ..., 0.5588, 0.5655, 0.5751],
          [0.4330, 0.4369, 0.3956,  ..., 0.5489, 0.4140, 0.5860]],

         ...,

         [[0.4828, 0.5917, 0.4301,  ..., 0.4875, 0.3451, 0.5251],
          [0.4106, 0.5450, 0.6478,  ..., 0.4173, 0.4967, 0.5263],
          [0.5245, 0.5627, 0.5727,  ..., 0.5251, 0.3975, 0.4785],
          [0.6334, 0.3979, 0.2697,  ..., 0.5931, 0.5010, 0.5603]],

         [[0.5641, 0.4163, 0.5308,  ..., 0.3730, 0.4751, 0.4724],
          [0.4884, 0.4666, 0.5799,  ..., 0.4140, 0.3905, 0.3840],
          [0.4858, 0.5889, 0.5903,  ..., 0.5344, 0.4102, 0.3495],
          [0.6280, 0.4989, 0.4177,  ..., 0.5363, 0.5523, 0.4688]],

         [[0.4364, 0.4642, 0.3933,  ..., 0.4841, 0.4492, 0.6142],
          [0.5508, 0.6334, 0.5808,  ..., 0.4849, 0.3648, 0.5185],
          [0.4177, 0.5908, 0.6100,  ..., 0.5917, 0.5607, 0.5245],
          [0.5057, 0.3648, 0.5085,  ..., 0.5646, 0.4711, 0.4550]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0030, -0.0010, -0.0050,  0.0050,  0.0010, -0.0130,  0.0090, -0.0210,
         0.0250, -0.0250], device='cuda:0')
selected experts tensor([1788, 1662, 1655, 1824, 1228, 2171, 1558, 1988,  935, 1575],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5338, 0.5672, 0.5126,  ..., 0.4091, 0.5130, 0.5132],
          [0.5127, 0.5875, 0.6032,  ..., 0.4360, 0.5227, 0.5503],
          [0.4146, 0.4867, 0.3821,  ..., 0.6410, 0.6151, 0.4408],
          [0.6489, 0.5589, 0.4047,  ..., 0.5066, 0.4470, 0.5580]],

         [[0.5507, 0.6140, 0.6013,  ..., 0.4588, 0.4760, 0.4384],
          [0.5114, 0.5349, 0.5136,  ..., 0.5236, 0.4670, 0.4441],
          [0.4708, 0.5366, 0.5138,  ..., 0.4217, 0.6896, 0.6644],
          [0.5017, 0.5422, 0.3225,  ..., 0.6248, 0.4887, 0.4687]],

         [[0.5266, 0.6215, 0.6293,  ..., 0.4503, 0.4290, 0.3979],
          [0.6399, 0.6098, 0.4708,  ..., 0.4806, 0.5018, 0.4213],
          [0.4469, 0.5219, 0.4623,  ..., 0.4208, 0.6521, 0.4494],
          [0.5263, 0.5852, 0.5055,  ..., 0.6455, 0.5094, 0.5457]],

         ...,

         [[0.5110, 0.5110, 0.5110,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5130, 0.5070, 0.5130]],

         [[0.5110, 0.5110, 0.5110,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5130, 0.5070, 0.5130]],

         [[0.5110, 0.5110, 0.5110,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5130, 0.5070, 0.5130]]],


        [[[0.5211, 0.6344, 0.6650,  ..., 0.3271, 0.5692, 0.4879],
          [0.6284, 0.5320, 0.3890,  ..., 0.6464, 0.5874, 0.5060],
          [0.3517, 0.4819, 0.3526,  ..., 0.5010, 0.5176, 0.6989],
          [0.5691, 0.5628, 0.4793,  ..., 0.4728, 0.5878, 0.5730]],

         [[0.5218, 0.6219, 0.6353,  ..., 0.3869, 0.4210, 0.3796],
          [0.4664, 0.6238, 0.4744,  ..., 0.5275, 0.5155, 0.3860],
          [0.5119, 0.5667, 0.4677,  ..., 0.5075, 0.7350, 0.3697],
          [0.6807, 0.6126, 0.3973,  ..., 0.3398, 0.5378, 0.6599]],

         [[0.6747, 0.6293, 0.7828,  ..., 0.4544, 0.5467, 0.3805],
          [0.4103, 0.5153, 0.4859,  ..., 0.6066, 0.5322, 0.4217],
          [0.3758, 0.5582, 0.4127,  ..., 0.4007, 0.6485, 0.3706],
          [0.5217, 0.6390, 0.3931,  ..., 0.4119, 0.4120, 0.5766]],

         ...,

         [[0.5110, 0.5110, 0.5110,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5130, 0.5070, 0.5130]],

         [[0.5110, 0.5110, 0.5110,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5130, 0.5070, 0.5130]],

         [[0.5110, 0.5110, 0.5110,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5110,  ..., 0.5130, 0.5070, 0.5130]]]],
       device='cuda:0')/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([[[[0.5228, 0.5562, 0.5016,  ..., 0.3961, 0.5060, 0.5002],
          [0.5017, 0.5765, 0.5922,  ..., 0.4230, 0.5157, 0.5373],
          [0.4036, 0.4757, 0.3711,  ..., 0.6280, 0.6081, 0.4278],
          [0.6379, 0.5479, 0.3937,  ..., 0.4936, 0.4400, 0.5450]],

         [[0.5397, 0.6030, 0.5903,  ..., 0.4458, 0.4690, 0.4254],
          [0.5004, 0.5239, 0.5026,  ..., 0.5106, 0.4600, 0.4311],
          [0.4598, 0.5256, 0.5028,  ..., 0.4087, 0.6826, 0.6514],
          [0.4907, 0.5312, 0.3115,  ..., 0.6118, 0.4817, 0.4557]],

         [[0.5156, 0.6105, 0.6183,  ..., 0.4373, 0.4220, 0.3849],
          [0.6289, 0.5988, 0.4598,  ..., 0.4676, 0.4948, 0.4083],
          [0.4359, 0.5109, 0.4513,  ..., 0.4078, 0.6451, 0.4364],
          [0.5153, 0.5742, 0.4945,  ..., 0.6325, 0.5024, 0.5327]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5101, 0.6234, 0.6540,  ..., 0.3141, 0.5622, 0.4749],
          [0.6174, 0.5210, 0.3780,  ..., 0.6334, 0.5804, 0.4930],
          [0.3407, 0.4709, 0.3416,  ..., 0.4880, 0.5106, 0.6859],
          [0.5581, 0.5518, 0.4683,  ..., 0.4598, 0.5808, 0.5600]],

         [[0.5108, 0.6109, 0.6243,  ..., 0.3739, 0.4140, 0.3666],
          [0.4554, 0.6128, 0.4634,  ..., 0.5145, 0.5085, 0.3730],
          [0.5009, 0.5557, 0.4567,  ..., 0.4945, 0.7280, 0.3567],
          [0.6697, 0.6016, 0.3863,  ..., 0.3268, 0.5308, 0.6469]],

         [[0.6637, 0.6183, 0.7718,  ..., 0.4414, 0.5397, 0.3675],
          [0.3993, 0.5043, 0.4749,  ..., 0.5936, 0.5252, 0.4087],
          [0.3648, 0.5472, 0.4017,  ..., 0.3877, 0.6415, 0.3576],
          [0.5107, 0.6280, 0.3821,  ..., 0.3989, 0.4050, 0.5636]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0110, 0.0110, 0.0110, 0.0110, 0.0130, 0.0110, 0.0110, 0.0130, 0.0070,
        0.0130], device='cuda:0')
selected experts tensor([1084, 1551, 1065,  498, 3132, 2114, 1576, 2920, 1631,  813],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4000, 0.5174, 0.4801,  ..., 0.4736, 0.5391, 0.3887],
          [0.4085, 0.6183, 0.3810,  ..., 0.5513, 0.4460, 0.5744],
          [0.5366, 0.4733, 0.3271,  ..., 0.4929, 0.4445, 0.5677],
          [0.4470, 0.4235, 0.5362,  ..., 0.5607, 0.4638, 0.4117]],

         [[0.5678, 0.5651, 0.4903,  ..., 0.4143, 0.7423, 0.5895],
          [0.5366, 0.4106, 0.5955,  ..., 0.4690, 0.3741, 0.4477],
          [0.5937, 0.5260, 0.6011,  ..., 0.5244, 0.5452, 0.5144],
          [0.3746, 0.3486, 0.3952,  ..., 0.4347, 0.5196, 0.4921]],

         [[0.3943, 0.4811, 0.6030,  ..., 0.3790, 0.4359, 0.5876],
          [0.4913, 0.2674, 0.4182,  ..., 0.5280, 0.6687, 0.5427],
          [0.5269, 0.5799, 0.4106,  ..., 0.4063, 0.5319, 0.3418],
          [0.5245, 0.5103, 0.4482,  ..., 0.4016, 0.5071, 0.3561]],

         ...,

         [[0.3691, 0.5266, 0.5281,  ..., 0.3951, 0.6206, 0.6183],
          [0.5857, 0.4316, 0.3791,  ..., 0.4252, 0.5229, 0.5829],
          [0.3746, 0.4111, 0.5386,  ..., 0.4376, 0.3960, 0.4652],
          [0.3947, 0.6165, 0.4538,  ..., 0.5825, 0.6436, 0.5819]],

         [[0.4880, 0.3390, 0.5740,  ..., 0.3226, 0.5780, 0.4261],
          [0.4533, 0.4856, 0.4296,  ..., 0.5347, 0.4378, 0.5041],
          [0.4618, 0.4492, 0.5399,  ..., 0.5949, 0.4842, 0.4533],
          [0.3943, 0.5499, 0.5137,  ..., 0.6330, 0.5505, 0.4690]],

         [[0.4219, 0.4564, 0.5869,  ..., 0.5197, 0.4977, 0.4075],
          [0.5458, 0.4877, 0.5005,  ..., 0.5897, 0.6535, 0.4623],
          [0.4308, 0.4721, 0.6396,  ..., 0.6995, 0.4510, 0.5629],
          [0.4356, 0.4182, 0.6063,  ..., 0.5610, 0.5686, 0.4042]]],


        [[[0.4638, 0.5108, 0.4912,  ..., 0.4622, 0.3822, 0.4811],
          [0.5735, 0.5475, 0.5489,  ..., 0.3636, 0.5196, 0.5965],
          [0.4138, 0.4378, 0.5144,  ..., 0.5253, 0.6066, 0.5562],
          [0.4385, 0.6118, 0.4315,  ..., 0.5013, 0.5809, 0.5989]],

         [[0.5099, 0.4359, 0.3022,  ..., 0.5460, 0.5050, 0.5470],
          [0.4774, 0.4297, 0.6058,  ..., 0.4214, 0.4174, 0.5156],
          [0.5664, 0.5997, 0.3487,  ..., 0.5119, 0.5435, 0.5480],
          [0.5230, 0.6091, 0.3915,  ..., 0.6188, 0.4880, 0.5006]],

         [[0.4076, 0.4797, 0.3549,  ..., 0.6421, 0.4759, 0.5653],
          [0.4587, 0.4335, 0.4177,  ..., 0.5228, 0.4689, 0.4175],
          [0.6176, 0.5010, 0.6838,  ..., 0.4357, 0.4737, 0.5829],
          [0.5330, 0.5983, 0.5225,  ..., 0.5258, 0.6271, 0.5853]],

         ...,

         [[0.4393, 0.7130, 0.5591,  ..., 0.6057, 0.6526, 0.5715],
          [0.3863, 0.5050, 0.5060,  ..., 0.5328, 0.6826, 0.5744],
          [0.3830, 0.4588, 0.5190,  ..., 0.5303, 0.5107, 0.5979],
          [0.4701, 0.4494, 0.4434,  ..., 0.5199, 0.5938, 0.6663]],

         [[0.3672, 0.5665, 0.5841,  ..., 0.5452, 0.5766, 0.6017],
          [0.4583, 0.3975, 0.5306,  ..., 0.4896, 0.5819, 0.4299],
          [0.6427, 0.4994, 0.4167,  ..., 0.5489, 0.4383, 0.4108],
          [0.5539, 0.4463, 0.5550,  ..., 0.4479, 0.4776, 0.4557]],

         [[0.4638, 0.5108, 0.4912,  ..., 0.4622, 0.3822, 0.4811],
          [0.5735, 0.5475, 0.5489,  ..., 0.3636, 0.5196, 0.5965],
          [0.4138, 0.4378, 0.5144,  ..., 0.5253, 0.6066, 0.5562],
          [0.4385, 0.6118, 0.4315,  ..., 0.5013, 0.5809, 0.5989]]]],
       device='cuda:0')
tensor([[[[0.4140, 0.5174, 0.4721,  ..., 0.4676, 0.5271, 0.3947],
          [0.4225, 0.6183, 0.3730,  ..., 0.5453, 0.4340, 0.5804],
          [0.5506, 0.4733, 0.3191,  ..., 0.4869, 0.4325, 0.5737],
          [0.4610, 0.4235, 0.5282,  ..., 0.5547, 0.4518, 0.4177]],

         [[0.5818, 0.5651, 0.4823,  ..., 0.4083, 0.7303, 0.5955],
          [0.5506, 0.4106, 0.5875,  ..., 0.4630, 0.3621, 0.4537],
          [0.6077, 0.5260, 0.5931,  ..., 0.5184, 0.5332, 0.5204],
          [0.3886, 0.3486, 0.3872,  ..., 0.4287, 0.5076, 0.4981]],

         [[0.4083, 0.4811, 0.5950,  ..., 0.3730, 0.4239, 0.5936],
          [0.5053, 0.2674, 0.4102,  ..., 0.5220, 0.6567, 0.5487],
          [0.5409, 0.5799, 0.4026,  ..., 0.4003, 0.5199, 0.3478],
          [0.5385, 0.5103, 0.4402,  ..., 0.3956, 0.4951, 0.3621]],

         ...,

         [[0.3831, 0.5266, 0.5201,  ..., 0.3891, 0.6086, 0.6243],
          [0.5997, 0.4316, 0.3711,  ..., 0.4192, 0.5109, 0.5889],
          [0.3886, 0.4111, 0.5306,  ..., 0.4316, 0.3840, 0.4712],
          [0.4087, 0.6165, 0.4458,  ..., 0.5765, 0.6316, 0.5879]],

         [[0.5020, 0.3390, 0.5660,  ..., 0.3166, 0.5660, 0.4321],
          [0.4673, 0.4856, 0.4216,  ..., 0.5287, 0.4258, 0.5101],
          [0.4758, 0.4492, 0.5319,  ..., 0.5889, 0.4722, 0.4593],
          [0.4083, 0.5499, 0.5057,  ..., 0.6270, 0.5385, 0.4750]],

         [[0.4359, 0.4564, 0.5789,  ..., 0.5137, 0.4857, 0.4135],
          [0.5598, 0.4877, 0.4925,  ..., 0.5837, 0.6415, 0.4683],
          [0.4448, 0.4721, 0.6316,  ..., 0.6935, 0.4390, 0.5689],
          [0.4496, 0.4182, 0.5983,  ..., 0.5550, 0.5566, 0.4102]]],


        [[[0.4778, 0.5108, 0.4832,  ..., 0.4562, 0.3702, 0.4871],
          [0.5875, 0.5475, 0.5409,  ..., 0.3576, 0.5076, 0.6025],
          [0.4278, 0.4378, 0.5064,  ..., 0.5193, 0.5946, 0.5622],
          [0.4525, 0.6118, 0.4235,  ..., 0.4953, 0.5689, 0.6049]],

         [[0.5239, 0.4359, 0.2942,  ..., 0.5400, 0.4930, 0.5530],
          [0.4914, 0.4297, 0.5978,  ..., 0.4154, 0.4054, 0.5216],
          [0.5804, 0.5997, 0.3407,  ..., 0.5059, 0.5315, 0.5540],
          [0.5370, 0.6091, 0.3835,  ..., 0.6128, 0.4760, 0.5066]],

         [[0.4216, 0.4797, 0.3469,  ..., 0.6361, 0.4639, 0.5713],
          [0.4727, 0.4335, 0.4097,  ..., 0.5168, 0.4569, 0.4235],
          [0.6316, 0.5010, 0.6758,  ..., 0.4297, 0.4617, 0.5889],
          [0.5470, 0.5983, 0.5145,  ..., 0.5198, 0.6151, 0.5913]],

         ...,

         [[0.4533, 0.7130, 0.5511,  ..., 0.5997, 0.6406, 0.5775],
          [0.4003, 0.5050, 0.4980,  ..., 0.5268, 0.6706, 0.5804],
          [0.3970, 0.4588, 0.5110,  ..., 0.5243, 0.4987, 0.6039],
          [0.4841, 0.4494, 0.4354,  ..., 0.5139, 0.5818, 0.6723]],

         [[0.3812, 0.5665, 0.5761,  ..., 0.5392, 0.5646, 0.6077],
          [0.4723, 0.3975, 0.5226,  ..., 0.4836, 0.5699, 0.4359],
          [0.6567, 0.4994, 0.4087,  ..., 0.5429, 0.4263, 0.4168],
          [0.5679, 0.4463, 0.5470,  ..., 0.4419, 0.4656, 0.4617]],

         [[0.4778, 0.5108, 0.4832,  ..., 0.4562, 0.3702, 0.4871],
          [0.5875, 0.5475, 0.5409,  ..., 0.3576, 0.5076, 0.6025],
          [0.4278, 0.4378, 0.5064,  ..., 0.5193, 0.5946, 0.5622],
          [0.4525, 0.6118, 0.4235,  ..., 0.4953, 0.5689, 0.6049]]]],
       device='cuda:0', requires_grad=True)
tensor([-1.4000e-02,  2.3283e-10,  8.0000e-03,  2.0000e-03,  8.0000e-03,
         6.0000e-03, -4.0000e-03,  6.0000e-03,  1.2000e-02, -6.0000e-03],
       device='cuda:0')
selected experts tensor([1560, 1640, 1559, 1690, 1602, 1665, 1678, 1602, 1665, 1723],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6516, 0.5709, 0.6403,  ..., 0.3581, 0.5795, 0.5384],
          [0.5215, 0.5421, 0.7595,  ..., 0.5200, 0.5055, 0.4948],
          [0.7053, 0.5997, 0.5782,  ..., 0.5372, 0.4724, 0.7154],
          [0.5156, 0.5525, 0.4758,  ..., 0.4775, 0.3805, 0.5796]],

         [[0.4961, 0.5414, 0.4780,  ..., 0.4505, 0.4331, 0.6899],
          [0.3976, 0.5847, 0.4242,  ..., 0.4900, 0.4905, 0.5805],
          [0.2740, 0.5833, 0.4745,  ..., 0.5189, 0.5666, 0.3875],
          [0.4785, 0.4715, 0.5665,  ..., 0.4737, 0.4122, 0.4687]],

         [[0.5596, 0.5168, 0.4405,  ..., 0.5255, 0.3254, 0.3064],
          [0.7346, 0.5671, 0.3872,  ..., 0.3662, 0.5294, 0.3829],
          [0.5352, 0.3853, 0.4552,  ..., 0.3772, 0.3768, 0.4677],
          [0.5342, 0.3559, 0.5368,  ..., 0.6221, 0.4776, 0.6365]],

         ...,

         [[0.5947, 0.5551, 0.4328,  ..., 0.5208, 0.4538, 0.5214],
          [0.5410, 0.6094, 0.5440,  ..., 0.6023, 0.3401, 0.3447],
          [0.4951, 0.4678, 0.4962,  ..., 0.2918, 0.4562, 0.3963],
          [0.6120, 0.3257, 0.5720,  ..., 0.3599, 0.5567, 0.4375]],

         [[0.4056, 0.5004, 0.5193,  ..., 0.3804, 0.5331, 0.5264],
          [0.4389, 0.6099, 0.5091,  ..., 0.5882, 0.5110, 0.6191],
          [0.4684, 0.5409, 0.4784,  ..., 0.3888, 0.4027, 0.4906],
          [0.3489, 0.4853, 0.5133,  ..., 0.5802, 0.5994, 0.6392]],

         [[0.5286, 0.4967, 0.4252,  ..., 0.5591, 0.4226, 0.3105],
          [0.5984, 0.4436, 0.5130,  ..., 0.5180, 0.6761, 0.3308],
          [0.4811, 0.6451, 0.4665,  ..., 0.4309, 0.4983, 0.5335],
          [0.6097, 0.5714, 0.3914,  ..., 0.5711, 0.4832, 0.5393]]],


        [[[0.5603, 0.6788, 0.4181,  ..., 0.5924, 0.4984, 0.5156],
          [0.4856, 0.6362, 0.3845,  ..., 0.6240, 0.5485, 0.5488],
          [0.5003, 0.3487, 0.6202,  ..., 0.4823, 0.6290, 0.3197],
          [0.4345, 0.2936, 0.4898,  ..., 0.4847, 0.5376, 0.5640]],

         [[0.3805, 0.5585, 0.5501,  ..., 0.6666, 0.6498, 0.6899],
          [0.5984, 0.3970, 0.4242,  ..., 0.6482, 0.2842, 0.6356],
          [0.5317, 0.5125, 0.5474,  ..., 0.6097, 0.5272, 0.4825],
          [0.5504, 0.4169, 0.6394,  ..., 0.4019, 0.5258, 0.4024]],

         [[0.3939, 0.5662, 0.5987,  ..., 0.5630, 0.5904, 0.5910],
          [0.5984, 0.4064, 0.6266,  ..., 0.4568, 0.4744, 0.4653],
          [0.3587, 0.5842, 0.4964,  ..., 0.5505, 0.4212, 0.4645],
          [0.5463, 0.4990, 0.5887,  ..., 0.6339, 0.4355, 0.6232]],

         ...,

         [[0.5461, 0.5941, 0.4554,  ..., 0.7271, 0.4611, 0.4033],
          [0.3920, 0.4310, 0.4804,  ..., 0.5519, 0.5781, 0.5657],
          [0.5037, 0.4848, 0.4063,  ..., 0.3271, 0.6263, 0.5163],
          [0.5347, 0.6478, 0.5042,  ..., 0.5191, 0.6194, 0.5471]],

         [[0.5824, 0.3989, 0.4699,  ..., 0.6718, 0.5661, 0.3806],
          [0.4961, 0.4325, 0.3808,  ..., 0.5990, 0.3883, 0.5128],
          [0.6180, 0.5566, 0.4295,  ..., 0.5271, 0.5115, 0.6633],
          [0.5210, 0.5120, 0.5267,  ..., 0.4047, 0.6129, 0.5201]],

         [[0.5603, 0.6788, 0.4181,  ..., 0.5924, 0.4984, 0.5156],
          [0.4856, 0.6362, 0.3845,  ..., 0.6240, 0.5485, 0.5488],
          [0.5003, 0.3487, 0.6202,  ..., 0.4823, 0.6290, 0.3197],
          [0.4345, 0.2936, 0.4898,  ..., 0.4847, 0.5376, 0.5640]]]],
       device='cuda:0')
tensor([[[[0.6496, 0.5789, 0.6343,  ..., 0.3621, 0.5775, 0.5344],
          [0.5195, 0.5501, 0.7535,  ..., 0.5240, 0.5035, 0.4908],
          [0.7033, 0.6077, 0.5722,  ..., 0.5412, 0.4704, 0.7114],
          [0.5136, 0.5605, 0.4698,  ..., 0.4815, 0.3785, 0.5756]],

         [[0.4941, 0.5494, 0.4720,  ..., 0.4545, 0.4311, 0.6859],
          [0.3956, 0.5927, 0.4182,  ..., 0.4940, 0.4885, 0.5765],
          [0.2720, 0.5913, 0.4685,  ..., 0.5229, 0.5646, 0.3835],
          [0.4765, 0.4795, 0.5605,  ..., 0.4777, 0.4102, 0.4647]],

         [[0.5576, 0.5248, 0.4345,  ..., 0.5295, 0.3234, 0.3024],
          [0.7326, 0.5751, 0.3812,  ..., 0.3702, 0.5274, 0.3789],
          [0.5332, 0.3933, 0.4492,  ..., 0.3812, 0.3748, 0.4637],
          [0.5322, 0.3639, 0.5308,  ..., 0.6261, 0.4756, 0.6325]],

         ...,

         [[0.5927, 0.5631, 0.4268,  ..., 0.5248, 0.4518, 0.5174],
          [0.5390, 0.6174, 0.5380,  ..., 0.6063, 0.3381, 0.3407],
          [0.4931, 0.4758, 0.4902,  ..., 0.2958, 0.4542, 0.3923],
          [0.6100, 0.3337, 0.5660,  ..., 0.3639, 0.5547, 0.4335]],

         [[0.4036, 0.5084, 0.5133,  ..., 0.3844, 0.5311, 0.5224],
          [0.4369, 0.6179, 0.5031,  ..., 0.5922, 0.5090, 0.6151],
          [0.4664, 0.5489, 0.4724,  ..., 0.3928, 0.4007, 0.4866],
          [0.3469, 0.4933, 0.5073,  ..., 0.5842, 0.5974, 0.6352]],

         [[0.5266, 0.5047, 0.4192,  ..., 0.5631, 0.4206, 0.3065],
          [0.5964, 0.4516, 0.5070,  ..., 0.5220, 0.6741, 0.3268],
          [0.4791, 0.6531, 0.4605,  ..., 0.4349, 0.4963, 0.5295],
          [0.6077, 0.5794, 0.3854,  ..., 0.5751, 0.4812, 0.5353]]],


        [[[0.5583, 0.6868, 0.4121,  ..., 0.5964, 0.4964, 0.5116],
          [0.4836, 0.6442, 0.3785,  ..., 0.6280, 0.5465, 0.5448],
          [0.4983, 0.3567, 0.6142,  ..., 0.4863, 0.6270, 0.3157],
          [0.4325, 0.3016, 0.4838,  ..., 0.4887, 0.5356, 0.5600]],

         [[0.3785, 0.5665, 0.5441,  ..., 0.6706, 0.6478, 0.6859],
          [0.5964, 0.4050, 0.4182,  ..., 0.6522, 0.2822, 0.6316],
          [0.5297, 0.5205, 0.5414,  ..., 0.6137, 0.5252, 0.4785],
          [0.5484, 0.4249, 0.6334,  ..., 0.4059, 0.5238, 0.3984]],

         [[0.3919, 0.5742, 0.5927,  ..., 0.5670, 0.5884, 0.5870],
          [0.5964, 0.4144, 0.6206,  ..., 0.4608, 0.4724, 0.4613],
          [0.3567, 0.5922, 0.4904,  ..., 0.5545, 0.4192, 0.4605],
          [0.5443, 0.5070, 0.5827,  ..., 0.6379, 0.4335, 0.6192]],

         ...,

         [[0.5441, 0.6021, 0.4494,  ..., 0.7311, 0.4591, 0.3993],
          [0.3900, 0.4390, 0.4744,  ..., 0.5559, 0.5761, 0.5617],
          [0.5017, 0.4928, 0.4003,  ..., 0.3311, 0.6243, 0.5123],
          [0.5327, 0.6558, 0.4982,  ..., 0.5231, 0.6174, 0.5431]],

         [[0.5804, 0.4069, 0.4639,  ..., 0.6758, 0.5641, 0.3766],
          [0.4941, 0.4405, 0.3748,  ..., 0.6030, 0.3863, 0.5088],
          [0.6160, 0.5646, 0.4235,  ..., 0.5311, 0.5095, 0.6593],
          [0.5190, 0.5200, 0.5207,  ..., 0.4087, 0.6109, 0.5161]],

         [[0.5583, 0.6868, 0.4121,  ..., 0.5964, 0.4964, 0.5116],
          [0.4836, 0.6442, 0.3785,  ..., 0.6280, 0.5465, 0.5448],
          [0.4983, 0.3567, 0.6142,  ..., 0.4863, 0.6270, 0.3157],
          [0.4325, 0.3016, 0.4838,  ..., 0.4887, 0.5356, 0.5600]]]],
       device='cuda:0', requires_grad=True)
tensor([ 2.0000e-03, -8.0000e-03,  6.0000e-03,  2.3283e-10,  2.0000e-03,
         6.0000e-03, -6.0000e-03, -4.0000e-03,  2.0000e-03,  4.0000e-03],
       device='cuda:0')
selected experts tensor([1516, 1694, 1758, 1711, 1489, 1813, 1680, 1584, 1598, 1541],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.3786, 0.5142, 0.4625,  ..., 0.6014, 0.3615, 0.5497],
          [0.4992, 0.4816, 0.4886,  ..., 0.3510, 0.4385, 0.6238],
          [0.4197, 0.5935, 0.5434,  ..., 0.4158, 0.4609, 0.4495],
          [0.5128, 0.5097, 0.4649,  ..., 0.6177, 0.5684, 0.4637]],

         [[0.5819, 0.5597, 0.3789,  ..., 0.4245, 0.4132, 0.4208],
          [0.4691, 0.4172, 0.4938,  ..., 0.3135, 0.4400, 0.4857],
          [0.4995, 0.5595, 0.6049,  ..., 0.4424, 0.3971, 0.4831],
          [0.5795, 0.3180, 0.3435,  ..., 0.6177, 0.5142, 0.5259]],

         [[0.5201, 0.5059, 0.3615,  ..., 0.6797, 0.3460, 0.5160],
          [0.6041, 0.6368, 0.5918,  ..., 0.4058, 0.4499, 0.6184],
          [0.4307, 0.4472, 0.4475,  ..., 0.4288, 0.4571, 0.5966],
          [0.5174, 0.5580, 0.4553,  ..., 0.4687, 0.5684, 0.5729]],

         ...,

         [[0.5169, 0.5616, 0.5956,  ..., 0.4778, 0.4747, 0.3862],
          [0.4079, 0.4696, 0.4208,  ..., 0.3031, 0.5454, 0.6103],
          [0.4810, 0.4934, 0.5667,  ..., 0.5852, 0.4100, 0.5218],
          [0.3453, 0.3811, 0.5097,  ..., 0.6078, 0.4528, 0.4564]],

         [[0.5223, 0.5380, 0.5648,  ..., 0.2929, 0.4155, 0.5382],
          [0.5601, 0.5626, 0.4170,  ..., 0.4410, 0.4366, 0.5658],
          [0.5432, 0.3875, 0.5777,  ..., 0.5664, 0.3292, 0.5888],
          [0.5533, 0.3449, 0.5610,  ..., 0.5416, 0.5935, 0.4506]],

         [[0.5106, 0.5059, 0.5202,  ..., 0.5550, 0.3773, 0.4095],
          [0.5627, 0.4706, 0.5824,  ..., 0.4424, 0.4188, 0.4659],
          [0.5885, 0.5869, 0.6689,  ..., 0.5279, 0.6276, 0.5865],
          [0.6031, 0.4044, 0.3734,  ..., 0.6150, 0.6073, 0.5244]]],


        [[[0.5299, 0.4622, 0.4094,  ..., 0.5075, 0.3685, 0.4896],
          [0.4873, 0.4791, 0.5220,  ..., 0.4206, 0.4183, 0.4665],
          [0.3786, 0.5916, 0.5511,  ..., 0.5281, 0.4708, 0.4899],
          [0.5439, 0.4549, 0.3418,  ..., 0.6355, 0.5206, 0.5625]],

         [[0.5637, 0.4663, 0.4993,  ..., 0.3929, 0.4188, 0.4536],
          [0.5151, 0.4115, 0.5077,  ..., 0.4766, 0.3917, 0.4028],
          [0.4943, 0.5285, 0.6418,  ..., 0.4730, 0.4614, 0.4004],
          [0.4623, 0.4344, 0.4565,  ..., 0.6123, 0.5596, 0.5883]],

         [[0.5483, 0.4916, 0.4688,  ..., 0.6285, 0.4235, 0.4459],
          [0.5347, 0.5375, 0.4842,  ..., 0.3410, 0.3563, 0.5288],
          [0.4360, 0.5631, 0.5194,  ..., 0.4502, 0.4352, 0.5338],
          [0.4917, 0.4409, 0.5177,  ..., 0.7329, 0.4966, 0.4772]],

         ...,

         [[0.4764, 0.5282, 0.6137,  ..., 0.6258, 0.4114, 0.4709],
          [0.6354, 0.4979, 0.6174,  ..., 0.5707, 0.3854, 0.4186],
          [0.4642, 0.6404, 0.6220,  ..., 0.5871, 0.4919, 0.5252],
          [0.5186, 0.4648, 0.3697,  ..., 0.5889, 0.5564, 0.5620]],

         [[0.5453, 0.5722, 0.5497,  ..., 0.3117, 0.4305, 0.3327],
          [0.5524, 0.3824, 0.5610,  ..., 0.4864, 0.4211, 0.5061],
          [0.3340, 0.5116, 0.4959,  ..., 0.5796, 0.5566, 0.4237],
          [0.3957, 0.3820, 0.4198,  ..., 0.7167, 0.5735, 0.6543]],

         [[0.5690, 0.3755, 0.3525,  ..., 0.4824, 0.4638, 0.4797],
          [0.5049, 0.5433, 0.6017,  ..., 0.4679, 0.3267, 0.5266],
          [0.4832, 0.5095, 0.5610,  ..., 0.5407, 0.6388, 0.5082],
          [0.5412, 0.4912, 0.4779,  ..., 0.6495, 0.4706, 0.4801]]]],
       device='cuda:0')
tensor([[[[0.3766, 0.5162, 0.4685,  ..., 0.6234, 0.3355, 0.5737],
          [0.4972, 0.4836, 0.4946,  ..., 0.3730, 0.4125, 0.6478],
          [0.4177, 0.5955, 0.5494,  ..., 0.4378, 0.4349, 0.4735],
          [0.5108, 0.5117, 0.4709,  ..., 0.6397, 0.5424, 0.4877]],

         [[0.5799, 0.5617, 0.3849,  ..., 0.4465, 0.3872, 0.4448],
          [0.4671, 0.4192, 0.4998,  ..., 0.3355, 0.4140, 0.5097],
          [0.4975, 0.5615, 0.6109,  ..., 0.4644, 0.3711, 0.5071],
          [0.5775, 0.3200, 0.3495,  ..., 0.6397, 0.4882, 0.5499]],

         [[0.5181, 0.5079, 0.3675,  ..., 0.7017, 0.3200, 0.5400],
          [0.6021, 0.6388, 0.5978,  ..., 0.4278, 0.4239, 0.6424],
          [0.4287, 0.4492, 0.4535,  ..., 0.4508, 0.4311, 0.6206],
          [0.5154, 0.5600, 0.4613,  ..., 0.4907, 0.5424, 0.5969]],

         ...,

         [[0.5149, 0.5636, 0.6016,  ..., 0.4998, 0.4487, 0.4102],
          [0.4059, 0.4716, 0.4268,  ..., 0.3251, 0.5194, 0.6343],
          [0.4790, 0.4954, 0.5727,  ..., 0.6072, 0.3840, 0.5458],
          [0.3433, 0.3831, 0.5157,  ..., 0.6298, 0.4268, 0.4804]],

         [[0.5203, 0.5400, 0.5708,  ..., 0.3149, 0.3895, 0.5622],
          [0.5581, 0.5646, 0.4230,  ..., 0.4630, 0.4106, 0.5898],
          [0.5412, 0.3895, 0.5837,  ..., 0.5884, 0.3032, 0.6128],
          [0.5513, 0.3469, 0.5670,  ..., 0.5636, 0.5675, 0.4746]],

         [[0.5086, 0.5079, 0.5262,  ..., 0.5770, 0.3513, 0.4335],
          [0.5607, 0.4726, 0.5884,  ..., 0.4644, 0.3928, 0.4899],
          [0.5865, 0.5889, 0.6749,  ..., 0.5499, 0.6016, 0.6105],
          [0.6011, 0.4064, 0.3794,  ..., 0.6370, 0.5813, 0.5484]]],


        [[[0.5279, 0.4642, 0.4154,  ..., 0.5295, 0.3425, 0.5136],
          [0.4853, 0.4811, 0.5280,  ..., 0.4426, 0.3923, 0.4905],
          [0.3766, 0.5936, 0.5571,  ..., 0.5501, 0.4448, 0.5139],
          [0.5419, 0.4569, 0.3478,  ..., 0.6575, 0.4946, 0.5865]],

         [[0.5617, 0.4683, 0.5053,  ..., 0.4149, 0.3928, 0.4776],
          [0.5131, 0.4135, 0.5137,  ..., 0.4986, 0.3657, 0.4268],
          [0.4923, 0.5305, 0.6478,  ..., 0.4950, 0.4354, 0.4244],
          [0.4603, 0.4364, 0.4625,  ..., 0.6343, 0.5336, 0.6123]],

         [[0.5463, 0.4936, 0.4748,  ..., 0.6505, 0.3975, 0.4699],
          [0.5327, 0.5395, 0.4902,  ..., 0.3630, 0.3303, 0.5528],
          [0.4340, 0.5651, 0.5254,  ..., 0.4722, 0.4092, 0.5578],
          [0.4897, 0.4429, 0.5237,  ..., 0.7549, 0.4706, 0.5012]],

         ...,

         [[0.4744, 0.5302, 0.6197,  ..., 0.6478, 0.3854, 0.4949],
          [0.6334, 0.4999, 0.6234,  ..., 0.5927, 0.3594, 0.4426],
          [0.4622, 0.6424, 0.6280,  ..., 0.6091, 0.4659, 0.5492],
          [0.5166, 0.4668, 0.3757,  ..., 0.6109, 0.5304, 0.5860]],

         [[0.5433, 0.5742, 0.5557,  ..., 0.3337, 0.4045, 0.3567],
          [0.5504, 0.3844, 0.5670,  ..., 0.5084, 0.3951, 0.5301],
          [0.3320, 0.5136, 0.5019,  ..., 0.6016, 0.5306, 0.4477],
          [0.3937, 0.3840, 0.4258,  ..., 0.7387, 0.5475, 0.6783]],

         [[0.5670, 0.3775, 0.3585,  ..., 0.5044, 0.4378, 0.5037],
          [0.5029, 0.5453, 0.6077,  ..., 0.4899, 0.3007, 0.5506],
          [0.4812, 0.5115, 0.5670,  ..., 0.5627, 0.6128, 0.5322],
          [0.5392, 0.4932, 0.4839,  ..., 0.6715, 0.4446, 0.5041]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0020, -0.0020, -0.0060,  0.0040,  0.0020, -0.0140,  0.0100, -0.0220,
         0.0260, -0.0240], device='cuda:0')
selected experts tensor([1747, 1292, 1535, 1818, 1535, 1946, 1439, 2474, 1000, 1598],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.6739, 0.5580, 0.5775,  ..., 0.4179, 0.5707, 0.4668],
          [0.5252, 0.4184, 0.3633,  ..., 0.6912, 0.4444, 0.4523],
          [0.3337, 0.5439, 0.3946,  ..., 0.4469, 0.4502, 0.5624],
          [0.5061, 0.5570, 0.4609,  ..., 0.5943, 0.5206, 0.5771]],

         [[0.6023, 0.7392, 0.4840,  ..., 0.4942, 0.5250, 0.5245],
          [0.5471, 0.4672, 0.3087,  ..., 0.4932, 0.5268, 0.5400],
          [0.4090, 0.4876, 0.5198,  ..., 0.4975, 0.6002, 0.5547],
          [0.6289, 0.6117, 0.5737,  ..., 0.7392, 0.5210, 0.5853]],

         [[0.5626, 0.4554, 0.5919,  ..., 0.4212, 0.4425, 0.4360],
          [0.5730, 0.6047, 0.3562,  ..., 0.6400, 0.5716, 0.4489],
          [0.3804, 0.4805, 0.3194,  ..., 0.3964, 0.6477, 0.5848],
          [0.5914, 0.4791, 0.4694,  ..., 0.5397, 0.5874, 0.4668]],

         ...,

         [[0.5120, 0.5120, 0.5120,  ..., 0.5120, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5120, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5120, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5120, 0.5080, 0.5140]],

         [[0.5120, 0.5120, 0.5120,  ..., 0.5120, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5120, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5120, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5120, 0.5080, 0.5140]],

         [[0.5120, 0.5120, 0.5120,  ..., 0.5120, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5120, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5120, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5120, 0.5080, 0.5140]]],


        [[[0.6372, 0.5428, 0.5597,  ..., 0.4815, 0.5695, 0.5307],
          [0.5766, 0.4813, 0.4786,  ..., 0.4922, 0.5010, 0.3689],
          [0.4015, 0.4823, 0.4819,  ..., 0.4892, 0.5817, 0.3716],
          [0.6598, 0.4865, 0.4025,  ..., 0.5785, 0.4125, 0.5733]],

         [[0.4350, 0.5476, 0.6912,  ..., 0.4109, 0.7406, 0.3984],
          [0.4628, 0.6122, 0.5599,  ..., 0.5089, 0.4668, 0.4930],
          [0.4336, 0.4460, 0.5670,  ..., 0.4928, 0.5513, 0.5195],
          [0.5842, 0.6544, 0.4520,  ..., 0.5469, 0.4892, 0.4951]],

         [[0.6526, 0.6562, 0.6517,  ..., 0.5616, 0.7391, 0.4403],
          [0.5597, 0.5890, 0.5900,  ..., 0.5583, 0.4584, 0.3365],
          [0.4369, 0.4484, 0.5665,  ..., 0.4179, 0.4348, 0.5920],
          [0.4837, 0.6294, 0.3144,  ..., 0.6730, 0.5569, 0.5872]],

         ...,

         [[0.5120, 0.5120, 0.5120,  ..., 0.5120, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5120, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5120, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5120, 0.5080, 0.5140]],

         [[0.5120, 0.5120, 0.5120,  ..., 0.5120, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5120, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5120, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5120, 0.5080, 0.5140]],

         [[0.5120, 0.5120, 0.5120,  ..., 0.5120, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5120, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5120, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5120, 0.5080, 0.5140]]]],
       device='cuda:0')
tensor([[[[0.6619, 0.5460, 0.5655,  ..., 0.4059, 0.5627, 0.4528],
          [0.5132, 0.4064, 0.3513,  ..., 0.6792, 0.4364, 0.4383],
          [0.3217, 0.5319, 0.3826,  ..., 0.4349, 0.4422, 0.5484],
          [0.4941, 0.5450, 0.4489,  ..., 0.5823, 0.5126, 0.5631]],

         [[0.5903, 0.7272, 0.4720,  ..., 0.4822, 0.5170, 0.5105],
          [0.5351, 0.4552, 0.2967,  ..., 0.4812, 0.5188, 0.5260],
          [0.3970, 0.4756, 0.5078,  ..., 0.4855, 0.5922, 0.5407],
          [0.6169, 0.5997, 0.5617,  ..., 0.7272, 0.5130, 0.5713]],

         [[0.5506, 0.4434, 0.5799,  ..., 0.4092, 0.4345, 0.4220],
          [0.5610, 0.5927, 0.3442,  ..., 0.6280, 0.5636, 0.4349],
          [0.3684, 0.4685, 0.3074,  ..., 0.3844, 0.6397, 0.5708],
          [0.5794, 0.4671, 0.4574,  ..., 0.5277, 0.5794, 0.4528]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.6252, 0.5308, 0.5477,  ..., 0.4695, 0.5615, 0.5167],
          [0.5646, 0.4693, 0.4666,  ..., 0.4802, 0.4930, 0.3549],
          [0.3895, 0.4703, 0.4699,  ..., 0.4772, 0.5737, 0.3576],
          [0.6478, 0.4745, 0.3905,  ..., 0.5665, 0.4045, 0.5593]],

         [[0.4230, 0.5356, 0.6792,  ..., 0.3989, 0.7326, 0.3844],
          [0.4508, 0.6002, 0.5479,  ..., 0.4969, 0.4588, 0.4790],
          [0.4216, 0.4340, 0.5550,  ..., 0.4808, 0.5433, 0.5055],
          [0.5722, 0.6424, 0.4400,  ..., 0.5349, 0.4812, 0.4811]],

         [[0.6406, 0.6442, 0.6397,  ..., 0.5496, 0.7311, 0.4263],
          [0.5477, 0.5770, 0.5780,  ..., 0.5463, 0.4504, 0.3225],
          [0.4249, 0.4364, 0.5545,  ..., 0.4059, 0.4268, 0.5780],
          [0.4717, 0.6174, 0.3024,  ..., 0.6610, 0.5489, 0.5732]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([0.0120, 0.0120, 0.0120, 0.0120, 0.0120, 0.0100, 0.0120, 0.0120, 0.0080,
        0.0140], device='cuda:0')
selected experts tensor([1793, 1344,  966,  494,  846, 1619, 1498,  637, 1525, 1566],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5592, 0.4959, 0.3792,  ..., 0.4933, 0.5380, 0.4919],
          [0.5301, 0.6521, 0.4710,  ..., 0.6530, 0.5172, 0.4017],
          [0.3104, 0.4537, 0.6370,  ..., 0.4837, 0.5074, 0.5504],
          [0.3714, 0.4586, 0.3870,  ..., 0.4707, 0.5536, 0.5662]],

         [[0.4670, 0.5100, 0.4922,  ..., 0.4632, 0.3830, 0.4821],
          [0.5740, 0.5467, 0.5499,  ..., 0.3646, 0.5187, 0.5955],
          [0.4143, 0.4368, 0.5154,  ..., 0.5262, 0.6056, 0.5552],
          [0.4395, 0.6108, 0.4325,  ..., 0.5024, 0.5799, 0.5979]],

         [[0.3682, 0.3224, 0.4173,  ..., 0.5255, 0.4311, 0.5054],
          [0.4415, 0.5501, 0.5016,  ..., 0.4893, 0.4828, 0.4979],
          [0.5323, 0.4651, 0.5207,  ..., 0.3891, 0.5190, 0.5513],
          [0.5079, 0.6029, 0.6078,  ..., 0.4110, 0.6182, 0.3863]],

         ...,

         [[0.3967, 0.5097, 0.5908,  ..., 0.5307, 0.6098, 0.4122],
          [0.3609, 0.5246, 0.6388,  ..., 0.4443, 0.3794, 0.4516],
          [0.5816, 0.5164, 0.4980,  ..., 0.4096, 0.5084, 0.4487],
          [0.4512, 0.4253, 0.6045,  ..., 0.5487, 0.4479, 0.6372]],

         [[0.5501, 0.4934, 0.4557,  ..., 0.7576, 0.4359, 0.5120],
          [0.4793, 0.2812, 0.5741,  ..., 0.5523, 0.6065, 0.5484],
          [0.4937, 0.5597, 0.6568,  ..., 0.6870, 0.6330, 0.4674],
          [0.5308, 0.6661, 0.4415,  ..., 0.4181, 0.4085, 0.4284]],

         [[0.6445, 0.5450, 0.4981,  ..., 0.4499, 0.6159, 0.5652],
          [0.5937, 0.4617, 0.4512,  ..., 0.4797, 0.5240, 0.3999],
          [0.4576, 0.4330, 0.4877,  ..., 0.5327, 0.5894, 0.4787],
          [0.3765, 0.4506, 0.4969,  ..., 0.5365, 0.6098, 0.4222]]],


        [[[0.4635, 0.4896, 0.6924,  ..., 0.4547, 0.4440, 0.4878],
          [0.4746, 0.5319, 0.5656,  ..., 0.4841, 0.6289, 0.3891],
          [0.4955, 0.4712, 0.4771,  ..., 0.6988, 0.5693, 0.5201],
          [0.5085, 0.6486, 0.4659,  ..., 0.5874, 0.3876, 0.4141]],

         [[0.6348, 0.5423, 0.4187,  ..., 0.4986, 0.5222, 0.4958],
          [0.4716, 0.3459, 0.4887,  ..., 0.5304, 0.6088, 0.4613],
          [0.4162, 0.4656, 0.6125,  ..., 0.5248, 0.6084, 0.5123],
          [0.5811, 0.5214, 0.3792,  ..., 0.5120, 0.4747, 0.3104]],

         [[0.5284, 0.4805, 0.4435,  ..., 0.5730, 0.4879, 0.5001],
          [0.5588, 0.3512, 0.4577,  ..., 0.5552, 0.6168, 0.5130],
          [0.6204, 0.5382, 0.4325,  ..., 0.4899, 0.4061, 0.2864],
          [0.6524, 0.4933, 0.5088,  ..., 0.6128, 0.4703, 0.5988]],

         ...,

         [[0.6679, 0.6661, 0.5482,  ..., 0.5320, 0.4686, 0.5446],
          [0.5612, 0.4695, 0.4676,  ..., 0.3984, 0.4522, 0.5824],
          [0.4043, 0.6414, 0.4225,  ..., 0.5290, 0.5952, 0.3849],
          [0.5492, 0.7554, 0.5998,  ..., 0.6350, 0.3695, 0.4409]],

         [[0.4311, 0.5578, 0.4088,  ..., 0.4054, 0.5502, 0.5743],
          [0.5142, 0.5590, 0.4069,  ..., 0.6175, 0.5737, 0.5819],
          [0.3590, 0.5129, 0.6907,  ..., 0.4499, 0.4534, 0.4964],
          [0.4345, 0.3683, 0.5199,  ..., 0.5897, 0.5166, 0.5681]],

         [[0.3554, 0.4059, 0.4248,  ..., 0.3924, 0.5404, 0.5719],
          [0.4604, 0.5438, 0.3765,  ..., 0.3781, 0.4609, 0.6309],
          [0.4766, 0.5655, 0.5606,  ..., 0.6281, 0.5710, 0.5549],
          [0.4935, 0.5474, 0.5553,  ..., 0.5419, 0.4402, 0.5472]]]],
       device='cuda:0')
tensor([[[[0.5722, 0.4969, 0.3702,  ..., 0.4863, 0.5270, 0.4989],
          [0.5431, 0.6531, 0.4620,  ..., 0.6460, 0.5062, 0.4087],
          [0.3234, 0.4547, 0.6280,  ..., 0.4767, 0.4964, 0.5574],
          [0.3844, 0.4596, 0.3780,  ..., 0.4637, 0.5426, 0.5732]],

         [[0.4800, 0.5110, 0.4832,  ..., 0.4562, 0.3720, 0.4891],
          [0.5870, 0.5477, 0.5409,  ..., 0.3576, 0.5077, 0.6025],
          [0.4273, 0.4378, 0.5064,  ..., 0.5192, 0.5946, 0.5622],
          [0.4525, 0.6118, 0.4235,  ..., 0.4954, 0.5689, 0.6049]],

         [[0.3812, 0.3234, 0.4083,  ..., 0.5185, 0.4201, 0.5124],
          [0.4545, 0.5511, 0.4926,  ..., 0.4823, 0.4718, 0.5049],
          [0.5453, 0.4661, 0.5117,  ..., 0.3821, 0.5080, 0.5583],
          [0.5209, 0.6039, 0.5988,  ..., 0.4040, 0.6072, 0.3933]],

         ...,

         [[0.4097, 0.5107, 0.5818,  ..., 0.5237, 0.5988, 0.4192],
          [0.3739, 0.5256, 0.6298,  ..., 0.4373, 0.3684, 0.4586],
          [0.5946, 0.5174, 0.4890,  ..., 0.4026, 0.4974, 0.4557],
          [0.4642, 0.4263, 0.5955,  ..., 0.5417, 0.4369, 0.6442]],

         [[0.5631, 0.4944, 0.4467,  ..., 0.7506, 0.4249, 0.5190],
          [0.4923, 0.2822, 0.5651,  ..., 0.5453, 0.5955, 0.5554],
          [0.5067, 0.5607, 0.6478,  ..., 0.6800, 0.6220, 0.4744],
          [0.5438, 0.6671, 0.4325,  ..., 0.4111, 0.3975, 0.4354]],

         [[0.6575, 0.5460, 0.4891,  ..., 0.4429, 0.6049, 0.5722],
          [0.6067, 0.4627, 0.4422,  ..., 0.4727, 0.5130, 0.4069],
          [0.4706, 0.4340, 0.4787,  ..., 0.5257, 0.5784, 0.4857],
          [0.3895, 0.4516, 0.4879,  ..., 0.5295, 0.5988, 0.4292]]],


        [[[0.4765, 0.4906, 0.6834,  ..., 0.4477, 0.4330, 0.4948],
          [0.4876, 0.5329, 0.5566,  ..., 0.4771, 0.6179, 0.3961],
          [0.5085, 0.4722, 0.4681,  ..., 0.6918, 0.5583, 0.5271],
          [0.5215, 0.6496, 0.4569,  ..., 0.5804, 0.3766, 0.4211]],

         [[0.6478, 0.5433, 0.4097,  ..., 0.4916, 0.5112, 0.5028],
          [0.4846, 0.3469, 0.4797,  ..., 0.5234, 0.5978, 0.4683],
          [0.4292, 0.4666, 0.6035,  ..., 0.5178, 0.5974, 0.5193],
          [0.5941, 0.5224, 0.3702,  ..., 0.5050, 0.4637, 0.3174]],

         [[0.5414, 0.4815, 0.4345,  ..., 0.5660, 0.4769, 0.5071],
          [0.5718, 0.3522, 0.4487,  ..., 0.5482, 0.6058, 0.5200],
          [0.6334, 0.5392, 0.4235,  ..., 0.4829, 0.3951, 0.2934],
          [0.6654, 0.4943, 0.4998,  ..., 0.6058, 0.4593, 0.6058]],

         ...,

         [[0.6809, 0.6671, 0.5392,  ..., 0.5250, 0.4576, 0.5516],
          [0.5742, 0.4705, 0.4586,  ..., 0.3914, 0.4412, 0.5894],
          [0.4173, 0.6424, 0.4135,  ..., 0.5220, 0.5842, 0.3919],
          [0.5622, 0.7564, 0.5908,  ..., 0.6280, 0.3585, 0.4479]],

         [[0.4441, 0.5588, 0.3998,  ..., 0.3984, 0.5392, 0.5813],
          [0.5272, 0.5600, 0.3979,  ..., 0.6105, 0.5627, 0.5889],
          [0.3720, 0.5139, 0.6817,  ..., 0.4429, 0.4424, 0.5034],
          [0.4475, 0.3693, 0.5109,  ..., 0.5827, 0.5056, 0.5751]],

         [[0.3684, 0.4069, 0.4158,  ..., 0.3854, 0.5294, 0.5789],
          [0.4734, 0.5448, 0.3675,  ..., 0.3711, 0.4499, 0.6379],
          [0.4896, 0.5665, 0.5516,  ..., 0.6211, 0.5600, 0.5619],
          [0.5065, 0.5484, 0.5463,  ..., 0.5349, 0.4292, 0.5542]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0130, -0.0010,  0.0090,  0.0010,  0.0090,  0.0050, -0.0050,  0.0070,
         0.0110, -0.0070], device='cuda:0')
selected experts tensor([1531, 1687, 1629, 1593, 1617, 1574, 1666, 1637, 1733, 1717],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5120, 0.5261, 0.5025,  ..., 0.4124, 0.5791, 0.4308],
          [0.4331, 0.3773, 0.4692,  ..., 0.5187, 0.4874, 0.5146],
          [0.6632, 0.4385, 0.6247,  ..., 0.5411, 0.5666, 0.2984],
          [0.4507, 0.5180, 0.3904,  ..., 0.5544, 0.5933, 0.5029]],

         [[0.5613, 0.6786, 0.4166,  ..., 0.5920, 0.4996, 0.5168],
          [0.4866, 0.6352, 0.3839,  ..., 0.6250, 0.5495, 0.5500],
          [0.5012, 0.3477, 0.6192,  ..., 0.4833, 0.6300, 0.3207],
          [0.4355, 0.2926, 0.4888,  ..., 0.4858, 0.5386, 0.5650]],

         [[0.5800, 0.5093, 0.6366,  ..., 0.4967, 0.6093, 0.5351],
          [0.4774, 0.4845, 0.6833,  ..., 0.6139, 0.4907, 0.5768],
          [0.3714, 0.3819, 0.4926,  ..., 0.4205, 0.6856, 0.4648],
          [0.5451, 0.4564, 0.5868,  ..., 0.6112, 0.4346, 0.4744]],

         ...,

         [[0.5126, 0.5033, 0.5705,  ..., 0.3875, 0.4983, 0.4161],
          [0.4647, 0.5541, 0.6739,  ..., 0.3963, 0.5114, 0.5710],
          [0.5291, 0.4544, 0.5532,  ..., 0.6088, 0.5058, 0.5972],
          [0.5224, 0.5982, 0.3936,  ..., 0.5568, 0.3732, 0.4824]],

         [[0.4858, 0.5312, 0.3936,  ..., 0.4949, 0.5279, 0.5340],
          [0.4184, 0.6010, 0.5873,  ..., 0.6728, 0.4375, 0.5996],
          [0.4075, 0.6199, 0.4280,  ..., 0.3035, 0.5748, 0.4694],
          [0.4756, 0.5131, 0.5285,  ..., 0.3903, 0.5094, 0.5720]],

         [[0.4854, 0.5884, 0.5042,  ..., 0.5606, 0.4284, 0.5479],
          [0.3120, 0.4754, 0.4839,  ..., 0.5769, 0.5652, 0.5701],
          [0.4403, 0.4748, 0.4227,  ..., 0.3351, 0.4856, 0.4361],
          [0.5886, 0.5987, 0.4076,  ..., 0.5488, 0.4160, 0.4697]]],


        [[[0.6148, 0.4435, 0.4824,  ..., 0.5110, 0.4312, 0.5447],
          [0.5045, 0.5103, 0.6617,  ..., 0.5289, 0.6328, 0.3250],
          [0.5507, 0.5575, 0.4109,  ..., 0.5061, 0.4524, 0.6247],
          [0.4726, 0.5331, 0.4943,  ..., 0.5055, 0.5743, 0.6643]],

         [[0.5320, 0.5525, 0.5304,  ..., 0.4408, 0.6889, 0.4313],
          [0.4459, 0.6477, 0.6617,  ..., 0.3681, 0.3615, 0.4171],
          [0.5748, 0.4983, 0.3780,  ..., 0.6598, 0.5141, 0.4766],
          [0.4061, 0.4756, 0.7406,  ..., 0.5911, 0.5080, 0.5038]],

         [[0.4548, 0.4045, 0.4498,  ..., 0.5072, 0.3778, 0.6150],
          [0.6640, 0.6190, 0.4624,  ..., 0.4435, 0.4502, 0.5645],
          [0.6018, 0.4707, 0.5696,  ..., 0.6403, 0.5052, 0.5386],
          [0.6544, 0.5259, 0.4915,  ..., 0.4205, 0.5483, 0.4843]],

         ...,

         [[0.5890, 0.6307, 0.4723,  ..., 0.5673, 0.5025, 0.4638],
          [0.4957, 0.4611, 0.4323,  ..., 0.6331, 0.4284, 0.3848],
          [0.4555, 0.7016, 0.4435,  ..., 0.4829, 0.4480, 0.5725],
          [0.5752, 0.5153, 0.3876,  ..., 0.4746, 0.5473, 0.4337]],

         [[0.6915, 0.4726, 0.6660,  ..., 0.5384, 0.5560, 0.5474],
          [0.4023, 0.5008, 0.3997,  ..., 0.4614, 0.5219, 0.3780],
          [0.3516, 0.3885, 0.4898,  ..., 0.6061, 0.4459, 0.4409],
          [0.4274, 0.4645, 0.5057,  ..., 0.6268, 0.4698, 0.6850]],

         [[0.4203, 0.4744, 0.3825,  ..., 0.5539, 0.4207, 0.5556],
          [0.4863, 0.4616, 0.4510,  ..., 0.4837, 0.4399, 0.5232],
          [0.5026, 0.5642, 0.5333,  ..., 0.5020, 0.3815, 0.4409],
          [0.5176, 0.4535, 0.5825,  ..., 0.4636, 0.3893, 0.4081]]]],
       device='cuda:0')
tensor([[[[0.5090, 0.5351, 0.4975,  ..., 0.4154, 0.5761, 0.4258],
          [0.4301, 0.3863, 0.4642,  ..., 0.5217, 0.4844, 0.5096],
          [0.6602, 0.4475, 0.6197,  ..., 0.5441, 0.5636, 0.2934],
          [0.4477, 0.5270, 0.3854,  ..., 0.5574, 0.5903, 0.4979]],

         [[0.5583, 0.6876, 0.4116,  ..., 0.5950, 0.4966, 0.5118],
          [0.4836, 0.6442, 0.3789,  ..., 0.6280, 0.5465, 0.5450],
          [0.4982, 0.3567, 0.6142,  ..., 0.4863, 0.6270, 0.3157],
          [0.4325, 0.3016, 0.4838,  ..., 0.4888, 0.5356, 0.5600]],

         [[0.5770, 0.5183, 0.6316,  ..., 0.4997, 0.6063, 0.5301],
          [0.4744, 0.4935, 0.6783,  ..., 0.6169, 0.4877, 0.5718],
          [0.3684, 0.3909, 0.4876,  ..., 0.4235, 0.6826, 0.4598],
          [0.5421, 0.4654, 0.5818,  ..., 0.6142, 0.4316, 0.4694]],

         ...,

         [[0.5096, 0.5123, 0.5655,  ..., 0.3905, 0.4953, 0.4111],
          [0.4617, 0.5631, 0.6689,  ..., 0.3993, 0.5084, 0.5660],
          [0.5261, 0.4634, 0.5482,  ..., 0.6118, 0.5028, 0.5922],
          [0.5194, 0.6072, 0.3886,  ..., 0.5598, 0.3702, 0.4774]],

         [[0.4828, 0.5402, 0.3886,  ..., 0.4979, 0.5249, 0.5290],
          [0.4154, 0.6100, 0.5823,  ..., 0.6758, 0.4345, 0.5946],
          [0.4045, 0.6289, 0.4230,  ..., 0.3065, 0.5718, 0.4644],
          [0.4726, 0.5221, 0.5235,  ..., 0.3933, 0.5064, 0.5670]],

         [[0.4824, 0.5974, 0.4992,  ..., 0.5636, 0.4254, 0.5429],
          [0.3090, 0.4844, 0.4789,  ..., 0.5799, 0.5622, 0.5651],
          [0.4373, 0.4838, 0.4177,  ..., 0.3381, 0.4826, 0.4311],
          [0.5856, 0.6077, 0.4026,  ..., 0.5518, 0.4130, 0.4647]]],


        [[[0.6118, 0.4525, 0.4774,  ..., 0.5140, 0.4282, 0.5397],
          [0.5015, 0.5193, 0.6567,  ..., 0.5319, 0.6298, 0.3200],
          [0.5477, 0.5665, 0.4059,  ..., 0.5091, 0.4494, 0.6197],
          [0.4696, 0.5421, 0.4893,  ..., 0.5085, 0.5713, 0.6593]],

         [[0.5290, 0.5615, 0.5254,  ..., 0.4438, 0.6859, 0.4263],
          [0.4429, 0.6567, 0.6567,  ..., 0.3711, 0.3585, 0.4121],
          [0.5718, 0.5073, 0.3730,  ..., 0.6628, 0.5111, 0.4716],
          [0.4031, 0.4846, 0.7356,  ..., 0.5941, 0.5050, 0.4988]],

         [[0.4518, 0.4135, 0.4448,  ..., 0.5102, 0.3748, 0.6100],
          [0.6610, 0.6280, 0.4574,  ..., 0.4465, 0.4472, 0.5595],
          [0.5988, 0.4797, 0.5646,  ..., 0.6433, 0.5022, 0.5336],
          [0.6514, 0.5349, 0.4865,  ..., 0.4235, 0.5453, 0.4793]],

         ...,

         [[0.5860, 0.6397, 0.4673,  ..., 0.5703, 0.4995, 0.4588],
          [0.4927, 0.4701, 0.4273,  ..., 0.6361, 0.4254, 0.3798],
          [0.4525, 0.7106, 0.4385,  ..., 0.4859, 0.4450, 0.5675],
          [0.5722, 0.5243, 0.3826,  ..., 0.4776, 0.5443, 0.4287]],

         [[0.6885, 0.4816, 0.6610,  ..., 0.5414, 0.5530, 0.5424],
          [0.3993, 0.5098, 0.3947,  ..., 0.4644, 0.5189, 0.3730],
          [0.3486, 0.3975, 0.4848,  ..., 0.6091, 0.4429, 0.4359],
          [0.4244, 0.4735, 0.5007,  ..., 0.6298, 0.4668, 0.6800]],

         [[0.4173, 0.4834, 0.3775,  ..., 0.5569, 0.4177, 0.5506],
          [0.4833, 0.4706, 0.4460,  ..., 0.4867, 0.4369, 0.5182],
          [0.4996, 0.5732, 0.5283,  ..., 0.5050, 0.3785, 0.4359],
          [0.5146, 0.4625, 0.5775,  ..., 0.4666, 0.3863, 0.4031]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0030, -0.0090,  0.0050, -0.0010,  0.0030,  0.0050, -0.0070, -0.0030,
         0.0030,  0.0050], device='cuda:0')
selected experts tensor([1624, 1662, 1622, 1642, 1671, 1677, 1630, 1653, 1608, 1595],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5267, 0.5336, 0.4727,  ..., 0.4606, 0.3172, 0.4620],
          [0.4764, 0.5626, 0.4960,  ..., 0.3364, 0.3990, 0.3824],
          [0.4698, 0.5065, 0.5408,  ..., 0.6221, 0.5597, 0.5502],
          [0.6156, 0.4491, 0.3589,  ..., 0.5967, 0.4651, 0.5416]],

         [[0.5239, 0.4092, 0.3906,  ..., 0.5365, 0.3599, 0.4824],
          [0.5704, 0.4689, 0.5022,  ..., 0.4816, 0.3846, 0.4753],
          [0.3730, 0.5245, 0.5739,  ..., 0.5411, 0.4776, 0.5138],
          [0.6115, 0.4486, 0.3544,  ..., 0.5870, 0.5279, 0.5259]],

         [[0.3878, 0.6324, 0.5282,  ..., 0.6795, 0.4438, 0.4395],
          [0.5456, 0.4707, 0.6138,  ..., 0.5266, 0.3470, 0.5578],
          [0.5429, 0.5010, 0.4415,  ..., 0.5814, 0.5554, 0.4927],
          [0.5533, 0.4805, 0.4497,  ..., 0.5958, 0.5465, 0.4043]],

         ...,

         [[0.4202, 0.3765, 0.4982,  ..., 0.5884, 0.5408, 0.4731],
          [0.3985, 0.5341, 0.6338,  ..., 0.4278, 0.3963, 0.5259],
          [0.6012, 0.4926, 0.5313,  ..., 0.3391, 0.6412, 0.4417],
          [0.5780, 0.5260, 0.5168,  ..., 0.4293, 0.5611, 0.4856]],

         [[0.5141, 0.4335, 0.4170,  ..., 0.4095, 0.3140, 0.3509],
          [0.4074, 0.3674, 0.6092,  ..., 0.3843, 0.3573, 0.4368],
          [0.4460, 0.5983, 0.6835,  ..., 0.5254, 0.5485, 0.5230],
          [0.4949, 0.2981, 0.3192,  ..., 0.5194, 0.4718, 0.5416]],

         [[0.5837, 0.3593, 0.4808,  ..., 0.5066, 0.4771, 0.3815],
          [0.4540, 0.5171, 0.4942,  ..., 0.4664, 0.4428, 0.3928],
          [0.4560, 0.3885, 0.6767,  ..., 0.4353, 0.5348, 0.6663],
          [0.4278, 0.3899, 0.4405,  ..., 0.4387, 0.6398, 0.5673]]],


        [[[0.4720, 0.5168, 0.3859,  ..., 0.5833, 0.3068, 0.4134],
          [0.3808, 0.4092, 0.4741,  ..., 0.4167, 0.4036, 0.5621],
          [0.3287, 0.5343, 0.4942,  ..., 0.4544, 0.4803, 0.4192],
          [0.4311, 0.4785, 0.4213,  ..., 0.6467, 0.5373, 0.6319]],

         [[0.4737, 0.5850, 0.4280,  ..., 0.5060, 0.4175, 0.4941],
          [0.5014, 0.3371, 0.4739,  ..., 0.4630, 0.4400, 0.5814],
          [0.6461, 0.6900, 0.5521,  ..., 0.6795, 0.6211, 0.5218],
          [0.4933, 0.4610, 0.4524,  ..., 0.6104, 0.6145, 0.6167]],

         [[0.6235, 0.4092, 0.4355,  ..., 0.4170, 0.3495, 0.4756],
          [0.4951, 0.4059, 0.5112,  ..., 0.3427, 0.3810, 0.4682],
          [0.4369, 0.4344, 0.5330,  ..., 0.3642, 0.4161, 0.4855],
          [0.5131, 0.4002, 0.4762,  ..., 0.4163, 0.4689, 0.5879]],

         ...,

         [[0.5723, 0.5817, 0.4757,  ..., 0.5170, 0.3651, 0.3707],
          [0.4799, 0.4801, 0.5867,  ..., 0.4124, 0.4586, 0.5324],
          [0.5195, 0.3190, 0.6202,  ..., 0.5099, 0.5670, 0.6663],
          [0.5637, 0.4068, 0.4454,  ..., 0.6122, 0.4438, 0.4734]],

         [[0.6827, 0.4696, 0.5410,  ..., 0.5116, 0.4462, 0.4790],
          [0.5885, 0.4474, 0.3428,  ..., 0.4799, 0.3555, 0.6122],
          [0.5555, 0.4244, 0.5103,  ..., 0.4208, 0.6342, 0.4801],
          [0.5998, 0.4622, 0.4716,  ..., 0.6059, 0.4694, 0.4368]],

         [[0.4947, 0.4908, 0.4990,  ..., 0.5315, 0.4198, 0.5535],
          [0.4121, 0.4663, 0.3901,  ..., 0.4158, 0.3504, 0.4204],
          [0.5572, 0.4913, 0.4922,  ..., 0.5293, 0.5208, 0.5569],
          [0.5833, 0.4523, 0.4331,  ..., 0.5888, 0.4610, 0.5150]]]],
       device='cuda:0')
tensor([[[[0.5257, 0.5346, 0.4777,  ..., 0.4836, 0.2902, 0.4850],
          [0.4754, 0.5636, 0.5010,  ..., 0.3594, 0.3720, 0.4054],
          [0.4688, 0.5075, 0.5458,  ..., 0.6451, 0.5327, 0.5732],
          [0.6146, 0.4501, 0.3639,  ..., 0.6197, 0.4381, 0.5646]],

         [[0.5229, 0.4102, 0.3956,  ..., 0.5595, 0.3329, 0.5054],
          [0.5694, 0.4699, 0.5072,  ..., 0.5046, 0.3576, 0.4983],
          [0.3720, 0.5255, 0.5789,  ..., 0.5641, 0.4506, 0.5368],
          [0.6105, 0.4496, 0.3594,  ..., 0.6100, 0.5009, 0.5489]],

         [[0.3868, 0.6334, 0.5332,  ..., 0.7025, 0.4168, 0.4625],
          [0.5446, 0.4717, 0.6188,  ..., 0.5496, 0.3200, 0.5808],
          [0.5419, 0.5020, 0.4465,  ..., 0.6044, 0.5284, 0.5157],
          [0.5523, 0.4815, 0.4547,  ..., 0.6188, 0.5195, 0.4273]],

         ...,

         [[0.4192, 0.3775, 0.5032,  ..., 0.6114, 0.5138, 0.4961],
          [0.3975, 0.5351, 0.6388,  ..., 0.4508, 0.3693, 0.5489],
          [0.6002, 0.4936, 0.5363,  ..., 0.3621, 0.6142, 0.4647],
          [0.5770, 0.5270, 0.5218,  ..., 0.4523, 0.5341, 0.5086]],

         [[0.5131, 0.4345, 0.4220,  ..., 0.4325, 0.2870, 0.3739],
          [0.4064, 0.3684, 0.6142,  ..., 0.4073, 0.3303, 0.4598],
          [0.4450, 0.5993, 0.6885,  ..., 0.5484, 0.5215, 0.5460],
          [0.4939, 0.2991, 0.3242,  ..., 0.5424, 0.4448, 0.5646]],

         [[0.5827, 0.3603, 0.4858,  ..., 0.5296, 0.4501, 0.4045],
          [0.4530, 0.5181, 0.4992,  ..., 0.4894, 0.4158, 0.4158],
          [0.4550, 0.3895, 0.6817,  ..., 0.4583, 0.5078, 0.6893],
          [0.4268, 0.3909, 0.4455,  ..., 0.4617, 0.6128, 0.5903]]],


        [[[0.4710, 0.5178, 0.3909,  ..., 0.6063, 0.2798, 0.4364],
          [0.3798, 0.4102, 0.4791,  ..., 0.4397, 0.3766, 0.5851],
          [0.3277, 0.5353, 0.4992,  ..., 0.4774, 0.4533, 0.4422],
          [0.4301, 0.4795, 0.4263,  ..., 0.6697, 0.5103, 0.6549]],

         [[0.4727, 0.5860, 0.4330,  ..., 0.5290, 0.3905, 0.5171],
          [0.5004, 0.3381, 0.4789,  ..., 0.4860, 0.4130, 0.6044],
          [0.6451, 0.6910, 0.5571,  ..., 0.7025, 0.5941, 0.5448],
          [0.4923, 0.4620, 0.4574,  ..., 0.6334, 0.5875, 0.6397]],

         [[0.6225, 0.4102, 0.4405,  ..., 0.4400, 0.3225, 0.4986],
          [0.4941, 0.4069, 0.5162,  ..., 0.3657, 0.3540, 0.4912],
          [0.4359, 0.4354, 0.5380,  ..., 0.3872, 0.3891, 0.5085],
          [0.5121, 0.4012, 0.4812,  ..., 0.4393, 0.4419, 0.6109]],

         ...,

         [[0.5713, 0.5827, 0.4807,  ..., 0.5400, 0.3381, 0.3937],
          [0.4789, 0.4811, 0.5917,  ..., 0.4354, 0.4316, 0.5554],
          [0.5185, 0.3200, 0.6252,  ..., 0.5329, 0.5400, 0.6893],
          [0.5627, 0.4078, 0.4504,  ..., 0.6352, 0.4168, 0.4964]],

         [[0.6817, 0.4706, 0.5460,  ..., 0.5346, 0.4192, 0.5020],
          [0.5875, 0.4484, 0.3478,  ..., 0.5029, 0.3285, 0.6352],
          [0.5545, 0.4254, 0.5153,  ..., 0.4438, 0.6072, 0.5031],
          [0.5988, 0.4632, 0.4766,  ..., 0.6289, 0.4424, 0.4598]],

         [[0.4937, 0.4918, 0.5040,  ..., 0.5545, 0.3928, 0.5765],
          [0.4111, 0.4673, 0.3951,  ..., 0.4388, 0.3234, 0.4434],
          [0.5562, 0.4923, 0.4972,  ..., 0.5523, 0.4938, 0.5799],
          [0.5823, 0.4533, 0.4381,  ..., 0.6118, 0.4340, 0.5380]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([ 0.0010, -0.0010, -0.0050,  0.0030,  0.0030, -0.0150,  0.0110, -0.0230,
         0.0270, -0.0230], device='cuda:0')
selected experts tensor([1547, 1623, 1501, 1895, 1569, 2071, 1387, 2247,  732, 1812],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.5708, 0.5693, 0.7048,  ..., 0.5454, 0.5350, 0.4781],
          [0.5643, 0.5669, 0.4222,  ..., 0.4825, 0.5242, 0.4926],
          [0.3659, 0.4103, 0.4665,  ..., 0.4890, 0.6828, 0.4436],
          [0.7061, 0.5999, 0.5197,  ..., 0.5064, 0.4903, 0.5483]],

         [[0.5628, 0.5761, 0.5752,  ..., 0.4408, 0.5145, 0.5154],
          [0.5247, 0.4871, 0.5063,  ..., 0.4840, 0.5533, 0.4786],
          [0.4245, 0.4904, 0.4811,  ..., 0.4246, 0.5821, 0.4293],
          [0.6408, 0.4875, 0.4600,  ..., 0.5566, 0.4847, 0.5795]],

         [[0.5072, 0.6480, 0.7031,  ..., 0.4908, 0.6422, 0.4837],
          [0.4335, 0.5162, 0.4982,  ..., 0.5378, 0.5453, 0.5612],
          [0.5295, 0.4368, 0.5469,  ..., 0.3751, 0.5651, 0.4470],
          [0.6362, 0.6498, 0.3751,  ..., 0.5544, 0.4678, 0.5420]],

         ...,

         [[0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130]],

         [[0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130]],

         [[0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130]]],


        [[[0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130]],

         [[0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130]],

         [[0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130]],

         ...,

         [[0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130]],

         [[0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130]],

         [[0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130],
          [0.5110, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5130]]]],
       device='cuda:0')
tensor([[[[0.5598, 0.5583, 0.6918,  ..., 0.5324, 0.5280, 0.4651],
          [0.5533, 0.5559, 0.4092,  ..., 0.4695, 0.5172, 0.4796],
          [0.3549, 0.3993, 0.4535,  ..., 0.4760, 0.6758, 0.4306],
          [0.6951, 0.5889, 0.5067,  ..., 0.4934, 0.4833, 0.5353]],

         [[0.5518, 0.5651, 0.5622,  ..., 0.4278, 0.5075, 0.5024],
          [0.5137, 0.4761, 0.4933,  ..., 0.4710, 0.5463, 0.4656],
          [0.4135, 0.4794, 0.4681,  ..., 0.4116, 0.5751, 0.4163],
          [0.6298, 0.4765, 0.4470,  ..., 0.5436, 0.4777, 0.5665]],

         [[0.4962, 0.6370, 0.6901,  ..., 0.4778, 0.6352, 0.4707],
          [0.4225, 0.5052, 0.4852,  ..., 0.5248, 0.5383, 0.5482],
          [0.5185, 0.4258, 0.5339,  ..., 0.3621, 0.5581, 0.4340],
          [0.6252, 0.6388, 0.3621,  ..., 0.5414, 0.4608, 0.5290]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0110, 0.0110, 0.0130, 0.0130, 0.0130, 0.0090, 0.0110, 0.0130, 0.0070,
        0.0130], device='cuda:0')
selected experts tensor([ 427,  743, 3849, 3651,  445,  822,  826,  390,  817,  318],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5243, 0.3079, 0.5134,  ..., 0.4557, 0.5026, 0.4678],
          [0.5265, 0.4848, 0.5129,  ..., 0.5716, 0.3739, 0.4022],
          [0.4158, 0.5850, 0.4822,  ..., 0.4712, 0.4548, 0.4342],
          [0.4713, 0.6232, 0.5279,  ..., 0.5257, 0.5623, 0.5496]],

         [[0.4053, 0.3361, 0.4815,  ..., 0.4598, 0.6452, 0.5049],
          [0.3619, 0.4784, 0.3811,  ..., 0.5094, 0.3676, 0.4226],
          [0.6706, 0.4248, 0.5337,  ..., 0.5523, 0.6251, 0.3677],
          [0.4328, 0.5486, 0.4282,  ..., 0.5054, 0.4992, 0.4260]],

         [[0.5250, 0.4129, 0.5700,  ..., 0.3487, 0.5818, 0.4511],
          [0.5703, 0.4224, 0.5875,  ..., 0.5421, 0.6107, 0.4174],
          [0.4355, 0.5042, 0.5946,  ..., 0.4480, 0.4693, 0.5305],
          [0.5171, 0.5812, 0.5794,  ..., 0.6194, 0.4979, 0.5008]],

         ...,

         [[0.6049, 0.4491, 0.5565,  ..., 0.5307, 0.5765, 0.6236],
          [0.5195, 0.5655, 0.6389,  ..., 0.4163, 0.4216, 0.5969],
          [0.5095, 0.5440, 0.4880,  ..., 0.4196, 0.4693, 0.3487],
          [0.2962, 0.6089, 0.4926,  ..., 0.4879, 0.3775, 0.4552]],

         [[0.3831, 0.6712, 0.4952,  ..., 0.5275, 0.4277, 0.3774],
          [0.5364, 0.4472, 0.5592,  ..., 0.4856, 0.6815, 0.4198],
          [0.5177, 0.5850, 0.5490,  ..., 0.5499, 0.5731, 0.5183],
          [0.5555, 0.5517, 0.4282,  ..., 0.5280, 0.5035, 0.5146]],

         [[0.4699, 0.5089, 0.4933,  ..., 0.4639, 0.3839, 0.4830],
          [0.5750, 0.5457, 0.5504,  ..., 0.3656, 0.5177, 0.5945],
          [0.4153, 0.4358, 0.5164,  ..., 0.5272, 0.6046, 0.5542],
          [0.4405, 0.6094, 0.4335,  ..., 0.5034, 0.5789, 0.5969]]],


        [[[0.5516, 0.5549, 0.4211,  ..., 0.4739, 0.3586, 0.5208],
          [0.4430, 0.3769, 0.6163,  ..., 0.5289, 0.4669, 0.5537],
          [0.5736, 0.5883, 0.4985,  ..., 0.4041, 0.5164, 0.5503],
          [0.5067, 0.4411, 0.4725,  ..., 0.4281, 0.5533, 0.5799]],

         [[0.4321, 0.5563, 0.4103,  ..., 0.4069, 0.5490, 0.5743],
          [0.5152, 0.5580, 0.4079,  ..., 0.6185, 0.5727, 0.5809],
          [0.3600, 0.5117, 0.6917,  ..., 0.4506, 0.4524, 0.4955],
          [0.4355, 0.3673, 0.5207,  ..., 0.5907, 0.5155, 0.5666]],

         [[0.4273, 0.6898, 0.5705,  ..., 0.4593, 0.5979, 0.3895],
          [0.6612, 0.6010, 0.5846,  ..., 0.3773, 0.5294, 0.4308],
          [0.3789, 0.3583, 0.4608,  ..., 0.5016, 0.4277, 0.4474],
          [0.5345, 0.5070, 0.4349,  ..., 0.5263, 0.5313, 0.3433]],

         ...,

         [[0.6723, 0.3274, 0.3830,  ..., 0.5678, 0.4211, 0.4241],
          [0.6385, 0.5265, 0.4703,  ..., 0.4906, 0.4028, 0.5671],
          [0.4852, 0.4955, 0.4254,  ..., 0.5605, 0.4330, 0.5015],
          [0.7058, 0.4576, 0.5282,  ..., 0.4153, 0.3982, 0.3197]],

         [[0.5535, 0.5822, 0.5385,  ..., 0.5062, 0.7325, 0.6172],
          [0.5545, 0.4933, 0.5960,  ..., 0.5591, 0.5401, 0.4849],
          [0.3831, 0.5421, 0.5326,  ..., 0.4550, 0.5485, 0.5799],
          [0.3892, 0.3737, 0.5002,  ..., 0.5370, 0.3857, 0.4308]],

         [[0.4699, 0.5089, 0.4933,  ..., 0.4639, 0.3839, 0.4830],
          [0.5750, 0.5457, 0.5504,  ..., 0.3656, 0.5177, 0.5945],
          [0.4153, 0.4358, 0.5164,  ..., 0.5272, 0.6046, 0.5542],
          [0.4405, 0.6094, 0.4335,  ..., 0.5034, 0.5789, 0.5969]]]],
       device='cuda:0')
tensor([[[[0.5363, 0.3099, 0.5034,  ..., 0.4477, 0.4926, 0.4758],
          [0.5385, 0.4868, 0.5029,  ..., 0.5636, 0.3639, 0.4102],
          [0.4278, 0.5870, 0.4722,  ..., 0.4632, 0.4448, 0.4422],
          [0.4833, 0.6252, 0.5179,  ..., 0.5177, 0.5523, 0.5576]],

         [[0.4173, 0.3381, 0.4715,  ..., 0.4518, 0.6352, 0.5129],
          [0.3739, 0.4804, 0.3711,  ..., 0.5014, 0.3576, 0.4306],
          [0.6826, 0.4268, 0.5237,  ..., 0.5443, 0.6151, 0.3757],
          [0.4448, 0.5506, 0.4182,  ..., 0.4974, 0.4892, 0.4340]],

         [[0.5370, 0.4149, 0.5600,  ..., 0.3407, 0.5718, 0.4591],
          [0.5823, 0.4244, 0.5775,  ..., 0.5341, 0.6007, 0.4254],
          [0.4475, 0.5062, 0.5846,  ..., 0.4400, 0.4593, 0.5385],
          [0.5291, 0.5832, 0.5694,  ..., 0.6114, 0.4879, 0.5088]],

         ...,

         [[0.6169, 0.4511, 0.5465,  ..., 0.5227, 0.5665, 0.6316],
          [0.5315, 0.5675, 0.6289,  ..., 0.4083, 0.4116, 0.6049],
          [0.5215, 0.5460, 0.4780,  ..., 0.4116, 0.4593, 0.3567],
          [0.3082, 0.6109, 0.4826,  ..., 0.4799, 0.3675, 0.4632]],

         [[0.3951, 0.6732, 0.4852,  ..., 0.5195, 0.4177, 0.3854],
          [0.5484, 0.4492, 0.5492,  ..., 0.4776, 0.6715, 0.4278],
          [0.5297, 0.5870, 0.5390,  ..., 0.5419, 0.5631, 0.5263],
          [0.5675, 0.5537, 0.4182,  ..., 0.5200, 0.4935, 0.5226]],

         [[0.4819, 0.5109, 0.4833,  ..., 0.4559, 0.3739, 0.4910],
          [0.5870, 0.5477, 0.5404,  ..., 0.3576, 0.5077, 0.6025],
          [0.4273, 0.4378, 0.5064,  ..., 0.5192, 0.5946, 0.5622],
          [0.4525, 0.6114, 0.4235,  ..., 0.4954, 0.5689, 0.6049]]],


        [[[0.5636, 0.5569, 0.4111,  ..., 0.4659, 0.3486, 0.5288],
          [0.4550, 0.3789, 0.6063,  ..., 0.5209, 0.4569, 0.5617],
          [0.5856, 0.5903, 0.4885,  ..., 0.3961, 0.5064, 0.5583],
          [0.5187, 0.4431, 0.4625,  ..., 0.4201, 0.5433, 0.5879]],

         [[0.4441, 0.5583, 0.4003,  ..., 0.3989, 0.5390, 0.5823],
          [0.5272, 0.5600, 0.3979,  ..., 0.6105, 0.5627, 0.5889],
          [0.3720, 0.5137, 0.6817,  ..., 0.4426, 0.4424, 0.5035],
          [0.4475, 0.3693, 0.5107,  ..., 0.5827, 0.5055, 0.5746]],

         [[0.4393, 0.6918, 0.5605,  ..., 0.4513, 0.5879, 0.3975],
          [0.6732, 0.6030, 0.5746,  ..., 0.3693, 0.5194, 0.4388],
          [0.3909, 0.3603, 0.4508,  ..., 0.4936, 0.4177, 0.4554],
          [0.5465, 0.5090, 0.4249,  ..., 0.5183, 0.5213, 0.3513]],

         ...,

         [[0.6843, 0.3294, 0.3730,  ..., 0.5598, 0.4111, 0.4321],
          [0.6505, 0.5285, 0.4603,  ..., 0.4826, 0.3928, 0.5751],
          [0.4972, 0.4975, 0.4154,  ..., 0.5525, 0.4230, 0.5095],
          [0.7178, 0.4596, 0.5182,  ..., 0.4073, 0.3882, 0.3277]],

         [[0.5655, 0.5842, 0.5285,  ..., 0.4982, 0.7225, 0.6252],
          [0.5665, 0.4953, 0.5860,  ..., 0.5511, 0.5301, 0.4929],
          [0.3951, 0.5441, 0.5226,  ..., 0.4470, 0.5385, 0.5879],
          [0.4012, 0.3757, 0.4902,  ..., 0.5290, 0.3757, 0.4388]],

         [[0.4819, 0.5109, 0.4833,  ..., 0.4559, 0.3739, 0.4910],
          [0.5870, 0.5477, 0.5404,  ..., 0.3576, 0.5077, 0.6025],
          [0.4273, 0.4378, 0.5064,  ..., 0.5192, 0.5946, 0.5622],
          [0.4525, 0.6114, 0.4235,  ..., 0.4954, 0.5689, 0.6049]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0120, -0.0020,  0.0100,  0.0020,  0.0100,  0.0060, -0.0060,  0.0080,
         0.0100, -0.0080], device='cuda:0')
selected experts tensor([1565, 1605, 1729, 1692, 1602, 1615, 1538, 1720, 1683, 1635],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4194, 0.3903, 0.4950,  ..., 0.4314, 0.5347, 0.6132],
          [0.5188, 0.4748, 0.5537,  ..., 0.5797, 0.5113, 0.5687],
          [0.3715, 0.5963, 0.4771,  ..., 0.4384, 0.5471, 0.5311],
          [0.4672, 0.5878, 0.5120,  ..., 0.4814, 0.5602, 0.5493]],

         [[0.3291, 0.3512, 0.4767,  ..., 0.4858, 0.6763, 0.5387],
          [0.5194, 0.5024, 0.4455,  ..., 0.4582, 0.4404, 0.5963],
          [0.6172, 0.4016, 0.5063,  ..., 0.4633, 0.4423, 0.5520],
          [0.5616, 0.5297, 0.4856,  ..., 0.5555, 0.6815, 0.4223]],

         [[0.5976, 0.1995, 0.5663,  ..., 0.4998, 0.4175, 0.5593],
          [0.4655, 0.3675, 0.6556,  ..., 0.5730, 0.6004, 0.5052],
          [0.4428, 0.4564, 0.4185,  ..., 0.4917, 0.4289, 0.5730],
          [0.5164, 0.4192, 0.4510,  ..., 0.4717, 0.5829, 0.3654]],

         ...,

         [[0.5200, 0.4211, 0.5074,  ..., 0.4790, 0.4507, 0.3109],
          [0.4636, 0.5195, 0.5782,  ..., 0.5873, 0.5990, 0.2658],
          [0.4779, 0.4918, 0.5141,  ..., 0.5004, 0.5662, 0.4067],
          [0.5340, 0.4908, 0.5830,  ..., 0.3814, 0.3257, 0.5209]],

         [[0.4941, 0.6000, 0.5612,  ..., 0.6221, 0.3518, 0.6591],
          [0.3779, 0.5001, 0.5209,  ..., 0.4663, 0.4409, 0.5583],
          [0.6789, 0.6161, 0.6266,  ..., 0.3473, 0.5066, 0.4658],
          [0.2838, 0.3884, 0.6151,  ..., 0.5237, 0.5341, 0.5261]],

         [[0.5623, 0.6776, 0.4171,  ..., 0.5896, 0.5011, 0.5174],
          [0.4875, 0.6342, 0.3849,  ..., 0.6240, 0.5500, 0.5508],
          [0.5023, 0.3467, 0.6202,  ..., 0.4820, 0.6310, 0.3217],
          [0.4365, 0.2916, 0.4896,  ..., 0.4850, 0.5396, 0.5658]]],


        [[[0.4534, 0.6563, 0.6349,  ..., 0.4619, 0.4740, 0.6670],
          [0.5459, 0.4030, 0.6043,  ..., 0.5398, 0.4332, 0.4390],
          [0.4602, 0.4430, 0.6001,  ..., 0.3879, 0.6117, 0.6151],
          [0.4409, 0.4923, 0.4185,  ..., 0.6180, 0.5413, 0.4921]],

         [[0.6916, 0.4715, 0.6662,  ..., 0.5369, 0.5570, 0.5496],
          [0.4033, 0.4999, 0.4007,  ..., 0.4607, 0.5228, 0.3790],
          [0.3526, 0.3875, 0.4907,  ..., 0.6051, 0.4469, 0.4419],
          [0.4284, 0.4634, 0.5067,  ..., 0.6258, 0.4711, 0.6860]],

         [[0.3908, 0.5878, 0.6109,  ..., 0.4109, 0.4469, 0.5203],
          [0.5372, 0.4974, 0.4395,  ..., 0.6961, 0.3206, 0.4925],
          [0.3386, 0.6152, 0.5331,  ..., 0.3786, 0.4894, 0.4176],
          [0.4043, 0.5236, 0.6731,  ..., 0.5175, 0.6014, 0.6099]],

         ...,

         [[0.4440, 0.5575, 0.5293,  ..., 0.3893, 0.7114, 0.4498],
          [0.4650, 0.4073, 0.3762,  ..., 0.2484, 0.5558, 0.4627],
          [0.4706, 0.4960, 0.4912,  ..., 0.4118, 0.4156, 0.5111],
          [0.4803, 0.4382, 0.3965,  ..., 0.6157, 0.5082, 0.4385]],

         [[0.4495, 0.3800, 0.4218,  ..., 0.3893, 0.4123, 0.5849],
          [0.5938, 0.3917, 0.3717,  ..., 0.4889, 0.5519, 0.4295],
          [0.5519, 0.3024, 0.5864,  ..., 0.5418, 0.4864, 0.5821],
          [0.6283, 0.6597, 0.4610,  ..., 0.4870, 0.4241, 0.4999]],

         [[0.5623, 0.6776, 0.4171,  ..., 0.5896, 0.5011, 0.5174],
          [0.4875, 0.6342, 0.3849,  ..., 0.6240, 0.5500, 0.5508],
          [0.5023, 0.3467, 0.6202,  ..., 0.4820, 0.6310, 0.3217],
          [0.4365, 0.2916, 0.4896,  ..., 0.4850, 0.5396, 0.5658]]]],
       device='cuda:0')
tensor([[[[0.4154, 0.4003, 0.4890,  ..., 0.4354, 0.5307, 0.6072],
          [0.5148, 0.4848, 0.5477,  ..., 0.5837, 0.5073, 0.5627],
          [0.3675, 0.6063, 0.4711,  ..., 0.4424, 0.5431, 0.5251],
          [0.4632, 0.5978, 0.5060,  ..., 0.4854, 0.5562, 0.5433]],

         [[0.3251, 0.3612, 0.4707,  ..., 0.4898, 0.6723, 0.5327],
          [0.5154, 0.5124, 0.4395,  ..., 0.4622, 0.4364, 0.5903],
          [0.6132, 0.4116, 0.5003,  ..., 0.4673, 0.4383, 0.5460],
          [0.5576, 0.5397, 0.4796,  ..., 0.5595, 0.6775, 0.4163]],

         [[0.5936, 0.2095, 0.5603,  ..., 0.5038, 0.4135, 0.5533],
          [0.4615, 0.3775, 0.6496,  ..., 0.5770, 0.5964, 0.4992],
          [0.4388, 0.4664, 0.4125,  ..., 0.4957, 0.4249, 0.5670],
          [0.5124, 0.4292, 0.4450,  ..., 0.4757, 0.5789, 0.3594]],

         ...,

         [[0.5160, 0.4311, 0.5014,  ..., 0.4830, 0.4467, 0.3049],
          [0.4596, 0.5295, 0.5722,  ..., 0.5913, 0.5950, 0.2598],
          [0.4739, 0.5018, 0.5081,  ..., 0.5044, 0.5622, 0.4007],
          [0.5300, 0.5008, 0.5770,  ..., 0.3854, 0.3217, 0.5149]],

         [[0.4901, 0.6100, 0.5552,  ..., 0.6261, 0.3478, 0.6531],
          [0.3739, 0.5101, 0.5149,  ..., 0.4703, 0.4369, 0.5523],
          [0.6749, 0.6261, 0.6206,  ..., 0.3513, 0.5026, 0.4598],
          [0.2798, 0.3984, 0.6091,  ..., 0.5277, 0.5301, 0.5201]],

         [[0.5583, 0.6876, 0.4111,  ..., 0.5936, 0.4971, 0.5114],
          [0.4835, 0.6442, 0.3789,  ..., 0.6280, 0.5460, 0.5448],
          [0.4983, 0.3567, 0.6142,  ..., 0.4860, 0.6270, 0.3157],
          [0.4325, 0.3016, 0.4836,  ..., 0.4890, 0.5356, 0.5598]]],


        [[[0.4494, 0.6663, 0.6289,  ..., 0.4659, 0.4700, 0.6610],
          [0.5419, 0.4130, 0.5983,  ..., 0.5438, 0.4292, 0.4330],
          [0.4562, 0.4530, 0.5941,  ..., 0.3919, 0.6077, 0.6091],
          [0.4369, 0.5023, 0.4125,  ..., 0.6220, 0.5373, 0.4861]],

         [[0.6876, 0.4815, 0.6602,  ..., 0.5409, 0.5530, 0.5436],
          [0.3993, 0.5099, 0.3947,  ..., 0.4647, 0.5188, 0.3730],
          [0.3486, 0.3975, 0.4847,  ..., 0.6091, 0.4429, 0.4359],
          [0.4244, 0.4734, 0.5007,  ..., 0.6298, 0.4671, 0.6800]],

         [[0.3868, 0.5978, 0.6049,  ..., 0.4149, 0.4429, 0.5143],
          [0.5332, 0.5074, 0.4335,  ..., 0.7001, 0.3166, 0.4865],
          [0.3346, 0.6252, 0.5271,  ..., 0.3826, 0.4854, 0.4116],
          [0.4003, 0.5336, 0.6671,  ..., 0.5215, 0.5974, 0.6039]],

         ...,

         [[0.4400, 0.5675, 0.5233,  ..., 0.3933, 0.7074, 0.4438],
          [0.4610, 0.4173, 0.3702,  ..., 0.2524, 0.5518, 0.4567],
          [0.4666, 0.5060, 0.4852,  ..., 0.4158, 0.4116, 0.5051],
          [0.4763, 0.4482, 0.3905,  ..., 0.6197, 0.5042, 0.4325]],

         [[0.4455, 0.3900, 0.4158,  ..., 0.3933, 0.4083, 0.5789],
          [0.5898, 0.4017, 0.3657,  ..., 0.4929, 0.5479, 0.4235],
          [0.5479, 0.3124, 0.5804,  ..., 0.5458, 0.4824, 0.5761],
          [0.6243, 0.6697, 0.4550,  ..., 0.4910, 0.4201, 0.4939]],

         [[0.5583, 0.6876, 0.4111,  ..., 0.5936, 0.4971, 0.5114],
          [0.4835, 0.6442, 0.3789,  ..., 0.6280, 0.5460, 0.5448],
          [0.4983, 0.3567, 0.6142,  ..., 0.4860, 0.6270, 0.3157],
          [0.4325, 0.3016, 0.4836,  ..., 0.4890, 0.5356, 0.5598]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0040, -0.0100,  0.0060, -0.0020,  0.0020,  0.0040, -0.0060, -0.0040,
         0.0040,  0.0060], device='cuda:0')
selected experts tensor([1677, 1617, 1671, 1685, 1488, 1687, 1629, 1610, 1693, 1627],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4776, 0.4908, 0.5692,  ..., 0.5307, 0.4524, 0.4090],
          [0.5325, 0.4861, 0.3874,  ..., 0.4416, 0.3661, 0.4583],
          [0.5223, 0.4528, 0.6357,  ..., 0.4598, 0.4092, 0.4481],
          [0.6171, 0.3895, 0.4100,  ..., 0.4644, 0.6873, 0.5846]],

         [[0.5594, 0.5908, 0.4300,  ..., 0.3508, 0.4208, 0.3660],
          [0.4079, 0.4537, 0.5663,  ..., 0.4076, 0.4358, 0.4426],
          [0.4326, 0.5397, 0.4868,  ..., 0.6058, 0.5056, 0.5387],
          [0.5011, 0.4040, 0.4156,  ..., 0.5544, 0.4953, 0.6388]],

         [[0.4475, 0.6049, 0.5990,  ..., 0.5521, 0.3766, 0.4820],
          [0.6111, 0.5139, 0.5056,  ..., 0.4831, 0.4278, 0.6405],
          [0.5627, 0.4441, 0.5352,  ..., 0.4954, 0.4805, 0.4768],
          [0.4961, 0.5453, 0.5625,  ..., 0.6030, 0.5885, 0.5943]],

         ...,

         [[0.3805, 0.5433, 0.3953,  ..., 0.5094, 0.3437, 0.4568],
          [0.4567, 0.5433, 0.6357,  ..., 0.4380, 0.4302, 0.6744],
          [0.5591, 0.6142, 0.5943,  ..., 0.4186, 0.5820, 0.3767],
          [0.5142, 0.3730, 0.3599,  ..., 0.6653, 0.7038, 0.5487]],

         [[0.4721, 0.4144, 0.4534,  ..., 0.4454, 0.3574, 0.5126],
          [0.4732, 0.4434, 0.6614,  ..., 0.4293, 0.5224, 0.4506],
          [0.4587, 0.5222, 0.5639,  ..., 0.5162, 0.4931, 0.5439],
          [0.4122, 0.4196, 0.5007,  ..., 0.5157, 0.5870, 0.6335]],

         [[0.5685, 0.3720, 0.3563,  ..., 0.4858, 0.4620, 0.4813],
          [0.5040, 0.5373, 0.6088,  ..., 0.4577, 0.3263, 0.5300],
          [0.4776, 0.5103, 0.5611,  ..., 0.5415, 0.6417, 0.5135],
          [0.5407, 0.4955, 0.4796,  ..., 0.6492, 0.4726, 0.4820]]],


        [[[0.6245, 0.3849, 0.4782,  ..., 0.4522, 0.4581, 0.4490],
          [0.4912, 0.5727, 0.3635,  ..., 0.4805, 0.4269, 0.4085],
          [0.4293, 0.6723, 0.5391,  ..., 0.5391, 0.6041, 0.4573],
          [0.4853, 0.4463, 0.4845,  ..., 0.5473, 0.4567, 0.4672]],

         [[0.4992, 0.5260, 0.5270,  ..., 0.6012, 0.4028, 0.6577],
          [0.4928, 0.5950, 0.5100,  ..., 0.4129, 0.3678, 0.4387],
          [0.4502, 0.4603, 0.5678,  ..., 0.3372, 0.5854, 0.5391],
          [0.4283, 0.5067, 0.5181,  ..., 0.6238, 0.5663, 0.4803]],

         [[0.5198, 0.5099, 0.4085,  ..., 0.4652, 0.4143, 0.5396],
          [0.5871, 0.3840, 0.3818,  ..., 0.3097, 0.2211, 0.4469],
          [0.4379, 0.4593, 0.5206,  ..., 0.5288, 0.5275, 0.4878],
          [0.5060, 0.4794, 0.4825,  ..., 0.7055, 0.4912, 0.4853]],

         ...,

         [[0.3427, 0.4249, 0.4095,  ..., 0.3309, 0.3802, 0.5435],
          [0.4882, 0.4340, 0.4737,  ..., 0.4842, 0.4429, 0.4487],
          [0.4910, 0.4800, 0.5625,  ..., 0.4872, 0.5699, 0.4174],
          [0.4169, 0.4426, 0.4430,  ..., 0.4983, 0.5796, 0.4424]],

         [[0.4393, 0.3909, 0.3883,  ..., 0.4751, 0.5393, 0.4290],
          [0.4686, 0.6128, 0.5410,  ..., 0.4726, 0.4548, 0.4866],
          [0.5742, 0.5006, 0.5507,  ..., 0.4965, 0.4854, 0.3777],
          [0.4878, 0.4552, 0.4531,  ..., 0.6793, 0.6230, 0.5387]],

         [[0.5685, 0.3720, 0.3563,  ..., 0.4858, 0.4620, 0.4813],
          [0.5040, 0.5373, 0.6088,  ..., 0.4577, 0.3263, 0.5300],
          [0.4776, 0.5103, 0.5611,  ..., 0.5415, 0.6417, 0.5135],
          [0.5407, 0.4955, 0.4796,  ..., 0.6492, 0.4726, 0.4820]]]],
       device='cuda:0')
tensor([[[[0.4756, 0.4908, 0.5732,  ..., 0.5547, 0.4244, 0.4330],
          [0.5305, 0.4861, 0.3914,  ..., 0.4656, 0.3381, 0.4823],
          [0.5203, 0.4528, 0.6397,  ..., 0.4838, 0.3812, 0.4721],
          [0.6151, 0.3895, 0.4140,  ..., 0.4884, 0.6593, 0.6086]],

         [[0.5574, 0.5908, 0.4340,  ..., 0.3748, 0.3928, 0.3900],
          [0.4059, 0.4537, 0.5703,  ..., 0.4316, 0.4078, 0.4666],
          [0.4306, 0.5397, 0.4908,  ..., 0.6298, 0.4776, 0.5627],
          [0.4991, 0.4040, 0.4196,  ..., 0.5784, 0.4673, 0.6628]],

         [[0.4455, 0.6049, 0.6030,  ..., 0.5761, 0.3486, 0.5060],
          [0.6091, 0.5139, 0.5096,  ..., 0.5071, 0.3998, 0.6645],
          [0.5607, 0.4441, 0.5392,  ..., 0.5194, 0.4525, 0.5008],
          [0.4941, 0.5453, 0.5665,  ..., 0.6270, 0.5605, 0.6183]],

         ...,

         [[0.3785, 0.5433, 0.3993,  ..., 0.5334, 0.3157, 0.4808],
          [0.4547, 0.5433, 0.6397,  ..., 0.4620, 0.4022, 0.6984],
          [0.5571, 0.6142, 0.5983,  ..., 0.4426, 0.5540, 0.4007],
          [0.5122, 0.3730, 0.3639,  ..., 0.6893, 0.6758, 0.5727]],

         [[0.4701, 0.4144, 0.4574,  ..., 0.4694, 0.3294, 0.5366],
          [0.4712, 0.4434, 0.6654,  ..., 0.4533, 0.4944, 0.4746],
          [0.4567, 0.5222, 0.5679,  ..., 0.5402, 0.4651, 0.5679],
          [0.4102, 0.4196, 0.5047,  ..., 0.5397, 0.5590, 0.6575]],

         [[0.5665, 0.3720, 0.3603,  ..., 0.5098, 0.4340, 0.5053],
          [0.5020, 0.5373, 0.6128,  ..., 0.4817, 0.2983, 0.5540],
          [0.4756, 0.5103, 0.5651,  ..., 0.5655, 0.6137, 0.5375],
          [0.5387, 0.4955, 0.4836,  ..., 0.6732, 0.4446, 0.5060]]],


        [[[0.6225, 0.3849, 0.4822,  ..., 0.4762, 0.4301, 0.4730],
          [0.4892, 0.5727, 0.3675,  ..., 0.5045, 0.3989, 0.4325],
          [0.4273, 0.6723, 0.5431,  ..., 0.5631, 0.5761, 0.4813],
          [0.4833, 0.4463, 0.4885,  ..., 0.5713, 0.4287, 0.4912]],

         [[0.4972, 0.5260, 0.5310,  ..., 0.6252, 0.3748, 0.6817],
          [0.4908, 0.5950, 0.5140,  ..., 0.4369, 0.3398, 0.4627],
          [0.4482, 0.4603, 0.5718,  ..., 0.3612, 0.5574, 0.5631],
          [0.4263, 0.5067, 0.5221,  ..., 0.6478, 0.5383, 0.5043]],

         [[0.5178, 0.5099, 0.4125,  ..., 0.4892, 0.3863, 0.5636],
          [0.5851, 0.3840, 0.3858,  ..., 0.3337, 0.1931, 0.4709],
          [0.4359, 0.4593, 0.5246,  ..., 0.5528, 0.4995, 0.5118],
          [0.5040, 0.4794, 0.4865,  ..., 0.7295, 0.4632, 0.5093]],

         ...,

         [[0.3407, 0.4249, 0.4135,  ..., 0.3549, 0.3522, 0.5675],
          [0.4862, 0.4340, 0.4777,  ..., 0.5082, 0.4149, 0.4727],
          [0.4890, 0.4800, 0.5665,  ..., 0.5112, 0.5419, 0.4414],
          [0.4149, 0.4426, 0.4470,  ..., 0.5223, 0.5516, 0.4664]],

         [[0.4373, 0.3909, 0.3923,  ..., 0.4991, 0.5113, 0.4530],
          [0.4666, 0.6128, 0.5450,  ..., 0.4966, 0.4268, 0.5106],
          [0.5722, 0.5006, 0.5547,  ..., 0.5205, 0.4574, 0.4017],
          [0.4858, 0.4552, 0.4571,  ..., 0.7033, 0.5950, 0.5627]],

         [[0.5665, 0.3720, 0.3603,  ..., 0.5098, 0.4340, 0.5053],
          [0.5020, 0.5373, 0.6128,  ..., 0.4817, 0.2983, 0.5540],
          [0.4756, 0.5103, 0.5651,  ..., 0.5655, 0.6137, 0.5375],
          [0.5387, 0.4955, 0.4836,  ..., 0.6732, 0.4446, 0.5060]]]],
       device='cuda:0', requires_grad=True)
tensor([ 2.0000e-03,  6.9849e-10, -4.0000e-03,  2.0000e-03,  4.0000e-03,
        -1.6000e-02,  1.2000e-02, -2.4000e-02,  2.8000e-02, -2.4000e-02],
       device='cuda:0')
selected experts tensor([1331, 1400, 1500, 1908, 1488, 2261, 1439, 2336, 1110, 1611],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5609, 0.6183, 0.7162,  ..., 0.3734, 0.4898, 0.3938],
          [0.5655, 0.5229, 0.4791,  ..., 0.6165, 0.3855, 0.5366],
          [0.4872, 0.4961, 0.5427,  ..., 0.3098, 0.5812, 0.5479],
          [0.5747, 0.4842, 0.4786,  ..., 0.4743, 0.4968, 0.4432]],

         [[0.5924, 0.5804, 0.7345,  ..., 0.5048, 0.4780, 0.5377],
          [0.6954, 0.5195, 0.4796,  ..., 0.5503, 0.5783, 0.5200],
          [0.4260, 0.4737, 0.3895,  ..., 0.4171, 0.6405, 0.4480],
          [0.6651, 0.6070, 0.5842,  ..., 0.5269, 0.5409, 0.5771]],

         [[0.5241, 0.5218, 0.6454,  ..., 0.5417, 0.3764, 0.4808],
          [0.5756, 0.5852, 0.3960,  ..., 0.6474, 0.5788, 0.4668],
          [0.3580, 0.5260, 0.3741,  ..., 0.4509, 0.5341, 0.4470],
          [0.5551, 0.5077, 0.4590,  ..., 0.5610, 0.5189, 0.6170]],

         ...,

         [[0.5120, 0.5120, 0.5120,  ..., 0.5140, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5140, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5140, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5140, 0.5080, 0.5140]],

         [[0.5120, 0.5120, 0.5120,  ..., 0.5140, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5140, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5140, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5140, 0.5080, 0.5140]],

         [[0.5120, 0.5120, 0.5120,  ..., 0.5140, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5140, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5140, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5140, 0.5080, 0.5140]]],


        [[[0.5909, 0.6526, 0.7337,  ..., 0.4008, 0.5841, 0.3975],
          [0.6018, 0.5682, 0.5771,  ..., 0.5427, 0.5151, 0.3591],
          [0.3978, 0.5276, 0.3633,  ..., 0.3725, 0.4800, 0.4059],
          [0.5539, 0.6472, 0.3786,  ..., 0.4867, 0.6096, 0.6071]],

         [[0.4943, 0.6660, 0.6748,  ..., 0.4675, 0.4700, 0.4091],
          [0.4992, 0.6243, 0.4866,  ..., 0.6724, 0.5301, 0.4629],
          [0.4053, 0.4832, 0.5281,  ..., 0.3915, 0.6450, 0.4031],
          [0.6317, 0.6580, 0.3946,  ..., 0.3582, 0.5065, 0.4867]],

         [[0.5125, 0.5775, 0.6580,  ..., 0.4180, 0.4262, 0.3547],
          [0.5024, 0.6372, 0.4988,  ..., 0.5996, 0.4608, 0.4564],
          [0.5396, 0.5833, 0.4269,  ..., 0.4607, 0.6998, 0.3443],
          [0.6526, 0.6266, 0.4431,  ..., 0.3281, 0.5236, 0.6627]],

         ...,

         [[0.5120, 0.5120, 0.5120,  ..., 0.5140, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5140, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5140, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5140, 0.5080, 0.5140]],

         [[0.5120, 0.5120, 0.5120,  ..., 0.5140, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5140, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5140, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5140, 0.5080, 0.5140]],

         [[0.5120, 0.5120, 0.5120,  ..., 0.5140, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5140, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5140, 0.5080, 0.5140],
          [0.5120, 0.5120, 0.5120,  ..., 0.5140, 0.5080, 0.5140]]]],
       device='cuda:0')/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([[[[0.5489, 0.6063, 0.7042,  ..., 0.3594, 0.4818, 0.3798],
          [0.5535, 0.5109, 0.4671,  ..., 0.6025, 0.3775, 0.5226],
          [0.4752, 0.4841, 0.5307,  ..., 0.2958, 0.5732, 0.5339],
          [0.5627, 0.4722, 0.4666,  ..., 0.4603, 0.4888, 0.4292]],

         [[0.5804, 0.5684, 0.7225,  ..., 0.4908, 0.4700, 0.5237],
          [0.6834, 0.5075, 0.4676,  ..., 0.5363, 0.5703, 0.5060],
          [0.4140, 0.4617, 0.3775,  ..., 0.4031, 0.6325, 0.4340],
          [0.6531, 0.5950, 0.5722,  ..., 0.5129, 0.5329, 0.5631]],

         [[0.5121, 0.5098, 0.6334,  ..., 0.5277, 0.3684, 0.4668],
          [0.5636, 0.5732, 0.3840,  ..., 0.6334, 0.5708, 0.4528],
          [0.3460, 0.5140, 0.3621,  ..., 0.4369, 0.5261, 0.4330],
          [0.5431, 0.4957, 0.4470,  ..., 0.5470, 0.5109, 0.6030]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5789, 0.6406, 0.7217,  ..., 0.3868, 0.5761, 0.3835],
          [0.5898, 0.5562, 0.5651,  ..., 0.5287, 0.5071, 0.3451],
          [0.3858, 0.5156, 0.3513,  ..., 0.3585, 0.4720, 0.3919],
          [0.5419, 0.6352, 0.3666,  ..., 0.4727, 0.6016, 0.5931]],

         [[0.4823, 0.6540, 0.6628,  ..., 0.4535, 0.4620, 0.3951],
          [0.4872, 0.6123, 0.4746,  ..., 0.6584, 0.5221, 0.4489],
          [0.3933, 0.4712, 0.5161,  ..., 0.3775, 0.6370, 0.3891],
          [0.6197, 0.6460, 0.3826,  ..., 0.3442, 0.4985, 0.4727]],

         [[0.5005, 0.5655, 0.6460,  ..., 0.4040, 0.4182, 0.3407],
          [0.4904, 0.6252, 0.4868,  ..., 0.5856, 0.4528, 0.4424],
          [0.5276, 0.5713, 0.4149,  ..., 0.4467, 0.6918, 0.3303],
          [0.6406, 0.6146, 0.4311,  ..., 0.3141, 0.5156, 0.6487]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0120, 0.0120, 0.0120, 0.0120, 0.0140, 0.0100, 0.0120, 0.0140, 0.0080,
        0.0140], device='cuda:0')
selected experts tensor([1004, 1668, 1137,  519, 2828, 2057, 1729, 3080, 1665,  697],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6673, 0.4172, 0.6305,  ..., 0.5869, 0.4487, 0.5264],
          [0.3795, 0.6043, 0.5135,  ..., 0.5457, 0.5237, 0.4327],
          [0.4641, 0.6122, 0.4925,  ..., 0.4542, 0.6415, 0.4825],
          [0.4415, 0.5213, 0.5238,  ..., 0.5639, 0.5898, 0.6113]],

         [[0.6699, 0.6713, 0.5480,  ..., 0.5325, 0.4664, 0.5460],
          [0.5632, 0.4695, 0.4676,  ..., 0.3984, 0.4502, 0.5824],
          [0.4058, 0.6414, 0.4225,  ..., 0.5290, 0.5932, 0.3849],
          [0.5509, 0.7554, 0.5998,  ..., 0.6350, 0.3675, 0.4412]],

         [[0.4863, 0.5674, 0.5065,  ..., 0.5788, 0.4253, 0.5128],
          [0.6332, 0.4301, 0.5470,  ..., 0.5632, 0.5005, 0.5051],
          [0.3748, 0.3485, 0.5150,  ..., 0.5663, 0.5507, 0.4074],
          [0.4345, 0.4311, 0.5865,  ..., 0.4487, 0.5246, 0.5890]],

         ...,

         [[0.3841, 0.5879, 0.6031,  ..., 0.3718, 0.4671, 0.2856],
          [0.4918, 0.4853, 0.5422,  ..., 0.5375, 0.5387, 0.3614],
          [0.5083, 0.4781, 0.5051,  ..., 0.5119, 0.4613, 0.6255],
          [0.5579, 0.5789, 0.4817,  ..., 0.5150, 0.4406, 0.5230]],

         [[0.4011, 0.2588, 0.5207,  ..., 0.3373, 0.4644, 0.4715],
          [0.4517, 0.4825, 0.2718,  ..., 0.6584, 0.4673, 0.4251],
          [0.4924, 0.4869, 0.5368,  ..., 0.3691, 0.5046, 0.3835],
          [0.4638, 0.4272, 0.4439,  ..., 0.5236, 0.4363, 0.5293]],

         [[0.3799, 0.4847, 0.5288,  ..., 0.7038, 0.6083, 0.4883],
          [0.4350, 0.4751, 0.6822,  ..., 0.4702, 0.4557, 0.3215],
          [0.5190, 0.4115, 0.5081,  ..., 0.6340, 0.5177, 0.5359],
          [0.5338, 0.4383, 0.6176,  ..., 0.4571, 0.4325, 0.5443]]],


        [[[0.4196, 0.4234, 0.5625,  ..., 0.5496, 0.5671, 0.3742],
          [0.5473, 0.5518, 0.4572,  ..., 0.5180, 0.6966, 0.5097],
          [0.5224, 0.5067, 0.5291,  ..., 0.3868, 0.5680, 0.5805],
          [0.4478, 0.4564, 0.4724,  ..., 0.5409, 0.4981, 0.6309]],

         [[0.6110, 0.6251, 0.4041,  ..., 0.4424, 0.4618, 0.5152],
          [0.4006, 0.5465, 0.3410,  ..., 0.5299, 0.5038, 0.5714],
          [0.4134, 0.6333, 0.6007,  ..., 0.4956, 0.5346, 0.4696],
          [0.5219, 0.5547, 0.5598,  ..., 0.5088, 0.5130, 0.4540]],

         [[0.5698, 0.5736, 0.6907,  ..., 0.6548, 0.5185, 0.3560],
          [0.4680, 0.5069, 0.5372,  ..., 0.3574, 0.5555, 0.5533],
          [0.4858, 0.6196, 0.5267,  ..., 0.5318, 0.5647, 0.4647],
          [0.4862, 0.6414, 0.6604,  ..., 0.5214, 0.4720, 0.4438]],

         ...,

         [[0.5123, 0.4824, 0.6991,  ..., 0.7071, 0.4377, 0.4480],
          [0.3565, 0.5301, 0.5908,  ..., 0.4803, 0.5697, 0.5866],
          [0.5248, 0.5898, 0.4306,  ..., 0.3512, 0.6388, 0.4017],
          [0.4343, 0.4002, 0.5898,  ..., 0.4871, 0.3290, 0.6662]],

         [[0.4725, 0.5102, 0.4922,  ..., 0.4629, 0.3838, 0.4862],
          [0.5760, 0.5467, 0.5494,  ..., 0.3646, 0.5166, 0.5955],
          [0.4163, 0.4368, 0.5154,  ..., 0.5260, 0.6036, 0.5552],
          [0.4415, 0.6104, 0.4325,  ..., 0.5025, 0.5779, 0.5979]],

         [[0.4139, 0.4306, 0.3856,  ..., 0.5850, 0.4553, 0.5927],
          [0.6639, 0.4663, 0.3506,  ..., 0.3961, 0.5846, 0.6141],
          [0.4583, 0.3983, 0.4976,  ..., 0.5576, 0.6305, 0.4217],
          [0.3288, 0.4296, 0.6036,  ..., 0.4581, 0.6301, 0.5048]]]],
       device='cuda:0')
tensor([[[[0.6783, 0.4182, 0.6215,  ..., 0.5799, 0.4397, 0.5334],
          [0.3905, 0.6053, 0.5045,  ..., 0.5387, 0.5147, 0.4397],
          [0.4751, 0.6132, 0.4835,  ..., 0.4472, 0.6325, 0.4895],
          [0.4525, 0.5223, 0.5148,  ..., 0.5569, 0.5808, 0.6183]],

         [[0.6809, 0.6723, 0.5390,  ..., 0.5255, 0.4574, 0.5530],
          [0.5742, 0.4705, 0.4586,  ..., 0.3914, 0.4412, 0.5894],
          [0.4168, 0.6424, 0.4135,  ..., 0.5220, 0.5842, 0.3919],
          [0.5619, 0.7564, 0.5908,  ..., 0.6280, 0.3585, 0.4482]],

         [[0.4973, 0.5684, 0.4975,  ..., 0.5718, 0.4163, 0.5198],
          [0.6442, 0.4311, 0.5380,  ..., 0.5562, 0.4915, 0.5121],
          [0.3858, 0.3495, 0.5060,  ..., 0.5593, 0.5417, 0.4144],
          [0.4455, 0.4321, 0.5775,  ..., 0.4417, 0.5156, 0.5960]],

         ...,

         [[0.3951, 0.5889, 0.5941,  ..., 0.3648, 0.4581, 0.2926],
          [0.5028, 0.4863, 0.5332,  ..., 0.5305, 0.5297, 0.3684],
          [0.5193, 0.4791, 0.4961,  ..., 0.5049, 0.4523, 0.6325],
          [0.5689, 0.5799, 0.4727,  ..., 0.5080, 0.4316, 0.5300]],

         [[0.4121, 0.2598, 0.5117,  ..., 0.3303, 0.4554, 0.4785],
          [0.4627, 0.4835, 0.2628,  ..., 0.6514, 0.4583, 0.4321],
          [0.5034, 0.4879, 0.5278,  ..., 0.3621, 0.4956, 0.3905],
          [0.4748, 0.4282, 0.4349,  ..., 0.5166, 0.4273, 0.5363]],

         [[0.3909, 0.4857, 0.5198,  ..., 0.6968, 0.5993, 0.4953],
          [0.4460, 0.4761, 0.6732,  ..., 0.4632, 0.4467, 0.3285],
          [0.5300, 0.4125, 0.4991,  ..., 0.6270, 0.5087, 0.5429],
          [0.5448, 0.4393, 0.6086,  ..., 0.4501, 0.4235, 0.5513]]],


        [[[0.4306, 0.4244, 0.5535,  ..., 0.5426, 0.5581, 0.3812],
          [0.5583, 0.5528, 0.4482,  ..., 0.5110, 0.6876, 0.5167],
          [0.5334, 0.5077, 0.5201,  ..., 0.3798, 0.5590, 0.5875],
          [0.4588, 0.4574, 0.4634,  ..., 0.5339, 0.4891, 0.6379]],

         [[0.6220, 0.6261, 0.3951,  ..., 0.4354, 0.4528, 0.5222],
          [0.4116, 0.5475, 0.3320,  ..., 0.5229, 0.4948, 0.5784],
          [0.4244, 0.6343, 0.5917,  ..., 0.4886, 0.5256, 0.4766],
          [0.5329, 0.5557, 0.5508,  ..., 0.5018, 0.5040, 0.4610]],

         [[0.5808, 0.5746, 0.6817,  ..., 0.6478, 0.5095, 0.3630],
          [0.4790, 0.5079, 0.5282,  ..., 0.3504, 0.5465, 0.5603],
          [0.4968, 0.6206, 0.5177,  ..., 0.5248, 0.5557, 0.4717],
          [0.4972, 0.6424, 0.6514,  ..., 0.5144, 0.4630, 0.4508]],

         ...,

         [[0.5233, 0.4834, 0.6901,  ..., 0.7001, 0.4287, 0.4550],
          [0.3675, 0.5311, 0.5818,  ..., 0.4733, 0.5607, 0.5936],
          [0.5358, 0.5908, 0.4216,  ..., 0.3442, 0.6298, 0.4087],
          [0.4453, 0.4012, 0.5808,  ..., 0.4801, 0.3200, 0.6732]],

         [[0.4835, 0.5112, 0.4832,  ..., 0.4559, 0.3748, 0.4932],
          [0.5870, 0.5477, 0.5404,  ..., 0.3576, 0.5076, 0.6025],
          [0.4273, 0.4378, 0.5064,  ..., 0.5190, 0.5946, 0.5622],
          [0.4525, 0.6114, 0.4235,  ..., 0.4955, 0.5689, 0.6049]],

         [[0.4249, 0.4316, 0.3766,  ..., 0.5780, 0.4463, 0.5997],
          [0.6749, 0.4673, 0.3416,  ..., 0.3891, 0.5756, 0.6211],
          [0.4693, 0.3993, 0.4886,  ..., 0.5506, 0.6215, 0.4287],
          [0.3398, 0.4306, 0.5946,  ..., 0.4511, 0.6211, 0.5118]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0110, -0.0010,  0.0090,  0.0010,  0.0110,  0.0070, -0.0050,  0.0070,
         0.0090, -0.0070], device='cuda:0')
selected experts tensor([1643, 1686, 1739, 1583, 1703, 1556, 1628, 1671, 1642, 1533],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4497, 0.4886, 0.5301,  ..., 0.4799, 0.3898, 0.4748],
          [0.4312, 0.5220, 0.5452,  ..., 0.5278, 0.4480, 0.4489],
          [0.5280, 0.4797, 0.4733,  ..., 0.3861, 0.6013, 0.4371],
          [0.5748, 0.5756, 0.5551,  ..., 0.4491, 0.4193, 0.4475]],

         [[0.5876, 0.6325, 0.4721,  ..., 0.5673, 0.5042, 0.4656],
          [0.4958, 0.4614, 0.4323,  ..., 0.6331, 0.4284, 0.3868],
          [0.4558, 0.7016, 0.4438,  ..., 0.4828, 0.4480, 0.5740],
          [0.5757, 0.5154, 0.3881,  ..., 0.4747, 0.5473, 0.4357]],

         [[0.6346, 0.5121, 0.5430,  ..., 0.6598, 0.4819, 0.3347],
          [0.5099, 0.4605, 0.3207,  ..., 0.6181, 0.2868, 0.6440],
          [0.6291, 0.4803, 0.5027,  ..., 0.5012, 0.5814, 0.6733],
          [0.6597, 0.5515, 0.4076,  ..., 0.4534, 0.5352, 0.5797]],

         ...,

         [[0.4165, 0.5271, 0.4933,  ..., 0.5280, 0.3696, 0.5660],
          [0.5321, 0.4593, 0.5317,  ..., 0.4838, 0.5466, 0.3637],
          [0.4958, 0.5327, 0.6136,  ..., 0.4927, 0.5033, 0.4513],
          [0.5834, 0.5059, 0.5537,  ..., 0.3430, 0.6864, 0.6331]],

         [[0.6605, 0.5106, 0.5358,  ..., 0.4883, 0.5819, 0.5037],
          [0.5345, 0.4885, 0.3941,  ..., 0.3035, 0.4743, 0.5516],
          [0.4611, 0.5266, 0.6747,  ..., 0.4411, 0.5264, 0.5783],
          [0.5478, 0.4542, 0.5503,  ..., 0.6779, 0.4543, 0.5406]],

         [[0.3446, 0.3764, 0.6094,  ..., 0.5423, 0.5219, 0.3460],
          [0.4399, 0.4544, 0.7391,  ..., 0.4534, 0.5349, 0.4410],
          [0.5239, 0.6135, 0.4771,  ..., 0.4949, 0.3949, 0.5883],
          [0.4023, 0.4578, 0.3335,  ..., 0.7012, 0.5332, 0.3610]]],


        [[[0.4122, 0.4984, 0.6330,  ..., 0.4590, 0.3624, 0.4989],
          [0.4808, 0.4961, 0.5500,  ..., 0.3555, 0.4394, 0.3347],
          [0.5001, 0.5353, 0.3361,  ..., 0.6176, 0.5393, 0.3781],
          [0.4468, 0.5156, 0.6019,  ..., 0.4838, 0.3455, 0.6179]],

         [[0.3651, 0.5959, 0.5705,  ..., 0.4147, 0.4269, 0.4348],
          [0.4526, 0.5493, 0.5042,  ..., 0.3787, 0.6454, 0.5783],
          [0.6209, 0.5747, 0.4356,  ..., 0.5621, 0.4572, 0.5273],
          [0.4454, 0.6010, 0.4409,  ..., 0.4233, 0.4308, 0.4874]],

         [[0.5795, 0.5737, 0.4119,  ..., 0.6065, 0.5522, 0.4257],
          [0.3898, 0.4269, 0.5244,  ..., 0.6259, 0.4812, 0.5840],
          [0.4375, 0.5373, 0.4675,  ..., 0.4653, 0.5862, 0.4764],
          [0.6162, 0.4630, 0.5134,  ..., 0.5098, 0.5048, 0.4049]],

         ...,

         [[0.5281, 0.4455, 0.5844,  ..., 0.6930, 0.4736, 0.4092],
          [0.4575, 0.4894, 0.5337,  ..., 0.6322, 0.4856, 0.4017],
          [0.5900, 0.5239, 0.5522,  ..., 0.5316, 0.5971, 0.6011],
          [0.5512, 0.4576, 0.4893,  ..., 0.5064, 0.4522, 0.4026]],

         [[0.5613, 0.6786, 0.4152,  ..., 0.5887, 0.5002, 0.5184],
          [0.4866, 0.6361, 0.3835,  ..., 0.6250, 0.5490, 0.5518],
          [0.5013, 0.3477, 0.6192,  ..., 0.4829, 0.6300, 0.3227],
          [0.4355, 0.2926, 0.4885,  ..., 0.4858, 0.5386, 0.5670]],

         [[0.3828, 0.4226, 0.3662,  ..., 0.4067, 0.3977, 0.5276],
          [0.4945, 0.6116, 0.5542,  ..., 0.3247, 0.4643, 0.5864],
          [0.6013, 0.3993, 0.4493,  ..., 0.5575, 0.6382, 0.5685],
          [0.5130, 0.4486, 0.4289,  ..., 0.5264, 0.4193, 0.4981]]]],
       device='cuda:0')
tensor([[[[0.4467, 0.4976, 0.5251,  ..., 0.4829, 0.3868, 0.4678],
          [0.4282, 0.5310, 0.5402,  ..., 0.5308, 0.4450, 0.4419],
          [0.5250, 0.4887, 0.4683,  ..., 0.3891, 0.5983, 0.4301],
          [0.5718, 0.5846, 0.5501,  ..., 0.4521, 0.4163, 0.4405]],

         [[0.5846, 0.6415, 0.4671,  ..., 0.5703, 0.5012, 0.4586],
          [0.4928, 0.4704, 0.4273,  ..., 0.6361, 0.4254, 0.3798],
          [0.4528, 0.7106, 0.4388,  ..., 0.4858, 0.4450, 0.5670],
          [0.5727, 0.5244, 0.3831,  ..., 0.4777, 0.5443, 0.4287]],

         [[0.6316, 0.5211, 0.5380,  ..., 0.6628, 0.4789, 0.3277],
          [0.5069, 0.4695, 0.3157,  ..., 0.6211, 0.2838, 0.6370],
          [0.6261, 0.4893, 0.4977,  ..., 0.5042, 0.5784, 0.6663],
          [0.6567, 0.5605, 0.4026,  ..., 0.4564, 0.5322, 0.5727]],

         ...,

         [[0.4135, 0.5361, 0.4883,  ..., 0.5310, 0.3666, 0.5590],
          [0.5291, 0.4683, 0.5267,  ..., 0.4868, 0.5436, 0.3567],
          [0.4928, 0.5417, 0.6086,  ..., 0.4957, 0.5003, 0.4443],
          [0.5804, 0.5149, 0.5487,  ..., 0.3460, 0.6834, 0.6261]],

         [[0.6575, 0.5196, 0.5308,  ..., 0.4913, 0.5789, 0.4967],
          [0.5315, 0.4975, 0.3891,  ..., 0.3065, 0.4713, 0.5446],
          [0.4581, 0.5356, 0.6697,  ..., 0.4441, 0.5234, 0.5713],
          [0.5448, 0.4632, 0.5453,  ..., 0.6809, 0.4513, 0.5336]],

         [[0.3416, 0.3854, 0.6044,  ..., 0.5453, 0.5189, 0.3390],
          [0.4369, 0.4634, 0.7341,  ..., 0.4564, 0.5319, 0.4340],
          [0.5209, 0.6225, 0.4721,  ..., 0.4979, 0.3919, 0.5813],
          [0.3993, 0.4668, 0.3285,  ..., 0.7042, 0.5302, 0.3540]]],


        [[[0.4092, 0.5074, 0.6280,  ..., 0.4620, 0.3594, 0.4919],
          [0.4778, 0.5051, 0.5450,  ..., 0.3585, 0.4364, 0.3277],
          [0.4971, 0.5443, 0.3311,  ..., 0.6206, 0.5363, 0.3711],
          [0.4438, 0.5246, 0.5969,  ..., 0.4868, 0.3425, 0.6109]],

         [[0.3621, 0.6049, 0.5655,  ..., 0.4177, 0.4239, 0.4278],
          [0.4496, 0.5583, 0.4992,  ..., 0.3817, 0.6424, 0.5713],
          [0.6179, 0.5837, 0.4306,  ..., 0.5651, 0.4542, 0.5203],
          [0.4424, 0.6100, 0.4359,  ..., 0.4263, 0.4278, 0.4804]],

         [[0.5765, 0.5827, 0.4069,  ..., 0.6095, 0.5492, 0.4187],
          [0.3868, 0.4359, 0.5194,  ..., 0.6289, 0.4782, 0.5770],
          [0.4345, 0.5463, 0.4625,  ..., 0.4683, 0.5832, 0.4694],
          [0.6132, 0.4720, 0.5084,  ..., 0.5128, 0.5018, 0.3979]],

         ...,

         [[0.5251, 0.4545, 0.5794,  ..., 0.6960, 0.4706, 0.4022],
          [0.4545, 0.4984, 0.5287,  ..., 0.6352, 0.4826, 0.3947],
          [0.5870, 0.5329, 0.5472,  ..., 0.5346, 0.5941, 0.5941],
          [0.5482, 0.4666, 0.4843,  ..., 0.5094, 0.4492, 0.3956]],

         [[0.5583, 0.6876, 0.4102,  ..., 0.5917, 0.4972, 0.5114],
          [0.4836, 0.6451, 0.3785,  ..., 0.6280, 0.5460, 0.5448],
          [0.4983, 0.3567, 0.6142,  ..., 0.4859, 0.6270, 0.3157],
          [0.4325, 0.3016, 0.4835,  ..., 0.4888, 0.5356, 0.5600]],

         [[0.3798, 0.4316, 0.3612,  ..., 0.4097, 0.3947, 0.5206],
          [0.4915, 0.6206, 0.5492,  ..., 0.3277, 0.4613, 0.5794],
          [0.5983, 0.4083, 0.4443,  ..., 0.5605, 0.6352, 0.5615],
          [0.5100, 0.4576, 0.4239,  ..., 0.5294, 0.4163, 0.4911]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0030, -0.0090,  0.0050, -0.0030,  0.0030,  0.0030, -0.0050, -0.0030,
         0.0030,  0.0070], device='cuda:0')
selected experts tensor([1686, 1620, 1673, 1582, 1597, 1596, 1792, 1665, 1528, 1645],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5065, 0.6003, 0.4948,  ..., 0.6343, 0.3911, 0.5167],
          [0.5226, 0.4953, 0.4658,  ..., 0.3994, 0.4568, 0.5237],
          [0.5046, 0.4615, 0.5032,  ..., 0.4651, 0.6065, 0.5032],
          [0.4303, 0.4168, 0.4039,  ..., 0.5747, 0.4430, 0.5283]],

         [[0.5724, 0.4623, 0.4034,  ..., 0.4996, 0.3776, 0.3656],
          [0.5886, 0.4584, 0.5249,  ..., 0.3932, 0.3671, 0.4734],
          [0.4080, 0.4900, 0.5953,  ..., 0.6039, 0.6786, 0.5502],
          [0.5700, 0.4192, 0.3153,  ..., 0.6609, 0.4204, 0.5659]],

         [[0.4497, 0.4046, 0.5000,  ..., 0.5606, 0.2411, 0.5392],
          [0.6649, 0.4340, 0.5774,  ..., 0.3254, 0.4611, 0.5288],
          [0.3769, 0.5804, 0.5408,  ..., 0.4560, 0.4213, 0.4839],
          [0.4790, 0.4688, 0.4437,  ..., 0.6618, 0.4406, 0.5526]],

         ...,

         [[0.4284, 0.5581, 0.4696,  ..., 0.5040, 0.4321, 0.4610],
          [0.5047, 0.5095, 0.5558,  ..., 0.5686, 0.4227, 0.5109],
          [0.6172, 0.5269, 0.6821,  ..., 0.4535, 0.6150, 0.4727],
          [0.5966, 0.5127, 0.4015,  ..., 0.6147, 0.4620, 0.4839]],

         [[0.5347, 0.4477, 0.4653,  ..., 0.5210, 0.3144, 0.4129],
          [0.3977, 0.4911, 0.5830,  ..., 0.4888, 0.4056, 0.4522],
          [0.4241, 0.6077, 0.6385,  ..., 0.5004, 0.5955, 0.5488],
          [0.5546, 0.5342, 0.3127,  ..., 0.4590, 0.5506, 0.5497]],

         [[0.4207, 0.5183, 0.4372,  ..., 0.4051, 0.3715, 0.4633],
          [0.4430, 0.4756, 0.6181,  ..., 0.4324, 0.3112, 0.4266],
          [0.3930, 0.4395, 0.6693,  ..., 0.4725, 0.5330, 0.4491],
          [0.5829, 0.3649, 0.5122,  ..., 0.5128, 0.5624, 0.4332]]],


        [[[0.5752, 0.4917, 0.4655,  ..., 0.6343, 0.4134, 0.5554],
          [0.5461, 0.3952, 0.4360,  ..., 0.6228, 0.3464, 0.6068],
          [0.5724, 0.4891, 0.5503,  ..., 0.3894, 0.5397, 0.4974],
          [0.5738, 0.4817, 0.4067,  ..., 0.5686, 0.5171, 0.5373]],

         [[0.4028, 0.5006, 0.4585,  ..., 0.5757, 0.4587, 0.4875],
          [0.5449, 0.4898, 0.5510,  ..., 0.4978, 0.4373, 0.4957],
          [0.3230, 0.5177, 0.6896,  ..., 0.5302, 0.6315, 0.4992],
          [0.4351, 0.4518, 0.3555,  ..., 0.5915, 0.4307, 0.4999]],

         [[0.5536, 0.4749, 0.5069,  ..., 0.5203, 0.3018, 0.4491],
          [0.6250, 0.4439, 0.5673,  ..., 0.3833, 0.3601, 0.6803],
          [0.4852, 0.5390, 0.4846,  ..., 0.5150, 0.4951, 0.5021],
          [0.4802, 0.4041, 0.4590,  ..., 0.6508, 0.4903, 0.5430]],

         ...,

         [[0.4758, 0.6262, 0.5536,  ..., 0.6066, 0.4486, 0.4249],
          [0.5471, 0.4754, 0.5745,  ..., 0.5135, 0.4448, 0.6212],
          [0.4136, 0.5689, 0.4600,  ..., 0.4824, 0.5864, 0.3536],
          [0.4814, 0.5608, 0.3810,  ..., 0.6609, 0.3839, 0.4593]],

         [[0.5097, 0.4383, 0.5098,  ..., 0.5686, 0.4453, 0.6149],
          [0.5829, 0.4545, 0.5740,  ..., 0.5677, 0.3723, 0.4766],
          [0.5872, 0.4584, 0.5697,  ..., 0.5780, 0.4985, 0.4086],
          [0.5781, 0.3767, 0.6033,  ..., 0.5667, 0.6408, 0.4887]],

         [[0.4370, 0.4221, 0.5972,  ..., 0.5183, 0.3992, 0.5569],
          [0.5743, 0.4876, 0.5673,  ..., 0.3725, 0.4093, 0.4310],
          [0.3196, 0.5256, 0.4781,  ..., 0.3380, 0.5408, 0.5322],
          [0.5300, 0.4055, 0.4955,  ..., 0.6246, 0.4690, 0.4924]]]],
       device='cuda:0')
tensor([[[[0.5035, 0.5993, 0.4978,  ..., 0.6593, 0.3621, 0.5397],
          [0.5196, 0.4943, 0.4688,  ..., 0.4244, 0.4278, 0.5467],
          [0.5016, 0.4605, 0.5062,  ..., 0.4901, 0.5775, 0.5262],
          [0.4273, 0.4158, 0.4069,  ..., 0.5997, 0.4140, 0.5513]],

         [[0.5694, 0.4613, 0.4064,  ..., 0.5246, 0.3486, 0.3886],
          [0.5856, 0.4574, 0.5279,  ..., 0.4182, 0.3381, 0.4964],
          [0.4050, 0.4890, 0.5983,  ..., 0.6289, 0.6496, 0.5732],
          [0.5670, 0.4182, 0.3183,  ..., 0.6859, 0.3914, 0.5889]],

         [[0.4467, 0.4036, 0.5030,  ..., 0.5856, 0.2121, 0.5622],
          [0.6619, 0.4330, 0.5804,  ..., 0.3504, 0.4321, 0.5518],
          [0.3739, 0.5794, 0.5438,  ..., 0.4810, 0.3923, 0.5069],
          [0.4760, 0.4678, 0.4467,  ..., 0.6868, 0.4116, 0.5756]],

         ...,

         [[0.4254, 0.5571, 0.4726,  ..., 0.5290, 0.4031, 0.4840],
          [0.5017, 0.5085, 0.5588,  ..., 0.5936, 0.3937, 0.5339],
          [0.6142, 0.5259, 0.6851,  ..., 0.4785, 0.5860, 0.4957],
          [0.5936, 0.5117, 0.4045,  ..., 0.6397, 0.4330, 0.5069]],

         [[0.5317, 0.4467, 0.4683,  ..., 0.5460, 0.2854, 0.4359],
          [0.3947, 0.4901, 0.5860,  ..., 0.5138, 0.3766, 0.4752],
          [0.4211, 0.6067, 0.6415,  ..., 0.5254, 0.5665, 0.5718],
          [0.5516, 0.5332, 0.3157,  ..., 0.4840, 0.5216, 0.5727]],

         [[0.4177, 0.5173, 0.4402,  ..., 0.4301, 0.3425, 0.4863],
          [0.4400, 0.4746, 0.6211,  ..., 0.4574, 0.2822, 0.4496],
          [0.3900, 0.4385, 0.6723,  ..., 0.4975, 0.5040, 0.4721],
          [0.5799, 0.3639, 0.5152,  ..., 0.5378, 0.5334, 0.4562]]],


        [[[0.5722, 0.4907, 0.4685,  ..., 0.6593, 0.3844, 0.5784],
          [0.5431, 0.3942, 0.4390,  ..., 0.6478, 0.3174, 0.6298],
          [0.5694, 0.4881, 0.5533,  ..., 0.4144, 0.5107, 0.5204],
          [0.5708, 0.4807, 0.4097,  ..., 0.5936, 0.4881, 0.5603]],

         [[0.3998, 0.4996, 0.4615,  ..., 0.6007, 0.4297, 0.5105],
          [0.5419, 0.4888, 0.5540,  ..., 0.5228, 0.4083, 0.5187],
          [0.3200, 0.5167, 0.6926,  ..., 0.5552, 0.6025, 0.5222],
          [0.4321, 0.4508, 0.3585,  ..., 0.6165, 0.4017, 0.5229]],

         [[0.5506, 0.4739, 0.5099,  ..., 0.5453, 0.2728, 0.4721],
          [0.6220, 0.4429, 0.5703,  ..., 0.4083, 0.3311, 0.7033],
          [0.4822, 0.5380, 0.4876,  ..., 0.5400, 0.4661, 0.5251],
          [0.4772, 0.4031, 0.4620,  ..., 0.6758, 0.4613, 0.5660]],

         ...,

         [[0.4728, 0.6252, 0.5566,  ..., 0.6316, 0.4196, 0.4479],
          [0.5441, 0.4744, 0.5775,  ..., 0.5385, 0.4158, 0.6442],
          [0.4106, 0.5679, 0.4630,  ..., 0.5074, 0.5574, 0.3766],
          [0.4784, 0.5598, 0.3840,  ..., 0.6859, 0.3549, 0.4823]],

         [[0.5067, 0.4373, 0.5128,  ..., 0.5936, 0.4163, 0.6379],
          [0.5799, 0.4535, 0.5770,  ..., 0.5927, 0.3433, 0.4996],
          [0.5842, 0.4574, 0.5727,  ..., 0.6030, 0.4695, 0.4316],
          [0.5751, 0.3757, 0.6063,  ..., 0.5917, 0.6118, 0.5117]],

         [[0.4340, 0.4211, 0.6002,  ..., 0.5433, 0.3702, 0.5799],
          [0.5713, 0.4866, 0.5703,  ..., 0.3975, 0.3803, 0.4540],
          [0.3166, 0.5246, 0.4811,  ..., 0.3630, 0.5118, 0.5552],
          [0.5270, 0.4045, 0.4985,  ..., 0.6496, 0.4400, 0.5154]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0030,  0.0010, -0.0030,  0.0010,  0.0050, -0.0170,  0.0130, -0.0250,
         0.0290, -0.0230], device='cuda:0')
selected experts tensor([1377, 1414, 1521, 1664, 1837, 2104, 1378, 2265,  993, 1831],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.5194, 0.5780, 0.6028,  ..., 0.3823, 0.4448, 0.3880],
          [0.4879, 0.6816, 0.4936,  ..., 0.5646, 0.4925, 0.4385],
          [0.5362, 0.5684, 0.4859,  ..., 0.4152, 0.7311, 0.3807],
          [0.6345, 0.6107, 0.4384,  ..., 0.3670, 0.5421, 0.6194]],

         [[0.5093, 0.5236, 0.6253,  ..., 0.3415, 0.5845, 0.4552],
          [0.4270, 0.5582, 0.4824,  ..., 0.5867, 0.5685, 0.5374],
          [0.3928, 0.4708, 0.5062,  ..., 0.5240, 0.5759, 0.4097],
          [0.5452, 0.6344, 0.4403,  ..., 0.6009, 0.4787, 0.5506]],

         [[0.6705, 0.6060, 0.5561,  ..., 0.4903, 0.5658, 0.5161],
          [0.5532, 0.4621, 0.4436,  ..., 0.4499, 0.4530, 0.4906],
          [0.4479, 0.4713, 0.4091,  ..., 0.4412, 0.4791, 0.5214],
          [0.4854, 0.5401, 0.3146,  ..., 0.6853, 0.5716, 0.6015]],

         ...,

         [[0.5814, 0.6228, 0.6581,  ..., 0.3769, 0.5797, 0.4634],
          [0.5209, 0.4833, 0.5881,  ..., 0.5805, 0.5636, 0.4375],
          [0.3528, 0.3686, 0.4232,  ..., 0.6151, 0.6044, 0.5451],
          [0.4021, 0.5703, 0.5515,  ..., 0.6332, 0.5246, 0.5025]],

         [[0.6066, 0.5933, 0.5809,  ..., 0.2728, 0.5680, 0.4181],
          [0.4455, 0.4989, 0.5006,  ..., 0.5415, 0.5689, 0.4733],
          [0.3581, 0.3830, 0.4750,  ..., 0.4597, 0.4499, 0.4451],
          [0.5761, 0.5096, 0.3998,  ..., 0.4910, 0.4191, 0.5697]],

         [[0.5130, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5150]]],


        [[[0.5139, 0.4951, 0.7131,  ..., 0.3476, 0.4687, 0.3990],
          [0.4511, 0.5568, 0.4709,  ..., 0.5466, 0.5450, 0.3690],
          [0.5079, 0.4725, 0.4747,  ..., 0.4747, 0.6512, 0.4978],
          [0.6221, 0.6552, 0.5440,  ..., 0.4689, 0.4367, 0.6484]],

         [[0.6350, 0.4503, 0.6391,  ..., 0.4255, 0.6030, 0.4509],
          [0.5976, 0.4383, 0.4786,  ..., 0.5289, 0.4026, 0.4985],
          [0.5016, 0.4364, 0.5134,  ..., 0.5708, 0.6802, 0.5583],
          [0.6043, 0.5539, 0.5585,  ..., 0.4508, 0.4040, 0.5311]],

         [[0.5447, 0.5684, 0.7821,  ..., 0.4859, 0.5453, 0.3852],
          [0.3910, 0.5698, 0.4175,  ..., 0.6714, 0.5482, 0.4917],
          [0.4713, 0.5875, 0.4825,  ..., 0.3887, 0.5945, 0.4134],
          [0.5249, 0.6876, 0.4762,  ..., 0.4784, 0.5296, 0.5135]],

         ...,

         [[0.5130, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5150]],

         [[0.5130, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5150]],

         [[0.5130, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5130, 0.5070, 0.5150]]]],
       device='cuda:0')
tensor([[[[0.5064, 0.5670, 0.5898,  ..., 0.3693, 0.4378, 0.3730],
          [0.4749, 0.6706, 0.4806,  ..., 0.5516, 0.4855, 0.4235],
          [0.5232, 0.5574, 0.4729,  ..., 0.4022, 0.7241, 0.3657],
          [0.6215, 0.5997, 0.4254,  ..., 0.3540, 0.5351, 0.6044]],

         [[0.4963, 0.5126, 0.6123,  ..., 0.3285, 0.5775, 0.4402],
          [0.4140, 0.5472, 0.4694,  ..., 0.5737, 0.5615, 0.5224],
          [0.3798, 0.4598, 0.4932,  ..., 0.5110, 0.5689, 0.3947],
          [0.5322, 0.6234, 0.4273,  ..., 0.5879, 0.4717, 0.5356]],

         [[0.6575, 0.5950, 0.5431,  ..., 0.4773, 0.5588, 0.5011],
          [0.5402, 0.4511, 0.4306,  ..., 0.4369, 0.4460, 0.4756],
          [0.4349, 0.4603, 0.3961,  ..., 0.4282, 0.4721, 0.5064],
          [0.4724, 0.5291, 0.3016,  ..., 0.6723, 0.5646, 0.5865]],

         ...,

         [[0.5684, 0.6118, 0.6451,  ..., 0.3639, 0.5727, 0.4484],
          [0.5079, 0.4723, 0.5751,  ..., 0.5675, 0.5566, 0.4225],
          [0.3398, 0.3576, 0.4102,  ..., 0.6021, 0.5974, 0.5301],
          [0.3891, 0.5593, 0.5385,  ..., 0.6202, 0.5176, 0.4875]],

         [[0.5936, 0.5823, 0.5679,  ..., 0.2598, 0.5610, 0.4031],
          [0.4325, 0.4879, 0.4876,  ..., 0.5285, 0.5619, 0.4583],
          [0.3451, 0.3720, 0.4620,  ..., 0.4467, 0.4429, 0.4301],
          [0.5631, 0.4986, 0.3868,  ..., 0.4780, 0.4121, 0.5547]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5009, 0.4841, 0.7001,  ..., 0.3346, 0.4617, 0.3840],
          [0.4381, 0.5458, 0.4579,  ..., 0.5336, 0.5380, 0.3540],
          [0.4949, 0.4615, 0.4617,  ..., 0.4617, 0.6442, 0.4828],
          [0.6091, 0.6442, 0.5310,  ..., 0.4559, 0.4297, 0.6334]],

         [[0.6220, 0.4393, 0.6261,  ..., 0.4125, 0.5960, 0.4359],
          [0.5846, 0.4273, 0.4656,  ..., 0.5159, 0.3956, 0.4835],
          [0.4886, 0.4254, 0.5004,  ..., 0.5578, 0.6732, 0.5433],
          [0.5913, 0.5429, 0.5455,  ..., 0.4378, 0.3970, 0.5161]],

         [[0.5317, 0.5574, 0.7691,  ..., 0.4729, 0.5383, 0.3702],
          [0.3780, 0.5588, 0.4045,  ..., 0.6584, 0.5412, 0.4767],
          [0.4583, 0.5765, 0.4695,  ..., 0.3757, 0.5875, 0.3984],
          [0.5119, 0.6766, 0.4632,  ..., 0.4654, 0.5226, 0.4985]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
[batch=30/40]:
	 Train time/batch: 29
	 Train time/sample: 58
	 Train time/batch_in_epoch: 29
	 Train time/sample_in_epoch: 58
	 Train time/token: 59392
	 Train time/token_in_epoch: 59392
	 Train memory/current_allocated_mem: 1.1652
	 Train memory/current_active_mem: 1.1652
	 Train memory/current_inactive_mem: 0.3385
	 Train memory/current_reserved_mem: 3.8231
	 Train memory/peak_allocated_mem: 2.7979
	 Train memory/peak_active_mem: 2.7979
	 Train memory/peak_inactive_mem: 0.8560
	 Train memory/peak_reserved_mem: 3.8231
	 Train memory/alloc_retries: 0
	 Train trainer/device_train_microbatch_size: 10
	 Train loss/train/total: 0.0047
	 Train metrics/train/LanguageCrossEntropy: 9.5717
	 Train metrics/train/LanguagePerplexity: 14352.4141
	 Train metrics/train/TokenAccuracy: 0.1663
	 Train throughput/batches_per_sec: 0.2064
	 Train throughput/samples_per_sec: 0.4128
	 Train throughput/device/batches_per_sec: 0.2064
	 Train throughput/device/samples_per_sec: 0.4128
	 Train throughput/tokens_per_sec: 422.7016
	 Train throughput/device/tokens_per_sec: 422.7016
	 Train time/train: 0.0488
	 Train time/val: 0.0000
	 Train time/total: 0.0488
	 Train lr-DecoupledAdamW/group0: 0.0000
	 Train time/remaining_estimate: 0.0134
	 Train metrics/shannon_entropy: 10.6097
	 Train metrics/batch_shannon_entropy: <wandb.sdk.data_types.table.Table object at 0x7097b028cfb0>
	 Train metrics/seq_shannon_entropy: CustomChart(table=<wandb.sdk.data_types.table.Table object at 0x70958bf1fe90>, spec=CustomChartSpec(spec_name='wandb/lineseries/v0', fields={'step': 'step', 'lineKey': 'lineKey', 'lineVal': 'lineVal'}, string_fields={'title': 'Per-Layer Shannon Entropy', 'xname': 'Block Index'}, key='', panel_type='Vega2', split_table=False))
	 Train metrics/exit_entropy: 0.6641
	 Train metrics/batch_exit_entropy: <wandb.sdk.data_types.table.Table object at 0x70984213f1a0>
	 Train metrics/seq_exit_entropy: CustomChart(table=<wandb.sdk.data_types.table.Table object at 0x7095904773e0>, spec=CustomChartSpec(spec_name='wandb/lineseries/v0', fields={'step': 'step', 'lineKey': 'lineKey', 'lineVal': 'lineVal'}, string_fields={'title': 'Per-Layer Exit Entropy', 'xname': 'Block Index'}, key='', panel_type='Vega2', split_table=False))
	 Train expert_selection/ffn_layer: <wandb.sdk.data_types.image.Image object at 0x7097e0fc4800>
	 Train expert_selection/attn_o_layer: <wandb.sdk.data_types.image.Image object at 0x7095902d18e0>
	 Train expert_selection/attn_v_layer: <wandb.sdk.data_types.image.Image object at 0x709590265610>
	 Train l2_norm/moment/model.transformer.router: 0.0000
	 Train l2_norm/param/model.transformer.router: 0.3638
	 Train l2_norm/update/model.transformer.router: 0.0006
	 Train l2_norm/grad/model.transformer.router: 0.0001
	 Train l2_norm/moment/model.transformer.tau: 0.0000
	 Train l2_norm/param/model.transformer.tau: 1.0003
	 Train l2_norm/update/model.transformer.tau: 0.0000
	 Train l2_norm/grad/model.transformer.tau: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.attention.v: 0.0004
	 Train l2_norm/param/model.transformer.layers.0.attention.v: 20.4717
	 Train l2_norm/update/model.transformer.layers.0.attention.v: 0.0163
	 Train l2_norm/grad/model.transformer.layers.0.attention.v: 0.0005
	 Train l2_norm/moment/model.transformer.layers.0.attention.o: 0.0005
	 Train l2_norm/param/model.transformer.layers.0.attention.o: 22.6951
	 Train l2_norm/update/model.transformer.layers.0.attention.o: 0.0208
	 Train l2_norm/grad/model.transformer.layers.0.attention.o: 0.0005
	 Train l2_norm/moment/model.transformer.layers.0.attention.sel_v: 0.0001
	 Train l2_norm/param/model.transformer.layers.0.attention.sel_v: 2.2340
	 Train l2_norm/update/model.transformer.layers.0.attention.sel_v: 0.0013
	 Train l2_norm/grad/model.transformer.layers.0.attention.sel_v: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.attention.sel_o: 0.0001
	 Train l2_norm/param/model.transformer.layers.0.attention.sel_o: 2.2493
	 Train l2_norm/update/model.transformer.layers.0.attention.sel_o: 0.0014
	 Train l2_norm/grad/model.transformer.layers.0.attention.sel_o: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.attention.q.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attention.q.weight: 6.4681
	 Train l2_norm/update/model.transformer.layers.0.attention.q.weight: 0.0063
	 Train l2_norm/grad/model.transformer.layers.0.attention.q.weight: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.attention.k.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attention.k.weight: 6.4741
	 Train l2_norm/update/model.transformer.layers.0.attention.k.weight: 0.0062
	 Train l2_norm/grad/model.transformer.layers.0.attention.k.weight: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.ffn.keys: 0.0004
	 Train l2_norm/param/model.transformer.layers.0.ffn.keys: 14.9973
	 Train l2_norm/update/model.transformer.layers.0.ffn.keys: 0.0166
	 Train l2_norm/grad/model.transformer.layers.0.ffn.keys: 0.0004
	 Train l2_norm/moment/model.transformer.layers.0.ffn.values: 0.0009
	 Train l2_norm/param/model.transformer.layers.0.ffn.values: 7.1928
	 Train l2_norm/update/model.transformer.layers.0.ffn.values: 0.0162
	 Train l2_norm/grad/model.transformer.layers.0.ffn.values: 0.0009
	 Train l2_norm/moment/model.transformer.layers.0.ffn.expert_sel: 0.0001
	 Train l2_norm/param/model.transformer.layers.0.ffn.expert_sel: 4.7512
	 Train l2_norm/update/model.transformer.layers.0.ffn.expert_sel: 0.0054
	 Train l2_norm/grad/model.transformer.layers.0.ffn.expert_sel: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.attn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attn_pre.weight: 20.2975
	 Train l2_norm/update/model.transformer.layers.0.attn_pre.weight: 0.0003
	 Train l2_norm/grad/model.transformer.layers.0.attn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.attn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attn_post.weight: 20.2970
	 Train l2_norm/update/model.transformer.layers.0.attn_post.weight: 0.0004
	 Train l2_norm/grad/model.transformer.layers.0.attn_post.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.ffn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.ffn_pre.weight: 20.2977
	 Train l2_norm/update/model.transformer.layers.0.ffn_pre.weight: 0.0004
	 Train l2_norm/grad/model.transformer.layers.0.ffn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.ffn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.ffn_post.weight: 20.2997
	 Train l2_norm/update/model.transformer.layers.0.ffn_post.weight: 0.0005
	 Train l2_norm/grad/model.transformer.layers.0.ffn_post.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.v: 0.0002
	 Train l2_norm/param/model.transformer.layers.1.attention.v: 20.4829
	 Train l2_norm/update/model.transformer.layers.1.attention.v: 0.0136
	 Train l2_norm/grad/model.transformer.layers.1.attention.v: 0.0004
	 Train l2_norm/moment/model.transformer.layers.1.attention.o: 0.0002
	 Train l2_norm/param/model.transformer.layers.1.attention.o: 22.6891
	 Train l2_norm/update/model.transformer.layers.1.attention.o: 0.0139
	 Train l2_norm/grad/model.transformer.layers.1.attention.o: 0.0003
	 Train l2_norm/moment/model.transformer.layers.1.attention.sel_v: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.sel_v: 2.2358
	 Train l2_norm/update/model.transformer.layers.1.attention.sel_v: 0.0009
	 Train l2_norm/grad/model.transformer.layers.1.attention.sel_v: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.sel_o: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.sel_o: 2.2254
	 Train l2_norm/update/model.transformer.layers.1.attention.sel_o: 0.0010
	 Train l2_norm/grad/model.transformer.layers.1.attention.sel_o: 0.0001
	 Train l2_norm/moment/model.transformer.layers.1.attention.q.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.q.weight: 6.4889
	 Train l2_norm/update/model.transformer.layers.1.attention.q.weight: 0.0027
	 Train l2_norm/grad/model.transformer.layers.1.attention.q.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.k.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.k.weight: 6.4777
	 Train l2_norm/update/model.transformer.layers.1.attention.k.weight: 0.0027
	 Train l2_norm/grad/model.transformer.layers.1.attention.k.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn.keys: 0.0001
	 Train l2_norm/param/model.transformer.layers.1.ffn.keys: 15.0067
	 Train l2_norm/update/model.transformer.layers.1.ffn.keys: 0.0124
	 Train l2_norm/grad/model.transformer.layers.1.ffn.keys: 0.0003
	 Train l2_norm/moment/model.transformer.layers.1.ffn.values: 0.0004
	 Train l2_norm/param/model.transformer.layers.1.ffn.values: 7.1658
	 Train l2_norm/update/model.transformer.layers.1.ffn.values: 0.0141
	 Train l2_norm/grad/model.transformer.layers.1.ffn.values: 0.0006
	 Train l2_norm/moment/model.transformer.layers.1.ffn.expert_sel: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.ffn.expert_sel: 4.7382
	 Train l2_norm/update/model.transformer.layers.1.ffn.expert_sel: 0.0039
	 Train l2_norm/grad/model.transformer.layers.1.ffn.expert_sel: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attn_pre.weight: 20.2975
	 Train l2_norm/update/model.transformer.layers.1.attn_pre.weight: 0.0003
	 Train l2_norm/grad/model.transformer.layers.1.attn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attn_post.weight: 20.2959
	 Train l2_norm/update/model.transformer.layers.1.attn_post.weight: 0.0004
	 Train l2_norm/grad/model.transformer.layers.1.attn_post.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.ffn_pre.weight: 20.2975
	 Train l2_norm/update/model.transformer.layers.1.ffn_pre.weight: 0.0003
	 Train l2_norm/grad/model.transformer.layers.1.ffn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.ffn_post.weight: 20.2977
	 Train l2_norm/update/model.transformer.layers.1.ffn_post.weight: 0.0003
	 Train l2_norm/grad/model.transformer.layers.1.ffn_post.weight: 0.0000
	 Train l2_norm/moment/model.embedding.weight: 0.0002
	 Train l2_norm/param/model.embedding.weight: 221.7548
	 Train l2_norm/update/model.embedding.weight: 0.0119
	 Train l2_norm/grad/model.embedding.weight: 0.0002
	 Train l2_norm/moment/model.lm_head.weight: 0.0006
	 Train l2_norm/param/model.lm_head.weight: 127.9705
	 Train l2_norm/update/model.lm_head.weight: 0.0481
	 Train l2_norm/grad/model.lm_head.weight: 0.0008
	 Train l2_norm/moment/model.lm_head.bias: 0.0000
	 Train l2_norm/param/model.lm_head.bias: 6.2883
	 Train l2_norm/update/model.lm_head.bias: 0.0037
	 Train l2_norm/grad/model.lm_head.bias: 0.0000
	 Train l2_norm/moment/model.out_norm.weight: 0.0000
	 Train l2_norm/param/model.out_norm.weight: 20.2997
	 Train l2_norm/update/model.out_norm.weight: 0.0006
	 Train l2_norm/grad/model.out_norm.weight: 0.0000
	 Train l2_norm/grad/global: 0.0017
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([0.0130, 0.0110, 0.0130, 0.0130, 0.0130, 0.0090, 0.0110, 0.0130, 0.0070,
        0.0150], device='cuda:0')
selected experts tensor([1697, 1691, 1065,  413,  669, 1551, 1425,  859, 1655, 1263],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5001, 0.5411, 0.5509,  ..., 0.5544, 0.4967, 0.3597],
          [0.4043, 0.5561, 0.5077,  ..., 0.4544, 0.3869, 0.5121],
          [0.4909, 0.4418, 0.4969,  ..., 0.5017, 0.6124, 0.4947],
          [0.5422, 0.5102, 0.4453,  ..., 0.5198, 0.5275, 0.4345]],

         [[0.5422, 0.4272, 0.3755,  ..., 0.4936, 0.6655, 0.4103],
          [0.6114, 0.3574, 0.4177,  ..., 0.4162, 0.4596, 0.3947],
          [0.4100, 0.5336, 0.5039,  ..., 0.5537, 0.6263, 0.5422],
          [0.5103, 0.5072, 0.4550,  ..., 0.4820, 0.4490, 0.3391]],

         [[0.3340, 0.3801, 0.5841,  ..., 0.6225, 0.4832, 0.3543],
          [0.3678, 0.3871, 0.5118,  ..., 0.4876, 0.3962, 0.4675],
          [0.5617, 0.6386, 0.4041,  ..., 0.5330, 0.5716, 0.2645],
          [0.3546, 0.5270, 0.5402,  ..., 0.3904, 0.5274, 0.5039]],

         ...,

         [[0.4451, 0.4551, 0.4410,  ..., 0.5377, 0.5042, 0.5388],
          [0.4057, 0.4810, 0.7226,  ..., 0.5443, 0.4055, 0.5163],
          [0.4790, 0.5346, 0.3846,  ..., 0.4834, 0.4985, 0.5677],
          [0.4430, 0.5426, 0.3071,  ..., 0.4470, 0.4234, 0.6698]],

         [[0.6946, 0.4729, 0.6812,  ..., 0.5107, 0.4714, 0.5007],
          [0.4636, 0.4418, 0.5888,  ..., 0.4381, 0.5521, 0.6550],
          [0.4951, 0.5597, 0.4954,  ..., 0.5754, 0.5455, 0.4951],
          [0.4818, 0.7321, 0.4509,  ..., 0.6475, 0.5384, 0.4531]],

         [[0.6603, 0.4522, 0.5731,  ..., 0.6766, 0.5250, 0.6949],
          [0.5588, 0.4143, 0.4125,  ..., 0.6067, 0.5610, 0.4037],
          [0.6277, 0.4639, 0.4630,  ..., 0.5643, 0.5678, 0.4103],
          [0.5401, 0.5372, 0.5083,  ..., 0.5792, 0.4205, 0.5405]]],


        [[[0.4451, 0.4895, 0.5254,  ..., 0.5447, 0.4497, 0.4543],
          [0.5821, 0.5750, 0.4305,  ..., 0.4508, 0.5356, 0.4084],
          [0.4724, 0.5640, 0.6007,  ..., 0.5455, 0.5574, 0.6310],
          [0.5297, 0.4406, 0.6594,  ..., 0.3538, 0.4386, 0.3588]],

         [[0.3967, 0.4782, 0.6035,  ..., 0.3780, 0.4315, 0.5862],
          [0.4934, 0.2654, 0.4186,  ..., 0.5282, 0.6647, 0.5429],
          [0.5292, 0.5774, 0.4106,  ..., 0.4063, 0.5275, 0.3418],
          [0.5267, 0.5083, 0.4482,  ..., 0.4016, 0.5030, 0.3561]],

         [[0.6105, 0.4840, 0.4852,  ..., 0.4912, 0.4468, 0.4803],
          [0.3600, 0.4590, 0.4707,  ..., 0.5194, 0.5250, 0.5077],
          [0.5058, 0.6154, 0.5112,  ..., 0.4704, 0.4319, 0.5050],
          [0.5265, 0.5129, 0.4473,  ..., 0.7028, 0.3540, 0.5951]],

         ...,

         [[0.4916, 0.3834, 0.5922,  ..., 0.4328, 0.5453, 0.5308],
          [0.4777, 0.5011, 0.4262,  ..., 0.5323, 0.5183, 0.4718],
          [0.4343, 0.3637, 0.4727,  ..., 0.3690, 0.4027, 0.3868],
          [0.4020, 0.5954, 0.4793,  ..., 0.5911, 0.7210, 0.4117]],

         [[0.5703, 0.3955, 0.5841,  ..., 0.4978, 0.5287, 0.6247],
          [0.3483, 0.5462, 0.4324,  ..., 0.5481, 0.4651, 0.5039],
          [0.4463, 0.4200, 0.4804,  ..., 0.3406, 0.6110, 0.6100],
          [0.4867, 0.6200, 0.2870,  ..., 0.4648, 0.4739, 0.4949]],

         [[0.3546, 0.5161, 0.3540,  ..., 0.5013, 0.3221, 0.5337],
          [0.4143, 0.4508, 0.3989,  ..., 0.5123, 0.6495, 0.5325],
          [0.6905, 0.3969, 0.5123,  ..., 0.6118, 0.4238, 0.5315],
          [0.5311, 0.5231, 0.4319,  ..., 0.5291, 0.4149, 0.5424]]]],
       device='cuda:0')
tensor([[[[0.5121, 0.5431, 0.5429,  ..., 0.5484, 0.4887, 0.3657],
          [0.4163, 0.5581, 0.4997,  ..., 0.4484, 0.3789, 0.5181],
          [0.5029, 0.4438, 0.4889,  ..., 0.4957, 0.6044, 0.5007],
          [0.5542, 0.5122, 0.4373,  ..., 0.5138, 0.5195, 0.4405]],

         [[0.5542, 0.4292, 0.3675,  ..., 0.4876, 0.6575, 0.4163],
          [0.6234, 0.3594, 0.4097,  ..., 0.4102, 0.4516, 0.4007],
          [0.4220, 0.5356, 0.4959,  ..., 0.5477, 0.6183, 0.5482],
          [0.5223, 0.5092, 0.4470,  ..., 0.4760, 0.4410, 0.3451]],

         [[0.3460, 0.3821, 0.5761,  ..., 0.6165, 0.4752, 0.3603],
          [0.3798, 0.3891, 0.5038,  ..., 0.4816, 0.3882, 0.4735],
          [0.5737, 0.6406, 0.3961,  ..., 0.5270, 0.5636, 0.2705],
          [0.3666, 0.5290, 0.5322,  ..., 0.3844, 0.5194, 0.5099]],

         ...,

         [[0.4571, 0.4571, 0.4330,  ..., 0.5317, 0.4962, 0.5448],
          [0.4177, 0.4830, 0.7146,  ..., 0.5383, 0.3975, 0.5223],
          [0.4910, 0.5366, 0.3766,  ..., 0.4774, 0.4905, 0.5737],
          [0.4550, 0.5446, 0.2991,  ..., 0.4410, 0.4154, 0.6758]],

         [[0.7066, 0.4749, 0.6732,  ..., 0.5047, 0.4634, 0.5067],
          [0.4756, 0.4438, 0.5808,  ..., 0.4321, 0.5441, 0.6610],
          [0.5071, 0.5617, 0.4874,  ..., 0.5694, 0.5375, 0.5011],
          [0.4938, 0.7341, 0.4429,  ..., 0.6415, 0.5304, 0.4591]],

         [[0.6723, 0.4542, 0.5651,  ..., 0.6706, 0.5170, 0.7009],
          [0.5708, 0.4163, 0.4045,  ..., 0.6007, 0.5530, 0.4097],
          [0.6397, 0.4659, 0.4550,  ..., 0.5583, 0.5598, 0.4163],
          [0.5521, 0.5392, 0.5003,  ..., 0.5732, 0.4125, 0.5465]]],


        [[[0.4571, 0.4915, 0.5174,  ..., 0.5387, 0.4417, 0.4603],
          [0.5941, 0.5770, 0.4225,  ..., 0.4448, 0.5276, 0.4144],
          [0.4844, 0.5660, 0.5927,  ..., 0.5395, 0.5494, 0.6370],
          [0.5417, 0.4426, 0.6514,  ..., 0.3478, 0.4306, 0.3648]],

         [[0.4087, 0.4802, 0.5955,  ..., 0.3720, 0.4235, 0.5922],
          [0.5054, 0.2674, 0.4106,  ..., 0.5222, 0.6567, 0.5489],
          [0.5412, 0.5794, 0.4026,  ..., 0.4003, 0.5195, 0.3478],
          [0.5387, 0.5103, 0.4402,  ..., 0.3956, 0.4950, 0.3621]],

         [[0.6225, 0.4860, 0.4772,  ..., 0.4852, 0.4388, 0.4863],
          [0.3720, 0.4610, 0.4627,  ..., 0.5134, 0.5170, 0.5137],
          [0.5178, 0.6174, 0.5032,  ..., 0.4644, 0.4239, 0.5110],
          [0.5385, 0.5149, 0.4393,  ..., 0.6968, 0.3460, 0.6011]],

         ...,

         [[0.5036, 0.3854, 0.5842,  ..., 0.4268, 0.5373, 0.5368],
          [0.4897, 0.5031, 0.4182,  ..., 0.5263, 0.5103, 0.4778],
          [0.4463, 0.3657, 0.4647,  ..., 0.3630, 0.3947, 0.3928],
          [0.4140, 0.5974, 0.4713,  ..., 0.5851, 0.7130, 0.4177]],

         [[0.5823, 0.3975, 0.5761,  ..., 0.4918, 0.5207, 0.6307],
          [0.3603, 0.5482, 0.4244,  ..., 0.5421, 0.4571, 0.5099],
          [0.4583, 0.4220, 0.4724,  ..., 0.3346, 0.6030, 0.6160],
          [0.4987, 0.6220, 0.2790,  ..., 0.4588, 0.4659, 0.5009]],

         [[0.3666, 0.5181, 0.3460,  ..., 0.4953, 0.3141, 0.5397],
          [0.4263, 0.4528, 0.3909,  ..., 0.5063, 0.6415, 0.5385],
          [0.7025, 0.3989, 0.5043,  ..., 0.6058, 0.4158, 0.5375],
          [0.5431, 0.5251, 0.4239,  ..., 0.5231, 0.4069, 0.5484]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0120, -0.0020,  0.0080,  0.0020,  0.0100,  0.0080, -0.0040,  0.0060,
         0.0080, -0.0060], device='cuda:0')
selected experts tensor([1556, 1554, 1708, 1660, 1669, 1650, 1635, 1768, 1569, 1615],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5276, 0.2579, 0.5900,  ..., 0.4493, 0.3912, 0.4761],
          [0.4574, 0.4669, 0.5753,  ..., 0.4712, 0.5820, 0.5392],
          [0.4298, 0.5987, 0.5282,  ..., 0.5067, 0.7170, 0.6071],
          [0.4098, 0.4623, 0.5279,  ..., 0.3473, 0.5500, 0.5153]],

         [[0.5105, 0.4370, 0.5971,  ..., 0.4638, 0.4751, 0.5660],
          [0.4703, 0.4954, 0.5493,  ..., 0.3916, 0.5106, 0.5622],
          [0.7284, 0.3257, 0.4807,  ..., 0.5454, 0.6320, 0.7270],
          [0.5274, 0.5585, 0.5853,  ..., 0.5706, 0.3473, 0.5102]],

         [[0.5272, 0.4116, 0.4628,  ..., 0.5096, 0.5004, 0.5821],
          [0.5149, 0.5604, 0.5556,  ..., 0.4712, 0.4936, 0.4338],
          [0.5651, 0.3677, 0.4260,  ..., 0.6474, 0.5834, 0.4414],
          [0.4098, 0.5496, 0.5592,  ..., 0.4076, 0.5437, 0.4918]],

         ...,

         [[0.4842, 0.4448, 0.4922,  ..., 0.5144, 0.4375, 0.5797],
          [0.4293, 0.5479, 0.4318,  ..., 0.6230, 0.5128, 0.4881],
          [0.5066, 0.4598, 0.5544,  ..., 0.5343, 0.5510, 0.3699],
          [0.5138, 0.5070, 0.4884,  ..., 0.4133, 0.5452, 0.5996]],

         [[0.4778, 0.4516, 0.6209,  ..., 0.4447, 0.5183, 0.5061],
          [0.5647, 0.4069, 0.5110,  ..., 0.5526, 0.6009, 0.4091],
          [0.5115, 0.5164, 0.4948,  ..., 0.5323, 0.5844, 0.4756],
          [0.4902, 0.2564, 0.5077,  ..., 0.5328, 0.4746, 0.6216]],

         [[0.4912, 0.4293, 0.5430,  ..., 0.4592, 0.4609, 0.6123],
          [0.6399, 0.5470, 0.5333,  ..., 0.3800, 0.6000, 0.4856],
          [0.5747, 0.4207, 0.5734,  ..., 0.4324, 0.4318, 0.5413],
          [0.5632, 0.2548, 0.4862,  ..., 0.4794, 0.4066, 0.4318]]],


        [[[0.5221, 0.4431, 0.5038,  ..., 0.5078, 0.4137, 0.6349],
          [0.5424, 0.3839, 0.4425,  ..., 0.3527, 0.5031, 0.4314],
          [0.4599, 0.5945, 0.6518,  ..., 0.4271, 0.4057, 0.5754],
          [0.3777, 0.3723, 0.4818,  ..., 0.4266, 0.3465, 0.4532]],

         [[0.5589, 0.5160, 0.4385,  ..., 0.5242, 0.3274, 0.3084],
          [0.7346, 0.5676, 0.3852,  ..., 0.3662, 0.5317, 0.3845],
          [0.5349, 0.3853, 0.4534,  ..., 0.3772, 0.3797, 0.4699],
          [0.5339, 0.3559, 0.5351,  ..., 0.6221, 0.4791, 0.6385]],

         [[0.4084, 0.4520, 0.5481,  ..., 0.6491, 0.4146, 0.6340],
          [0.4990, 0.5642, 0.4764,  ..., 0.4865, 0.4413, 0.5987],
          [0.5364, 0.6140, 0.5650,  ..., 0.5787, 0.4677, 0.4223],
          [0.5099, 0.5494, 0.6196,  ..., 0.5050, 0.4517, 0.5157]],

         ...,

         [[0.6083, 0.4455, 0.4981,  ..., 0.4698, 0.5544, 0.4513],
          [0.4475, 0.6085, 0.4435,  ..., 0.4786, 0.5393, 0.5440],
          [0.6120, 0.5419, 0.4862,  ..., 0.4495, 0.5302, 0.3699],
          [0.4526, 0.6353, 0.4582,  ..., 0.6037, 0.5676, 0.4588]],

         [[0.6613, 0.4494, 0.5667,  ..., 0.5957, 0.5060, 0.3100],
          [0.5586, 0.5181, 0.6446,  ..., 0.5567, 0.5943, 0.5308],
          [0.4930, 0.5168, 0.5337,  ..., 0.4624, 0.5005, 0.5077],
          [0.5321, 0.3806, 0.4318,  ..., 0.7162, 0.4452, 0.5289]],

         [[0.5785, 0.6398, 0.4824,  ..., 0.5188, 0.6177, 0.3817],
          [0.4572, 0.5633, 0.6042,  ..., 0.6420, 0.5435, 0.6202],
          [0.5738, 0.4472, 0.4462,  ..., 0.4261, 0.5471, 0.4916],
          [0.6003, 0.4412, 0.4375,  ..., 0.6928, 0.5403, 0.4086]]]],
       device='cuda:0')
tensor([[[[0.5256, 0.2659, 0.5860,  ..., 0.4533, 0.3872, 0.4701],
          [0.4554, 0.4749, 0.5713,  ..., 0.4752, 0.5780, 0.5332],
          [0.4278, 0.6067, 0.5242,  ..., 0.5107, 0.7130, 0.6011],
          [0.4078, 0.4703, 0.5239,  ..., 0.3513, 0.5460, 0.5093]],

         [[0.5085, 0.4450, 0.5931,  ..., 0.4678, 0.4711, 0.5600],
          [0.4683, 0.5034, 0.5453,  ..., 0.3956, 0.5066, 0.5562],
          [0.7264, 0.3337, 0.4767,  ..., 0.5494, 0.6280, 0.7210],
          [0.5254, 0.5665, 0.5813,  ..., 0.5746, 0.3433, 0.5042]],

         [[0.5252, 0.4196, 0.4588,  ..., 0.5136, 0.4964, 0.5761],
          [0.5129, 0.5684, 0.5516,  ..., 0.4752, 0.4896, 0.4278],
          [0.5631, 0.3757, 0.4220,  ..., 0.6514, 0.5794, 0.4354],
          [0.4078, 0.5576, 0.5552,  ..., 0.4116, 0.5397, 0.4858]],

         ...,

         [[0.4822, 0.4528, 0.4882,  ..., 0.5184, 0.4335, 0.5737],
          [0.4273, 0.5559, 0.4278,  ..., 0.6270, 0.5088, 0.4821],
          [0.5046, 0.4678, 0.5504,  ..., 0.5383, 0.5470, 0.3639],
          [0.5118, 0.5150, 0.4844,  ..., 0.4173, 0.5412, 0.5936]],

         [[0.4758, 0.4596, 0.6169,  ..., 0.4487, 0.5143, 0.5001],
          [0.5627, 0.4149, 0.5070,  ..., 0.5566, 0.5969, 0.4031],
          [0.5095, 0.5244, 0.4908,  ..., 0.5363, 0.5804, 0.4696],
          [0.4882, 0.2644, 0.5037,  ..., 0.5368, 0.4706, 0.6156]],

         [[0.4892, 0.4373, 0.5390,  ..., 0.4632, 0.4569, 0.6063],
          [0.6379, 0.5550, 0.5293,  ..., 0.3840, 0.5960, 0.4796],
          [0.5727, 0.4287, 0.5694,  ..., 0.4364, 0.4278, 0.5353],
          [0.5612, 0.2628, 0.4822,  ..., 0.4834, 0.4026, 0.4258]]],


        [[[0.5201, 0.4511, 0.4998,  ..., 0.5118, 0.4097, 0.6289],
          [0.5404, 0.3919, 0.4385,  ..., 0.3567, 0.4991, 0.4254],
          [0.4579, 0.6025, 0.6478,  ..., 0.4311, 0.4017, 0.5694],
          [0.3757, 0.3803, 0.4778,  ..., 0.4306, 0.3425, 0.4472]],

         [[0.5569, 0.5240, 0.4345,  ..., 0.5282, 0.3234, 0.3024],
          [0.7326, 0.5756, 0.3812,  ..., 0.3702, 0.5277, 0.3785],
          [0.5329, 0.3933, 0.4494,  ..., 0.3812, 0.3757, 0.4639],
          [0.5319, 0.3639, 0.5311,  ..., 0.6261, 0.4751, 0.6325]],

         [[0.4064, 0.4600, 0.5441,  ..., 0.6531, 0.4106, 0.6280],
          [0.4970, 0.5722, 0.4724,  ..., 0.4905, 0.4373, 0.5927],
          [0.5344, 0.6220, 0.5610,  ..., 0.5827, 0.4637, 0.4163],
          [0.5079, 0.5574, 0.6156,  ..., 0.5090, 0.4477, 0.5097]],

         ...,

         [[0.6063, 0.4535, 0.4941,  ..., 0.4738, 0.5504, 0.4453],
          [0.4455, 0.6165, 0.4395,  ..., 0.4826, 0.5353, 0.5380],
          [0.6100, 0.5499, 0.4822,  ..., 0.4535, 0.5262, 0.3639],
          [0.4506, 0.6433, 0.4542,  ..., 0.6077, 0.5636, 0.4528]],

         [[0.6593, 0.4574, 0.5627,  ..., 0.5997, 0.5020, 0.3040],
          [0.5566, 0.5261, 0.6406,  ..., 0.5607, 0.5903, 0.5248],
          [0.4910, 0.5248, 0.5297,  ..., 0.4664, 0.4965, 0.5017],
          [0.5301, 0.3886, 0.4278,  ..., 0.7202, 0.4412, 0.5229]],

         [[0.5765, 0.6478, 0.4784,  ..., 0.5228, 0.6137, 0.3757],
          [0.4552, 0.5713, 0.6002,  ..., 0.6460, 0.5395, 0.6142],
          [0.5718, 0.4552, 0.4422,  ..., 0.4301, 0.5431, 0.4856],
          [0.5983, 0.4492, 0.4335,  ..., 0.6968, 0.5363, 0.4026]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0020, -0.0080,  0.0040, -0.0020,  0.0040,  0.0040, -0.0060, -0.0040,
         0.0040,  0.0060], device='cuda:0')
selected experts tensor([1583, 1582, 1631, 1670, 1641, 1705, 1641, 1539, 1728, 1664],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4835, 0.5666, 0.5093,  ..., 0.6666, 0.3466, 0.4489],
          [0.4825, 0.4298, 0.5263,  ..., 0.4246, 0.3258, 0.5478],
          [0.4974, 0.5331, 0.5165,  ..., 0.5186, 0.5489, 0.5396],
          [0.4619, 0.4492, 0.4624,  ..., 0.7027, 0.5439, 0.5096]],

         [[0.3679, 0.5451, 0.6214,  ..., 0.5302, 0.4520, 0.5874],
          [0.4294, 0.4747, 0.5062,  ..., 0.4488, 0.4463, 0.4419],
          [0.3625, 0.5258, 0.6260,  ..., 0.6394, 0.5907, 0.4559],
          [0.4992, 0.3695, 0.5211,  ..., 0.6481, 0.6099, 0.5540]],

         [[0.5260, 0.4212, 0.4882,  ..., 0.5634, 0.5755, 0.5006],
          [0.4515, 0.5140, 0.4718,  ..., 0.4923, 0.6118, 0.5118],
          [0.5486, 0.4524, 0.6232,  ..., 0.3965, 0.4731, 0.4999],
          [0.4337, 0.2826, 0.3765,  ..., 0.4439, 0.6325, 0.5319]],

         ...,

         [[0.4057, 0.5914, 0.4016,  ..., 0.5040, 0.5344, 0.5021],
          [0.4476, 0.3578, 0.6814,  ..., 0.4494, 0.3769, 0.6021],
          [0.4303, 0.5762, 0.5963,  ..., 0.5347, 0.4915, 0.3381],
          [0.3751, 0.4509, 0.5134,  ..., 0.5539, 0.5876, 0.5487]],

         [[0.5753, 0.4212, 0.4551,  ..., 0.4960, 0.4640, 0.5271],
          [0.5180, 0.6363, 0.5822,  ..., 0.5115, 0.5495, 0.4402],
          [0.4691, 0.5603, 0.6033,  ..., 0.4770, 0.4864, 0.5087],
          [0.4447, 0.4742, 0.5977,  ..., 0.6001, 0.5871, 0.5201]],

         [[0.5553, 0.4288, 0.4072,  ..., 0.4803, 0.3474, 0.5396],
          [0.5197, 0.5695, 0.5081,  ..., 0.4712, 0.3316, 0.3937],
          [0.4094, 0.4846, 0.6296,  ..., 0.4128, 0.6822, 0.4218],
          [0.6186, 0.4793, 0.4537,  ..., 0.6455, 0.3840, 0.4676]]],


        [[[0.4822, 0.4502, 0.6168,  ..., 0.3922, 0.3603, 0.5087],
          [0.4614, 0.5444, 0.4967,  ..., 0.2974, 0.4726, 0.4288],
          [0.4370, 0.5191, 0.5836,  ..., 0.4350, 0.5016, 0.5818],
          [0.4418, 0.4293, 0.4479,  ..., 0.5567, 0.5333, 0.5602]],

         [[0.5427, 0.4681, 0.3440,  ..., 0.6523, 0.3707, 0.5535],
          [0.5829, 0.6318, 0.5935,  ..., 0.3870, 0.4307, 0.6229],
          [0.4208, 0.4732, 0.3973,  ..., 0.3534, 0.4066, 0.5851],
          [0.4938, 0.5709, 0.4455,  ..., 0.5072, 0.5315, 0.5106]],

         [[0.5495, 0.5785, 0.5243,  ..., 0.3673, 0.4200, 0.4475],
          [0.4275, 0.4098, 0.6250,  ..., 0.4195, 0.3822, 0.4508],
          [0.3706, 0.5675, 0.5864,  ..., 0.4413, 0.4710, 0.5406],
          [0.4840, 0.3750, 0.4874,  ..., 0.6137, 0.5561, 0.6577]],

         ...,

         [[0.5580, 0.5545, 0.5023,  ..., 0.5019, 0.4597, 0.3471],
          [0.3908, 0.3623, 0.5845,  ..., 0.3724, 0.3162, 0.4886],
          [0.4428, 0.4989, 0.6043,  ..., 0.4939, 0.3795, 0.5549],
          [0.5184, 0.4997, 0.3737,  ..., 0.5798, 0.5398, 0.4949]],

         [[0.6772, 0.4432, 0.6865,  ..., 0.4494, 0.4303, 0.5079],
          [0.6857, 0.3614, 0.4889,  ..., 0.4401, 0.5234, 0.5943],
          [0.4308, 0.4245, 0.6052,  ..., 0.5062, 0.5069, 0.5116],
          [0.4706, 0.3985, 0.4982,  ..., 0.6245, 0.4611, 0.5687]],

         [[0.5782, 0.5251, 0.5000,  ..., 0.5217, 0.4568, 0.4703],
          [0.4734, 0.5574, 0.4467,  ..., 0.4858, 0.3130, 0.5696],
          [0.4483, 0.6180, 0.6071,  ..., 0.4150, 0.4934, 0.4840],
          [0.6205, 0.4393, 0.4447,  ..., 0.4468, 0.4233, 0.4124]]]],
       device='cuda:0')
tensor([[[[0.4795, 0.5646, 0.5113,  ..., 0.6926, 0.3166, 0.4729],
          [0.4785, 0.4278, 0.5283,  ..., 0.4506, 0.2958, 0.5718],
          [0.4934, 0.5311, 0.5185,  ..., 0.5446, 0.5189, 0.5636],
          [0.4579, 0.4472, 0.4644,  ..., 0.7287, 0.5139, 0.5336]],

         [[0.3639, 0.5431, 0.6234,  ..., 0.5562, 0.4220, 0.6114],
          [0.4254, 0.4727, 0.5082,  ..., 0.4748, 0.4163, 0.4659],
          [0.3585, 0.5238, 0.6280,  ..., 0.6654, 0.5607, 0.4799],
          [0.4952, 0.3675, 0.5231,  ..., 0.6741, 0.5799, 0.5780]],

         [[0.5220, 0.4192, 0.4902,  ..., 0.5894, 0.5455, 0.5246],
          [0.4475, 0.5120, 0.4738,  ..., 0.5183, 0.5818, 0.5358],
          [0.5446, 0.4504, 0.6252,  ..., 0.4225, 0.4431, 0.5239],
          [0.4297, 0.2806, 0.3785,  ..., 0.4699, 0.6025, 0.5559]],

         ...,

         [[0.4017, 0.5894, 0.4036,  ..., 0.5300, 0.5044, 0.5261],
          [0.4436, 0.3558, 0.6834,  ..., 0.4754, 0.3469, 0.6261],
          [0.4263, 0.5742, 0.5983,  ..., 0.5607, 0.4615, 0.3621],
          [0.3711, 0.4489, 0.5154,  ..., 0.5799, 0.5576, 0.5727]],

         [[0.5713, 0.4192, 0.4571,  ..., 0.5220, 0.4340, 0.5511],
          [0.5140, 0.6343, 0.5842,  ..., 0.5375, 0.5195, 0.4642],
          [0.4651, 0.5583, 0.6053,  ..., 0.5030, 0.4564, 0.5327],
          [0.4407, 0.4722, 0.5997,  ..., 0.6261, 0.5571, 0.5441]],

         [[0.5513, 0.4268, 0.4092,  ..., 0.5063, 0.3174, 0.5636],
          [0.5157, 0.5675, 0.5101,  ..., 0.4972, 0.3016, 0.4177],
          [0.4054, 0.4826, 0.6316,  ..., 0.4388, 0.6522, 0.4458],
          [0.6146, 0.4773, 0.4557,  ..., 0.6715, 0.3540, 0.4916]]],


        [[[0.4782, 0.4482, 0.6188,  ..., 0.4182, 0.3303, 0.5327],
          [0.4574, 0.5424, 0.4987,  ..., 0.3234, 0.4426, 0.4528],
          [0.4330, 0.5171, 0.5856,  ..., 0.4610, 0.4716, 0.6058],
          [0.4378, 0.4273, 0.4499,  ..., 0.5827, 0.5033, 0.5842]],

         [[0.5387, 0.4661, 0.3460,  ..., 0.6783, 0.3407, 0.5775],
          [0.5789, 0.6298, 0.5955,  ..., 0.4130, 0.4007, 0.6469],
          [0.4168, 0.4712, 0.3993,  ..., 0.3794, 0.3766, 0.6091],
          [0.4898, 0.5689, 0.4475,  ..., 0.5332, 0.5015, 0.5346]],

         [[0.5455, 0.5765, 0.5263,  ..., 0.3933, 0.3900, 0.4715],
          [0.4235, 0.4078, 0.6270,  ..., 0.4455, 0.3522, 0.4748],
          [0.3666, 0.5655, 0.5884,  ..., 0.4673, 0.4410, 0.5646],
          [0.4800, 0.3730, 0.4894,  ..., 0.6397, 0.5261, 0.6817]],

         ...,

         [[0.5540, 0.5525, 0.5043,  ..., 0.5279, 0.4297, 0.3711],
          [0.3868, 0.3603, 0.5865,  ..., 0.3984, 0.2862, 0.5126],
          [0.4388, 0.4969, 0.6063,  ..., 0.5199, 0.3495, 0.5789],
          [0.5144, 0.4977, 0.3757,  ..., 0.6058, 0.5098, 0.5189]],

         [[0.6732, 0.4412, 0.6885,  ..., 0.4754, 0.4003, 0.5319],
          [0.6817, 0.3594, 0.4909,  ..., 0.4661, 0.4934, 0.6183],
          [0.4268, 0.4225, 0.6072,  ..., 0.5322, 0.4769, 0.5356],
          [0.4666, 0.3965, 0.5002,  ..., 0.6505, 0.4311, 0.5927]],

         [[0.5742, 0.5231, 0.5020,  ..., 0.5477, 0.4268, 0.4943],
          [0.4694, 0.5554, 0.4487,  ..., 0.5118, 0.2830, 0.5936],
          [0.4443, 0.6160, 0.6091,  ..., 0.4410, 0.4634, 0.5080],
          [0.6165, 0.4373, 0.4467,  ..., 0.4728, 0.3933, 0.4364]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([ 4.0000e-03,  2.0000e-03, -2.0000e-03, -6.9849e-10,  4.0000e-03,
        -1.8000e-02,  1.4000e-02, -2.6000e-02,  3.0000e-02, -2.4000e-02],
       device='cuda:0')
selected experts tensor([1496, 1362, 1625, 1572, 1518, 2282, 1492, 2102, 1113, 1822],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6481, 0.5775, 0.7173,  ..., 0.4542, 0.5816, 0.3626],
          [0.6669, 0.5630, 0.4922,  ..., 0.6137, 0.5481, 0.4877],
          [0.4915, 0.5965, 0.5318,  ..., 0.4176, 0.4697, 0.6346],
          [0.4099, 0.5623, 0.4702,  ..., 0.6600, 0.6132, 0.4627]],

         [[0.6066, 0.5310, 0.7041,  ..., 0.5179, 0.7332, 0.3635],
          [0.5252, 0.5040, 0.5518,  ..., 0.6360, 0.5203, 0.5202],
          [0.4156, 0.5128, 0.4709,  ..., 0.4702, 0.4783, 0.6067],
          [0.4460, 0.5357, 0.4762,  ..., 0.5537, 0.5319, 0.5165]],

         [[0.5962, 0.5640, 0.6582,  ..., 0.3635, 0.6001, 0.4394],
          [0.4844, 0.4459, 0.5334,  ..., 0.5329, 0.4809, 0.4237],
          [0.5129, 0.5045, 0.4557,  ..., 0.5285, 0.6565, 0.5067],
          [0.5111, 0.5524, 0.4869,  ..., 0.6519, 0.4049, 0.5891]],

         ...,

         [[0.5120, 0.5100, 0.5140,  ..., 0.5140, 0.5060, 0.5140],
          [0.5120, 0.5100, 0.5140,  ..., 0.5140, 0.5060, 0.5140],
          [0.5120, 0.5100, 0.5140,  ..., 0.5140, 0.5060, 0.5140],
          [0.5120, 0.5100, 0.5140,  ..., 0.5140, 0.5060, 0.5140]],

         [[0.5120, 0.5100, 0.5140,  ..., 0.5140, 0.5060, 0.5140],
          [0.5120, 0.5100, 0.5140,  ..., 0.5140, 0.5060, 0.5140],
          [0.5120, 0.5100, 0.5140,  ..., 0.5140, 0.5060, 0.5140],
          [0.5120, 0.5100, 0.5140,  ..., 0.5140, 0.5060, 0.5140]],

         [[0.5120, 0.5100, 0.5140,  ..., 0.5140, 0.5060, 0.5140],
          [0.5120, 0.5100, 0.5140,  ..., 0.5140, 0.5060, 0.5140],
          [0.5120, 0.5100, 0.5140,  ..., 0.5140, 0.5060, 0.5140],
          [0.5120, 0.5100, 0.5140,  ..., 0.5140, 0.5060, 0.5140]]],


        [[[0.5204, 0.6452, 0.7124,  ..., 0.4821, 0.6511, 0.3556],
          [0.5823, 0.5126, 0.3698,  ..., 0.6392, 0.5242, 0.5379],
          [0.4958, 0.5698, 0.5872,  ..., 0.3600, 0.5353, 0.5791],
          [0.5187, 0.6702, 0.6365,  ..., 0.7246, 0.4930, 0.5009]],

         [[0.6929, 0.5686, 0.5117,  ..., 0.4475, 0.5711, 0.4646],
          [0.5490, 0.4145, 0.4223,  ..., 0.6750, 0.4200, 0.4808],
          [0.3642, 0.5203, 0.3788,  ..., 0.4639, 0.3960, 0.5363],
          [0.4643, 0.5436, 0.4143,  ..., 0.5561, 0.5535, 0.4736]],

         [[0.4137, 0.5032, 0.5953,  ..., 0.2899, 0.7347, 0.4105],
          [0.5191, 0.5937, 0.4646,  ..., 0.5104, 0.4058, 0.5358],
          [0.4137, 0.5799, 0.4709,  ..., 0.4721, 0.5595, 0.5080],
          [0.4981, 0.5091, 0.5405,  ..., 0.5929, 0.5811, 0.5910]],

         ...,

         [[0.5120, 0.5100, 0.5140,  ..., 0.5140, 0.5060, 0.5140],
          [0.5120, 0.5100, 0.5140,  ..., 0.5140, 0.5060, 0.5140],
          [0.5120, 0.5100, 0.5140,  ..., 0.5140, 0.5060, 0.5140],
          [0.5120, 0.5100, 0.5140,  ..., 0.5140, 0.5060, 0.5140]],

         [[0.5120, 0.5100, 0.5140,  ..., 0.5140, 0.5060, 0.5140],
          [0.5120, 0.5100, 0.5140,  ..., 0.5140, 0.5060, 0.5140],
          [0.5120, 0.5100, 0.5140,  ..., 0.5140, 0.5060, 0.5140],
          [0.5120, 0.5100, 0.5140,  ..., 0.5140, 0.5060, 0.5140]],

         [[0.5120, 0.5100, 0.5140,  ..., 0.5140, 0.5060, 0.5140],
          [0.5120, 0.5100, 0.5140,  ..., 0.5140, 0.5060, 0.5140],
          [0.5120, 0.5100, 0.5140,  ..., 0.5140, 0.5060, 0.5140],
          [0.5120, 0.5100, 0.5140,  ..., 0.5140, 0.5060, 0.5140]]]],
       device='cuda:0')
tensor([[[[0.6361, 0.5675, 0.7033,  ..., 0.4402, 0.5756, 0.3486],
          [0.6549, 0.5530, 0.4782,  ..., 0.5997, 0.5421, 0.4737],
          [0.4795, 0.5865, 0.5178,  ..., 0.4036, 0.4637, 0.6206],
          [0.3979, 0.5523, 0.4562,  ..., 0.6460, 0.6072, 0.4487]],

         [[0.5946, 0.5210, 0.6901,  ..., 0.5039, 0.7272, 0.3495],
          [0.5132, 0.4940, 0.5378,  ..., 0.6220, 0.5143, 0.5062],
          [0.4036, 0.5028, 0.4569,  ..., 0.4562, 0.4723, 0.5927],
          [0.4340, 0.5257, 0.4622,  ..., 0.5397, 0.5259, 0.5025]],

         [[0.5842, 0.5540, 0.6442,  ..., 0.3495, 0.5941, 0.4254],
          [0.4724, 0.4359, 0.5194,  ..., 0.5189, 0.4749, 0.4097],
          [0.5009, 0.4945, 0.4417,  ..., 0.5145, 0.6505, 0.4927],
          [0.4991, 0.5424, 0.4729,  ..., 0.6379, 0.3989, 0.5751]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5084, 0.6352, 0.6984,  ..., 0.4681, 0.6451, 0.3416],
          [0.5703, 0.5026, 0.3558,  ..., 0.6252, 0.5182, 0.5239],
          [0.4838, 0.5598, 0.5732,  ..., 0.3460, 0.5293, 0.5651],
          [0.5067, 0.6602, 0.6225,  ..., 0.7106, 0.4870, 0.4869]],

         [[0.6809, 0.5586, 0.4977,  ..., 0.4335, 0.5651, 0.4506],
          [0.5370, 0.4045, 0.4083,  ..., 0.6610, 0.4140, 0.4668],
          [0.3522, 0.5103, 0.3648,  ..., 0.4499, 0.3900, 0.5223],
          [0.4523, 0.5336, 0.4003,  ..., 0.5421, 0.5475, 0.4596]],

         [[0.4017, 0.4932, 0.5813,  ..., 0.2759, 0.7287, 0.3965],
          [0.5071, 0.5837, 0.4506,  ..., 0.4964, 0.3998, 0.5218],
          [0.4017, 0.5699, 0.4569,  ..., 0.4581, 0.5535, 0.4940],
          [0.4861, 0.4991, 0.5265,  ..., 0.5789, 0.5751, 0.5770]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0120, 0.0100, 0.0140, 0.0140, 0.0140, 0.0080, 0.0100, 0.0140, 0.0060,
        0.0140], device='cuda:0')
selected experts tensor([1214, 1556, 2999, 2271,  780, 1867, 1879, 1096, 1983,  739],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6749, 0.3284, 0.3800,  ..., 0.5686, 0.4201, 0.4261],
          [0.6395, 0.5274, 0.4673,  ..., 0.4877, 0.4013, 0.5701],
          [0.4863, 0.4966, 0.4228,  ..., 0.5573, 0.4325, 0.5045],
          [0.7068, 0.4583, 0.5251,  ..., 0.4119, 0.3972, 0.3227]],

         [[0.4759, 0.5803, 0.6566,  ..., 0.6943, 0.4972, 0.4396],
          [0.4490, 0.5679, 0.5479,  ..., 0.4742, 0.4842, 0.4290],
          [0.4120, 0.4816, 0.4271,  ..., 0.5578, 0.5144, 0.4161],
          [0.3992, 0.5822, 0.5641,  ..., 0.5440, 0.5865, 0.4189]],

         [[0.5454, 0.6739, 0.4719,  ..., 0.4479, 0.4734, 0.3707],
          [0.4769, 0.5245, 0.5557,  ..., 0.4714, 0.4901, 0.5364],
          [0.5263, 0.4172, 0.4376,  ..., 0.4653, 0.5596, 0.5128],
          [0.5840, 0.6034, 0.5230,  ..., 0.5440, 0.4401, 0.3652]],

         ...,

         [[0.4996, 0.3656, 0.5964,  ..., 0.7092, 0.3820, 0.4295],
          [0.5826, 0.4167, 0.6081,  ..., 0.4285, 0.5082, 0.4213],
          [0.3944, 0.6635, 0.4842,  ..., 0.5839, 0.5203, 0.3348],
          [0.5054, 0.4738, 0.4381,  ..., 0.4455, 0.5678, 0.3436]],

         [[0.5971, 0.5078, 0.5195,  ..., 0.4308, 0.5207, 0.4926],
          [0.4430, 0.4848, 0.5826,  ..., 0.5811, 0.5922, 0.5316],
          [0.4612, 0.5841, 0.4729,  ..., 0.4194, 0.4664, 0.6759],
          [0.3090, 0.5518, 0.4451,  ..., 0.5406, 0.5215, 0.4199]],

         [[0.6206, 0.5360, 0.4348,  ..., 0.5792, 0.4372, 0.6230],
          [0.5010, 0.4610, 0.3859,  ..., 0.6429, 0.5023, 0.4519],
          [0.5732, 0.3064, 0.3416,  ..., 0.4185, 0.5436, 0.5129],
          [0.3656, 0.4404, 0.5508,  ..., 0.5270, 0.4959, 0.5730]]],


        [[[0.5948, 0.4344, 0.4859,  ..., 0.4878, 0.4140, 0.7485],
          [0.6004, 0.5377, 0.3451,  ..., 0.3413, 0.5339, 0.6621],
          [0.5162, 0.3584, 0.5426,  ..., 0.4899, 0.5741, 0.4584],
          [0.6323, 0.5280, 0.5199,  ..., 0.4886, 0.4463, 0.6156]],

         [[0.5430, 0.3692, 0.5479,  ..., 0.4043, 0.6487, 0.6428],
          [0.6260, 0.3692, 0.4782,  ..., 0.5334, 0.4116, 0.4028],
          [0.4343, 0.5585, 0.5721,  ..., 0.5691, 0.4524, 0.5734],
          [0.5386, 0.5498, 0.4867,  ..., 0.5149, 0.4642, 0.4384]],

         [[0.4048, 0.5874, 0.3700,  ..., 0.3959, 0.5487, 0.5577],
          [0.3376, 0.4225, 0.4709,  ..., 0.5245, 0.5077, 0.5891],
          [0.4163, 0.4689, 0.6105,  ..., 0.3422, 0.4325, 0.4251],
          [0.4011, 0.3512, 0.4903,  ..., 0.4208, 0.5165, 0.4408]],

         ...,

         [[0.4328, 0.5573, 0.4096,  ..., 0.4067, 0.5475, 0.5792],
          [0.5162, 0.5590, 0.4049,  ..., 0.6159, 0.5712, 0.5839],
          [0.3610, 0.5129, 0.6887,  ..., 0.4476, 0.4514, 0.4984],
          [0.4367, 0.3683, 0.5179,  ..., 0.5877, 0.5147, 0.5701]],

         [[0.4928, 0.4673, 0.4886,  ..., 0.5575, 0.5601, 0.4266],
          [0.3493, 0.3647, 0.6025,  ..., 0.3918, 0.4586, 0.4679],
          [0.3583, 0.6405, 0.5075,  ..., 0.5474, 0.4415, 0.4228],
          [0.5550, 0.4368, 0.6119,  ..., 0.5396, 0.5611, 0.5966]],

         [[0.4244, 0.5631, 0.6887,  ..., 0.5580, 0.6621, 0.4280],
          [0.5883, 0.5636, 0.6072,  ..., 0.5358, 0.5955, 0.4582],
          [0.6179, 0.3476, 0.4832,  ..., 0.6085, 0.5456, 0.4900],
          [0.6359, 0.4629, 0.4424,  ..., 0.5619, 0.6040, 0.6175]]]],
       device='cuda:0')
tensor([[[[0.6859, 0.3294, 0.3730,  ..., 0.5636, 0.4111, 0.4311],
          [0.6505, 0.5284, 0.4603,  ..., 0.4827, 0.3923, 0.5751],
          [0.4973, 0.4976, 0.4158,  ..., 0.5523, 0.4235, 0.5095],
          [0.7178, 0.4593, 0.5181,  ..., 0.4069, 0.3882, 0.3277]],

         [[0.4869, 0.5813, 0.6496,  ..., 0.6893, 0.4882, 0.4446],
          [0.4600, 0.5689, 0.5409,  ..., 0.4692, 0.4752, 0.4340],
          [0.4230, 0.4826, 0.4201,  ..., 0.5528, 0.5054, 0.4211],
          [0.4102, 0.5832, 0.5571,  ..., 0.5390, 0.5775, 0.4239]],

         [[0.5564, 0.6749, 0.4649,  ..., 0.4429, 0.4644, 0.3757],
          [0.4879, 0.5255, 0.5487,  ..., 0.4664, 0.4811, 0.5414],
          [0.5373, 0.4182, 0.4306,  ..., 0.4603, 0.5506, 0.5178],
          [0.5950, 0.6044, 0.5160,  ..., 0.5390, 0.4311, 0.3702]],

         ...,

         [[0.5106, 0.3666, 0.5894,  ..., 0.7042, 0.3730, 0.4345],
          [0.5936, 0.4177, 0.6011,  ..., 0.4235, 0.4992, 0.4263],
          [0.4054, 0.6645, 0.4772,  ..., 0.5789, 0.5113, 0.3398],
          [0.5164, 0.4748, 0.4311,  ..., 0.4405, 0.5588, 0.3486]],

         [[0.6081, 0.5088, 0.5125,  ..., 0.4258, 0.5117, 0.4976],
          [0.4540, 0.4858, 0.5756,  ..., 0.5761, 0.5832, 0.5366],
          [0.4722, 0.5851, 0.4659,  ..., 0.4144, 0.4574, 0.6809],
          [0.3200, 0.5528, 0.4381,  ..., 0.5356, 0.5125, 0.4249]],

         [[0.6316, 0.5370, 0.4278,  ..., 0.5742, 0.4282, 0.6280],
          [0.5120, 0.4620, 0.3789,  ..., 0.6379, 0.4933, 0.4569],
          [0.5842, 0.3074, 0.3346,  ..., 0.4135, 0.5346, 0.5179],
          [0.3766, 0.4414, 0.5438,  ..., 0.5220, 0.4869, 0.5780]]],


        [[[0.6058, 0.4354, 0.4789,  ..., 0.4828, 0.4050, 0.7535],
          [0.6114, 0.5387, 0.3381,  ..., 0.3363, 0.5249, 0.6671],
          [0.5272, 0.3594, 0.5356,  ..., 0.4849, 0.5651, 0.4634],
          [0.6433, 0.5290, 0.5129,  ..., 0.4836, 0.4373, 0.6206]],

         [[0.5540, 0.3702, 0.5409,  ..., 0.3993, 0.6397, 0.6478],
          [0.6370, 0.3702, 0.4712,  ..., 0.5284, 0.4026, 0.4078],
          [0.4453, 0.5595, 0.5651,  ..., 0.5641, 0.4434, 0.5784],
          [0.5496, 0.5508, 0.4797,  ..., 0.5099, 0.4552, 0.4434]],

         [[0.4158, 0.5884, 0.3630,  ..., 0.3909, 0.5397, 0.5627],
          [0.3486, 0.4235, 0.4639,  ..., 0.5195, 0.4987, 0.5941],
          [0.4273, 0.4699, 0.6035,  ..., 0.3372, 0.4235, 0.4301],
          [0.4121, 0.3522, 0.4833,  ..., 0.4158, 0.5075, 0.4458]],

         ...,

         [[0.4438, 0.5583, 0.4026,  ..., 0.4017, 0.5385, 0.5842],
          [0.5272, 0.5600, 0.3979,  ..., 0.6109, 0.5622, 0.5889],
          [0.3720, 0.5139, 0.6817,  ..., 0.4426, 0.4424, 0.5034],
          [0.4477, 0.3693, 0.5109,  ..., 0.5827, 0.5057, 0.5751]],

         [[0.5038, 0.4683, 0.4816,  ..., 0.5525, 0.5511, 0.4316],
          [0.3603, 0.3657, 0.5955,  ..., 0.3868, 0.4496, 0.4729],
          [0.3693, 0.6415, 0.5005,  ..., 0.5424, 0.4325, 0.4278],
          [0.5660, 0.4378, 0.6049,  ..., 0.5346, 0.5521, 0.6016]],

         [[0.4354, 0.5641, 0.6817,  ..., 0.5530, 0.6531, 0.4330],
          [0.5993, 0.5646, 0.6002,  ..., 0.5308, 0.5865, 0.4632],
          [0.6289, 0.3486, 0.4762,  ..., 0.6035, 0.5366, 0.4950],
          [0.6469, 0.4639, 0.4354,  ..., 0.5569, 0.5950, 0.6225]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0110, -0.0010,  0.0070,  0.0010,  0.0090,  0.0070, -0.0030,  0.0050,
         0.0090, -0.0050], device='cuda:0')
selected experts tensor([1694, 1566, 1597, 1323, 1810, 1761, 1784, 1439, 1631, 1779],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4425, 0.5561, 0.5282,  ..., 0.3884, 0.7112, 0.4537],
          [0.4638, 0.4107, 0.3752,  ..., 0.2494, 0.5548, 0.4617],
          [0.4698, 0.4990, 0.4898,  ..., 0.4128, 0.4141, 0.5102],
          [0.4793, 0.4417, 0.3955,  ..., 0.6167, 0.5071, 0.4380]],

         [[0.5437, 0.5633, 0.5195,  ..., 0.5311, 0.4643, 0.5234],
          [0.4874, 0.4653, 0.5597,  ..., 0.5183, 0.6027, 0.4053],
          [0.4952, 0.3569, 0.4474,  ..., 0.5510, 0.5999, 0.4614],
          [0.4146, 0.4788, 0.5389,  ..., 0.6107, 0.4151, 0.5379]],

         [[0.3902, 0.4371, 0.6210,  ..., 0.4291, 0.5881, 0.5357],
          [0.3508, 0.4993, 0.3987,  ..., 0.5075, 0.6051, 0.5981],
          [0.6273, 0.4103, 0.5772,  ..., 0.4248, 0.3939, 0.5910],
          [0.4399, 0.5134, 0.4431,  ..., 0.5563, 0.5695, 0.4665]],

         ...,

         [[0.4681, 0.4960, 0.3922,  ..., 0.4095, 0.4075, 0.4469],
          [0.4476, 0.5585, 0.5124,  ..., 0.3819, 0.5415, 0.5459],
          [0.4122, 0.4753, 0.6850,  ..., 0.5252, 0.5928, 0.4954],
          [0.4793, 0.4543, 0.4660,  ..., 0.4663, 0.3411, 0.4915]],

         [[0.4792, 0.4994, 0.6210,  ..., 0.4411, 0.5630, 0.5720],
          [0.5287, 0.5643, 0.6192,  ..., 0.4399, 0.4427, 0.6339],
          [0.5295, 0.5215, 0.7482,  ..., 0.5587, 0.4415, 0.6071],
          [0.3815, 0.4530, 0.5242,  ..., 0.6313, 0.3455, 0.5563]],

         [[0.5405, 0.4986, 0.5616,  ..., 0.5563, 0.6391, 0.6438],
          [0.4502, 0.5274, 0.4380,  ..., 0.4713, 0.5112, 0.4194],
          [0.5182, 0.4777, 0.5016,  ..., 0.5331, 0.6605, 0.6366],
          [0.5400, 0.4974, 0.4953,  ..., 0.5029, 0.5776, 0.4989]]],


        [[[0.4580, 0.3961, 0.6284,  ..., 0.3307, 0.4587, 0.3599],
          [0.5195, 0.3937, 0.5210,  ..., 0.4854, 0.7386, 0.6270],
          [0.5374, 0.4467, 0.4484,  ..., 0.2994, 0.4265, 0.3881],
          [0.4903, 0.5106, 0.5474,  ..., 0.6728, 0.6222, 0.5445]],

         [[0.4601, 0.4633, 0.3644,  ..., 0.6719, 0.6074, 0.6057],
          [0.4871, 0.5266, 0.3387,  ..., 0.3273, 0.4122, 0.4419],
          [0.6623, 0.6291, 0.4076,  ..., 0.4010, 0.4741, 0.6265],
          [0.6915, 0.3259, 0.4399,  ..., 0.4181, 0.5415, 0.6094]],

         [[0.5695, 0.4279, 0.4728,  ..., 0.3921, 0.5810, 0.6330],
          [0.5310, 0.5705, 0.3969,  ..., 0.4365, 0.6889, 0.4237],
          [0.4122, 0.3363, 0.5161,  ..., 0.4481, 0.3696, 0.5135],
          [0.3446, 0.4964, 0.4886,  ..., 0.4348, 0.5065, 0.6357]],

         ...,

         [[0.6873, 0.4742, 0.6643,  ..., 0.5377, 0.5563, 0.5522],
          [0.4023, 0.5029, 0.3997,  ..., 0.4617, 0.5219, 0.3780],
          [0.3516, 0.3905, 0.4898,  ..., 0.6061, 0.4461, 0.4409],
          [0.4274, 0.4665, 0.5058,  ..., 0.6268, 0.4701, 0.6850]],

         [[0.6472, 0.3497, 0.6546,  ..., 0.5478, 0.4403, 0.5189],
          [0.5616, 0.6264, 0.4464,  ..., 0.4162, 0.5250, 0.3862],
          [0.4052, 0.3886, 0.4787,  ..., 0.4573, 0.5230, 0.4773],
          [0.5007, 0.5320, 0.6219,  ..., 0.6572, 0.3437, 0.5362]],

         [[0.4260, 0.4036, 0.5288,  ..., 0.6213, 0.3446, 0.4568],
          [0.3624, 0.4759, 0.5624,  ..., 0.6213, 0.3778, 0.6293],
          [0.5287, 0.4270, 0.5192,  ..., 0.6139, 0.4887, 0.4822],
          [0.5666, 0.5395, 0.6103,  ..., 0.4503, 0.5233, 0.3396]]]],
       device='cuda:0')
tensor([[[[0.4395, 0.5631, 0.5232,  ..., 0.3914, 0.7082, 0.4487],
          [0.4608, 0.4177, 0.3702,  ..., 0.2524, 0.5518, 0.4567],
          [0.4668, 0.5060, 0.4848,  ..., 0.4158, 0.4111, 0.5052],
          [0.4763, 0.4487, 0.3905,  ..., 0.6197, 0.5041, 0.4330]],

         [[0.5407, 0.5703, 0.5145,  ..., 0.5341, 0.4613, 0.5184],
          [0.4844, 0.4723, 0.5547,  ..., 0.5213, 0.5997, 0.4003],
          [0.4922, 0.3639, 0.4424,  ..., 0.5540, 0.5969, 0.4564],
          [0.4116, 0.4858, 0.5339,  ..., 0.6137, 0.4121, 0.5329]],

         [[0.3872, 0.4441, 0.6160,  ..., 0.4321, 0.5851, 0.5307],
          [0.3478, 0.5063, 0.3937,  ..., 0.5105, 0.6021, 0.5931],
          [0.6243, 0.4173, 0.5722,  ..., 0.4278, 0.3909, 0.5860],
          [0.4369, 0.5204, 0.4381,  ..., 0.5593, 0.5665, 0.4615]],

         ...,

         [[0.4651, 0.5030, 0.3872,  ..., 0.4125, 0.4045, 0.4419],
          [0.4446, 0.5655, 0.5074,  ..., 0.3849, 0.5385, 0.5409],
          [0.4092, 0.4823, 0.6800,  ..., 0.5282, 0.5898, 0.4904],
          [0.4763, 0.4613, 0.4610,  ..., 0.4693, 0.3381, 0.4865]],

         [[0.4762, 0.5064, 0.6160,  ..., 0.4441, 0.5600, 0.5670],
          [0.5257, 0.5713, 0.6142,  ..., 0.4429, 0.4397, 0.6289],
          [0.5265, 0.5285, 0.7432,  ..., 0.5617, 0.4385, 0.6021],
          [0.3785, 0.4600, 0.5192,  ..., 0.6343, 0.3425, 0.5513]],

         [[0.5375, 0.5056, 0.5566,  ..., 0.5593, 0.6361, 0.6388],
          [0.4472, 0.5344, 0.4330,  ..., 0.4743, 0.5082, 0.4144],
          [0.5152, 0.4847, 0.4966,  ..., 0.5361, 0.6575, 0.6316],
          [0.5370, 0.5044, 0.4903,  ..., 0.5059, 0.5746, 0.4939]]],


        [[[0.4550, 0.4031, 0.6234,  ..., 0.3337, 0.4557, 0.3549],
          [0.5165, 0.4007, 0.5160,  ..., 0.4884, 0.7356, 0.6220],
          [0.5344, 0.4537, 0.4434,  ..., 0.3024, 0.4235, 0.3831],
          [0.4873, 0.5176, 0.5424,  ..., 0.6758, 0.6192, 0.5395]],

         [[0.4571, 0.4703, 0.3594,  ..., 0.6749, 0.6044, 0.6007],
          [0.4841, 0.5336, 0.3337,  ..., 0.3303, 0.4092, 0.4369],
          [0.6593, 0.6361, 0.4026,  ..., 0.4040, 0.4711, 0.6215],
          [0.6885, 0.3329, 0.4349,  ..., 0.4211, 0.5385, 0.6044]],

         [[0.5665, 0.4349, 0.4678,  ..., 0.3951, 0.5780, 0.6280],
          [0.5280, 0.5775, 0.3919,  ..., 0.4395, 0.6859, 0.4187],
          [0.4092, 0.3433, 0.5111,  ..., 0.4511, 0.3666, 0.5085],
          [0.3416, 0.5034, 0.4836,  ..., 0.4378, 0.5035, 0.6307]],

         ...,

         [[0.6843, 0.4812, 0.6593,  ..., 0.5407, 0.5533, 0.5472],
          [0.3993, 0.5099, 0.3947,  ..., 0.4647, 0.5189, 0.3730],
          [0.3486, 0.3975, 0.4848,  ..., 0.6091, 0.4431, 0.4359],
          [0.4244, 0.4735, 0.5008,  ..., 0.6298, 0.4671, 0.6800]],

         [[0.6442, 0.3567, 0.6496,  ..., 0.5508, 0.4373, 0.5139],
          [0.5586, 0.6334, 0.4414,  ..., 0.4192, 0.5220, 0.3812],
          [0.4022, 0.3956, 0.4737,  ..., 0.4603, 0.5200, 0.4723],
          [0.4977, 0.5390, 0.6169,  ..., 0.6602, 0.3407, 0.5312]],

         [[0.4230, 0.4106, 0.5238,  ..., 0.6243, 0.3416, 0.4518],
          [0.3594, 0.4829, 0.5574,  ..., 0.6243, 0.3748, 0.6243],
          [0.5257, 0.4340, 0.5142,  ..., 0.6169, 0.4857, 0.4772],
          [0.5636, 0.5465, 0.6053,  ..., 0.4533, 0.5203, 0.3346]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0030, -0.0070,  0.0050, -0.0030,  0.0030,  0.0030, -0.0070, -0.0030,
         0.0030,  0.0050], device='cuda:0')
selected experts tensor([1561, 1659, 1571, 1614, 1791, 1676, 1516, 1655, 1685, 1656],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5849, 0.4452, 0.4419,  ..., 0.5357, 0.4154, 0.3890],
          [0.4636, 0.5118, 0.5712,  ..., 0.5347, 0.3639, 0.4423],
          [0.4062, 0.6649, 0.5224,  ..., 0.5732, 0.6752, 0.5203],
          [0.4989, 0.5172, 0.4177,  ..., 0.5510, 0.4327, 0.4360]],

         [[0.4828, 0.5319, 0.4774,  ..., 0.6796, 0.5143, 0.4358],
          [0.4053, 0.4113, 0.5564,  ..., 0.3874, 0.3805, 0.5686],
          [0.4385, 0.6544, 0.5722,  ..., 0.5071, 0.6377, 0.4628],
          [0.4578, 0.4394, 0.3844,  ..., 0.5553, 0.4976, 0.4782]],

         [[0.4479, 0.4117, 0.3872,  ..., 0.5788, 0.4397, 0.4903],
          [0.4624, 0.5010, 0.6122,  ..., 0.3146, 0.3665, 0.4426],
          [0.3839, 0.5560, 0.6085,  ..., 0.4843, 0.6232, 0.5062],
          [0.4428, 0.5120, 0.4987,  ..., 0.5918, 0.3922, 0.4234]],

         ...,

         [[0.5474, 0.4953, 0.4600,  ..., 0.4768, 0.3484, 0.4793],
          [0.5882, 0.5359, 0.4438,  ..., 0.3208, 0.5083, 0.5328],
          [0.4204, 0.5417, 0.3793,  ..., 0.4394, 0.3587, 0.6011],
          [0.5715, 0.3154, 0.3415,  ..., 0.5462, 0.4327, 0.4685]],

         [[0.5534, 0.4519, 0.5221,  ..., 0.4658, 0.3285, 0.5360],
          [0.5873, 0.4696, 0.4928,  ..., 0.4394, 0.5994, 0.5307],
          [0.5265, 0.6291, 0.6288,  ..., 0.5448, 0.4173, 0.4331],
          [0.6528, 0.3769, 0.5201,  ..., 0.5586, 0.6194, 0.3885]],

         [[0.6581, 0.4312, 0.5674,  ..., 0.4495, 0.5600, 0.6816],
          [0.6224, 0.4830, 0.6210,  ..., 0.4593, 0.3985, 0.5482],
          [0.5243, 0.3741, 0.6076,  ..., 0.4318, 0.5985, 0.5258],
          [0.5341, 0.4141, 0.5817,  ..., 0.6145, 0.5013, 0.6369]]],


        [[[0.5418, 0.6291, 0.4973,  ..., 0.6539, 0.4478, 0.5468],
          [0.4390, 0.4132, 0.5411,  ..., 0.3752, 0.4862, 0.5696],
          [0.4261, 0.3796, 0.5307,  ..., 0.3279, 0.5104, 0.5620],
          [0.4782, 0.5671, 0.4822,  ..., 0.5156, 0.5439, 0.5251]],

         [[0.5346, 0.5252, 0.4139,  ..., 0.4872, 0.3212, 0.4460],
          [0.5198, 0.4677, 0.6618,  ..., 0.2989, 0.3613, 0.5220],
          [0.4128, 0.6418, 0.5717,  ..., 0.5105, 0.4736, 0.5577],
          [0.5686, 0.5810, 0.4201,  ..., 0.6261, 0.4012, 0.5006]],

         [[0.5437, 0.2727, 0.4115,  ..., 0.5163, 0.3359, 0.4547],
          [0.3224, 0.4897, 0.4272,  ..., 0.3414, 0.4187, 0.3562],
          [0.3199, 0.6889, 0.6043,  ..., 0.4185, 0.5680, 0.5492],
          [0.4072, 0.4613, 0.3895,  ..., 0.6375, 0.4910, 0.4314]],

         ...,

         [[0.4918, 0.5700, 0.4325,  ..., 0.6261, 0.4215, 0.5171],
          [0.2401, 0.6719, 0.5535,  ..., 0.4745, 0.5136, 0.5929],
          [0.4275, 0.4958, 0.7032,  ..., 0.5294, 0.4602, 0.3113],
          [0.7188, 0.2628, 0.4694,  ..., 0.4234, 0.5610, 0.5137]],

         [[0.4670, 0.6579, 0.5445,  ..., 0.5265, 0.4802, 0.5246],
          [0.4347, 0.5526, 0.4515,  ..., 0.4479, 0.5263, 0.5183],
          [0.4629, 0.4217, 0.6477,  ..., 0.4464, 0.4626, 0.3562],
          [0.8082, 0.4094, 0.4997,  ..., 0.4246, 0.5543, 0.4591]],

         [[0.5529, 0.4577, 0.3602,  ..., 0.4490, 0.3770, 0.5873],
          [0.5854, 0.5524, 0.6053,  ..., 0.4381, 0.4021, 0.5229],
          [0.4595, 0.5509, 0.5689,  ..., 0.3120, 0.6028, 0.6525],
          [0.5986, 0.3981, 0.4752,  ..., 0.5438, 0.5467, 0.4680]]]],
       device='cuda:0')
tensor([[[[0.5799, 0.4422, 0.4429,  ..., 0.5627, 0.3844, 0.4140],
          [0.4586, 0.5088, 0.5722,  ..., 0.5617, 0.3329, 0.4673],
          [0.4012, 0.6619, 0.5234,  ..., 0.6002, 0.6442, 0.5453],
          [0.4939, 0.5142, 0.4187,  ..., 0.5780, 0.4017, 0.4610]],

         [[0.4778, 0.5289, 0.4784,  ..., 0.7066, 0.4833, 0.4608],
          [0.4003, 0.4083, 0.5574,  ..., 0.4144, 0.3495, 0.5936],
          [0.4335, 0.6514, 0.5732,  ..., 0.5341, 0.6067, 0.4878],
          [0.4528, 0.4364, 0.3854,  ..., 0.5823, 0.4666, 0.5032]],

         [[0.4429, 0.4087, 0.3882,  ..., 0.6058, 0.4087, 0.5153],
          [0.4574, 0.4980, 0.6132,  ..., 0.3416, 0.3355, 0.4676],
          [0.3789, 0.5530, 0.6095,  ..., 0.5113, 0.5922, 0.5312],
          [0.4378, 0.5090, 0.4997,  ..., 0.6188, 0.3612, 0.4484]],

         ...,

         [[0.5424, 0.4923, 0.4610,  ..., 0.5038, 0.3174, 0.5043],
          [0.5832, 0.5329, 0.4448,  ..., 0.3478, 0.4773, 0.5578],
          [0.4154, 0.5387, 0.3803,  ..., 0.4664, 0.3277, 0.6261],
          [0.5665, 0.3124, 0.3425,  ..., 0.5732, 0.4017, 0.4935]],

         [[0.5484, 0.4489, 0.5231,  ..., 0.4928, 0.2975, 0.5610],
          [0.5823, 0.4666, 0.4938,  ..., 0.4664, 0.5684, 0.5557],
          [0.5215, 0.6261, 0.6298,  ..., 0.5718, 0.3863, 0.4581],
          [0.6478, 0.3739, 0.5211,  ..., 0.5856, 0.5884, 0.4135]],

         [[0.6531, 0.4282, 0.5684,  ..., 0.4765, 0.5290, 0.7066],
          [0.6174, 0.4800, 0.6220,  ..., 0.4863, 0.3675, 0.5732],
          [0.5193, 0.3711, 0.6086,  ..., 0.4588, 0.5675, 0.5508],
          [0.5291, 0.4111, 0.5827,  ..., 0.6415, 0.4703, 0.6619]]],


        [[[0.5368, 0.6261, 0.4983,  ..., 0.6809, 0.4168, 0.5718],
          [0.4340, 0.4102, 0.5421,  ..., 0.4022, 0.4552, 0.5946],
          [0.4211, 0.3766, 0.5317,  ..., 0.3549, 0.4794, 0.5870],
          [0.4732, 0.5641, 0.4832,  ..., 0.5426, 0.5129, 0.5501]],

         [[0.5296, 0.5222, 0.4149,  ..., 0.5142, 0.2902, 0.4710],
          [0.5148, 0.4647, 0.6628,  ..., 0.3259, 0.3303, 0.5470],
          [0.4078, 0.6388, 0.5727,  ..., 0.5375, 0.4426, 0.5827],
          [0.5636, 0.5780, 0.4211,  ..., 0.6531, 0.3702, 0.5256]],

         [[0.5387, 0.2697, 0.4125,  ..., 0.5433, 0.3049, 0.4797],
          [0.3174, 0.4867, 0.4282,  ..., 0.3684, 0.3877, 0.3812],
          [0.3149, 0.6859, 0.6053,  ..., 0.4455, 0.5370, 0.5742],
          [0.4022, 0.4583, 0.3905,  ..., 0.6645, 0.4600, 0.4564]],

         ...,

         [[0.4868, 0.5670, 0.4335,  ..., 0.6531, 0.3905, 0.5421],
          [0.2351, 0.6689, 0.5545,  ..., 0.5015, 0.4826, 0.6179],
          [0.4225, 0.4928, 0.7042,  ..., 0.5564, 0.4292, 0.3363],
          [0.7138, 0.2598, 0.4704,  ..., 0.4504, 0.5300, 0.5387]],

         [[0.4620, 0.6549, 0.5455,  ..., 0.5535, 0.4492, 0.5496],
          [0.4297, 0.5496, 0.4525,  ..., 0.4749, 0.4953, 0.5433],
          [0.4579, 0.4187, 0.6487,  ..., 0.4734, 0.4316, 0.3812],
          [0.8032, 0.4064, 0.5007,  ..., 0.4516, 0.5233, 0.4841]],

         [[0.5479, 0.4547, 0.3612,  ..., 0.4760, 0.3460, 0.6123],
          [0.5804, 0.5494, 0.6063,  ..., 0.4651, 0.3711, 0.5479],
          [0.4545, 0.5479, 0.5699,  ..., 0.3390, 0.5718, 0.6775],
          [0.5936, 0.3951, 0.4762,  ..., 0.5708, 0.5157, 0.4930]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0050,  0.0030, -0.0010,  0.0010,  0.0050, -0.0190,  0.0150, -0.0270,
         0.0310, -0.0250], device='cuda:0')
selected experts tensor([1455, 1602, 1581, 1622, 1570, 2160, 1894, 2080, 1053, 1367],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.5201, 0.4638, 0.6500,  ..., 0.3843, 0.5493, 0.4610],
          [0.5370, 0.5133, 0.4559,  ..., 0.5791, 0.5892, 0.3898],
          [0.4475, 0.4769, 0.4288,  ..., 0.5083, 0.5834, 0.5215],
          [0.6304, 0.5527, 0.4777,  ..., 0.5105, 0.5915, 0.6804]],

         [[0.5580, 0.5297, 0.6179,  ..., 0.4413, 0.4072, 0.4153],
          [0.5425, 0.6353, 0.4629,  ..., 0.5915, 0.5045, 0.4680],
          [0.5790, 0.5614, 0.4341,  ..., 0.4428, 0.7180, 0.3487],
          [0.6545, 0.6098, 0.4571,  ..., 0.3215, 0.5391, 0.6583]],

         [[0.6644, 0.7168, 0.6981,  ..., 0.4115, 0.3608, 0.5163],
          [0.6123, 0.5837, 0.4508,  ..., 0.5853, 0.5977, 0.3744],
          [0.4213, 0.4164, 0.3956,  ..., 0.4186, 0.6348, 0.5021],
          [0.5680, 0.6380, 0.3988,  ..., 0.6185, 0.6219, 0.3583]],

         ...,

         [[0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150]],

         [[0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150]],

         [[0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150]]],


        [[[0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150]],

         [[0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150]],

         [[0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150]],

         ...,

         [[0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150]],

         [[0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150]],

         [[0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150],
          [0.5130, 0.5110, 0.5130,  ..., 0.5150, 0.5050, 0.5150]]]],
       device='cuda:0')/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([[[[0.5071, 0.4528, 0.6370,  ..., 0.3693, 0.5443, 0.4460],
          [0.5240, 0.5023, 0.4429,  ..., 0.5641, 0.5842, 0.3748],
          [0.4345, 0.4659, 0.4158,  ..., 0.4933, 0.5784, 0.5065],
          [0.6174, 0.5417, 0.4647,  ..., 0.4955, 0.5865, 0.6654]],

         [[0.5450, 0.5187, 0.6049,  ..., 0.4263, 0.4022, 0.4003],
          [0.5295, 0.6243, 0.4499,  ..., 0.5765, 0.4995, 0.4530],
          [0.5660, 0.5504, 0.4211,  ..., 0.4278, 0.7130, 0.3337],
          [0.6415, 0.5988, 0.4441,  ..., 0.3065, 0.5341, 0.6433]],

         [[0.6514, 0.7058, 0.6851,  ..., 0.3965, 0.3558, 0.5013],
          [0.5993, 0.5727, 0.4378,  ..., 0.5703, 0.5927, 0.3594],
          [0.4083, 0.4054, 0.3826,  ..., 0.4036, 0.6298, 0.4871],
          [0.5550, 0.6270, 0.3858,  ..., 0.6035, 0.6169, 0.3433]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0130, 0.0110, 0.0130, 0.0130, 0.0150, 0.0070, 0.0090, 0.0150, 0.0050,
        0.0150], device='cuda:0')
selected experts tensor([ 597,  758,  499,  175, 3795,  880,  581, 3889,  779,  335],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5545, 0.5324, 0.4487,  ..., 0.6202, 0.5087, 0.4533],
          [0.5330, 0.4354, 0.4924,  ..., 0.6818, 0.5094, 0.4519],
          [0.3911, 0.4854, 0.4291,  ..., 0.4082, 0.4650, 0.3022],
          [0.4225, 0.3425, 0.3305,  ..., 0.4758, 0.5751, 0.5344]],

         [[0.4318, 0.5583, 0.4116,  ..., 0.4082, 0.5487, 0.5791],
          [0.5152, 0.5600, 0.4059,  ..., 0.6165, 0.5722, 0.5829],
          [0.3600, 0.5140, 0.6897,  ..., 0.4486, 0.4524, 0.4973],
          [0.4355, 0.3693, 0.5189,  ..., 0.5887, 0.5158, 0.5691]],

         [[0.5636, 0.5033, 0.6314,  ..., 0.5559, 0.5359, 0.5975],
          [0.3411, 0.5908, 0.5936,  ..., 0.5720, 0.4009, 0.6732],
          [0.5617, 0.5559, 0.4272,  ..., 0.6439, 0.5618, 0.5147],
          [0.4758, 0.4773, 0.4463,  ..., 0.4195, 0.5827, 0.5083]],

         ...,

         [[0.4707, 0.5832, 0.6044,  ..., 0.5868, 0.3889, 0.2964],
          [0.4850, 0.3900, 0.5012,  ..., 0.5641, 0.5770, 0.4393],
          [0.6250, 0.5530, 0.4842,  ..., 0.6183, 0.4103, 0.6012],
          [0.6394, 0.4807, 0.5499,  ..., 0.5356, 0.4325, 0.4232]],

         [[0.3794, 0.5913, 0.5931,  ..., 0.6809, 0.5059, 0.3938],
          [0.4752, 0.3748, 0.3409,  ..., 0.4782, 0.5060, 0.5881],
          [0.4584, 0.5969, 0.4490,  ..., 0.4965, 0.5315, 0.5388],
          [0.6411, 0.5789, 0.4434,  ..., 0.4648, 0.4630, 0.5014]],

         [[0.5966, 0.4811, 0.5912,  ..., 0.6609, 0.4861, 0.4470],
          [0.5221, 0.4656, 0.5983,  ..., 0.5773, 0.5625, 0.3022],
          [0.5323, 0.3984, 0.4243,  ..., 0.5418, 0.5751, 0.5514],
          [0.3393, 0.4450, 0.5409,  ..., 0.4610, 0.4589, 0.5658]]],


        [[[0.4775, 0.5112, 0.4909,  ..., 0.4614, 0.3912, 0.4929],
          [0.5755, 0.5472, 0.5484,  ..., 0.3636, 0.5176, 0.5970],
          [0.4153, 0.4381, 0.5146,  ..., 0.5250, 0.6046, 0.5559],
          [0.4405, 0.6114, 0.4310,  ..., 0.5014, 0.5784, 0.5984]],

         [[0.4316, 0.4796, 0.4319,  ..., 0.4433, 0.5572, 0.5135],
          [0.5466, 0.5002, 0.6082,  ..., 0.5172, 0.6900, 0.5344],
          [0.3724, 0.4720, 0.4960,  ..., 0.4218, 0.5432, 0.5006],
          [0.5487, 0.6307, 0.4064,  ..., 0.5588, 0.5970, 0.6799]],

         [[0.4723, 0.4345, 0.6513,  ..., 0.5156, 0.5575, 0.5970],
          [0.4129, 0.5518, 0.5120,  ..., 0.5009, 0.4949, 0.4463],
          [0.4743, 0.3868, 0.4073,  ..., 0.6038, 0.4623, 0.4692],
          [0.5545, 0.5983, 0.5146,  ..., 0.4518, 0.4776, 0.3896]],

         ...,

         [[0.4318, 0.5583, 0.4116,  ..., 0.4082, 0.5487, 0.5791],
          [0.5152, 0.5600, 0.4059,  ..., 0.6165, 0.5722, 0.5829],
          [0.3600, 0.5140, 0.6897,  ..., 0.4486, 0.4524, 0.4973],
          [0.4355, 0.3693, 0.5189,  ..., 0.5887, 0.5158, 0.5691]],

         [[0.3897, 0.5737, 0.5533,  ..., 0.6565, 0.4967, 0.4755],
          [0.5330, 0.5458, 0.5198,  ..., 0.4784, 0.4846, 0.5417],
          [0.4476, 0.5823, 0.3846,  ..., 0.4314, 0.4982, 0.4902],
          [0.6141, 0.4948, 0.5841,  ..., 0.4438, 0.6279, 0.4993]],

         [[0.4775, 0.5112, 0.4909,  ..., 0.4614, 0.3912, 0.4929],
          [0.5755, 0.5472, 0.5484,  ..., 0.3636, 0.5176, 0.5970],
          [0.4153, 0.4381, 0.5146,  ..., 0.5250, 0.6046, 0.5559],
          [0.4405, 0.6114, 0.4310,  ..., 0.5014, 0.5784, 0.5984]]]],
       device='cuda:0')
tensor([[[[0.5665, 0.5324, 0.4407,  ..., 0.6142, 0.4987, 0.4593],
          [0.5450, 0.4354, 0.4844,  ..., 0.6758, 0.4994, 0.4579],
          [0.4031, 0.4854, 0.4211,  ..., 0.4022, 0.4550, 0.3082],
          [0.4345, 0.3425, 0.3225,  ..., 0.4698, 0.5651, 0.5404]],

         [[0.4438, 0.5583, 0.4036,  ..., 0.4022, 0.5387, 0.5851],
          [0.5272, 0.5600, 0.3979,  ..., 0.6105, 0.5622, 0.5889],
          [0.3720, 0.5140, 0.6817,  ..., 0.4426, 0.4424, 0.5033],
          [0.4475, 0.3693, 0.5109,  ..., 0.5827, 0.5058, 0.5751]],

         [[0.5756, 0.5033, 0.6234,  ..., 0.5499, 0.5259, 0.6035],
          [0.3531, 0.5908, 0.5856,  ..., 0.5660, 0.3909, 0.6792],
          [0.5737, 0.5559, 0.4192,  ..., 0.6379, 0.5518, 0.5207],
          [0.4878, 0.4773, 0.4383,  ..., 0.4135, 0.5727, 0.5143]],

         ...,

         [[0.4827, 0.5832, 0.5964,  ..., 0.5808, 0.3789, 0.3024],
          [0.4970, 0.3900, 0.4932,  ..., 0.5581, 0.5670, 0.4453],
          [0.6370, 0.5530, 0.4762,  ..., 0.6123, 0.4003, 0.6072],
          [0.6514, 0.4807, 0.5419,  ..., 0.5296, 0.4225, 0.4292]],

         [[0.3914, 0.5913, 0.5851,  ..., 0.6749, 0.4959, 0.3998],
          [0.4872, 0.3748, 0.3329,  ..., 0.4722, 0.4960, 0.5941],
          [0.4704, 0.5969, 0.4410,  ..., 0.4905, 0.5215, 0.5448],
          [0.6531, 0.5789, 0.4354,  ..., 0.4588, 0.4530, 0.5074]],

         [[0.6086, 0.4811, 0.5832,  ..., 0.6549, 0.4761, 0.4530],
          [0.5341, 0.4656, 0.5903,  ..., 0.5713, 0.5525, 0.3082],
          [0.5443, 0.3984, 0.4163,  ..., 0.5358, 0.5651, 0.5574],
          [0.3513, 0.4450, 0.5329,  ..., 0.4550, 0.4489, 0.5718]]],


        [[[0.4895, 0.5112, 0.4829,  ..., 0.4554, 0.3812, 0.4989],
          [0.5875, 0.5472, 0.5404,  ..., 0.3576, 0.5076, 0.6030],
          [0.4273, 0.4381, 0.5066,  ..., 0.5190, 0.5946, 0.5619],
          [0.4525, 0.6114, 0.4230,  ..., 0.4954, 0.5684, 0.6044]],

         [[0.4436, 0.4796, 0.4239,  ..., 0.4373, 0.5472, 0.5195],
          [0.5586, 0.5002, 0.6002,  ..., 0.5112, 0.6800, 0.5404],
          [0.3844, 0.4720, 0.4880,  ..., 0.4158, 0.5332, 0.5066],
          [0.5607, 0.6307, 0.3984,  ..., 0.5528, 0.5870, 0.6859]],

         [[0.4843, 0.4345, 0.6433,  ..., 0.5096, 0.5475, 0.6030],
          [0.4249, 0.5518, 0.5040,  ..., 0.4949, 0.4849, 0.4523],
          [0.4863, 0.3868, 0.3993,  ..., 0.5978, 0.4523, 0.4752],
          [0.5665, 0.5983, 0.5066,  ..., 0.4458, 0.4676, 0.3956]],

         ...,

         [[0.4438, 0.5583, 0.4036,  ..., 0.4022, 0.5387, 0.5851],
          [0.5272, 0.5600, 0.3979,  ..., 0.6105, 0.5622, 0.5889],
          [0.3720, 0.5140, 0.6817,  ..., 0.4426, 0.4424, 0.5033],
          [0.4475, 0.3693, 0.5109,  ..., 0.5827, 0.5058, 0.5751]],

         [[0.4017, 0.5737, 0.5453,  ..., 0.6505, 0.4867, 0.4815],
          [0.5450, 0.5458, 0.5118,  ..., 0.4724, 0.4746, 0.5477],
          [0.4596, 0.5823, 0.3766,  ..., 0.4254, 0.4882, 0.4962],
          [0.6261, 0.4948, 0.5761,  ..., 0.4378, 0.6179, 0.5053]],

         [[0.4895, 0.5112, 0.4829,  ..., 0.4554, 0.3812, 0.4989],
          [0.5875, 0.5472, 0.5404,  ..., 0.3576, 0.5076, 0.6030],
          [0.4273, 0.4381, 0.5066,  ..., 0.5190, 0.5946, 0.5619],
          [0.4525, 0.6114, 0.4230,  ..., 0.4954, 0.5684, 0.6044]]]],
       device='cuda:0', requires_grad=True)
tensor([-1.2000e-02,  2.3283e-10,  8.0000e-03,  2.0000e-03,  8.0000e-03,
         6.0000e-03, -4.0000e-03,  6.0000e-03,  1.0000e-02, -6.0000e-03],
       device='cuda:0')
selected experts tensor([1534, 1713, 1744, 1512, 1733, 1706, 1626, 1566, 1683, 1567],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4602, 0.4854, 0.7118,  ..., 0.6023, 0.6534, 0.5138],
          [0.4498, 0.5453, 0.4975,  ..., 0.5393, 0.4553, 0.5005],
          [0.5853, 0.5068, 0.5333,  ..., 0.3921, 0.6078, 0.5500],
          [0.4351, 0.6838, 0.4472,  ..., 0.5938, 0.4255, 0.2579]],

         [[0.6874, 0.4735, 0.6653,  ..., 0.5374, 0.5550, 0.5524],
          [0.4038, 0.5018, 0.4007,  ..., 0.4607, 0.5208, 0.3770],
          [0.3526, 0.3895, 0.4909,  ..., 0.6051, 0.4451, 0.4399],
          [0.4284, 0.4655, 0.5067,  ..., 0.6258, 0.4691, 0.6840]],

         [[0.5341, 0.4759, 0.4185,  ..., 0.3855, 0.5061, 0.2799],
          [0.6329, 0.4854, 0.4823,  ..., 0.5882, 0.6309, 0.4399],
          [0.4399, 0.5038, 0.4276,  ..., 0.5083, 0.4483, 0.3562],
          [0.5141, 0.4791, 0.5297,  ..., 0.5323, 0.3704, 0.5896]],

         ...,

         [[0.4265, 0.4918, 0.4704,  ..., 0.6018, 0.4947, 0.6000],
          [0.5019, 0.3631, 0.6330,  ..., 0.2649, 0.6254, 0.3105],
          [0.4194, 0.3932, 0.4680,  ..., 0.5435, 0.5719, 0.3072],
          [0.4862, 0.6344, 0.5136,  ..., 0.4109, 0.6134, 0.6205]],

         [[0.5302, 0.6209, 0.4767,  ..., 0.6928, 0.4691, 0.5410],
          [0.6177, 0.3389, 0.4702,  ..., 0.2951, 0.5203, 0.3634],
          [0.5410, 0.5387, 0.4731,  ..., 0.5257, 0.4858, 0.3982],
          [0.4127, 0.4721, 0.4607,  ..., 0.5825, 0.4379, 0.5406]],

         [[0.4478, 0.4445, 0.5058,  ..., 0.5301, 0.4940, 0.6310],
          [0.3945, 0.4494, 0.5314,  ..., 0.3177, 0.6083, 0.4544],
          [0.4532, 0.4689, 0.5525,  ..., 0.4507, 0.5625, 0.4964],
          [0.3751, 0.5180, 0.5266,  ..., 0.4543, 0.3846, 0.5505]]],


        [[[0.5630, 0.6805, 0.4152,  ..., 0.5835, 0.5005, 0.5157],
          [0.4875, 0.6371, 0.3849,  ..., 0.6249, 0.5480, 0.5490],
          [0.5026, 0.3496, 0.6197,  ..., 0.4818, 0.6290, 0.3197],
          [0.4365, 0.2927, 0.4893,  ..., 0.4851, 0.5373, 0.5640]],

         [[0.6320, 0.4760, 0.3826,  ..., 0.6482, 0.5128, 0.4246],
          [0.4597, 0.4887, 0.3932,  ..., 0.6675, 0.4872, 0.3670],
          [0.6410, 0.5222, 0.5392,  ..., 0.5173, 0.3985, 0.4099],
          [0.4227, 0.4700, 0.5097,  ..., 0.6143, 0.5630, 0.4495]],

         [[0.5943, 0.4717, 0.5259,  ..., 0.4071, 0.4709, 0.4619],
          [0.5844, 0.5426, 0.5811,  ..., 0.5415, 0.3596, 0.4284],
          [0.6004, 0.4116, 0.4252,  ..., 0.5476, 0.4971, 0.4847],
          [0.3751, 0.4957, 0.5211,  ..., 0.5920, 0.4060, 0.3991]],

         ...,

         [[0.6874, 0.4735, 0.6653,  ..., 0.5374, 0.5550, 0.5524],
          [0.4038, 0.5018, 0.4007,  ..., 0.4607, 0.5208, 0.3770],
          [0.3526, 0.3895, 0.4909,  ..., 0.6051, 0.4451, 0.4399],
          [0.4284, 0.4655, 0.5067,  ..., 0.6258, 0.4691, 0.6840]],

         [[0.5067, 0.5256, 0.5111,  ..., 0.3786, 0.5390, 0.5820],
          [0.4404, 0.5152, 0.5968,  ..., 0.3996, 0.4974, 0.5075],
          [0.4752, 0.5619, 0.4318,  ..., 0.3818, 0.4633, 0.4498],
          [0.3829, 0.6057, 0.5682,  ..., 0.5478, 0.4921, 0.4679]],

         [[0.5630, 0.6805, 0.4152,  ..., 0.5835, 0.5005, 0.5157],
          [0.4875, 0.6371, 0.3849,  ..., 0.6249, 0.5480, 0.5490],
          [0.5026, 0.3496, 0.6197,  ..., 0.4818, 0.6290, 0.3197],
          [0.4365, 0.2927, 0.4893,  ..., 0.4851, 0.5373, 0.5640]]]],
       device='cuda:0')
tensor([[[[0.4562, 0.4934, 0.7058,  ..., 0.6063, 0.6514, 0.5098],
          [0.4458, 0.5533, 0.4915,  ..., 0.5433, 0.4533, 0.4965],
          [0.5813, 0.5148, 0.5273,  ..., 0.3961, 0.6058, 0.5460],
          [0.4311, 0.6918, 0.4412,  ..., 0.5978, 0.4235, 0.2539]],

         [[0.6834, 0.4815, 0.6593,  ..., 0.5414, 0.5530, 0.5484],
          [0.3998, 0.5098, 0.3947,  ..., 0.4647, 0.5188, 0.3730],
          [0.3486, 0.3975, 0.4849,  ..., 0.6091, 0.4431, 0.4359],
          [0.4244, 0.4735, 0.5007,  ..., 0.6298, 0.4671, 0.6800]],

         [[0.5301, 0.4839, 0.4125,  ..., 0.3895, 0.5041, 0.2759],
          [0.6289, 0.4934, 0.4763,  ..., 0.5922, 0.6289, 0.4359],
          [0.4359, 0.5118, 0.4216,  ..., 0.5123, 0.4463, 0.3522],
          [0.5101, 0.4871, 0.5237,  ..., 0.5363, 0.3684, 0.5856]],

         ...,

         [[0.4225, 0.4998, 0.4644,  ..., 0.6058, 0.4927, 0.5960],
          [0.4979, 0.3711, 0.6270,  ..., 0.2689, 0.6234, 0.3065],
          [0.4154, 0.4012, 0.4620,  ..., 0.5475, 0.5699, 0.3032],
          [0.4822, 0.6424, 0.5076,  ..., 0.4149, 0.6114, 0.6165]],

         [[0.5262, 0.6289, 0.4707,  ..., 0.6968, 0.4671, 0.5370],
          [0.6137, 0.3469, 0.4642,  ..., 0.2991, 0.5183, 0.3594],
          [0.5370, 0.5467, 0.4671,  ..., 0.5297, 0.4838, 0.3942],
          [0.4087, 0.4801, 0.4547,  ..., 0.5865, 0.4359, 0.5366]],

         [[0.4438, 0.4525, 0.4998,  ..., 0.5341, 0.4920, 0.6270],
          [0.3905, 0.4574, 0.5254,  ..., 0.3217, 0.6063, 0.4504],
          [0.4492, 0.4769, 0.5465,  ..., 0.4547, 0.5605, 0.4924],
          [0.3711, 0.5260, 0.5206,  ..., 0.4583, 0.3826, 0.5465]]],


        [[[0.5590, 0.6885, 0.4092,  ..., 0.5875, 0.4985, 0.5117],
          [0.4835, 0.6451, 0.3789,  ..., 0.6289, 0.5460, 0.5450],
          [0.4986, 0.3576, 0.6137,  ..., 0.4858, 0.6270, 0.3157],
          [0.4325, 0.3007, 0.4833,  ..., 0.4891, 0.5353, 0.5600]],

         [[0.6280, 0.4840, 0.3766,  ..., 0.6522, 0.5108, 0.4206],
          [0.4557, 0.4967, 0.3872,  ..., 0.6715, 0.4852, 0.3630],
          [0.6370, 0.5302, 0.5332,  ..., 0.5213, 0.3965, 0.4059],
          [0.4187, 0.4780, 0.5037,  ..., 0.6183, 0.5610, 0.4455]],

         [[0.5903, 0.4797, 0.5199,  ..., 0.4111, 0.4689, 0.4579],
          [0.5804, 0.5506, 0.5751,  ..., 0.5455, 0.3576, 0.4244],
          [0.5964, 0.4196, 0.4192,  ..., 0.5516, 0.4951, 0.4807],
          [0.3711, 0.5037, 0.5151,  ..., 0.5960, 0.4040, 0.3951]],

         ...,

         [[0.6834, 0.4815, 0.6593,  ..., 0.5414, 0.5530, 0.5484],
          [0.3998, 0.5098, 0.3947,  ..., 0.4647, 0.5188, 0.3730],
          [0.3486, 0.3975, 0.4849,  ..., 0.6091, 0.4431, 0.4359],
          [0.4244, 0.4735, 0.5007,  ..., 0.6298, 0.4671, 0.6800]],

         [[0.5027, 0.5336, 0.5051,  ..., 0.3826, 0.5370, 0.5780],
          [0.4364, 0.5232, 0.5908,  ..., 0.4036, 0.4954, 0.5035],
          [0.4712, 0.5699, 0.4258,  ..., 0.3858, 0.4613, 0.4458],
          [0.3789, 0.6137, 0.5622,  ..., 0.5518, 0.4901, 0.4639]],

         [[0.5590, 0.6885, 0.4092,  ..., 0.5875, 0.4985, 0.5117],
          [0.4835, 0.6451, 0.3789,  ..., 0.6289, 0.5460, 0.5450],
          [0.4986, 0.3576, 0.6137,  ..., 0.4858, 0.6270, 0.3157],
          [0.4325, 0.3007, 0.4833,  ..., 0.4891, 0.5353, 0.5600]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0040, -0.0080,  0.0060, -0.0020,  0.0020,  0.0020, -0.0060, -0.0040,
         0.0020,  0.0040], device='cuda:0')
selected experts tensor([1631, 1635, 1819, 1663, 1538, 1612, 1599, 1609, 1695, 1583],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4886, 0.4687, 0.3951,  ..., 0.4932, 0.5156, 0.4856],
          [0.5174, 0.5423, 0.6893,  ..., 0.3541, 0.4464, 0.5232],
          [0.4651, 0.6000, 0.5564,  ..., 0.5031, 0.6289, 0.5602],
          [0.4753, 0.4879, 0.4872,  ..., 0.4805, 0.4809, 0.4769]],

         [[0.4900, 0.5633, 0.5283,  ..., 0.5871, 0.4095, 0.6518],
          [0.4624, 0.5934, 0.5344,  ..., 0.3723, 0.3762, 0.4898],
          [0.4861, 0.5486, 0.5799,  ..., 0.3350, 0.5446, 0.5160],
          [0.4479, 0.5221, 0.5077,  ..., 0.5717, 0.6062, 0.4723]],

         [[0.6961, 0.4739, 0.4453,  ..., 0.4635, 0.3640, 0.4503],
          [0.4470, 0.5449, 0.6252,  ..., 0.2590, 0.3718, 0.4336],
          [0.4271, 0.4984, 0.5329,  ..., 0.4195, 0.5418, 0.6085],
          [0.5387, 0.4265, 0.3775,  ..., 0.5209, 0.6014, 0.5554]],

         ...,

         [[0.6132, 0.5403, 0.6215,  ..., 0.4045, 0.2964, 0.5738],
          [0.5360, 0.4696, 0.5699,  ..., 0.4436, 0.3745, 0.5616],
          [0.4366, 0.4862, 0.4771,  ..., 0.3569, 0.6042, 0.6353],
          [0.4868, 0.3395, 0.5088,  ..., 0.5769, 0.4758, 0.4582]],

         [[0.5130, 0.4165, 0.4944,  ..., 0.4657, 0.4464, 0.5911],
          [0.5396, 0.5986, 0.6128,  ..., 0.5003, 0.4318, 0.6220],
          [0.4559, 0.5157, 0.5327,  ..., 0.4026, 0.5995, 0.5406],
          [0.5481, 0.3788, 0.4886,  ..., 0.5289, 0.4436, 0.5011]],

         [[0.5691, 0.3240, 0.5997,  ..., 0.4896, 0.4299, 0.5813],
          [0.6081, 0.4820, 0.4763,  ..., 0.5270, 0.4574, 0.4266],
          [0.4888, 0.4370, 0.5837,  ..., 0.5533, 0.4118, 0.4392],
          [0.5498, 0.4085, 0.6388,  ..., 0.5727, 0.5246, 0.5511]]],


        [[[0.5289, 0.4493, 0.4518,  ..., 0.5466, 0.3360, 0.5089],
          [0.5365, 0.4227, 0.5780,  ..., 0.4446, 0.4742, 0.5060],
          [0.3458, 0.5214, 0.5465,  ..., 0.5141, 0.4641, 0.5179],
          [0.6109, 0.4488, 0.3495,  ..., 0.5466, 0.5160, 0.5261]],

         [[0.4900, 0.5107, 0.5032,  ..., 0.6295, 0.3477, 0.5346],
          [0.5873, 0.3553, 0.5455,  ..., 0.2920, 0.5427, 0.6686],
          [0.3690, 0.6061, 0.4980,  ..., 0.4442, 0.4225, 0.4975],
          [0.5056, 0.4156, 0.4321,  ..., 0.5466, 0.5285, 0.5223]],

         [[0.5883, 0.4500, 0.5363,  ..., 0.5694, 0.2994, 0.4729],
          [0.6043, 0.3894, 0.4904,  ..., 0.3468, 0.5175, 0.6327],
          [0.3432, 0.6255, 0.4980,  ..., 0.4642, 0.4370, 0.5370],
          [0.4670, 0.4203, 0.4149,  ..., 0.5361, 0.5635, 0.5160]],

         ...,

         [[0.4660, 0.4870, 0.5978,  ..., 0.6225, 0.4192, 0.4081],
          [0.5768, 0.4561, 0.5482,  ..., 0.3005, 0.4991, 0.5138],
          [0.3717, 0.5834, 0.5559,  ..., 0.3341, 0.5596, 0.4645],
          [0.3965, 0.4337, 0.4694,  ..., 0.6680, 0.5343, 0.5341]],

         [[0.4888, 0.5372, 0.4244,  ..., 0.5838, 0.4417, 0.5045],
          [0.4489, 0.5239, 0.6044,  ..., 0.4161, 0.3631, 0.5568],
          [0.5452, 0.5710, 0.6109,  ..., 0.6144, 0.4441, 0.4685],
          [0.5655, 0.4062, 0.3928,  ..., 0.5889, 0.5349, 0.5506]],

         [[0.5725, 0.3670, 0.3657,  ..., 0.4845, 0.4650, 0.4859],
          [0.5081, 0.5218, 0.6202,  ..., 0.4376, 0.3319, 0.5379],
          [0.4716, 0.5119, 0.5605,  ..., 0.5399, 0.6457, 0.5206],
          [0.5396, 0.5034, 0.4824,  ..., 0.6486, 0.4795, 0.4861]]]],
       device='cuda:0')
tensor([[[[0.4826, 0.4647, 0.3951,  ..., 0.5212, 0.4836, 0.5096],
          [0.5114, 0.5383, 0.6893,  ..., 0.3821, 0.4144, 0.5472],
          [0.4591, 0.5960, 0.5564,  ..., 0.5311, 0.5969, 0.5842],
          [0.4693, 0.4839, 0.4872,  ..., 0.5085, 0.4489, 0.5009]],

         [[0.4840, 0.5593, 0.5283,  ..., 0.6151, 0.3775, 0.6758],
          [0.4564, 0.5894, 0.5344,  ..., 0.4003, 0.3442, 0.5138],
          [0.4801, 0.5446, 0.5799,  ..., 0.3630, 0.5126, 0.5400],
          [0.4419, 0.5181, 0.5077,  ..., 0.5997, 0.5742, 0.4963]],

         [[0.6901, 0.4699, 0.4453,  ..., 0.4915, 0.3320, 0.4743],
          [0.4410, 0.5409, 0.6252,  ..., 0.2870, 0.3398, 0.4576],
          [0.4211, 0.4944, 0.5329,  ..., 0.4475, 0.5098, 0.6325],
          [0.5327, 0.4225, 0.3775,  ..., 0.5489, 0.5694, 0.5794]],

         ...,

         [[0.6072, 0.5363, 0.6215,  ..., 0.4325, 0.2644, 0.5978],
          [0.5300, 0.4656, 0.5699,  ..., 0.4716, 0.3425, 0.5856],
          [0.4306, 0.4822, 0.4771,  ..., 0.3849, 0.5722, 0.6593],
          [0.4808, 0.3355, 0.5088,  ..., 0.6049, 0.4438, 0.4822]],

         [[0.5070, 0.4125, 0.4944,  ..., 0.4937, 0.4144, 0.6151],
          [0.5336, 0.5946, 0.6128,  ..., 0.5283, 0.3998, 0.6460],
          [0.4499, 0.5117, 0.5327,  ..., 0.4306, 0.5675, 0.5646],
          [0.5421, 0.3748, 0.4886,  ..., 0.5569, 0.4116, 0.5251]],

         [[0.5631, 0.3200, 0.5997,  ..., 0.5176, 0.3979, 0.6053],
          [0.6021, 0.4780, 0.4763,  ..., 0.5550, 0.4254, 0.4506],
          [0.4828, 0.4330, 0.5837,  ..., 0.5813, 0.3798, 0.4632],
          [0.5438, 0.4045, 0.6388,  ..., 0.6007, 0.4926, 0.5751]]],


        [[[0.5229, 0.4453, 0.4518,  ..., 0.5746, 0.3040, 0.5329],
          [0.5305, 0.4187, 0.5780,  ..., 0.4726, 0.4422, 0.5300],
          [0.3398, 0.5174, 0.5465,  ..., 0.5421, 0.4321, 0.5419],
          [0.6049, 0.4448, 0.3495,  ..., 0.5746, 0.4840, 0.5501]],

         [[0.4840, 0.5067, 0.5032,  ..., 0.6575, 0.3157, 0.5586],
          [0.5813, 0.3513, 0.5455,  ..., 0.3200, 0.5107, 0.6926],
          [0.3630, 0.6021, 0.4980,  ..., 0.4722, 0.3905, 0.5215],
          [0.4996, 0.4116, 0.4321,  ..., 0.5746, 0.4965, 0.5463]],

         [[0.5823, 0.4460, 0.5363,  ..., 0.5974, 0.2674, 0.4969],
          [0.5983, 0.3854, 0.4904,  ..., 0.3748, 0.4855, 0.6567],
          [0.3372, 0.6215, 0.4980,  ..., 0.4922, 0.4050, 0.5610],
          [0.4610, 0.4163, 0.4149,  ..., 0.5641, 0.5315, 0.5400]],

         ...,

         [[0.4600, 0.4830, 0.5978,  ..., 0.6505, 0.3872, 0.4321],
          [0.5708, 0.4521, 0.5482,  ..., 0.3285, 0.4671, 0.5378],
          [0.3657, 0.5794, 0.5559,  ..., 0.3621, 0.5276, 0.4885],
          [0.3905, 0.4297, 0.4694,  ..., 0.6960, 0.5023, 0.5581]],

         [[0.4828, 0.5332, 0.4244,  ..., 0.6118, 0.4097, 0.5285],
          [0.4429, 0.5199, 0.6044,  ..., 0.4441, 0.3311, 0.5808],
          [0.5392, 0.5670, 0.6109,  ..., 0.6424, 0.4121, 0.4925],
          [0.5595, 0.4022, 0.3928,  ..., 0.6169, 0.5029, 0.5746]],

         [[0.5665, 0.3630, 0.3657,  ..., 0.5125, 0.4330, 0.5099],
          [0.5021, 0.5178, 0.6202,  ..., 0.4656, 0.2999, 0.5619],
          [0.4656, 0.5079, 0.5605,  ..., 0.5679, 0.6137, 0.5446],
          [0.5336, 0.4994, 0.4824,  ..., 0.6766, 0.4475, 0.5101]]]],
       device='cuda:0', requires_grad=True)
tensor([ 6.0000e-03,  4.0000e-03,  6.9849e-10,  2.0000e-03,  6.0000e-03,
        -2.0000e-02,  1.4000e-02, -2.8000e-02,  3.2000e-02, -2.4000e-02],
       device='cuda:0')
selected experts tensor([1530, 1232, 1945, 1628, 1670, 2315, 1616, 1654, 1400, 1394],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.5704, 0.5344, 0.6328,  ..., 0.4976, 0.5619, 0.4065],
          [0.5759, 0.4393, 0.3164,  ..., 0.5600, 0.5438, 0.4940],
          [0.5219, 0.5876, 0.5939,  ..., 0.4082, 0.5571, 0.4654],
          [0.5694, 0.5672, 0.5396,  ..., 0.5481, 0.4663, 0.5133]],

         [[0.5486, 0.5928, 0.6207,  ..., 0.4218, 0.6188, 0.4688],
          [0.6076, 0.4388, 0.4313,  ..., 0.5882, 0.5159, 0.5755],
          [0.3842, 0.5388, 0.3716,  ..., 0.4968, 0.4114, 0.5144],
          [0.5001, 0.5476, 0.4850,  ..., 0.5743, 0.5234, 0.5305]],

         [[0.6128, 0.5483, 0.5003,  ..., 0.4958, 0.5199, 0.5149],
          [0.6240, 0.4307, 0.4576,  ..., 0.5612, 0.5715, 0.4649],
          [0.4185, 0.5614, 0.4157,  ..., 0.4559, 0.5549, 0.3853],
          [0.5530, 0.4561, 0.4821,  ..., 0.5186, 0.4290, 0.5608]],

         ...,

         [[0.5140, 0.5120, 0.5140,  ..., 0.5140, 0.5060, 0.5160],
          [0.5140, 0.5120, 0.5140,  ..., 0.5140, 0.5060, 0.5160],
          [0.5140, 0.5120, 0.5140,  ..., 0.5140, 0.5060, 0.5160],
          [0.5140, 0.5120, 0.5140,  ..., 0.5140, 0.5060, 0.5160]],

         [[0.5140, 0.5120, 0.5140,  ..., 0.5140, 0.5060, 0.5160],
          [0.5140, 0.5120, 0.5140,  ..., 0.5140, 0.5060, 0.5160],
          [0.5140, 0.5120, 0.5140,  ..., 0.5140, 0.5060, 0.5160],
          [0.5140, 0.5120, 0.5140,  ..., 0.5140, 0.5060, 0.5160]],

         [[0.5140, 0.5120, 0.5140,  ..., 0.5140, 0.5060, 0.5160],
          [0.5140, 0.5120, 0.5140,  ..., 0.5140, 0.5060, 0.5160],
          [0.5140, 0.5120, 0.5140,  ..., 0.5140, 0.5060, 0.5160],
          [0.5140, 0.5120, 0.5140,  ..., 0.5140, 0.5060, 0.5160]]],


        [[[0.6067, 0.5224, 0.4816,  ..., 0.4562, 0.5486, 0.5461],
          [0.5692, 0.4718, 0.4697,  ..., 0.5469, 0.5503, 0.4777],
          [0.3920, 0.4988, 0.4687,  ..., 0.4576, 0.5203, 0.4037],
          [0.6258, 0.4383, 0.5336,  ..., 0.5523, 0.4641, 0.5470]],

         [[0.4972, 0.4915, 0.6474,  ..., 0.4451, 0.6165, 0.4681],
          [0.5896, 0.5323, 0.3230,  ..., 0.5843, 0.6448, 0.4998],
          [0.4096, 0.4863, 0.5412,  ..., 0.5226, 0.4761, 0.5496],
          [0.5283, 0.6047, 0.5948,  ..., 0.5420, 0.6118, 0.5528]],

         [[0.5815, 0.4984, 0.7083,  ..., 0.5288, 0.5367, 0.4153],
          [0.6019, 0.4708, 0.2930,  ..., 0.5610, 0.5622, 0.6171],
          [0.2907, 0.5360, 0.4610,  ..., 0.4956, 0.5353, 0.5705],
          [0.4590, 0.5114, 0.5196,  ..., 0.5388, 0.5474, 0.4615]],

         ...,

         [[0.5140, 0.5120, 0.5140,  ..., 0.5140, 0.5060, 0.5160],
          [0.5140, 0.5120, 0.5140,  ..., 0.5140, 0.5060, 0.5160],
          [0.5140, 0.5120, 0.5140,  ..., 0.5140, 0.5060, 0.5160],
          [0.5140, 0.5120, 0.5140,  ..., 0.5140, 0.5060, 0.5160]],

         [[0.5140, 0.5120, 0.5140,  ..., 0.5140, 0.5060, 0.5160],
          [0.5140, 0.5120, 0.5140,  ..., 0.5140, 0.5060, 0.5160],
          [0.5140, 0.5120, 0.5140,  ..., 0.5140, 0.5060, 0.5160],
          [0.5140, 0.5120, 0.5140,  ..., 0.5140, 0.5060, 0.5160]],

         [[0.5140, 0.5120, 0.5140,  ..., 0.5140, 0.5060, 0.5160],
          [0.5140, 0.5120, 0.5140,  ..., 0.5140, 0.5060, 0.5160],
          [0.5140, 0.5120, 0.5140,  ..., 0.5140, 0.5060, 0.5160],
          [0.5140, 0.5120, 0.5140,  ..., 0.5140, 0.5060, 0.5160]]]],
       device='cuda:0')
tensor([[[[0.5564, 0.5224, 0.6188,  ..., 0.4836, 0.5559, 0.3905],
          [0.5619, 0.4273, 0.3024,  ..., 0.5460, 0.5378, 0.4780],
          [0.5079, 0.5756, 0.5799,  ..., 0.3942, 0.5511, 0.4494],
          [0.5554, 0.5552, 0.5256,  ..., 0.5341, 0.4603, 0.4973]],

         [[0.5346, 0.5808, 0.6067,  ..., 0.4078, 0.6128, 0.4528],
          [0.5936, 0.4268, 0.4173,  ..., 0.5742, 0.5099, 0.5595],
          [0.3702, 0.5268, 0.3576,  ..., 0.4828, 0.4054, 0.4984],
          [0.4861, 0.5356, 0.4710,  ..., 0.5603, 0.5174, 0.5145]],

         [[0.5988, 0.5363, 0.4863,  ..., 0.4818, 0.5139, 0.4989],
          [0.6100, 0.4187, 0.4436,  ..., 0.5472, 0.5655, 0.4489],
          [0.4045, 0.5494, 0.4017,  ..., 0.4419, 0.5489, 0.3693],
          [0.5390, 0.4441, 0.4681,  ..., 0.5046, 0.4230, 0.5448]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5927, 0.5104, 0.4676,  ..., 0.4422, 0.5426, 0.5301],
          [0.5552, 0.4598, 0.4557,  ..., 0.5329, 0.5443, 0.4617],
          [0.3780, 0.4868, 0.4547,  ..., 0.4436, 0.5143, 0.3877],
          [0.6118, 0.4263, 0.5196,  ..., 0.5383, 0.4581, 0.5310]],

         [[0.4832, 0.4795, 0.6334,  ..., 0.4311, 0.6105, 0.4521],
          [0.5756, 0.5203, 0.3090,  ..., 0.5703, 0.6388, 0.4838],
          [0.3956, 0.4743, 0.5272,  ..., 0.5086, 0.4701, 0.5336],
          [0.5143, 0.5927, 0.5808,  ..., 0.5280, 0.6058, 0.5368]],

         [[0.5675, 0.4864, 0.6943,  ..., 0.5148, 0.5307, 0.3993],
          [0.5879, 0.4588, 0.2790,  ..., 0.5470, 0.5562, 0.6011],
          [0.2767, 0.5240, 0.4470,  ..., 0.4816, 0.5293, 0.5545],
          [0.4450, 0.4994, 0.5056,  ..., 0.5248, 0.5414, 0.4455]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([0.0140, 0.0120, 0.0140, 0.0140, 0.0140, 0.0080, 0.0100, 0.0140, 0.0060,
        0.0160], device='cuda:0')
selected experts tensor([1643, 1597, 1098,  487,  714, 1709, 1650,  979, 1441,  970],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4805, 0.5110, 0.4896,  ..., 0.4624, 0.3925, 0.4960],
          [0.5765, 0.5465, 0.5474,  ..., 0.3646, 0.5167, 0.5980],
          [0.4163, 0.4371, 0.5138,  ..., 0.5263, 0.6036, 0.5569],
          [0.4415, 0.6104, 0.4300,  ..., 0.5027, 0.5774, 0.5994]],

         [[0.5817, 0.4844, 0.4228,  ..., 0.4233, 0.5917, 0.6156],
          [0.4611, 0.6816, 0.4766,  ..., 0.5237, 0.5492, 0.4837],
          [0.5906, 0.4786, 0.6422,  ..., 0.6512, 0.4168, 0.5323],
          [0.6018, 0.6095, 0.4073,  ..., 0.5016, 0.4965, 0.4661]],

         [[0.5452, 0.5130, 0.4247,  ..., 0.3827, 0.4485, 0.4104],
          [0.3457, 0.4359, 0.4295,  ..., 0.4586, 0.3410, 0.4920],
          [0.5698, 0.3105, 0.4804,  ..., 0.6476, 0.4130, 0.5110],
          [0.4619, 0.5508, 0.6142,  ..., 0.5027, 0.5603, 0.5183]],

         ...,

         [[0.5372, 0.4234, 0.4017,  ..., 0.6198, 0.4449, 0.5900],
          [0.4583, 0.5770, 0.4371,  ..., 0.5566, 0.4676, 0.4965],
          [0.4841, 0.5450, 0.3763,  ..., 0.6067, 0.6397, 0.5810],
          [0.4956, 0.4344, 0.4641,  ..., 0.3646, 0.5836, 0.6560]],

         [[0.4805, 0.5110, 0.4896,  ..., 0.4624, 0.3925, 0.4960],
          [0.5765, 0.5465, 0.5474,  ..., 0.3646, 0.5167, 0.5980],
          [0.4163, 0.4371, 0.5138,  ..., 0.5263, 0.6036, 0.5569],
          [0.4415, 0.6104, 0.4300,  ..., 0.5027, 0.5774, 0.5994]],

         [[0.6766, 0.3293, 0.3800,  ..., 0.5735, 0.4192, 0.4266],
          [0.6395, 0.5275, 0.4670,  ..., 0.4896, 0.4013, 0.5701],
          [0.4863, 0.4966, 0.4228,  ..., 0.5593, 0.4325, 0.5045],
          [0.7068, 0.4583, 0.5253,  ..., 0.4139, 0.3967, 0.3227]]],


        [[[0.5108, 0.5684, 0.6175,  ..., 0.6253, 0.4041, 0.4922],
          [0.4872, 0.4859, 0.5394,  ..., 0.4668, 0.4391, 0.3689],
          [0.5384, 0.3557, 0.6091,  ..., 0.7200, 0.5855, 0.4372],
          [0.3846, 0.4220, 0.5406,  ..., 0.5389, 0.4790, 0.6329]],

         [[0.6724, 0.6807, 0.5455,  ..., 0.5361, 0.4659, 0.5524],
          [0.5636, 0.4696, 0.4653,  ..., 0.3984, 0.4502, 0.5844],
          [0.4063, 0.6414, 0.4205,  ..., 0.5291, 0.5932, 0.3869],
          [0.5509, 0.7554, 0.5978,  ..., 0.6350, 0.3675, 0.4434]],

         [[0.6188, 0.5679, 0.6119,  ..., 0.5144, 0.5151, 0.4218],
          [0.3610, 0.4414, 0.4649,  ..., 0.5094, 0.4135, 0.4427],
          [0.6160, 0.6001, 0.4876,  ..., 0.4286, 0.6370, 0.4587],
          [0.5270, 0.3960, 0.5419,  ..., 0.5576, 0.4771, 0.4468]],

         ...,

         [[0.3982, 0.4789, 0.6020,  ..., 0.3800, 0.4320, 0.5867],
          [0.4943, 0.2664, 0.4172,  ..., 0.5294, 0.6657, 0.5439],
          [0.5299, 0.5789, 0.4096,  ..., 0.4073, 0.5284, 0.3428],
          [0.5280, 0.5093, 0.4475,  ..., 0.4026, 0.5041, 0.3571]],

         [[0.5798, 0.4416, 0.4607,  ..., 0.4966, 0.4027, 0.5763],
          [0.4731, 0.5784, 0.6449,  ..., 0.4707, 0.3453, 0.5335],
          [0.5868, 0.4963, 0.4780,  ..., 0.4646, 0.5591, 0.5429],
          [0.2618, 0.6565, 0.4758,  ..., 0.5651, 0.5109, 0.4655]],

         [[0.4427, 0.4955, 0.3574,  ..., 0.5006, 0.5798, 0.5933],
          [0.4249, 0.5161, 0.5496,  ..., 0.4753, 0.4845, 0.5739],
          [0.4459, 0.4229, 0.5964,  ..., 0.4649, 0.4182, 0.5318],
          [0.4326, 0.2908, 0.5624,  ..., 0.5968, 0.4995, 0.4323]]]],
       device='cuda:0')
tensor([[[[0.4915, 0.5120, 0.4826,  ..., 0.4554, 0.3835, 0.5010],
          [0.5875, 0.5475, 0.5404,  ..., 0.3576, 0.5077, 0.6030],
          [0.4273, 0.4381, 0.5068,  ..., 0.5193, 0.5946, 0.5619],
          [0.4525, 0.6114, 0.4230,  ..., 0.4957, 0.5684, 0.6044]],

         [[0.5927, 0.4854, 0.4158,  ..., 0.4163, 0.5827, 0.6206],
          [0.4721, 0.6826, 0.4696,  ..., 0.5167, 0.5402, 0.4887],
          [0.6016, 0.4796, 0.6352,  ..., 0.6442, 0.4078, 0.5373],
          [0.6128, 0.6105, 0.4003,  ..., 0.4946, 0.4875, 0.4711]],

         [[0.5562, 0.5140, 0.4177,  ..., 0.3757, 0.4395, 0.4154],
          [0.3567, 0.4369, 0.4225,  ..., 0.4516, 0.3320, 0.4970],
          [0.5808, 0.3115, 0.4734,  ..., 0.6406, 0.4040, 0.5160],
          [0.4729, 0.5518, 0.6072,  ..., 0.4957, 0.5513, 0.5233]],

         ...,

         [[0.5482, 0.4244, 0.3947,  ..., 0.6128, 0.4359, 0.5950],
          [0.4693, 0.5780, 0.4301,  ..., 0.5496, 0.4586, 0.5015],
          [0.4951, 0.5460, 0.3693,  ..., 0.5997, 0.6307, 0.5860],
          [0.5066, 0.4354, 0.4571,  ..., 0.3576, 0.5746, 0.6610]],

         [[0.4915, 0.5120, 0.4826,  ..., 0.4554, 0.3835, 0.5010],
          [0.5875, 0.5475, 0.5404,  ..., 0.3576, 0.5077, 0.6030],
          [0.4273, 0.4381, 0.5068,  ..., 0.5193, 0.5946, 0.5619],
          [0.4525, 0.6114, 0.4230,  ..., 0.4957, 0.5684, 0.6044]],

         [[0.6876, 0.3303, 0.3730,  ..., 0.5665, 0.4102, 0.4316],
          [0.6505, 0.5285, 0.4600,  ..., 0.4826, 0.3923, 0.5751],
          [0.4973, 0.4976, 0.4158,  ..., 0.5523, 0.4235, 0.5095],
          [0.7178, 0.4593, 0.5183,  ..., 0.4069, 0.3877, 0.3277]]],


        [[[0.5218, 0.5694, 0.6105,  ..., 0.6183, 0.3951, 0.4972],
          [0.4982, 0.4869, 0.5324,  ..., 0.4598, 0.4301, 0.3739],
          [0.5494, 0.3567, 0.6021,  ..., 0.7130, 0.5765, 0.4422],
          [0.3956, 0.4230, 0.5336,  ..., 0.5319, 0.4700, 0.6379]],

         [[0.6834, 0.6817, 0.5385,  ..., 0.5291, 0.4569, 0.5574],
          [0.5746, 0.4706, 0.4583,  ..., 0.3914, 0.4412, 0.5894],
          [0.4173, 0.6424, 0.4135,  ..., 0.5221, 0.5842, 0.3919],
          [0.5619, 0.7564, 0.5908,  ..., 0.6280, 0.3585, 0.4484]],

         [[0.6298, 0.5689, 0.6049,  ..., 0.5074, 0.5061, 0.4268],
          [0.3720, 0.4424, 0.4579,  ..., 0.5024, 0.4045, 0.4477],
          [0.6270, 0.6011, 0.4806,  ..., 0.4216, 0.6280, 0.4637],
          [0.5380, 0.3970, 0.5349,  ..., 0.5506, 0.4681, 0.4518]],

         ...,

         [[0.4092, 0.4799, 0.5950,  ..., 0.3730, 0.4230, 0.5917],
          [0.5053, 0.2674, 0.4102,  ..., 0.5224, 0.6567, 0.5489],
          [0.5409, 0.5799, 0.4026,  ..., 0.4003, 0.5194, 0.3478],
          [0.5390, 0.5103, 0.4405,  ..., 0.3956, 0.4951, 0.3621]],

         [[0.5908, 0.4426, 0.4537,  ..., 0.4896, 0.3937, 0.5813],
          [0.4841, 0.5794, 0.6379,  ..., 0.4637, 0.3363, 0.5385],
          [0.5978, 0.4973, 0.4710,  ..., 0.4576, 0.5501, 0.5479],
          [0.2728, 0.6575, 0.4688,  ..., 0.5581, 0.5019, 0.4705]],

         [[0.4537, 0.4965, 0.3504,  ..., 0.4936, 0.5708, 0.5983],
          [0.4359, 0.5171, 0.5426,  ..., 0.4683, 0.4755, 0.5789],
          [0.4569, 0.4239, 0.5894,  ..., 0.4579, 0.4092, 0.5368],
          [0.4436, 0.2918, 0.5554,  ..., 0.5898, 0.4905, 0.4373]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0110, -0.0010,  0.0070,  0.0030,  0.0070,  0.0050, -0.0030,  0.0070,
         0.0090, -0.0050], device='cuda:0')
selected experts tensor([1607, 1751, 1618, 1653, 1620, 1635, 1677, 1569, 1593, 1661],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5638, 0.6823, 0.4142,  ..., 0.5830, 0.5003, 0.5162],
          [0.4885, 0.6381, 0.3839,  ..., 0.6250, 0.5468, 0.5498],
          [0.5036, 0.3506, 0.6187,  ..., 0.4827, 0.6280, 0.3207],
          [0.4375, 0.2937, 0.4883,  ..., 0.4862, 0.5366, 0.5648]],

         [[0.3510, 0.4742, 0.4242,  ..., 0.4481, 0.4564, 0.4571],
          [0.5901, 0.3087, 0.5401,  ..., 0.5076, 0.5380, 0.5578],
          [0.4443, 0.5757, 0.5153,  ..., 0.6501, 0.7188, 0.5796],
          [0.5677, 0.3952, 0.4433,  ..., 0.4394, 0.5718, 0.5628]],

         [[0.5580, 0.4615, 0.4218,  ..., 0.4932, 0.5315, 0.4765],
          [0.6876, 0.3994, 0.5135,  ..., 0.4091, 0.5560, 0.2486],
          [0.5113, 0.4757, 0.5631,  ..., 0.5106, 0.3730, 0.5459],
          [0.2976, 0.5249, 0.5207,  ..., 0.4486, 0.4704, 0.4680]],

         ...,

         [[0.4689, 0.6781, 0.5259,  ..., 0.6162, 0.3595, 0.5479],
          [0.5517, 0.4359, 0.2318,  ..., 0.6130, 0.5828, 0.5379],
          [0.6033, 0.6122, 0.7497,  ..., 0.3456, 0.4420, 0.4152],
          [0.5739, 0.5467, 0.3894,  ..., 0.4100, 0.5388, 0.5246]],

         [[0.5638, 0.6823, 0.4142,  ..., 0.5830, 0.5003, 0.5162],
          [0.4885, 0.6381, 0.3839,  ..., 0.6250, 0.5468, 0.5498],
          [0.5036, 0.3506, 0.6187,  ..., 0.4827, 0.6280, 0.3207],
          [0.4375, 0.2937, 0.4883,  ..., 0.4862, 0.5366, 0.5648]],

         [[0.4445, 0.5520, 0.5284,  ..., 0.3879, 0.7100, 0.4573],
          [0.4660, 0.4107, 0.3752,  ..., 0.2494, 0.5526, 0.4617],
          [0.4718, 0.4990, 0.4897,  ..., 0.4128, 0.4126, 0.5102],
          [0.4812, 0.4417, 0.3955,  ..., 0.6162, 0.5051, 0.4380]]],


        [[[0.5498, 0.5045, 0.5459,  ..., 0.3833, 0.5270, 0.4496],
          [0.4918, 0.4707, 0.6131,  ..., 0.6545, 0.6335, 0.5418],
          [0.5354, 0.3947, 0.3361,  ..., 0.3144, 0.5036, 0.6159],
          [0.5681, 0.4999, 0.5411,  ..., 0.3987, 0.5081, 0.5744]],

         [[0.5854, 0.6390, 0.4723,  ..., 0.5669, 0.5063, 0.4641],
          [0.4976, 0.4636, 0.4323,  ..., 0.6331, 0.4259, 0.3848],
          [0.4575, 0.7036, 0.4438,  ..., 0.4830, 0.4463, 0.5725],
          [0.5782, 0.5173, 0.3876,  ..., 0.4746, 0.5453, 0.4337]],

         [[0.6192, 0.4671, 0.4872,  ..., 0.4243, 0.4620, 0.4443],
          [0.4677, 0.6237, 0.6501,  ..., 0.4024, 0.3905, 0.4034],
          [0.5339, 0.5499, 0.5146,  ..., 0.6813, 0.4704, 0.5389],
          [0.4185, 0.4088, 0.3008,  ..., 0.3959, 0.4806, 0.4791]],

         ...,

         [[0.5616, 0.5169, 0.4395,  ..., 0.5247, 0.3244, 0.3082],
          [0.7376, 0.5681, 0.3867,  ..., 0.3672, 0.5287, 0.3839],
          [0.5377, 0.3867, 0.4546,  ..., 0.3782, 0.3767, 0.4689],
          [0.5372, 0.3569, 0.5361,  ..., 0.6240, 0.4764, 0.6375]],

         [[0.4946, 0.5053, 0.6247,  ..., 0.5659, 0.5297, 0.7092],
          [0.4617, 0.5266, 0.6447,  ..., 0.4162, 0.3365, 0.5844],
          [0.5042, 0.5429, 0.5568,  ..., 0.4899, 0.4331, 0.4609],
          [0.5882, 0.3372, 0.4834,  ..., 0.6181, 0.5181, 0.4982]],

         [[0.4385, 0.3669, 0.4694,  ..., 0.4524, 0.5053, 0.4385],
          [0.5086, 0.5380, 0.4375,  ..., 0.2928, 0.4079, 0.5820],
          [0.5573, 0.4596, 0.3501,  ..., 0.4985, 0.3244, 0.4034],
          [0.7330, 0.6426, 0.6492,  ..., 0.3954, 0.5315, 0.6201]]]],
       device='cuda:0')/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([[[[0.5588, 0.6893, 0.4092,  ..., 0.5860, 0.4993, 0.5112],
          [0.4835, 0.6451, 0.3789,  ..., 0.6280, 0.5458, 0.5448],
          [0.4986, 0.3576, 0.6137,  ..., 0.4857, 0.6270, 0.3157],
          [0.4325, 0.3007, 0.4833,  ..., 0.4892, 0.5356, 0.5598]],

         [[0.3460, 0.4812, 0.4192,  ..., 0.4511, 0.4554, 0.4521],
          [0.5851, 0.3157, 0.5351,  ..., 0.5106, 0.5370, 0.5528],
          [0.4393, 0.5827, 0.5103,  ..., 0.6531, 0.7178, 0.5746],
          [0.5627, 0.4022, 0.4383,  ..., 0.4424, 0.5708, 0.5578]],

         [[0.5530, 0.4685, 0.4168,  ..., 0.4962, 0.5305, 0.4715],
          [0.6826, 0.4064, 0.5085,  ..., 0.4121, 0.5550, 0.2436],
          [0.5063, 0.4827, 0.5581,  ..., 0.5136, 0.3720, 0.5409],
          [0.2926, 0.5319, 0.5157,  ..., 0.4516, 0.4694, 0.4630]],

         ...,

         [[0.4639, 0.6851, 0.5209,  ..., 0.6192, 0.3585, 0.5429],
          [0.5467, 0.4429, 0.2268,  ..., 0.6160, 0.5818, 0.5329],
          [0.5983, 0.6192, 0.7447,  ..., 0.3486, 0.4410, 0.4102],
          [0.5689, 0.5537, 0.3844,  ..., 0.4130, 0.5378, 0.5196]],

         [[0.5588, 0.6893, 0.4092,  ..., 0.5860, 0.4993, 0.5112],
          [0.4835, 0.6451, 0.3789,  ..., 0.6280, 0.5458, 0.5448],
          [0.4986, 0.3576, 0.6137,  ..., 0.4857, 0.6270, 0.3157],
          [0.4325, 0.3007, 0.4833,  ..., 0.4892, 0.5356, 0.5598]],

         [[0.4395, 0.5590, 0.5234,  ..., 0.3909, 0.7090, 0.4523],
          [0.4610, 0.4177, 0.3702,  ..., 0.2524, 0.5516, 0.4567],
          [0.4668, 0.5060, 0.4847,  ..., 0.4158, 0.4116, 0.5052],
          [0.4762, 0.4487, 0.3905,  ..., 0.6192, 0.5041, 0.4330]]],


        [[[0.5448, 0.5115, 0.5409,  ..., 0.3863, 0.5260, 0.4446],
          [0.4868, 0.4777, 0.6081,  ..., 0.6575, 0.6325, 0.5368],
          [0.5304, 0.4017, 0.3311,  ..., 0.3174, 0.5026, 0.6109],
          [0.5631, 0.5069, 0.5361,  ..., 0.4017, 0.5071, 0.5694]],

         [[0.5804, 0.6460, 0.4673,  ..., 0.5699, 0.5053, 0.4591],
          [0.4926, 0.4706, 0.4273,  ..., 0.6361, 0.4249, 0.3798],
          [0.4525, 0.7106, 0.4388,  ..., 0.4860, 0.4453, 0.5675],
          [0.5732, 0.5243, 0.3826,  ..., 0.4776, 0.5443, 0.4287]],

         [[0.6142, 0.4741, 0.4822,  ..., 0.4273, 0.4610, 0.4393],
          [0.4627, 0.6307, 0.6451,  ..., 0.4054, 0.3895, 0.3984],
          [0.5289, 0.5569, 0.5096,  ..., 0.6843, 0.4694, 0.5339],
          [0.4135, 0.4158, 0.2958,  ..., 0.3989, 0.4796, 0.4741]],

         ...,

         [[0.5566, 0.5239, 0.4345,  ..., 0.5277, 0.3234, 0.3032],
          [0.7326, 0.5751, 0.3817,  ..., 0.3702, 0.5277, 0.3789],
          [0.5327, 0.3937, 0.4496,  ..., 0.3812, 0.3757, 0.4639],
          [0.5322, 0.3639, 0.5311,  ..., 0.6270, 0.4754, 0.6325]],

         [[0.4896, 0.5123, 0.6197,  ..., 0.5689, 0.5287, 0.7042],
          [0.4567, 0.5336, 0.6397,  ..., 0.4192, 0.3355, 0.5794],
          [0.4992, 0.5499, 0.5518,  ..., 0.4929, 0.4321, 0.4559],
          [0.5832, 0.3442, 0.4784,  ..., 0.6211, 0.5171, 0.4932]],

         [[0.4335, 0.3739, 0.4644,  ..., 0.4554, 0.5043, 0.4335],
          [0.5036, 0.5450, 0.4325,  ..., 0.2958, 0.4069, 0.5770],
          [0.5523, 0.4666, 0.3451,  ..., 0.5015, 0.3234, 0.3984],
          [0.7280, 0.6496, 0.6442,  ..., 0.3984, 0.5305, 0.6151]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0050, -0.0070,  0.0050, -0.0030,  0.0030,  0.0030, -0.0050, -0.0030,
         0.0010,  0.0050], device='cuda:0')
selected experts tensor([1669, 1691, 1616, 1596, 1688, 1567, 1626, 1734, 1568, 1629],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.3431, 0.4430, 0.3934,  ..., 0.3162, 0.5478, 0.3738],
          [0.4471, 0.3592, 0.5359,  ..., 0.5178, 0.3926, 0.4415],
          [0.4273, 0.5602, 0.5178,  ..., 0.3989, 0.4705, 0.3651],
          [0.4096, 0.6214, 0.5262,  ..., 0.5501, 0.4850, 0.5914]],

         [[0.4025, 0.4176, 0.5898,  ..., 0.6332, 0.4368, 0.4683],
          [0.5089, 0.4476, 0.3837,  ..., 0.5489, 0.6578, 0.4628],
          [0.4130, 0.5926, 0.5642,  ..., 0.6751, 0.4751, 0.3525],
          [0.6422, 0.5007, 0.4865,  ..., 0.5788, 0.4863, 0.5310]],

         [[0.6046, 0.4172, 0.4644,  ..., 0.5845, 0.4277, 0.5810],
          [0.5112, 0.5053, 0.3846,  ..., 0.4485, 0.5487, 0.6990],
          [0.7030, 0.4110, 0.4856,  ..., 0.5711, 0.4397, 0.5056],
          [0.6589, 0.6332, 0.6414,  ..., 0.5692, 0.4349, 0.5838]],

         ...,

         [[0.4940, 0.3843, 0.5922,  ..., 0.4348, 0.5483, 0.5318],
          [0.4793, 0.5016, 0.4267,  ..., 0.5345, 0.5201, 0.4719],
          [0.4360, 0.3637, 0.4727,  ..., 0.3710, 0.4047, 0.3868],
          [0.4040, 0.5954, 0.4792,  ..., 0.5931, 0.7230, 0.4117]],

         [[0.4786, 0.5874, 0.3966,  ..., 0.4710, 0.4997, 0.5099],
          [0.5027, 0.4484, 0.4654,  ..., 0.5122, 0.5608, 0.6409],
          [0.5699, 0.4520, 0.6378,  ..., 0.4887, 0.5700, 0.4427],
          [0.6028, 0.4496, 0.6699,  ..., 0.6540, 0.4946, 0.4889]],

         [[0.6306, 0.4792, 0.4296,  ..., 0.6567, 0.4244, 0.4814],
          [0.6589, 0.5016, 0.4210,  ..., 0.6699, 0.4507, 0.3014],
          [0.5317, 0.5621, 0.5094,  ..., 0.4780, 0.4192, 0.5398],
          [0.5627, 0.4044, 0.6332,  ..., 0.3246, 0.3586, 0.6454]]],


        [[[0.4568, 0.2898, 0.6495,  ..., 0.4497, 0.6237, 0.3817],
          [0.6288, 0.4011, 0.5228,  ..., 0.4969, 0.4488, 0.5453],
          [0.5599, 0.3889, 0.4334,  ..., 0.5545, 0.5908, 0.5118],
          [0.4503, 0.4267, 0.4879,  ..., 0.5320, 0.4159, 0.5643]],

         [[0.4774, 0.4455, 0.4828,  ..., 0.4362, 0.5187, 0.4630],
          [0.3449, 0.5068, 0.4605,  ..., 0.5078, 0.4079, 0.4638],
          [0.6342, 0.4612, 0.4758,  ..., 0.4305, 0.4980, 0.4117],
          [0.5817, 0.5201, 0.6180,  ..., 0.4562, 0.5266, 0.5824]],

         [[0.4842, 0.3885, 0.6300,  ..., 0.3322, 0.5354, 0.5293],
          [0.5229, 0.4493, 0.3999,  ..., 0.5448, 0.5264, 0.4723],
          [0.5143, 0.4737, 0.5978,  ..., 0.6110, 0.6900, 0.6183],
          [0.5234, 0.4938, 0.4429,  ..., 0.5499, 0.4890, 0.4621]],

         ...,

         [[0.5093, 0.4110, 0.4654,  ..., 0.4008, 0.6315, 0.4516],
          [0.4496, 0.4934, 0.3470,  ..., 0.3971, 0.5923, 0.4237],
          [0.3721, 0.4525, 0.3237,  ..., 0.5540, 0.4150, 0.3480],
          [0.5317, 0.4728, 0.4396,  ..., 0.4579, 0.5177, 0.4950]],

         [[0.4940, 0.3843, 0.5922,  ..., 0.4348, 0.5483, 0.5318],
          [0.4793, 0.5016, 0.4267,  ..., 0.5345, 0.5201, 0.4719],
          [0.4360, 0.3637, 0.4727,  ..., 0.3710, 0.4047, 0.3868],
          [0.4040, 0.5954, 0.4792,  ..., 0.5931, 0.7230, 0.4117]],

         [[0.4479, 0.2971, 0.4685,  ..., 0.3989, 0.4277, 0.3651],
          [0.5536, 0.5926, 0.4396,  ..., 0.6138, 0.4416, 0.4509],
          [0.6901, 0.5312, 0.4106,  ..., 0.5176, 0.4811, 0.5739],
          [0.6189, 0.2593, 0.4547,  ..., 0.5229, 0.3498, 0.4859]]]],
       device='cuda:0')
tensor([[[[0.3531, 0.4450, 0.3854,  ..., 0.3082, 0.5378, 0.3798],
          [0.4571, 0.3612, 0.5279,  ..., 0.5098, 0.3826, 0.4475],
          [0.4373, 0.5622, 0.5098,  ..., 0.3909, 0.4605, 0.3711],
          [0.4196, 0.6234, 0.5182,  ..., 0.5421, 0.4750, 0.5974]],

         [[0.4125, 0.4196, 0.5818,  ..., 0.6252, 0.4268, 0.4743],
          [0.5189, 0.4496, 0.3757,  ..., 0.5409, 0.6478, 0.4688],
          [0.4230, 0.5946, 0.5562,  ..., 0.6671, 0.4651, 0.3585],
          [0.6522, 0.5027, 0.4785,  ..., 0.5708, 0.4763, 0.5370]],

         [[0.6146, 0.4192, 0.4564,  ..., 0.5765, 0.4177, 0.5870],
          [0.5212, 0.5073, 0.3766,  ..., 0.4405, 0.5387, 0.7050],
          [0.7130, 0.4130, 0.4776,  ..., 0.5631, 0.4297, 0.5116],
          [0.6689, 0.6352, 0.6334,  ..., 0.5612, 0.4249, 0.5898]],

         ...,

         [[0.5040, 0.3863, 0.5842,  ..., 0.4268, 0.5383, 0.5378],
          [0.4893, 0.5036, 0.4187,  ..., 0.5265, 0.5101, 0.4779],
          [0.4460, 0.3657, 0.4647,  ..., 0.3630, 0.3947, 0.3928],
          [0.4140, 0.5974, 0.4712,  ..., 0.5851, 0.7130, 0.4177]],

         [[0.4886, 0.5894, 0.3886,  ..., 0.4630, 0.4897, 0.5159],
          [0.5127, 0.4504, 0.4574,  ..., 0.5042, 0.5508, 0.6469],
          [0.5799, 0.4540, 0.6298,  ..., 0.4807, 0.5600, 0.4487],
          [0.6128, 0.4516, 0.6619,  ..., 0.6460, 0.4846, 0.4949]],

         [[0.6406, 0.4812, 0.4216,  ..., 0.6487, 0.4144, 0.4874],
          [0.6689, 0.5036, 0.4130,  ..., 0.6619, 0.4407, 0.3074],
          [0.5417, 0.5641, 0.5014,  ..., 0.4700, 0.4092, 0.5458],
          [0.5727, 0.4064, 0.6252,  ..., 0.3166, 0.3486, 0.6514]]],


        [[[0.4668, 0.2918, 0.6415,  ..., 0.4417, 0.6137, 0.3877],
          [0.6388, 0.4031, 0.5148,  ..., 0.4889, 0.4388, 0.5513],
          [0.5699, 0.3909, 0.4254,  ..., 0.5465, 0.5808, 0.5178],
          [0.4603, 0.4287, 0.4799,  ..., 0.5240, 0.4059, 0.5703]],

         [[0.4874, 0.4475, 0.4748,  ..., 0.4282, 0.5087, 0.4690],
          [0.3549, 0.5088, 0.4525,  ..., 0.4998, 0.3979, 0.4698],
          [0.6442, 0.4632, 0.4678,  ..., 0.4225, 0.4880, 0.4177],
          [0.5917, 0.5221, 0.6100,  ..., 0.4482, 0.5166, 0.5884]],

         [[0.4942, 0.3905, 0.6220,  ..., 0.3242, 0.5254, 0.5353],
          [0.5329, 0.4513, 0.3919,  ..., 0.5368, 0.5164, 0.4783],
          [0.5243, 0.4757, 0.5898,  ..., 0.6030, 0.6800, 0.6243],
          [0.5334, 0.4958, 0.4349,  ..., 0.5419, 0.4790, 0.4681]],

         ...,

         [[0.5193, 0.4130, 0.4574,  ..., 0.3928, 0.6215, 0.4576],
          [0.4596, 0.4954, 0.3390,  ..., 0.3891, 0.5823, 0.4297],
          [0.3821, 0.4545, 0.3157,  ..., 0.5460, 0.4050, 0.3540],
          [0.5417, 0.4748, 0.4316,  ..., 0.4499, 0.5077, 0.5010]],

         [[0.5040, 0.3863, 0.5842,  ..., 0.4268, 0.5383, 0.5378],
          [0.4893, 0.5036, 0.4187,  ..., 0.5265, 0.5101, 0.4779],
          [0.4460, 0.3657, 0.4647,  ..., 0.3630, 0.3947, 0.3928],
          [0.4140, 0.5974, 0.4712,  ..., 0.5851, 0.7130, 0.4177]],

         [[0.4579, 0.2991, 0.4605,  ..., 0.3909, 0.4177, 0.3711],
          [0.5636, 0.5946, 0.4316,  ..., 0.6058, 0.4316, 0.4569],
          [0.7001, 0.5332, 0.4026,  ..., 0.5096, 0.4711, 0.5799],
          [0.6289, 0.2613, 0.4467,  ..., 0.5149, 0.3398, 0.4919]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0100, -0.0020,  0.0080,  0.0020,  0.0080,  0.0060, -0.0040,  0.0080,
         0.0100, -0.0060], device='cuda:0')
selected experts tensor([1649, 1565, 1642, 1691, 1571, 1572, 1569, 1724, 1717, 1684],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4672, 0.2911, 0.5055,  ..., 0.5052, 0.4869, 0.5428],
          [0.5910, 0.4487, 0.5073,  ..., 0.5309, 0.5094, 0.4923],
          [0.5815, 0.4462, 0.4627,  ..., 0.5630, 0.4640, 0.4465],
          [0.5381, 0.3811, 0.4959,  ..., 0.4095, 0.6031, 0.5123]],

         [[0.4375, 0.5170, 0.4814,  ..., 0.5526, 0.5989, 0.5217],
          [0.4718, 0.5126, 0.3538,  ..., 0.6777, 0.5255, 0.5479],
          [0.5488, 0.4899, 0.4728,  ..., 0.4832, 0.5994, 0.5309],
          [0.4052, 0.6487, 0.5022,  ..., 0.4223, 0.5998, 0.4366]],

         [[0.5604, 0.4462, 0.5081,  ..., 0.3092, 0.5776, 0.4496],
          [0.6168, 0.6281, 0.3983,  ..., 0.5526, 0.4293, 0.6484],
          [0.5643, 0.4985, 0.3277,  ..., 0.5061, 0.4842, 0.5048],
          [0.5327, 0.3514, 0.6970,  ..., 0.5976, 0.4842, 0.5186]],

         ...,

         [[0.6135, 0.4494, 0.5003,  ..., 0.4690, 0.5531, 0.4515],
          [0.4495, 0.6085, 0.4455,  ..., 0.4782, 0.5373, 0.5438],
          [0.6135, 0.5419, 0.4877,  ..., 0.4495, 0.5285, 0.3699],
          [0.4544, 0.6353, 0.4602,  ..., 0.6037, 0.5656, 0.4583]],

         [[0.4146, 0.5462, 0.4929,  ..., 0.5430, 0.4772, 0.6970],
          [0.5512, 0.4325, 0.4745,  ..., 0.3726, 0.5738, 0.4619],
          [0.2714, 0.5950, 0.5338,  ..., 0.4353, 0.4623, 0.4578],
          [0.5981, 0.3659, 0.5072,  ..., 0.4461, 0.5928, 0.6053]],

         [[0.4674, 0.4680, 0.5792,  ..., 0.4698, 0.5270, 0.6262],
          [0.6437, 0.5021, 0.5100,  ..., 0.6683, 0.3578, 0.4668],
          [0.2668, 0.6263, 0.4966,  ..., 0.5272, 0.6336, 0.4578],
          [0.4657, 0.5048, 0.6457,  ..., 0.5957, 0.4659, 0.6024]]],


        [[[0.5153, 0.4863, 0.4503,  ..., 0.5242, 0.6263, 0.5347],
          [0.5018, 0.5676, 0.6010,  ..., 0.4415, 0.4439, 0.5389],
          [0.4970, 0.5339, 0.4838,  ..., 0.4495, 0.5557, 0.5797],
          [0.4772, 0.4426, 0.4581,  ..., 0.4732, 0.5185, 0.4477]],

         [[0.4711, 0.6583, 0.5146,  ..., 0.4905, 0.6795, 0.4634],
          [0.4621, 0.4236, 0.5958,  ..., 0.5045, 0.6157, 0.5259],
          [0.4546, 0.4516, 0.6183,  ..., 0.5439, 0.4654, 0.6466],
          [0.4955, 0.5074, 0.2301,  ..., 0.4137, 0.5057, 0.4585]],

         [[0.3456, 0.4007, 0.3881,  ..., 0.4943, 0.5514, 0.4827],
          [0.4730, 0.5030, 0.5542,  ..., 0.4413, 0.6208, 0.5038],
          [0.4213, 0.5790, 0.4252,  ..., 0.4779, 0.4941, 0.4491],
          [0.4478, 0.5256, 0.5787,  ..., 0.5155, 0.6417, 0.4627]],

         ...,

         [[0.5447, 0.4774, 0.6029,  ..., 0.6509, 0.4974, 0.5046],
          [0.4660, 0.3913, 0.5653,  ..., 0.4556, 0.3228, 0.5646],
          [0.5493, 0.5866, 0.4781,  ..., 0.4529, 0.5352, 0.4237],
          [0.5327, 0.5127, 0.4994,  ..., 0.6977, 0.7078, 0.4342]],

         [[0.6135, 0.4494, 0.5003,  ..., 0.4690, 0.5531, 0.4515],
          [0.4495, 0.6085, 0.4455,  ..., 0.4782, 0.5373, 0.5438],
          [0.6135, 0.5419, 0.4877,  ..., 0.4495, 0.5285, 0.3699],
          [0.4544, 0.6353, 0.4602,  ..., 0.6037, 0.5656, 0.4583]],

         [[0.5705, 0.4942, 0.4801,  ..., 0.4756, 0.3203, 0.4636],
          [0.6473, 0.3595, 0.5878,  ..., 0.5050, 0.5495, 0.5539],
          [0.4515, 0.6371, 0.4643,  ..., 0.4502, 0.5994, 0.4665],
          [0.5418, 0.4636, 0.4295,  ..., 0.3791, 0.4027, 0.7028]]]],
       device='cuda:0')
tensor([[[[0.4632, 0.2991, 0.4995,  ..., 0.5092, 0.4849, 0.5368],
          [0.5870, 0.4567, 0.5013,  ..., 0.5349, 0.5074, 0.4863],
          [0.5775, 0.4542, 0.4567,  ..., 0.5670, 0.4620, 0.4405],
          [0.5341, 0.3891, 0.4899,  ..., 0.4135, 0.6011, 0.5063]],

         [[0.4335, 0.5250, 0.4754,  ..., 0.5566, 0.5969, 0.5157],
          [0.4678, 0.5206, 0.3478,  ..., 0.6817, 0.5235, 0.5419],
          [0.5448, 0.4979, 0.4668,  ..., 0.4872, 0.5974, 0.5249],
          [0.4012, 0.6567, 0.4962,  ..., 0.4263, 0.5978, 0.4306]],

         [[0.5564, 0.4542, 0.5021,  ..., 0.3132, 0.5756, 0.4436],
          [0.6128, 0.6361, 0.3923,  ..., 0.5566, 0.4273, 0.6424],
          [0.5603, 0.5065, 0.3217,  ..., 0.5101, 0.4822, 0.4988],
          [0.5287, 0.3594, 0.6910,  ..., 0.6016, 0.4822, 0.5126]],

         ...,

         [[0.6095, 0.4574, 0.4943,  ..., 0.4730, 0.5511, 0.4455],
          [0.4455, 0.6165, 0.4395,  ..., 0.4822, 0.5353, 0.5378],
          [0.6095, 0.5499, 0.4817,  ..., 0.4535, 0.5265, 0.3639],
          [0.4504, 0.6433, 0.4542,  ..., 0.6077, 0.5636, 0.4523]],

         [[0.4106, 0.5542, 0.4869,  ..., 0.5470, 0.4752, 0.6910],
          [0.5472, 0.4405, 0.4685,  ..., 0.3766, 0.5718, 0.4559],
          [0.2674, 0.6030, 0.5278,  ..., 0.4393, 0.4603, 0.4518],
          [0.5941, 0.3739, 0.5012,  ..., 0.4501, 0.5908, 0.5993]],

         [[0.4634, 0.4760, 0.5732,  ..., 0.4738, 0.5250, 0.6202],
          [0.6397, 0.5101, 0.5040,  ..., 0.6723, 0.3558, 0.4608],
          [0.2628, 0.6343, 0.4906,  ..., 0.5312, 0.6316, 0.4518],
          [0.4617, 0.5128, 0.6397,  ..., 0.5997, 0.4639, 0.5964]]],


        [[[0.5113, 0.4943, 0.4443,  ..., 0.5282, 0.6243, 0.5287],
          [0.4978, 0.5756, 0.5950,  ..., 0.4455, 0.4419, 0.5329],
          [0.4930, 0.5419, 0.4778,  ..., 0.4535, 0.5537, 0.5737],
          [0.4732, 0.4506, 0.4521,  ..., 0.4772, 0.5165, 0.4417]],

         [[0.4671, 0.6663, 0.5086,  ..., 0.4945, 0.6775, 0.4574],
          [0.4581, 0.4316, 0.5898,  ..., 0.5085, 0.6137, 0.5199],
          [0.4506, 0.4596, 0.6123,  ..., 0.5479, 0.4634, 0.6406],
          [0.4915, 0.5154, 0.2241,  ..., 0.4177, 0.5037, 0.4525]],

         [[0.3416, 0.4087, 0.3821,  ..., 0.4983, 0.5494, 0.4767],
          [0.4690, 0.5110, 0.5482,  ..., 0.4453, 0.6188, 0.4978],
          [0.4173, 0.5870, 0.4192,  ..., 0.4819, 0.4921, 0.4431],
          [0.4438, 0.5336, 0.5727,  ..., 0.5195, 0.6397, 0.4567]],

         ...,

         [[0.5407, 0.4854, 0.5969,  ..., 0.6549, 0.4954, 0.4986],
          [0.4620, 0.3993, 0.5593,  ..., 0.4596, 0.3208, 0.5586],
          [0.5453, 0.5946, 0.4721,  ..., 0.4569, 0.5332, 0.4177],
          [0.5287, 0.5207, 0.4934,  ..., 0.7017, 0.7058, 0.4282]],

         [[0.6095, 0.4574, 0.4943,  ..., 0.4730, 0.5511, 0.4455],
          [0.4455, 0.6165, 0.4395,  ..., 0.4822, 0.5353, 0.5378],
          [0.6095, 0.5499, 0.4817,  ..., 0.4535, 0.5265, 0.3639],
          [0.4504, 0.6433, 0.4542,  ..., 0.6077, 0.5636, 0.4523]],

         [[0.5665, 0.5022, 0.4741,  ..., 0.4796, 0.3183, 0.4576],
          [0.6433, 0.3675, 0.5818,  ..., 0.5090, 0.5475, 0.5479],
          [0.4475, 0.6451, 0.4583,  ..., 0.4542, 0.5974, 0.4605],
          [0.5378, 0.4716, 0.4235,  ..., 0.3831, 0.4007, 0.6968]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0040, -0.0080,  0.0060, -0.0020,  0.0020,  0.0040, -0.0040, -0.0040,
         0.0020,  0.0060], device='cuda:0')
selected experts tensor([1567, 1700, 1650, 1580, 1578, 1728, 1561, 1682, 1648, 1690],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5453, 0.4484, 0.6118,  ..., 0.2454, 0.3933, 0.4005],
          [0.5011, 0.3309, 0.4995,  ..., 0.3652, 0.3685, 0.4738],
          [0.4205, 0.5479, 0.4598,  ..., 0.4732, 0.4060, 0.5242],
          [0.3910, 0.3725, 0.4349,  ..., 0.5613, 0.5343, 0.4951]],

         [[0.4518, 0.5476, 0.4215,  ..., 0.4905, 0.3641, 0.3633],
          [0.5503, 0.2936, 0.6450,  ..., 0.5061, 0.4446, 0.5630],
          [0.3970, 0.5696, 0.4933,  ..., 0.6215, 0.5303, 0.5271],
          [0.4646, 0.4609, 0.2749,  ..., 0.4880, 0.4882, 0.5564]],

         [[0.5162, 0.5340, 0.4753,  ..., 0.4779, 0.4281, 0.5249],
          [0.4262, 0.3950, 0.6459,  ..., 0.4561, 0.4660, 0.4615],
          [0.3827, 0.5616, 0.5390,  ..., 0.5337, 0.4952, 0.5545],
          [0.5064, 0.3830, 0.3415,  ..., 0.3968, 0.6532, 0.6754]],

         ...,

         [[0.6575, 0.4554, 0.4779,  ..., 0.4916, 0.3987, 0.4288],
          [0.5237, 0.4270, 0.4714,  ..., 0.5494, 0.4718, 0.5373],
          [0.4328, 0.5739, 0.5037,  ..., 0.4947, 0.6564, 0.4317],
          [0.6458, 0.5124, 0.4068,  ..., 0.4769, 0.5345, 0.5739]],

         [[0.4106, 0.5216, 0.5334,  ..., 0.6259, 0.3598, 0.5716],
          [0.5460, 0.4804, 0.5411,  ..., 0.5721, 0.3772, 0.5469],
          [0.6006, 0.5034, 0.4881,  ..., 0.3508, 0.4041, 0.3481],
          [0.5503, 0.4981, 0.4540,  ..., 0.6416, 0.4096, 0.3810]],

         [[0.5290, 0.3572, 0.4177,  ..., 0.4674, 0.4069, 0.5010],
          [0.4451, 0.4595, 0.6423,  ..., 0.4079, 0.4771, 0.4473],
          [0.4875, 0.5479, 0.4972,  ..., 0.4419, 0.5082, 0.4896],
          [0.5701, 0.3973, 0.5235,  ..., 0.5749, 0.4352, 0.5015]]],


        [[[0.5428, 0.3707, 0.5075,  ..., 0.5930, 0.3633, 0.6194],
          [0.4352, 0.5648, 0.4975,  ..., 0.4641, 0.4711, 0.5616],
          [0.4774, 0.5167, 0.4649,  ..., 0.5156, 0.5098, 0.5597],
          [0.5433, 0.4862, 0.4641,  ..., 0.5189, 0.4636, 0.5416]],

         [[0.5331, 0.4237, 0.4419,  ..., 0.4861, 0.4005, 0.3591],
          [0.5071, 0.4748, 0.5106,  ..., 0.4645, 0.3808, 0.4344],
          [0.3270, 0.5948, 0.5438,  ..., 0.7216, 0.6736, 0.6077],
          [0.4537, 0.5127, 0.3602,  ..., 0.6485, 0.3256, 0.4864]],

         [[0.5115, 0.6057, 0.4542,  ..., 0.4993, 0.3564, 0.3445],
          [0.5302, 0.4123, 0.5268,  ..., 0.3161, 0.3496, 0.4402],
          [0.3477, 0.6330, 0.5717,  ..., 0.5561, 0.5867, 0.5143],
          [0.5057, 0.3807, 0.3575,  ..., 0.6329, 0.4584, 0.6212]],

         ...,

         [[0.5382, 0.5515, 0.6146,  ..., 0.4454, 0.3843, 0.5220],
          [0.4376, 0.5825, 0.4248,  ..., 0.3783, 0.4841, 0.6167],
          [0.4003, 0.5386, 0.5407,  ..., 0.4189, 0.3897, 0.5119],
          [0.5204, 0.4660, 0.3131,  ..., 0.6116, 0.4756, 0.4598]],

         [[0.5340, 0.4147, 0.5214,  ..., 0.4129, 0.4899, 0.4474],
          [0.5185, 0.6182, 0.4957,  ..., 0.6152, 0.4272, 0.2576],
          [0.5499, 0.3922, 0.6118,  ..., 0.5777, 0.6076, 0.5046],
          [0.3975, 0.4443, 0.5257,  ..., 0.4233, 0.5657, 0.4324]],

         [[0.6377, 0.3922, 0.5291,  ..., 0.4170, 0.2943, 0.4947],
          [0.5126, 0.4753, 0.5055,  ..., 0.3750, 0.4872, 0.5673],
          [0.5037, 0.6219, 0.5096,  ..., 0.3840, 0.5398, 0.4669],
          [0.5940, 0.3644, 0.5134,  ..., 0.5726, 0.5190, 0.5673]]]],
       device='cuda:0')
tensor([[[[0.5383, 0.4434, 0.6128,  ..., 0.2744, 0.3603, 0.4235],
          [0.4941, 0.3259, 0.5005,  ..., 0.3942, 0.3355, 0.4968],
          [0.4135, 0.5429, 0.4608,  ..., 0.5022, 0.3730, 0.5472],
          [0.3840, 0.3675, 0.4359,  ..., 0.5903, 0.5013, 0.5181]],

         [[0.4448, 0.5426, 0.4225,  ..., 0.5195, 0.3311, 0.3863],
          [0.5433, 0.2886, 0.6460,  ..., 0.5351, 0.4116, 0.5860],
          [0.3900, 0.5646, 0.4943,  ..., 0.6505, 0.4973, 0.5501],
          [0.4576, 0.4559, 0.2759,  ..., 0.5170, 0.4552, 0.5794]],

         [[0.5092, 0.5290, 0.4763,  ..., 0.5069, 0.3951, 0.5479],
          [0.4192, 0.3900, 0.6469,  ..., 0.4851, 0.4330, 0.4845],
          [0.3757, 0.5566, 0.5400,  ..., 0.5627, 0.4622, 0.5775],
          [0.4994, 0.3780, 0.3425,  ..., 0.4258, 0.6202, 0.6984]],

         ...,

         [[0.6505, 0.4504, 0.4789,  ..., 0.5206, 0.3657, 0.4518],
          [0.5167, 0.4220, 0.4724,  ..., 0.5784, 0.4388, 0.5603],
          [0.4258, 0.5689, 0.5047,  ..., 0.5237, 0.6234, 0.4547],
          [0.6388, 0.5074, 0.4078,  ..., 0.5059, 0.5015, 0.5969]],

         [[0.4036, 0.5166, 0.5344,  ..., 0.6549, 0.3268, 0.5946],
          [0.5390, 0.4754, 0.5421,  ..., 0.6011, 0.3442, 0.5699],
          [0.5936, 0.4984, 0.4891,  ..., 0.3798, 0.3711, 0.3711],
          [0.5433, 0.4931, 0.4550,  ..., 0.6706, 0.3766, 0.4040]],

         [[0.5220, 0.3522, 0.4187,  ..., 0.4964, 0.3739, 0.5240],
          [0.4381, 0.4545, 0.6433,  ..., 0.4369, 0.4441, 0.4703],
          [0.4805, 0.5429, 0.4982,  ..., 0.4709, 0.4752, 0.5126],
          [0.5631, 0.3923, 0.5245,  ..., 0.6039, 0.4022, 0.5245]]],


        [[[0.5358, 0.3657, 0.5085,  ..., 0.6220, 0.3303, 0.6424],
          [0.4282, 0.5598, 0.4985,  ..., 0.4931, 0.4381, 0.5846],
          [0.4704, 0.5117, 0.4659,  ..., 0.5446, 0.4768, 0.5827],
          [0.5363, 0.4812, 0.4651,  ..., 0.5479, 0.4306, 0.5646]],

         [[0.5261, 0.4187, 0.4429,  ..., 0.5151, 0.3675, 0.3821],
          [0.5001, 0.4698, 0.5116,  ..., 0.4935, 0.3478, 0.4574],
          [0.3200, 0.5898, 0.5448,  ..., 0.7506, 0.6406, 0.6307],
          [0.4467, 0.5077, 0.3612,  ..., 0.6775, 0.2926, 0.5094]],

         [[0.5045, 0.6007, 0.4552,  ..., 0.5283, 0.3234, 0.3675],
          [0.5232, 0.4073, 0.5278,  ..., 0.3451, 0.3166, 0.4632],
          [0.3407, 0.6280, 0.5727,  ..., 0.5851, 0.5537, 0.5373],
          [0.4987, 0.3757, 0.3585,  ..., 0.6619, 0.4254, 0.6442]],

         ...,

         [[0.5312, 0.5465, 0.6156,  ..., 0.4744, 0.3513, 0.5450],
          [0.4306, 0.5775, 0.4258,  ..., 0.4073, 0.4511, 0.6397],
          [0.3933, 0.5336, 0.5417,  ..., 0.4479, 0.3567, 0.5349],
          [0.5134, 0.4610, 0.3141,  ..., 0.6406, 0.4426, 0.4828]],

         [[0.5270, 0.4097, 0.5224,  ..., 0.4419, 0.4569, 0.4704],
          [0.5115, 0.6132, 0.4967,  ..., 0.6442, 0.3942, 0.2806],
          [0.5429, 0.3872, 0.6128,  ..., 0.6067, 0.5746, 0.5276],
          [0.3905, 0.4393, 0.5267,  ..., 0.4523, 0.5327, 0.4554]],

         [[0.6307, 0.3872, 0.5301,  ..., 0.4460, 0.2613, 0.5177],
          [0.5056, 0.4703, 0.5065,  ..., 0.4040, 0.4542, 0.5903],
          [0.4967, 0.6169, 0.5106,  ..., 0.4130, 0.5068, 0.4899],
          [0.5870, 0.3594, 0.5144,  ..., 0.6016, 0.4860, 0.5903]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0070,  0.0050, -0.0010,  0.0030,  0.0050, -0.0210,  0.0150, -0.0290,
         0.0330, -0.0230], device='cuda:0')
selected experts tensor([1274, 1426, 1708, 1793, 1493, 2359, 1534, 1934, 1008, 1855],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4547, 0.5809, 0.6439,  ..., 0.3628, 0.6739, 0.3227],
          [0.5733, 0.5832, 0.4237,  ..., 0.6484, 0.5009, 0.5175],
          [0.4341, 0.3395, 0.5896,  ..., 0.3985, 0.5140, 0.3827],
          [0.6299, 0.6703, 0.6128,  ..., 0.5125, 0.3807, 0.5262]],

         [[0.5580, 0.4541, 0.7280,  ..., 0.3601, 0.6893, 0.4366],
          [0.4918, 0.4901, 0.5029,  ..., 0.5022, 0.4029, 0.4338],
          [0.4451, 0.4141, 0.4733,  ..., 0.4838, 0.5542, 0.3818],
          [0.6014, 0.5427, 0.5516,  ..., 0.4380, 0.4556, 0.5676]],

         [[0.4571, 0.4464, 0.6942,  ..., 0.4485, 0.4935, 0.4324],
          [0.5544, 0.5454, 0.5094,  ..., 0.6420, 0.4940, 0.4187],
          [0.4755, 0.4950, 0.4299,  ..., 0.4204, 0.5777, 0.5587],
          [0.6626, 0.5643, 0.4466,  ..., 0.4743, 0.4718, 0.4938]],

         ...,

         [[0.5130, 0.5110, 0.5150,  ..., 0.5150, 0.5050, 0.5170],
          [0.5130, 0.5110, 0.5150,  ..., 0.5150, 0.5050, 0.5170],
          [0.5130, 0.5110, 0.5150,  ..., 0.5150, 0.5050, 0.5170],
          [0.5130, 0.5110, 0.5150,  ..., 0.5150, 0.5050, 0.5170]],

         [[0.5130, 0.5110, 0.5150,  ..., 0.5150, 0.5050, 0.5170],
          [0.5130, 0.5110, 0.5150,  ..., 0.5150, 0.5050, 0.5170],
          [0.5130, 0.5110, 0.5150,  ..., 0.5150, 0.5050, 0.5170],
          [0.5130, 0.5110, 0.5150,  ..., 0.5150, 0.5050, 0.5170]],

         [[0.5130, 0.5110, 0.5150,  ..., 0.5150, 0.5050, 0.5170],
          [0.5130, 0.5110, 0.5150,  ..., 0.5150, 0.5050, 0.5170],
          [0.5130, 0.5110, 0.5150,  ..., 0.5150, 0.5050, 0.5170],
          [0.5130, 0.5110, 0.5150,  ..., 0.5150, 0.5050, 0.5170]]],


        [[[0.6518, 0.6046, 0.5743,  ..., 0.3780, 0.5047, 0.4409],
          [0.5957, 0.5765, 0.5412,  ..., 0.5323, 0.5113, 0.5613],
          [0.4307, 0.3996, 0.3994,  ..., 0.4256, 0.4721, 0.4486],
          [0.5886, 0.6117, 0.4115,  ..., 0.5045, 0.5146, 0.7029]],

         [[0.5445, 0.5325, 0.7264,  ..., 0.3100, 0.5500, 0.5254],
          [0.5209, 0.5737, 0.5119,  ..., 0.5351, 0.4940, 0.4281],
          [0.4355, 0.4546, 0.3852,  ..., 0.5078, 0.5796, 0.3978],
          [0.6455, 0.6056, 0.4181,  ..., 0.3654, 0.5493, 0.6841]],

         [[0.5056, 0.5212, 0.6420,  ..., 0.3916, 0.4484, 0.4314],
          [0.4750, 0.6597, 0.4629,  ..., 0.6420, 0.5110, 0.4481],
          [0.5563, 0.5837, 0.4671,  ..., 0.3985, 0.7108, 0.3674],
          [0.6714, 0.6182, 0.5140,  ..., 0.3291, 0.5391, 0.6431]],

         ...,

         [[0.5130, 0.5110, 0.5150,  ..., 0.5150, 0.5050, 0.5170],
          [0.5130, 0.5110, 0.5150,  ..., 0.5150, 0.5050, 0.5170],
          [0.5130, 0.5110, 0.5150,  ..., 0.5150, 0.5050, 0.5170],
          [0.5130, 0.5110, 0.5150,  ..., 0.5150, 0.5050, 0.5170]],

         [[0.5130, 0.5110, 0.5150,  ..., 0.5150, 0.5050, 0.5170],
          [0.5130, 0.5110, 0.5150,  ..., 0.5150, 0.5050, 0.5170],
          [0.5130, 0.5110, 0.5150,  ..., 0.5150, 0.5050, 0.5170],
          [0.5130, 0.5110, 0.5150,  ..., 0.5150, 0.5050, 0.5170]],

         [[0.5130, 0.5110, 0.5150,  ..., 0.5150, 0.5050, 0.5170],
          [0.5130, 0.5110, 0.5150,  ..., 0.5150, 0.5050, 0.5170],
          [0.5130, 0.5110, 0.5150,  ..., 0.5150, 0.5050, 0.5170],
          [0.5130, 0.5110, 0.5150,  ..., 0.5150, 0.5050, 0.5170]]]],
       device='cuda:0')
tensor([[[[0.4417, 0.5699, 0.6289,  ..., 0.3478, 0.6689, 0.3057],
          [0.5603, 0.5722, 0.4087,  ..., 0.6334, 0.4959, 0.5005],
          [0.4211, 0.3285, 0.5746,  ..., 0.3835, 0.5090, 0.3657],
          [0.6169, 0.6593, 0.5978,  ..., 0.4975, 0.3757, 0.5092]],

         [[0.5450, 0.4431, 0.7130,  ..., 0.3451, 0.6843, 0.4196],
          [0.4788, 0.4791, 0.4879,  ..., 0.4872, 0.3979, 0.4168],
          [0.4321, 0.4031, 0.4583,  ..., 0.4688, 0.5492, 0.3648],
          [0.5884, 0.5317, 0.5366,  ..., 0.4230, 0.4506, 0.5506]],

         [[0.4441, 0.4354, 0.6792,  ..., 0.4335, 0.4885, 0.4154],
          [0.5414, 0.5344, 0.4944,  ..., 0.6270, 0.4890, 0.4017],
          [0.4625, 0.4840, 0.4149,  ..., 0.4054, 0.5727, 0.5417],
          [0.6496, 0.5533, 0.4316,  ..., 0.4593, 0.4668, 0.4768]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.6388, 0.5936, 0.5593,  ..., 0.3630, 0.4997, 0.4239],
          [0.5827, 0.5655, 0.5262,  ..., 0.5173, 0.5063, 0.5443],
          [0.4177, 0.3886, 0.3844,  ..., 0.4106, 0.4671, 0.4316],
          [0.5756, 0.6007, 0.3965,  ..., 0.4895, 0.5096, 0.6859]],

         [[0.5315, 0.5215, 0.7114,  ..., 0.2950, 0.5450, 0.5084],
          [0.5079, 0.5627, 0.4969,  ..., 0.5201, 0.4890, 0.4111],
          [0.4225, 0.4436, 0.3702,  ..., 0.4928, 0.5746, 0.3808],
          [0.6325, 0.5946, 0.4031,  ..., 0.3504, 0.5443, 0.6671]],

         [[0.4926, 0.5102, 0.6270,  ..., 0.3766, 0.4434, 0.4144],
          [0.4620, 0.6487, 0.4479,  ..., 0.6270, 0.5060, 0.4311],
          [0.5433, 0.5727, 0.4521,  ..., 0.3835, 0.7058, 0.3504],
          [0.6584, 0.6072, 0.4990,  ..., 0.3141, 0.5341, 0.6261]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([0.0130, 0.0110, 0.0150, 0.0150, 0.0150, 0.0070, 0.0090, 0.0150, 0.0050,
        0.0170], device='cuda:0')
selected experts tensor([1275, 1675, 3039,  441,  717, 2121, 1539, 1220, 1806, 2551],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6741, 0.6858, 0.5455,  ..., 0.5387, 0.4657, 0.5528],
          [0.5636, 0.4696, 0.4653,  ..., 0.3989, 0.4502, 0.5824],
          [0.4063, 0.6414, 0.4210,  ..., 0.5288, 0.5932, 0.3849],
          [0.5512, 0.7554, 0.5978,  ..., 0.6350, 0.3675, 0.4414]],

         [[0.3730, 0.5046, 0.3655,  ..., 0.5002, 0.5774, 0.6141],
          [0.6215, 0.6342, 0.6175,  ..., 0.5600, 0.6208, 0.4686],
          [0.5026, 0.3647, 0.5706,  ..., 0.4607, 0.6195, 0.2937],
          [0.5999, 0.5262, 0.4148,  ..., 0.5119, 0.4853, 0.4518]],

         [[0.5971, 0.3747, 0.5697,  ..., 0.3565, 0.5028, 0.3416],
          [0.5217, 0.5650, 0.5533,  ..., 0.5675, 0.6269, 0.5114],
          [0.4034, 0.4373, 0.5375,  ..., 0.4429, 0.3497, 0.4668],
          [0.6874, 0.5482, 0.4760,  ..., 0.5096, 0.4037, 0.4889]],

         ...,

         [[0.4730, 0.4335, 0.6503,  ..., 0.5167, 0.5567, 0.5960],
          [0.4139, 0.5508, 0.5108,  ..., 0.5021, 0.4939, 0.4455],
          [0.4756, 0.3858, 0.4063,  ..., 0.6048, 0.4615, 0.4682],
          [0.5555, 0.5973, 0.5137,  ..., 0.4530, 0.4766, 0.3886]],

         [[0.3869, 0.3310, 0.5397,  ..., 0.4892, 0.3349, 0.4771],
          [0.6064, 0.4253, 0.3521,  ..., 0.4017, 0.5579, 0.4241],
          [0.4667, 0.4962, 0.4729,  ..., 0.5864, 0.6442, 0.4927],
          [0.5411, 0.4455, 0.5436,  ..., 0.5324, 0.3048, 0.5083]],

         [[0.4726, 0.5817, 0.6039,  ..., 0.5859, 0.3879, 0.2962],
          [0.4857, 0.3895, 0.5003,  ..., 0.5653, 0.5760, 0.4388],
          [0.6260, 0.5520, 0.4835,  ..., 0.6188, 0.4093, 0.6002],
          [0.6404, 0.4798, 0.5491,  ..., 0.5366, 0.4310, 0.4222]]],


        [[[0.4177, 0.3983, 0.4949,  ..., 0.5231, 0.5366, 0.4977],
          [0.4239, 0.4688, 0.5036,  ..., 0.4576, 0.4540, 0.5016],
          [0.3350, 0.3647, 0.4933,  ..., 0.6072, 0.3436, 0.5337],
          [0.5531, 0.4586, 0.5445,  ..., 0.4508, 0.5034, 0.3259]],

         [[0.2657, 0.4375, 0.3673,  ..., 0.4045, 0.3865, 0.4962],
          [0.6350, 0.4465, 0.4012,  ..., 0.4124, 0.5903, 0.4639],
          [0.4919, 0.2892, 0.6290,  ..., 0.4484, 0.3948, 0.4664],
          [0.6188, 0.3557, 0.3956,  ..., 0.5331, 0.6796, 0.4754]],

         [[0.5541, 0.4106, 0.5482,  ..., 0.4571, 0.5221, 0.4584],
          [0.5075, 0.4325, 0.3028,  ..., 0.4176, 0.5874, 0.4169],
          [0.3930, 0.5097, 0.5048,  ..., 0.3754, 0.3810, 0.4107],
          [0.4406, 0.5503, 0.5406,  ..., 0.3583, 0.4737, 0.4070]],

         ...,

         [[0.5367, 0.6351, 0.6359,  ..., 0.4591, 0.5267, 0.5117],
          [0.4249, 0.6187, 0.4021,  ..., 0.5859, 0.4391, 0.4208],
          [0.5741, 0.3692, 0.6404,  ..., 0.4957, 0.5077, 0.4806],
          [0.3926, 0.5669, 0.5302,  ..., 0.5523, 0.4720, 0.4687]],

         [[0.4563, 0.3267, 0.6207,  ..., 0.4709, 0.3462, 0.3933],
          [0.5000, 0.5813, 0.3691,  ..., 0.5061, 0.3550, 0.5719],
          [0.4048, 0.3674, 0.4726,  ..., 0.6020, 0.5644, 0.7835],
          [0.3297, 0.4675, 0.5893,  ..., 0.4424, 0.5446, 0.5288]],

         [[0.5427, 0.3692, 0.5482,  ..., 0.4063, 0.6487, 0.6399],
          [0.6260, 0.3683, 0.4779,  ..., 0.5354, 0.4112, 0.4008],
          [0.4343, 0.5585, 0.5721,  ..., 0.5711, 0.4524, 0.5714],
          [0.5384, 0.5501, 0.4869,  ..., 0.5168, 0.4644, 0.4366]]]],
       device='cuda:0')
tensor([[[[0.6851, 0.6868, 0.5385,  ..., 0.5317, 0.4567, 0.5598],
          [0.5746, 0.4706, 0.4583,  ..., 0.3919, 0.4412, 0.5894],
          [0.4173, 0.6424, 0.4140,  ..., 0.5218, 0.5842, 0.3919],
          [0.5622, 0.7564, 0.5908,  ..., 0.6280, 0.3585, 0.4484]],

         [[0.3840, 0.5056, 0.3585,  ..., 0.4932, 0.5684, 0.6211],
          [0.6325, 0.6352, 0.6105,  ..., 0.5530, 0.6118, 0.4756],
          [0.5136, 0.3657, 0.5636,  ..., 0.4537, 0.6105, 0.3007],
          [0.6109, 0.5272, 0.4078,  ..., 0.5049, 0.4763, 0.4588]],

         [[0.6081, 0.3757, 0.5627,  ..., 0.3495, 0.4938, 0.3486],
          [0.5327, 0.5660, 0.5463,  ..., 0.5605, 0.6179, 0.5184],
          [0.4144, 0.4383, 0.5305,  ..., 0.4359, 0.3407, 0.4738],
          [0.6984, 0.5492, 0.4690,  ..., 0.5026, 0.3947, 0.4959]],

         ...,

         [[0.4840, 0.4345, 0.6433,  ..., 0.5097, 0.5477, 0.6030],
          [0.4249, 0.5518, 0.5038,  ..., 0.4951, 0.4849, 0.4525],
          [0.4866, 0.3868, 0.3993,  ..., 0.5978, 0.4525, 0.4752],
          [0.5665, 0.5983, 0.5067,  ..., 0.4460, 0.4676, 0.3956]],

         [[0.3979, 0.3320, 0.5327,  ..., 0.4822, 0.3259, 0.4841],
          [0.6174, 0.4263, 0.3451,  ..., 0.3947, 0.5489, 0.4311],
          [0.4777, 0.4972, 0.4659,  ..., 0.5794, 0.6352, 0.4997],
          [0.5521, 0.4465, 0.5366,  ..., 0.5254, 0.2958, 0.5153]],

         [[0.4836, 0.5827, 0.5969,  ..., 0.5789, 0.3789, 0.3032],
          [0.4967, 0.3905, 0.4933,  ..., 0.5583, 0.5670, 0.4458],
          [0.6370, 0.5530, 0.4765,  ..., 0.6118, 0.4003, 0.6072],
          [0.6514, 0.4808, 0.5421,  ..., 0.5296, 0.4220, 0.4292]]],


        [[[0.4287, 0.3993, 0.4879,  ..., 0.5161, 0.5276, 0.5047],
          [0.4349, 0.4698, 0.4966,  ..., 0.4506, 0.4450, 0.5086],
          [0.3460, 0.3657, 0.4863,  ..., 0.6002, 0.3346, 0.5407],
          [0.5641, 0.4596, 0.5375,  ..., 0.4438, 0.4944, 0.3329]],

         [[0.2767, 0.4385, 0.3603,  ..., 0.3975, 0.3775, 0.5032],
          [0.6460, 0.4475, 0.3942,  ..., 0.4054, 0.5813, 0.4709],
          [0.5029, 0.2902, 0.6220,  ..., 0.4414, 0.3858, 0.4734],
          [0.6298, 0.3567, 0.3886,  ..., 0.5261, 0.6706, 0.4824]],

         [[0.5651, 0.4116, 0.5412,  ..., 0.4501, 0.5131, 0.4654],
          [0.5185, 0.4335, 0.2958,  ..., 0.4106, 0.5784, 0.4239],
          [0.4040, 0.5107, 0.4978,  ..., 0.3684, 0.3720, 0.4177],
          [0.4516, 0.5513, 0.5336,  ..., 0.3513, 0.4647, 0.4140]],

         ...,

         [[0.5477, 0.6361, 0.6289,  ..., 0.4521, 0.5177, 0.5187],
          [0.4359, 0.6197, 0.3951,  ..., 0.5789, 0.4301, 0.4278],
          [0.5851, 0.3702, 0.6334,  ..., 0.4887, 0.4987, 0.4876],
          [0.4036, 0.5679, 0.5232,  ..., 0.5453, 0.4630, 0.4757]],

         [[0.4673, 0.3277, 0.6137,  ..., 0.4639, 0.3372, 0.4003],
          [0.5110, 0.5823, 0.3621,  ..., 0.4991, 0.3460, 0.5789],
          [0.4158, 0.3684, 0.4656,  ..., 0.5950, 0.5554, 0.7905],
          [0.3407, 0.4685, 0.5823,  ..., 0.4354, 0.5356, 0.5358]],

         [[0.5537, 0.3702, 0.5412,  ..., 0.3993, 0.6397, 0.6469],
          [0.6370, 0.3693, 0.4709,  ..., 0.5284, 0.4022, 0.4078],
          [0.4453, 0.5595, 0.5651,  ..., 0.5641, 0.4434, 0.5784],
          [0.5494, 0.5511, 0.4799,  ..., 0.5098, 0.4554, 0.4436]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0110, -0.0010,  0.0070,  0.0010,  0.0090,  0.0070, -0.0030,  0.0070,
         0.0090, -0.0070], device='cuda:0')
selected experts tensor([1692, 1669, 1526, 1646, 1666, 1621, 1585, 1664, 1716, 1599],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5834, 0.6388, 0.4726,  ..., 0.5649, 0.5088, 0.4643],
          [0.4976, 0.4615, 0.4323,  ..., 0.6311, 0.4259, 0.3848],
          [0.4573, 0.7016, 0.4438,  ..., 0.4807, 0.4460, 0.5729],
          [0.5782, 0.5153, 0.3876,  ..., 0.4729, 0.5453, 0.4337]],

         [[0.5271, 0.5813, 0.4194,  ..., 0.6311, 0.5704, 0.4723],
          [0.4910, 0.4491, 0.4242,  ..., 0.5577, 0.4331, 0.6247],
          [0.5386, 0.7024, 0.4517,  ..., 0.6036, 0.4032, 0.5782],
          [0.5369, 0.3964, 0.4185,  ..., 0.6266, 0.5101, 0.4597]],

         [[0.5834, 0.4440, 0.5753,  ..., 0.4242, 0.4369, 0.6510],
          [0.4857, 0.4864, 0.5105,  ..., 0.5038, 0.5908, 0.3275],
          [0.3964, 0.4800, 0.5346,  ..., 0.4478, 0.4235, 0.4931],
          [0.3848, 0.5358, 0.4199,  ..., 0.5340, 0.5400, 0.3932]],

         ...,

         [[0.5948, 0.4705, 0.5257,  ..., 0.4066, 0.4688, 0.4624],
          [0.5854, 0.5416, 0.5796,  ..., 0.5405, 0.3586, 0.4299],
          [0.6014, 0.4102, 0.4242,  ..., 0.5463, 0.4960, 0.4858],
          [0.3761, 0.4949, 0.5200,  ..., 0.5914, 0.4050, 0.3997]],

         [[0.4778, 0.5053, 0.6411,  ..., 0.4127, 0.5111, 0.5583],
          [0.5335, 0.5799, 0.5877,  ..., 0.5446, 0.4097, 0.5624],
          [0.5527, 0.5084, 0.5030,  ..., 0.4718, 0.3841, 0.4549],
          [0.3997, 0.4874, 0.4760,  ..., 0.4261, 0.6280, 0.5624]],

         [[0.4275, 0.4902, 0.4699,  ..., 0.5975, 0.4958, 0.5981],
          [0.5030, 0.3621, 0.6320,  ..., 0.2639, 0.6244, 0.3115],
          [0.4204, 0.3922, 0.4667,  ..., 0.5422, 0.5709, 0.3090],
          [0.4869, 0.6334, 0.5124,  ..., 0.4099, 0.6124, 0.6215]]],


        [[[0.5061, 0.5541, 0.4308,  ..., 0.4490, 0.5689, 0.6825],
          [0.4435, 0.5935, 0.5423,  ..., 0.4151, 0.4178, 0.5107],
          [0.5051, 0.4970, 0.4166,  ..., 0.5483, 0.4192, 0.4161],
          [0.4133, 0.5500, 0.5491,  ..., 0.4485, 0.6179, 0.3955]],

         [[0.4822, 0.4810, 0.4882,  ..., 0.5644, 0.5670, 0.5505],
          [0.4830, 0.5632, 0.6159,  ..., 0.6055, 0.3836, 0.5621],
          [0.5288, 0.5474, 0.6411,  ..., 0.4550, 0.5130, 0.5739],
          [0.4190, 0.6042, 0.5583,  ..., 0.5352, 0.4480, 0.4095]],

         [[0.5305, 0.5201, 0.4665,  ..., 0.5924, 0.5689, 0.5462],
          [0.5369, 0.5491, 0.4185,  ..., 0.5142, 0.5057, 0.4549],
          [0.5462, 0.5785, 0.5454,  ..., 0.3836, 0.3966, 0.5052],
          [0.4760, 0.4450, 0.3536,  ..., 0.5359, 0.3827, 0.3635]],

         ...,

         [[0.5546, 0.6135, 0.4194,  ..., 0.2780, 0.6308, 0.3124],
          [0.3440, 0.4771, 0.5996,  ..., 0.4403, 0.3435, 0.3932],
          [0.5003, 0.4885, 0.5906,  ..., 0.4439, 0.5018, 0.6765],
          [0.4190, 0.5285, 0.4561,  ..., 0.6013, 0.5518, 0.3848]],

         [[0.5440, 0.4940, 0.5624,  ..., 0.4490, 0.5390, 0.4435],
          [0.5967, 0.4886, 0.5135,  ..., 0.4782, 0.6603, 0.2648],
          [0.4285, 0.4932, 0.5106,  ..., 0.5649, 0.5923, 0.4726],
          [0.4323, 0.4554, 0.5592,  ..., 0.5514, 0.4550, 0.5394]],

         [[0.4631, 0.4611, 0.3644,  ..., 0.6699, 0.6049, 0.6057],
          [0.4890, 0.5244, 0.3396,  ..., 0.3244, 0.4097, 0.4419],
          [0.6643, 0.6271, 0.4076,  ..., 0.4000, 0.4722, 0.6265],
          [0.6935, 0.3239, 0.4399,  ..., 0.4161, 0.5395, 0.6094]]]],
       device='cuda:0')
tensor([[[[0.5784, 0.6478, 0.4676,  ..., 0.5699, 0.5078, 0.4593],
          [0.4926, 0.4705, 0.4273,  ..., 0.6361, 0.4249, 0.3798],
          [0.4523, 0.7106, 0.4388,  ..., 0.4857, 0.4450, 0.5679],
          [0.5732, 0.5243, 0.3826,  ..., 0.4779, 0.5443, 0.4287]],

         [[0.5221, 0.5903, 0.4144,  ..., 0.6361, 0.5694, 0.4673],
          [0.4860, 0.4581, 0.4192,  ..., 0.5627, 0.4321, 0.6197],
          [0.5336, 0.7114, 0.4467,  ..., 0.6086, 0.4022, 0.5732],
          [0.5319, 0.4054, 0.4135,  ..., 0.6316, 0.5091, 0.4547]],

         [[0.5784, 0.4530, 0.5703,  ..., 0.4292, 0.4359, 0.6460],
          [0.4807, 0.4954, 0.5055,  ..., 0.5088, 0.5898, 0.3225],
          [0.3914, 0.4890, 0.5296,  ..., 0.4528, 0.4225, 0.4881],
          [0.3798, 0.5448, 0.4149,  ..., 0.5390, 0.5390, 0.3882]],

         ...,

         [[0.5898, 0.4795, 0.5207,  ..., 0.4116, 0.4678, 0.4574],
          [0.5804, 0.5506, 0.5746,  ..., 0.5455, 0.3576, 0.4249],
          [0.5964, 0.4192, 0.4192,  ..., 0.5513, 0.4950, 0.4808],
          [0.3711, 0.5039, 0.5150,  ..., 0.5964, 0.4040, 0.3947]],

         [[0.4728, 0.5143, 0.6361,  ..., 0.4177, 0.5101, 0.5533],
          [0.5285, 0.5889, 0.5827,  ..., 0.5496, 0.4087, 0.5574],
          [0.5477, 0.5174, 0.4980,  ..., 0.4768, 0.3831, 0.4499],
          [0.3947, 0.4964, 0.4710,  ..., 0.4311, 0.6270, 0.5574]],

         [[0.4225, 0.4992, 0.4649,  ..., 0.6025, 0.4948, 0.5931],
          [0.4980, 0.3711, 0.6270,  ..., 0.2689, 0.6234, 0.3065],
          [0.4154, 0.4012, 0.4617,  ..., 0.5472, 0.5699, 0.3040],
          [0.4819, 0.6424, 0.5074,  ..., 0.4149, 0.6114, 0.6165]]],


        [[[0.5011, 0.5631, 0.4258,  ..., 0.4540, 0.5679, 0.6775],
          [0.4385, 0.6025, 0.5373,  ..., 0.4201, 0.4168, 0.5057],
          [0.5001, 0.5060, 0.4116,  ..., 0.5533, 0.4182, 0.4111],
          [0.4083, 0.5590, 0.5441,  ..., 0.4535, 0.6169, 0.3905]],

         [[0.4772, 0.4900, 0.4832,  ..., 0.5694, 0.5660, 0.5455],
          [0.4780, 0.5722, 0.6109,  ..., 0.6105, 0.3826, 0.5571],
          [0.5238, 0.5564, 0.6361,  ..., 0.4600, 0.5120, 0.5689],
          [0.4140, 0.6132, 0.5533,  ..., 0.5402, 0.4470, 0.4045]],

         [[0.5255, 0.5291, 0.4615,  ..., 0.5974, 0.5679, 0.5412],
          [0.5319, 0.5581, 0.4135,  ..., 0.5192, 0.5047, 0.4499],
          [0.5412, 0.5875, 0.5404,  ..., 0.3886, 0.3956, 0.5002],
          [0.4710, 0.4540, 0.3486,  ..., 0.5409, 0.3817, 0.3585]],

         ...,

         [[0.5496, 0.6225, 0.4144,  ..., 0.2830, 0.6298, 0.3074],
          [0.3390, 0.4861, 0.5946,  ..., 0.4453, 0.3425, 0.3882],
          [0.4953, 0.4975, 0.5856,  ..., 0.4489, 0.5008, 0.6715],
          [0.4140, 0.5375, 0.4511,  ..., 0.6063, 0.5508, 0.3798]],

         [[0.5390, 0.5030, 0.5574,  ..., 0.4540, 0.5380, 0.4385],
          [0.5917, 0.4976, 0.5085,  ..., 0.4832, 0.6593, 0.2598],
          [0.4235, 0.5022, 0.5056,  ..., 0.5699, 0.5913, 0.4676],
          [0.4273, 0.4644, 0.5542,  ..., 0.5564, 0.4540, 0.5344]],

         [[0.4581, 0.4701, 0.3594,  ..., 0.6749, 0.6039, 0.6007],
          [0.4840, 0.5334, 0.3346,  ..., 0.3294, 0.4087, 0.4369],
          [0.6593, 0.6361, 0.4026,  ..., 0.4050, 0.4712, 0.6215],
          [0.6885, 0.3329, 0.4349,  ..., 0.4211, 0.5385, 0.6044]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0050, -0.0090,  0.0050, -0.0010,  0.0030,  0.0030, -0.0030, -0.0050,
         0.0010,  0.0050], device='cuda:0')
selected experts tensor([1685, 1625, 1641, 1682, 1742, 1499, 1637, 1616, 1586, 1671],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6016, 0.4804, 0.4067,  ..., 0.4757, 0.3898, 0.3895],
          [0.6594, 0.3891, 0.4950,  ..., 0.3375, 0.3695, 0.5002],
          [0.4078, 0.5188, 0.6232,  ..., 0.5998, 0.6977, 0.5767],
          [0.5750, 0.4828, 0.3466,  ..., 0.6576, 0.4291, 0.5673]],

         [[0.6110, 0.6085, 0.4593,  ..., 0.6115, 0.3514, 0.5559],
          [0.5043, 0.3909, 0.5418,  ..., 0.3339, 0.3988, 0.6211],
          [0.4627, 0.6220, 0.5855,  ..., 0.4697, 0.4863, 0.5391],
          [0.5783, 0.4252, 0.4663,  ..., 0.6302, 0.5208, 0.5795]],

         [[0.4386, 0.3423, 0.5193,  ..., 0.3949, 0.4579, 0.3702],
          [0.4186, 0.4333, 0.4788,  ..., 0.3911, 0.5039, 0.4568],
          [0.5678, 0.6178, 0.5597,  ..., 0.4551, 0.5636, 0.4397],
          [0.3683, 0.5261, 0.4975,  ..., 0.6160, 0.5383, 0.6318]],

         ...,

         [[0.6049, 0.4636, 0.5902,  ..., 0.5266, 0.3464, 0.2825],
          [0.5092, 0.4376, 0.4191,  ..., 0.4354, 0.3250, 0.4390],
          [0.3006, 0.6192, 0.5130,  ..., 0.4510, 0.6719, 0.4657],
          [0.5470, 0.4400, 0.5002,  ..., 0.4796, 0.5926, 0.4066]],

         [[0.4598, 0.5428, 0.5902,  ..., 0.6310, 0.4282, 0.4162],
          [0.5382, 0.4433, 0.4224,  ..., 0.4955, 0.5102, 0.5271],
          [0.4391, 0.5060, 0.6043,  ..., 0.5546, 0.6024, 0.4941],
          [0.5538, 0.4405, 0.5004,  ..., 0.3712, 0.6800, 0.6449]],

         [[0.5176, 0.5806, 0.4334,  ..., 0.5336, 0.4051, 0.4517],
          [0.4078, 0.4619, 0.6205,  ..., 0.3954, 0.3916, 0.4792],
          [0.4405, 0.5443, 0.4899,  ..., 0.3656, 0.5338, 0.3739],
          [0.5455, 0.4585, 0.4968,  ..., 0.6838, 0.4984, 0.5147]]],


        [[[0.6147, 0.5849, 0.5527,  ..., 0.3544, 0.3800, 0.4052],
          [0.3980, 0.5203, 0.4824,  ..., 0.3792, 0.4194, 0.3890],
          [0.3656, 0.5057, 0.5426,  ..., 0.4342, 0.4694, 0.6238],
          [0.5470, 0.3735, 0.4224,  ..., 0.5869, 0.5247, 0.6569]],

         [[0.5779, 0.5416, 0.5372,  ..., 0.5053, 0.4926, 0.5883],
          [0.4875, 0.4438, 0.4689,  ..., 0.4394, 0.3916, 0.4712],
          [0.5192, 0.5116, 0.5546,  ..., 0.4956, 0.5868, 0.5549],
          [0.3855, 0.5349, 0.4082,  ..., 0.6782, 0.5102, 0.5654]],

         [[0.5884, 0.5255, 0.5148,  ..., 0.5095, 0.3258, 0.4684],
          [0.4933, 0.5052, 0.4315,  ..., 0.2891, 0.3455, 0.5425],
          [0.3755, 0.5078, 0.5726,  ..., 0.4286, 0.4333, 0.5846],
          [0.5764, 0.3609, 0.4196,  ..., 0.6534, 0.6062, 0.5564]],

         ...,

         [[0.7040, 0.6178, 0.5602,  ..., 0.5702, 0.4603, 0.4827],
          [0.5678, 0.6099, 0.5139,  ..., 0.4432, 0.4015, 0.5062],
          [0.5774, 0.3564, 0.5726,  ..., 0.4611, 0.5803, 0.6300],
          [0.5798, 0.4883, 0.6940,  ..., 0.6774, 0.5511, 0.6211]],

         [[0.5740, 0.4489, 0.4469,  ..., 0.3558, 0.5016, 0.5070],
          [0.4962, 0.4982, 0.6305,  ..., 0.4753, 0.3625, 0.5492],
          [0.4787, 0.6430, 0.5270,  ..., 0.4708, 0.4757, 0.4317],
          [0.6504, 0.4280, 0.3737,  ..., 0.6774, 0.4603, 0.5177]],

         [[0.4434, 0.4460, 0.4782,  ..., 0.4073, 0.3414, 0.3572],
          [0.5063, 0.4147, 0.5907,  ..., 0.3276, 0.3712, 0.4469],
          [0.4391, 0.5266, 0.5930,  ..., 0.4568, 0.4865, 0.5075],
          [0.4205, 0.5051, 0.5544,  ..., 0.5897, 0.5102, 0.4769]]]],
       device='cuda:0')
tensor([[[[0.5936, 0.4744, 0.4087,  ..., 0.5057, 0.3558, 0.4135],
          [0.6514, 0.3831, 0.4970,  ..., 0.3675, 0.3355, 0.5242],
          [0.3998, 0.5128, 0.6252,  ..., 0.6298, 0.6637, 0.6007],
          [0.5670, 0.4768, 0.3486,  ..., 0.6876, 0.3951, 0.5913]],

         [[0.6030, 0.6025, 0.4613,  ..., 0.6415, 0.3174, 0.5799],
          [0.4963, 0.3849, 0.5438,  ..., 0.3639, 0.3648, 0.6451],
          [0.4547, 0.6160, 0.5875,  ..., 0.4997, 0.4523, 0.5631],
          [0.5703, 0.4192, 0.4683,  ..., 0.6602, 0.4868, 0.6035]],

         [[0.4306, 0.3363, 0.5213,  ..., 0.4249, 0.4239, 0.3942],
          [0.4106, 0.4273, 0.4808,  ..., 0.4211, 0.4699, 0.4808],
          [0.5598, 0.6118, 0.5617,  ..., 0.4851, 0.5296, 0.4637],
          [0.3603, 0.5201, 0.4995,  ..., 0.6460, 0.5043, 0.6558]],

         ...,

         [[0.5969, 0.4576, 0.5922,  ..., 0.5566, 0.3124, 0.3065],
          [0.5012, 0.4316, 0.4211,  ..., 0.4654, 0.2910, 0.4630],
          [0.2926, 0.6132, 0.5150,  ..., 0.4810, 0.6379, 0.4897],
          [0.5390, 0.4340, 0.5022,  ..., 0.5096, 0.5586, 0.4306]],

         [[0.4518, 0.5368, 0.5922,  ..., 0.6610, 0.3942, 0.4402],
          [0.5302, 0.4373, 0.4244,  ..., 0.5255, 0.4762, 0.5511],
          [0.4311, 0.5000, 0.6063,  ..., 0.5846, 0.5684, 0.5181],
          [0.5458, 0.4345, 0.5024,  ..., 0.4012, 0.6460, 0.6689]],

         [[0.5096, 0.5746, 0.4354,  ..., 0.5636, 0.3711, 0.4757],
          [0.3998, 0.4559, 0.6225,  ..., 0.4254, 0.3576, 0.5032],
          [0.4325, 0.5383, 0.4919,  ..., 0.3956, 0.4998, 0.3979],
          [0.5375, 0.4525, 0.4988,  ..., 0.7138, 0.4644, 0.5387]]],


        [[[0.6067, 0.5789, 0.5547,  ..., 0.3844, 0.3460, 0.4292],
          [0.3900, 0.5143, 0.4844,  ..., 0.4092, 0.3854, 0.4130],
          [0.3576, 0.4997, 0.5446,  ..., 0.4642, 0.4354, 0.6478],
          [0.5390, 0.3675, 0.4244,  ..., 0.6169, 0.4907, 0.6809]],

         [[0.5699, 0.5356, 0.5392,  ..., 0.5353, 0.4586, 0.6123],
          [0.4795, 0.4378, 0.4709,  ..., 0.4694, 0.3576, 0.4952],
          [0.5112, 0.5056, 0.5566,  ..., 0.5256, 0.5528, 0.5789],
          [0.3775, 0.5289, 0.4102,  ..., 0.7082, 0.4762, 0.5894]],

         [[0.5804, 0.5195, 0.5168,  ..., 0.5395, 0.2918, 0.4924],
          [0.4853, 0.4992, 0.4335,  ..., 0.3191, 0.3115, 0.5665],
          [0.3675, 0.5018, 0.5746,  ..., 0.4586, 0.3993, 0.6086],
          [0.5684, 0.3549, 0.4216,  ..., 0.6834, 0.5722, 0.5804]],

         ...,

         [[0.6960, 0.6118, 0.5622,  ..., 0.6002, 0.4263, 0.5067],
          [0.5598, 0.6039, 0.5159,  ..., 0.4732, 0.3675, 0.5302],
          [0.5694, 0.3504, 0.5746,  ..., 0.4911, 0.5463, 0.6540],
          [0.5718, 0.4823, 0.6960,  ..., 0.7074, 0.5171, 0.6451]],

         [[0.5660, 0.4429, 0.4489,  ..., 0.3858, 0.4676, 0.5310],
          [0.4882, 0.4922, 0.6325,  ..., 0.5053, 0.3285, 0.5732],
          [0.4707, 0.6370, 0.5290,  ..., 0.5008, 0.4417, 0.4557],
          [0.6424, 0.4220, 0.3757,  ..., 0.7074, 0.4263, 0.5417]],

         [[0.4354, 0.4400, 0.4802,  ..., 0.4373, 0.3074, 0.3812],
          [0.4983, 0.4087, 0.5927,  ..., 0.3576, 0.3372, 0.4709],
          [0.4311, 0.5206, 0.5950,  ..., 0.4868, 0.4525, 0.5315],
          [0.4125, 0.4991, 0.5564,  ..., 0.6197, 0.4762, 0.5009]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([ 0.0080,  0.0060, -0.0020,  0.0020,  0.0060, -0.0220,  0.0160, -0.0300,
         0.0340, -0.0240], device='cuda:0')
selected experts tensor([1466, 1439, 1542, 1831, 1648, 2160, 1704, 1770,  984, 1840],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.5182, 0.5099, 0.6447,  ..., 0.3835, 0.4351, 0.4545],
          [0.4475, 0.6728, 0.4984,  ..., 0.6260, 0.4913, 0.4582],
          [0.5394, 0.6022, 0.4576,  ..., 0.3709, 0.6916, 0.3532],
          [0.6698, 0.6050, 0.5055,  ..., 0.3118, 0.5667, 0.6674]],

         [[0.5259, 0.4960, 0.6898,  ..., 0.4816, 0.5218, 0.4519],
          [0.4022, 0.5502, 0.3743,  ..., 0.5726, 0.4655, 0.6316],
          [0.4398, 0.5904, 0.4437,  ..., 0.4290, 0.4950, 0.5336],
          [0.5019, 0.5287, 0.5092,  ..., 0.4877, 0.6428, 0.5238]],

         [[0.6081, 0.6631, 0.6492,  ..., 0.4423, 0.4884, 0.5053],
          [0.5561, 0.5458, 0.4687,  ..., 0.5940, 0.5469, 0.5333],
          [0.4437, 0.3542, 0.4298,  ..., 0.4543, 0.3935, 0.4787],
          [0.5273, 0.5451, 0.4518,  ..., 0.5820, 0.4457, 0.6421]],

         ...,

         [[0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160],
          [0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160],
          [0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160],
          [0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160]],

         [[0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160],
          [0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160],
          [0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160],
          [0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160]],

         [[0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160],
          [0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160],
          [0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160],
          [0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160]]],


        [[[0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160],
          [0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160],
          [0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160],
          [0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160]],

         [[0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160],
          [0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160],
          [0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160],
          [0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160]],

         [[0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160],
          [0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160],
          [0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160],
          [0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160]],

         ...,

         [[0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160],
          [0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160],
          [0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160],
          [0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160]],

         [[0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160],
          [0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160],
          [0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160],
          [0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160]],

         [[0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160],
          [0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160],
          [0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160],
          [0.5140, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5160]]]],
       device='cuda:0')
tensor([[[[0.5042, 0.4999, 0.6307,  ..., 0.3675, 0.4311, 0.4385],
          [0.4335, 0.6628, 0.4844,  ..., 0.6100, 0.4873, 0.4422],
          [0.5254, 0.5922, 0.4436,  ..., 0.3549, 0.6876, 0.3372],
          [0.6558, 0.5950, 0.4915,  ..., 0.2958, 0.5627, 0.6514]],

         [[0.5119, 0.4860, 0.6758,  ..., 0.4656, 0.5178, 0.4359],
          [0.3882, 0.5402, 0.3603,  ..., 0.5566, 0.4615, 0.6156],
          [0.4258, 0.5804, 0.4297,  ..., 0.4130, 0.4910, 0.5176],
          [0.4879, 0.5187, 0.4952,  ..., 0.4717, 0.6388, 0.5078]],

         [[0.5941, 0.6531, 0.6352,  ..., 0.4263, 0.4844, 0.4893],
          [0.5421, 0.5358, 0.4547,  ..., 0.5780, 0.5429, 0.5173],
          [0.4297, 0.3442, 0.4158,  ..., 0.4383, 0.3895, 0.4627],
          [0.5133, 0.5351, 0.4378,  ..., 0.5660, 0.4417, 0.6261]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0140, 0.0100, 0.0140, 0.0160, 0.0160, 0.0060, 0.0100, 0.0160, 0.0040,
        0.0160], device='cuda:0')
selected experts tensor([ 643,  641,  565, 3400, 3549,  925,  657,  625,  824,  459],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4719, 0.5807, 0.6049,  ..., 0.5840, 0.3874, 0.2972],
          [0.4847, 0.3885, 0.5013,  ..., 0.5643, 0.5750, 0.4398],
          [0.6250, 0.5510, 0.4843,  ..., 0.6183, 0.4083, 0.6012],
          [0.6394, 0.4788, 0.5499,  ..., 0.5357, 0.4305, 0.4232]],

         [[0.4774, 0.4981, 0.5031,  ..., 0.4448, 0.5530, 0.6265],
          [0.4471, 0.3520, 0.5755,  ..., 0.6731, 0.3305, 0.4802],
          [0.5626, 0.5631, 0.5424,  ..., 0.5387, 0.5429, 0.5643],
          [0.4674, 0.4880, 0.3683,  ..., 0.4981, 0.3593, 0.4179]],

         [[0.5612, 0.4568, 0.4756,  ..., 0.6053, 0.4272, 0.5485],
          [0.6008, 0.5817, 0.3674,  ..., 0.4537, 0.4396, 0.6310],
          [0.4393, 0.2548, 0.4163,  ..., 0.5095, 0.5912, 0.4855],
          [0.5150, 0.4605, 0.6291,  ..., 0.5103, 0.4125, 0.4507]],

         ...,

         [[0.3537, 0.5563, 0.6513,  ..., 0.4044, 0.5399, 0.4819],
          [0.5221, 0.4382, 0.6846,  ..., 0.4114, 0.6249, 0.5310],
          [0.5425, 0.4508, 0.6030,  ..., 0.6109, 0.4391, 0.4347],
          [0.4244, 0.4681, 0.5431,  ..., 0.5930, 0.5620, 0.4560]],

         [[0.5384, 0.2770, 0.4697,  ..., 0.5337, 0.5187, 0.3933],
          [0.5289, 0.5855, 0.4642,  ..., 0.6421, 0.6567, 0.5004],
          [0.5362, 0.5346, 0.5018,  ..., 0.5506, 0.4348, 0.5190],
          [0.4640, 0.5329, 0.4995,  ..., 0.6118, 0.6441, 0.5805]],

         [[0.3972, 0.4776, 0.6035,  ..., 0.3790, 0.4310, 0.5853],
          [0.4935, 0.2654, 0.4182,  ..., 0.5286, 0.6647, 0.5429],
          [0.5292, 0.5779, 0.4111,  ..., 0.4063, 0.5276, 0.3409],
          [0.5267, 0.5084, 0.4487,  ..., 0.4016, 0.5032, 0.3561]]],


        [[[0.4285, 0.5109, 0.4797,  ..., 0.4544, 0.5203, 0.5056],
          [0.5299, 0.6108, 0.4509,  ..., 0.4535, 0.3584, 0.5446],
          [0.4196, 0.4039, 0.4567,  ..., 0.5121, 0.4987, 0.5624],
          [0.5664, 0.5731, 0.5917,  ..., 0.5418, 0.5521, 0.4482]],

         [[0.5282, 0.4305, 0.5959,  ..., 0.4862, 0.5499, 0.5555],
          [0.5811, 0.4955, 0.4535,  ..., 0.5617, 0.4569, 0.4492],
          [0.2954, 0.3885, 0.5711,  ..., 0.4318, 0.4482, 0.5116],
          [0.4684, 0.5259, 0.4751,  ..., 0.6591, 0.5654, 0.5881]],

         [[0.6739, 0.6881, 0.5463,  ..., 0.5396, 0.4649, 0.5550],
          [0.5626, 0.4685, 0.4666,  ..., 0.3979, 0.4492, 0.5838],
          [0.4053, 0.6404, 0.4220,  ..., 0.5280, 0.5922, 0.3863],
          [0.5502, 0.7544, 0.5988,  ..., 0.6349, 0.3665, 0.4424]],

         ...,

         [[0.4618, 0.4373, 0.3966,  ..., 0.5055, 0.5817, 0.4689],
          [0.4602, 0.4983, 0.6129,  ..., 0.6146, 0.5287, 0.5318],
          [0.4586, 0.5117, 0.5038,  ..., 0.6475, 0.5584, 0.5619],
          [0.5722, 0.5103, 0.5855,  ..., 0.5797, 0.5511, 0.5061]],

         [[0.4459, 0.4542, 0.4405,  ..., 0.5360, 0.5049, 0.5378],
          [0.4057, 0.4808, 0.7218,  ..., 0.5440, 0.4050, 0.5163],
          [0.4790, 0.5348, 0.3846,  ..., 0.4831, 0.4985, 0.5677],
          [0.4430, 0.5423, 0.3071,  ..., 0.4470, 0.4234, 0.6698]],

         [[0.4352, 0.6103, 0.5165,  ..., 0.4309, 0.3638, 0.3624],
          [0.4401, 0.5212, 0.5149,  ..., 0.4595, 0.4215, 0.5330],
          [0.5929, 0.4243, 0.4538,  ..., 0.4049, 0.5441, 0.6068],
          [0.4986, 0.4358, 0.6690,  ..., 0.4333, 0.4557, 0.6400]]]],
       device='cuda:0')
tensor([[[[0.4839, 0.5827, 0.5969,  ..., 0.5780, 0.3794, 0.3032],
          [0.4967, 0.3905, 0.4933,  ..., 0.5583, 0.5670, 0.4458],
          [0.6370, 0.5530, 0.4763,  ..., 0.6123, 0.4003, 0.6072],
          [0.6514, 0.4808, 0.5419,  ..., 0.5297, 0.4225, 0.4292]],

         [[0.4894, 0.5001, 0.4951,  ..., 0.4388, 0.5450, 0.6325],
          [0.4591, 0.3540, 0.5675,  ..., 0.6671, 0.3225, 0.4862],
          [0.5746, 0.5651, 0.5344,  ..., 0.5327, 0.5349, 0.5703],
          [0.4794, 0.4900, 0.3603,  ..., 0.4921, 0.3513, 0.4239]],

         [[0.5732, 0.4588, 0.4676,  ..., 0.5993, 0.4192, 0.5545],
          [0.6128, 0.5837, 0.3594,  ..., 0.4477, 0.4316, 0.6370],
          [0.4513, 0.2568, 0.4083,  ..., 0.5035, 0.5832, 0.4915],
          [0.5270, 0.4625, 0.6211,  ..., 0.5043, 0.4045, 0.4567]],

         ...,

         [[0.3657, 0.5583, 0.6433,  ..., 0.3984, 0.5319, 0.4879],
          [0.5341, 0.4402, 0.6766,  ..., 0.4054, 0.6169, 0.5370],
          [0.5545, 0.4528, 0.5950,  ..., 0.6049, 0.4311, 0.4407],
          [0.4364, 0.4701, 0.5351,  ..., 0.5870, 0.5540, 0.4620]],

         [[0.5504, 0.2790, 0.4617,  ..., 0.5277, 0.5107, 0.3993],
          [0.5409, 0.5875, 0.4562,  ..., 0.6361, 0.6487, 0.5064],
          [0.5482, 0.5366, 0.4938,  ..., 0.5446, 0.4268, 0.5250],
          [0.4760, 0.5349, 0.4915,  ..., 0.6058, 0.6361, 0.5865]],

         [[0.4092, 0.4796, 0.5955,  ..., 0.3730, 0.4230, 0.5913],
          [0.5055, 0.2674, 0.4102,  ..., 0.5226, 0.6567, 0.5489],
          [0.5412, 0.5799, 0.4031,  ..., 0.4003, 0.5196, 0.3469],
          [0.5387, 0.5104, 0.4407,  ..., 0.3956, 0.4952, 0.3621]]],


        [[[0.4405, 0.5129, 0.4717,  ..., 0.4484, 0.5123, 0.5116],
          [0.5419, 0.6128, 0.4429,  ..., 0.4475, 0.3504, 0.5506],
          [0.4316, 0.4059, 0.4487,  ..., 0.5061, 0.4907, 0.5684],
          [0.5784, 0.5751, 0.5837,  ..., 0.5358, 0.5441, 0.4542]],

         [[0.5402, 0.4325, 0.5879,  ..., 0.4802, 0.5419, 0.5615],
          [0.5931, 0.4975, 0.4455,  ..., 0.5557, 0.4489, 0.4552],
          [0.3074, 0.3905, 0.5631,  ..., 0.4258, 0.4402, 0.5176],
          [0.4804, 0.5279, 0.4671,  ..., 0.6531, 0.5574, 0.5941]],

         [[0.6859, 0.6901, 0.5383,  ..., 0.5336, 0.4569, 0.5610],
          [0.5746, 0.4705, 0.4586,  ..., 0.3919, 0.4412, 0.5898],
          [0.4173, 0.6424, 0.4140,  ..., 0.5220, 0.5842, 0.3923],
          [0.5622, 0.7564, 0.5908,  ..., 0.6289, 0.3585, 0.4484]],

         ...,

         [[0.4738, 0.4393, 0.3886,  ..., 0.4995, 0.5737, 0.4749],
          [0.4722, 0.5003, 0.6049,  ..., 0.6086, 0.5207, 0.5378],
          [0.4706, 0.5137, 0.4958,  ..., 0.6415, 0.5504, 0.5679],
          [0.5842, 0.5123, 0.5775,  ..., 0.5737, 0.5431, 0.5121]],

         [[0.4579, 0.4562, 0.4325,  ..., 0.5300, 0.4969, 0.5438],
          [0.4177, 0.4828, 0.7138,  ..., 0.5380, 0.3970, 0.5223],
          [0.4910, 0.5368, 0.3766,  ..., 0.4771, 0.4905, 0.5737],
          [0.4550, 0.5443, 0.2991,  ..., 0.4410, 0.4154, 0.6758]],

         [[0.4472, 0.6123, 0.5085,  ..., 0.4249, 0.3558, 0.3684],
          [0.4521, 0.5232, 0.5069,  ..., 0.4535, 0.4135, 0.5390],
          [0.6049, 0.4263, 0.4458,  ..., 0.3989, 0.5361, 0.6128],
          [0.5106, 0.4378, 0.6610,  ..., 0.4273, 0.4477, 0.6460]]]],
       device='cuda:0', requires_grad=True)
tensor([-1.2000e-02, -2.0000e-03,  8.0000e-03, -2.3283e-10,  8.0000e-03,
         8.0000e-03, -2.0000e-03,  6.0000e-03,  8.0000e-03, -6.0000e-03],
       device='cuda:0')
selected experts tensor([1626, 1663, 1648, 1417, 1634, 1676, 1639, 1712, 1697, 1672],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4265, 0.4906, 0.4689,  ..., 0.5976, 0.4977, 0.5962],
          [0.5020, 0.3631, 0.6310,  ..., 0.2649, 0.6254, 0.3105],
          [0.4189, 0.3932, 0.4655,  ..., 0.5432, 0.5714, 0.3080],
          [0.4858, 0.6344, 0.5114,  ..., 0.4109, 0.6134, 0.6205]],

         [[0.4718, 0.5356, 0.4862,  ..., 0.5122, 0.4650, 0.5782],
          [0.5486, 0.4284, 0.6527,  ..., 0.4118, 0.5800, 0.5301],
          [0.5150, 0.6530, 0.4817,  ..., 0.5816, 0.5287, 0.3806],
          [0.4227, 0.4083, 0.4222,  ..., 0.5152, 0.4345, 0.4332]],

         [[0.4466, 0.3989, 0.4104,  ..., 0.4365, 0.5186, 0.4721],
          [0.5466, 0.3019, 0.6182,  ..., 0.3017, 0.5495, 0.5748],
          [0.5820, 0.5409, 0.3634,  ..., 0.5362, 0.4572, 0.5408],
          [0.4488, 0.5757, 0.5134,  ..., 0.4597, 0.5344, 0.5149]],

         ...,

         [[0.5498, 0.3728, 0.4908,  ..., 0.5065, 0.4422, 0.5344],
          [0.5057, 0.4727, 0.5515,  ..., 0.3438, 0.6578, 0.3903],
          [0.6329, 0.5506, 0.5995,  ..., 0.4546, 0.5236, 0.5046],
          [0.4883, 0.5809, 0.3820,  ..., 0.4242, 0.3846, 0.5116]],

         [[0.5410, 0.6181, 0.5934,  ..., 0.4972, 0.3596, 0.3968],
          [0.4459, 0.5922, 0.4929,  ..., 0.5582, 0.4860, 0.5647],
          [0.5289, 0.5738, 0.5882,  ..., 0.4803, 0.3722, 0.6140],
          [0.4968, 0.4888, 0.4450,  ..., 0.5074, 0.5271, 0.4394]],

         [[0.5602, 0.5159, 0.4385,  ..., 0.5230, 0.3254, 0.3064],
          [0.7366, 0.5671, 0.3857,  ..., 0.3662, 0.5299, 0.3829],
          [0.5364, 0.3853, 0.4536,  ..., 0.3772, 0.3777, 0.4679],
          [0.5359, 0.3559, 0.5351,  ..., 0.6230, 0.4774, 0.6365]]],


        [[[0.4621, 0.6094, 0.4912,  ..., 0.4792, 0.2987, 0.4951],
          [0.3848, 0.6504, 0.4303,  ..., 0.4868, 0.5497, 0.6237],
          [0.4662, 0.3993, 0.3917,  ..., 0.5294, 0.4164, 0.4878],
          [0.4251, 0.4559, 0.4853,  ..., 0.5668, 0.4780, 0.5264]],

         [[0.4655, 0.3532, 0.5715,  ..., 0.4762, 0.4750, 0.4558],
          [0.5971, 0.5180, 0.5425,  ..., 0.4403, 0.5366, 0.5408],
          [0.5214, 0.6117, 0.5967,  ..., 0.4611, 0.4858, 0.4094],
          [0.4113, 0.3998, 0.4761,  ..., 0.4338, 0.4764, 0.6338]],

         [[0.5810, 0.6407, 0.4716,  ..., 0.5659, 0.5109, 0.4628],
          [0.4965, 0.4626, 0.4318,  ..., 0.6321, 0.4269, 0.3838],
          [0.4565, 0.7034, 0.4428,  ..., 0.4818, 0.4470, 0.5719],
          [0.5772, 0.5159, 0.3866,  ..., 0.4738, 0.5463, 0.4327]],

         ...,

         [[0.4222, 0.3541, 0.4486,  ..., 0.6267, 0.5675, 0.5786],
          [0.7073, 0.5472, 0.3733,  ..., 0.6465, 0.4986, 0.5305],
          [0.5415, 0.5462, 0.4953,  ..., 0.5069, 0.4880, 0.5374],
          [0.4208, 0.4045, 0.6009,  ..., 0.5615, 0.4037, 0.4217]],

         [[0.4881, 0.4448, 0.4922,  ..., 0.5133, 0.4360, 0.5772],
          [0.4313, 0.5474, 0.4318,  ..., 0.6230, 0.5108, 0.4864],
          [0.5083, 0.4603, 0.5544,  ..., 0.5343, 0.5490, 0.3679],
          [0.5158, 0.5068, 0.4884,  ..., 0.4137, 0.5427, 0.5976]],

         [[0.4404, 0.4409, 0.6840,  ..., 0.3644, 0.5077, 0.6755],
          [0.4694, 0.5109, 0.5824,  ..., 0.5172, 0.5502, 0.4450],
          [0.5302, 0.5818, 0.3917,  ..., 0.4978, 0.4487, 0.3982],
          [0.3926, 0.4012, 0.5115,  ..., 0.3237, 0.3119, 0.4744]]]],
       device='cuda:0')
tensor([[[[0.4225, 0.4986, 0.4649,  ..., 0.6016, 0.4957, 0.5922],
          [0.4980, 0.3711, 0.6270,  ..., 0.2689, 0.6234, 0.3065],
          [0.4149, 0.4012, 0.4615,  ..., 0.5472, 0.5694, 0.3040],
          [0.4818, 0.6424, 0.5074,  ..., 0.4149, 0.6114, 0.6165]],

         [[0.4678, 0.5436, 0.4822,  ..., 0.5162, 0.4630, 0.5742],
          [0.5446, 0.4364, 0.6487,  ..., 0.4158, 0.5780, 0.5261],
          [0.5110, 0.6610, 0.4777,  ..., 0.5856, 0.5267, 0.3766],
          [0.4187, 0.4163, 0.4182,  ..., 0.5192, 0.4325, 0.4292]],

         [[0.4426, 0.4069, 0.4064,  ..., 0.4405, 0.5166, 0.4681],
          [0.5426, 0.3099, 0.6142,  ..., 0.3057, 0.5475, 0.5708],
          [0.5780, 0.5489, 0.3594,  ..., 0.5402, 0.4552, 0.5368],
          [0.4448, 0.5837, 0.5094,  ..., 0.4637, 0.5324, 0.5109]],

         ...,

         [[0.5458, 0.3808, 0.4868,  ..., 0.5105, 0.4402, 0.5304],
          [0.5017, 0.4807, 0.5475,  ..., 0.3478, 0.6558, 0.3863],
          [0.6289, 0.5586, 0.5955,  ..., 0.4586, 0.5216, 0.5006],
          [0.4843, 0.5889, 0.3780,  ..., 0.4282, 0.3826, 0.5076]],

         [[0.5370, 0.6261, 0.5894,  ..., 0.5012, 0.3576, 0.3928],
          [0.4419, 0.6002, 0.4889,  ..., 0.5622, 0.4840, 0.5607],
          [0.5249, 0.5818, 0.5842,  ..., 0.4843, 0.3702, 0.6100],
          [0.4928, 0.4968, 0.4410,  ..., 0.5114, 0.5251, 0.4354]],

         [[0.5562, 0.5239, 0.4345,  ..., 0.5270, 0.3234, 0.3024],
          [0.7326, 0.5751, 0.3817,  ..., 0.3702, 0.5279, 0.3789],
          [0.5324, 0.3933, 0.4496,  ..., 0.3812, 0.3757, 0.4639],
          [0.5319, 0.3639, 0.5311,  ..., 0.6270, 0.4754, 0.6325]]],


        [[[0.4581, 0.6174, 0.4872,  ..., 0.4832, 0.2967, 0.4911],
          [0.3808, 0.6584, 0.4263,  ..., 0.4908, 0.5477, 0.6197],
          [0.4622, 0.4073, 0.3877,  ..., 0.5334, 0.4144, 0.4838],
          [0.4211, 0.4639, 0.4813,  ..., 0.5708, 0.4760, 0.5224]],

         [[0.4615, 0.3612, 0.5675,  ..., 0.4802, 0.4730, 0.4518],
          [0.5931, 0.5260, 0.5385,  ..., 0.4443, 0.5346, 0.5368],
          [0.5174, 0.6197, 0.5927,  ..., 0.4651, 0.4838, 0.4054],
          [0.4073, 0.4078, 0.4721,  ..., 0.4378, 0.4744, 0.6298]],

         [[0.5770, 0.6487, 0.4676,  ..., 0.5699, 0.5089, 0.4588],
          [0.4925, 0.4706, 0.4278,  ..., 0.6361, 0.4249, 0.3798],
          [0.4525, 0.7114, 0.4388,  ..., 0.4858, 0.4450, 0.5679],
          [0.5732, 0.5239, 0.3826,  ..., 0.4778, 0.5443, 0.4287]],

         ...,

         [[0.4182, 0.3621, 0.4446,  ..., 0.6307, 0.5655, 0.5746],
          [0.7033, 0.5552, 0.3693,  ..., 0.6505, 0.4966, 0.5265],
          [0.5375, 0.5542, 0.4913,  ..., 0.5109, 0.4860, 0.5334],
          [0.4168, 0.4125, 0.5969,  ..., 0.5655, 0.4017, 0.4177]],

         [[0.4841, 0.4528, 0.4882,  ..., 0.5173, 0.4340, 0.5732],
          [0.4273, 0.5554, 0.4278,  ..., 0.6270, 0.5088, 0.4824],
          [0.5043, 0.4683, 0.5504,  ..., 0.5383, 0.5470, 0.3639],
          [0.5118, 0.5148, 0.4844,  ..., 0.4177, 0.5407, 0.5936]],

         [[0.4364, 0.4489, 0.6800,  ..., 0.3684, 0.5057, 0.6715],
          [0.4654, 0.5189, 0.5784,  ..., 0.5212, 0.5482, 0.4410],
          [0.5262, 0.5898, 0.3877,  ..., 0.5018, 0.4467, 0.3942],
          [0.3886, 0.4092, 0.5075,  ..., 0.3277, 0.3099, 0.4704]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([ 0.0040, -0.0080,  0.0040, -0.0020,  0.0020,  0.0040, -0.0020, -0.0040,
         0.0020,  0.0040], device='cuda:0')
selected experts tensor([1576, 1646, 1671, 1631, 1548, 1625, 1779, 1647, 1623, 1638],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4731, 0.5797, 0.6039,  ..., 0.5830, 0.3864, 0.2962],
          [0.4856, 0.3875, 0.5002,  ..., 0.5633, 0.5740, 0.4388],
          [0.6260, 0.5498, 0.4833,  ..., 0.6173, 0.4073, 0.6002],
          [0.6404, 0.4780, 0.5489,  ..., 0.5347, 0.4295, 0.4222]],

         [[0.5155, 0.5222, 0.4653,  ..., 0.3752, 0.3882, 0.5419],
          [0.4015, 0.6190, 0.5797,  ..., 0.5160, 0.3477, 0.2381],
          [0.5603, 0.5897, 0.4352,  ..., 0.4285, 0.5812, 0.5714],
          [0.4086, 0.4874, 0.4719,  ..., 0.3725, 0.5467, 0.5824]],

         [[0.6359, 0.5406, 0.4120,  ..., 0.4976, 0.5215, 0.4953],
          [0.4733, 0.3448, 0.4861,  ..., 0.5281, 0.6053, 0.4619],
          [0.4187, 0.4636, 0.6100,  ..., 0.5226, 0.6039, 0.5124],
          [0.5831, 0.5198, 0.3772,  ..., 0.5100, 0.4707, 0.3096]],

         ...,

         [[0.3968, 0.5774, 0.5217,  ..., 0.4851, 0.4477, 0.4540],
          [0.3688, 0.6268, 0.5431,  ..., 0.4428, 0.5167, 0.4736],
          [0.4541, 0.5196, 0.4731,  ..., 0.5190, 0.5675, 0.5866],
          [0.2964, 0.4476, 0.5907,  ..., 0.5384, 0.4463, 0.5941]],

         [[0.5201, 0.5659, 0.4475,  ..., 0.3599, 0.3961, 0.5238],
          [0.4386, 0.4348, 0.5552,  ..., 0.4911, 0.5251, 0.6053],
          [0.4297, 0.5085, 0.6793,  ..., 0.4133, 0.3329, 0.4003],
          [0.3996, 0.6448, 0.4818,  ..., 0.5317, 0.5460, 0.5220]],

         [[0.4936, 0.3838, 0.5916,  ..., 0.4318, 0.5460, 0.5313],
          [0.4783, 0.5007, 0.4252,  ..., 0.5311, 0.5171, 0.4709],
          [0.4348, 0.3627, 0.4714,  ..., 0.3680, 0.4021, 0.3858],
          [0.4030, 0.5944, 0.4779,  ..., 0.5901, 0.7200, 0.4103]]],


        [[[0.5990, 0.6412, 0.4818,  ..., 0.4053, 0.5007, 0.4051],
          [0.3556, 0.4425, 0.5559,  ..., 0.3890, 0.4271, 0.5518],
          [0.6008, 0.5340, 0.4357,  ..., 0.5069, 0.4624, 0.5286],
          [0.4063, 0.4464, 0.5997,  ..., 0.4744, 0.3574, 0.5293]],

         [[0.6758, 0.6896, 0.5453,  ..., 0.5406, 0.4639, 0.5552],
          [0.5636, 0.4675, 0.4656,  ..., 0.3964, 0.4482, 0.5828],
          [0.4063, 0.6394, 0.4210,  ..., 0.5271, 0.5912, 0.3853],
          [0.5512, 0.7534, 0.5978,  ..., 0.6339, 0.3655, 0.4417]],

         [[0.4326, 0.5544, 0.4153,  ..., 0.4100, 0.5453, 0.5805],
          [0.5164, 0.5573, 0.4054,  ..., 0.6159, 0.5689, 0.5819],
          [0.3620, 0.5106, 0.6887,  ..., 0.4476, 0.4494, 0.4963],
          [0.4365, 0.3663, 0.5178,  ..., 0.5877, 0.5128, 0.5681]],

         ...,

         [[0.4880, 0.5396, 0.5120,  ..., 0.4793, 0.5864, 0.3551],
          [0.6850, 0.5977, 0.4219,  ..., 0.5585, 0.5537, 0.4270],
          [0.5005, 0.3591, 0.5545,  ..., 0.5963, 0.5835, 0.4939],
          [0.4101, 0.4794, 0.4172,  ..., 0.4726, 0.5559, 0.4719]],

         [[0.4887, 0.5109, 0.4896,  ..., 0.4609, 0.4003, 0.5028],
          [0.5769, 0.5442, 0.5474,  ..., 0.3626, 0.5146, 0.5960],
          [0.4163, 0.4358, 0.5138,  ..., 0.5244, 0.6016, 0.5547],
          [0.4418, 0.6084, 0.4305,  ..., 0.5006, 0.5754, 0.5974]],

         [[0.6816, 0.3273, 0.3800,  ..., 0.5772, 0.4181, 0.4246],
          [0.6395, 0.5258, 0.4670,  ..., 0.4879, 0.3998, 0.5681],
          [0.4865, 0.4944, 0.4233,  ..., 0.5571, 0.4305, 0.5029],
          [0.7068, 0.4563, 0.5254,  ..., 0.4119, 0.3947, 0.3207]]]],
       device='cuda:0')
tensor([[[[0.4841, 0.5827, 0.5969,  ..., 0.5780, 0.3794, 0.3032],
          [0.4966, 0.3905, 0.4932,  ..., 0.5583, 0.5670, 0.4458],
          [0.6370, 0.5528, 0.4763,  ..., 0.6123, 0.4003, 0.6072],
          [0.6514, 0.4810, 0.5419,  ..., 0.5297, 0.4225, 0.4292]],

         [[0.5265, 0.5252, 0.4583,  ..., 0.3702, 0.3812, 0.5489],
          [0.4125, 0.6220, 0.5727,  ..., 0.5110, 0.3407, 0.2451],
          [0.5713, 0.5927, 0.4282,  ..., 0.4235, 0.5742, 0.5784],
          [0.4196, 0.4904, 0.4649,  ..., 0.3675, 0.5397, 0.5894]],

         [[0.6469, 0.5436, 0.4050,  ..., 0.4926, 0.5145, 0.5023],
          [0.4843, 0.3478, 0.4791,  ..., 0.5231, 0.5983, 0.4689],
          [0.4297, 0.4666, 0.6030,  ..., 0.5176, 0.5969, 0.5194],
          [0.5941, 0.5228, 0.3702,  ..., 0.5050, 0.4637, 0.3166]],

         ...,

         [[0.4078, 0.5804, 0.5147,  ..., 0.4801, 0.4407, 0.4610],
          [0.3798, 0.6298, 0.5361,  ..., 0.4378, 0.5097, 0.4806],
          [0.4651, 0.5226, 0.4661,  ..., 0.5140, 0.5605, 0.5936],
          [0.3074, 0.4506, 0.5837,  ..., 0.5334, 0.4393, 0.6011]],

         [[0.5311, 0.5689, 0.4405,  ..., 0.3549, 0.3891, 0.5308],
          [0.4496, 0.4378, 0.5482,  ..., 0.4861, 0.5181, 0.6123],
          [0.4407, 0.5115, 0.6723,  ..., 0.4083, 0.3259, 0.4073],
          [0.4106, 0.6478, 0.4748,  ..., 0.5267, 0.5390, 0.5290]],

         [[0.5046, 0.3868, 0.5846,  ..., 0.4268, 0.5390, 0.5383],
          [0.4893, 0.5037, 0.4182,  ..., 0.5261, 0.5101, 0.4779],
          [0.4458, 0.3657, 0.4644,  ..., 0.3630, 0.3951, 0.3928],
          [0.4140, 0.5974, 0.4709,  ..., 0.5851, 0.7130, 0.4173]]],


        [[[0.6100, 0.6442, 0.4748,  ..., 0.4003, 0.4937, 0.4121],
          [0.3666, 0.4455, 0.5489,  ..., 0.3840, 0.4201, 0.5588],
          [0.6118, 0.5370, 0.4287,  ..., 0.5019, 0.4554, 0.5356],
          [0.4173, 0.4494, 0.5927,  ..., 0.4694, 0.3504, 0.5363]],

         [[0.6868, 0.6926, 0.5383,  ..., 0.5356, 0.4569, 0.5622],
          [0.5746, 0.4705, 0.4586,  ..., 0.3914, 0.4412, 0.5898],
          [0.4173, 0.6424, 0.4140,  ..., 0.5221, 0.5842, 0.3923],
          [0.5622, 0.7564, 0.5908,  ..., 0.6289, 0.3585, 0.4487]],

         [[0.4436, 0.5574, 0.4083,  ..., 0.4050, 0.5383, 0.5875],
          [0.5274, 0.5603, 0.3984,  ..., 0.6109, 0.5619, 0.5889],
          [0.3730, 0.5136, 0.6817,  ..., 0.4426, 0.4424, 0.5033],
          [0.4475, 0.3693, 0.5108,  ..., 0.5827, 0.5058, 0.5751]],

         ...,

         [[0.4990, 0.5426, 0.5050,  ..., 0.4743, 0.5794, 0.3621],
          [0.6960, 0.6007, 0.4149,  ..., 0.5535, 0.5467, 0.4340],
          [0.5115, 0.3621, 0.5475,  ..., 0.5913, 0.5765, 0.5009],
          [0.4211, 0.4824, 0.4102,  ..., 0.4676, 0.5489, 0.4789]],

         [[0.4997, 0.5139, 0.4826,  ..., 0.4559, 0.3933, 0.5098],
          [0.5879, 0.5472, 0.5404,  ..., 0.3576, 0.5076, 0.6030],
          [0.4273, 0.4388, 0.5068,  ..., 0.5194, 0.5946, 0.5617],
          [0.4528, 0.6114, 0.4235,  ..., 0.4956, 0.5684, 0.6044]],

         [[0.6926, 0.3303, 0.3730,  ..., 0.5722, 0.4111, 0.4316],
          [0.6505, 0.5288, 0.4600,  ..., 0.4829, 0.3928, 0.5751],
          [0.4975, 0.4974, 0.4163,  ..., 0.5521, 0.4235, 0.5099],
          [0.7178, 0.4593, 0.5184,  ..., 0.4069, 0.3877, 0.3277]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0110, -0.0030,  0.0070,  0.0010,  0.0090,  0.0070, -0.0030,  0.0050,
         0.0070, -0.0070], device='cuda:0')
selected experts tensor([1624, 1555, 1569, 1642, 1690, 1575, 1732, 1629, 1627, 1741],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4270, 0.4891, 0.4681,  ..., 0.5952, 0.4995, 0.5958],
          [0.5030, 0.3621, 0.6300,  ..., 0.2639, 0.6264, 0.3115],
          [0.4204, 0.3922, 0.4643,  ..., 0.5422, 0.5724, 0.3090],
          [0.4867, 0.6334, 0.5105,  ..., 0.4099, 0.6144, 0.6215]],

         [[0.5416, 0.5599, 0.6499,  ..., 0.4483, 0.5030, 0.5631],
          [0.5356, 0.4928, 0.4317,  ..., 0.3508, 0.4113, 0.4375],
          [0.4900, 0.6262, 0.4464,  ..., 0.4575, 0.5834, 0.5934],
          [0.6224, 0.6836, 0.6956,  ..., 0.4090, 0.4922, 0.4904]],

         [[0.5332, 0.5561, 0.5302,  ..., 0.4420, 0.6889, 0.4337],
          [0.4474, 0.6477, 0.6588,  ..., 0.3652, 0.3615, 0.4171],
          [0.5768, 0.4986, 0.3760,  ..., 0.6569, 0.5142, 0.4765],
          [0.4086, 0.4754, 0.7386,  ..., 0.5886, 0.5075, 0.5036]],

         ...,

         [[0.4648, 0.5234, 0.6328,  ..., 0.2925, 0.5192, 0.3275],
          [0.5525, 0.6162, 0.6805,  ..., 0.3616, 0.7200, 0.4227],
          [0.4342, 0.4768, 0.4480,  ..., 0.4996, 0.6658, 0.5114],
          [0.5772, 0.5409, 0.5125,  ..., 0.3366, 0.5330, 0.6808]],

         [[0.5163, 0.4288, 0.5008,  ..., 0.3428, 0.6391, 0.5259],
          [0.6765, 0.7615, 0.3463,  ..., 0.4860, 0.5161, 0.4828],
          [0.5440, 0.5656, 0.4495,  ..., 0.6096, 0.4862, 0.4534],
          [0.5176, 0.5486, 0.7055,  ..., 0.4832, 0.4657, 0.6071]],

         [[0.6168, 0.4513, 0.4974,  ..., 0.4674, 0.5538, 0.4510],
          [0.4503, 0.6079, 0.4427,  ..., 0.4773, 0.5386, 0.5425],
          [0.6145, 0.5409, 0.4845,  ..., 0.4487, 0.5293, 0.3689],
          [0.4556, 0.6343, 0.4577,  ..., 0.6027, 0.5666, 0.4573]]],


        [[[0.4886, 0.3300, 0.4599,  ..., 0.4965, 0.6167, 0.5558],
          [0.4604, 0.6144, 0.4155,  ..., 0.4778, 0.4962, 0.4946],
          [0.5081, 0.3195, 0.4691,  ..., 0.5376, 0.4442, 0.5102],
          [0.5142, 0.3495, 0.3874,  ..., 0.6055, 0.4837, 0.5420]],

         [[0.5811, 0.6406, 0.4706,  ..., 0.5639, 0.5133, 0.4643],
          [0.4975, 0.4616, 0.4308,  ..., 0.6311, 0.4279, 0.3848],
          [0.4575, 0.7016, 0.4420,  ..., 0.4807, 0.4480, 0.5729],
          [0.5782, 0.5147, 0.3856,  ..., 0.4728, 0.5473, 0.4332]],

         [[0.6816, 0.4718, 0.6605,  ..., 0.5386, 0.5555, 0.5604],
          [0.4043, 0.5008, 0.3977,  ..., 0.4597, 0.5217, 0.3780],
          [0.3536, 0.3880, 0.4878,  ..., 0.6041, 0.4464, 0.4404],
          [0.4294, 0.4647, 0.5038,  ..., 0.6248, 0.4701, 0.6859]],

         ...,

         [[0.4633, 0.4686, 0.5628,  ..., 0.5610, 0.5657, 0.5310],
          [0.4773, 0.5389, 0.2750,  ..., 0.4806, 0.6300, 0.5103],
          [0.6687, 0.4336, 0.6111,  ..., 0.5199, 0.4810, 0.5396],
          [0.4660, 0.4506, 0.5317,  ..., 0.5167, 0.5334, 0.5135]],

         [[0.5645, 0.6803, 0.4113,  ..., 0.5734, 0.5060, 0.5165],
          [0.4878, 0.6361, 0.3819,  ..., 0.6239, 0.5485, 0.5498],
          [0.5038, 0.3486, 0.6172,  ..., 0.4808, 0.6300, 0.3207],
          [0.4375, 0.2917, 0.4864,  ..., 0.4839, 0.5381, 0.5645]],

         [[0.4431, 0.5435, 0.5268,  ..., 0.3855, 0.7120, 0.4648],
          [0.4658, 0.4087, 0.3732,  ..., 0.2474, 0.5541, 0.4621],
          [0.4718, 0.4973, 0.4879,  ..., 0.4108, 0.4146, 0.5105],
          [0.4815, 0.4399, 0.3935,  ..., 0.6142, 0.5069, 0.4380]]]],
       device='cuda:0')/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([[[[0.4220, 0.4981, 0.4651,  ..., 0.6002, 0.4965, 0.5908],
          [0.4980, 0.3711, 0.6270,  ..., 0.2689, 0.6234, 0.3065],
          [0.4154, 0.4012, 0.4613,  ..., 0.5472, 0.5694, 0.3040],
          [0.4817, 0.6424, 0.5075,  ..., 0.4149, 0.6114, 0.6165]],

         [[0.5366, 0.5689, 0.6469,  ..., 0.4533, 0.5000, 0.5581],
          [0.5306, 0.5018, 0.4287,  ..., 0.3558, 0.4083, 0.4325],
          [0.4850, 0.6352, 0.4434,  ..., 0.4625, 0.5804, 0.5884],
          [0.6174, 0.6926, 0.6926,  ..., 0.4140, 0.4892, 0.4854]],

         [[0.5282, 0.5651, 0.5272,  ..., 0.4470, 0.6859, 0.4287],
          [0.4424, 0.6567, 0.6558,  ..., 0.3702, 0.3585, 0.4121],
          [0.5718, 0.5076, 0.3730,  ..., 0.6619, 0.5112, 0.4715],
          [0.4036, 0.4844, 0.7356,  ..., 0.5936, 0.5045, 0.4986]],

         ...,

         [[0.4598, 0.5324, 0.6298,  ..., 0.2975, 0.5162, 0.3225],
          [0.5475, 0.6252, 0.6775,  ..., 0.3666, 0.7170, 0.4177],
          [0.4292, 0.4858, 0.4450,  ..., 0.5046, 0.6628, 0.5064],
          [0.5722, 0.5499, 0.5095,  ..., 0.3416, 0.5300, 0.6758]],

         [[0.5113, 0.4378, 0.4978,  ..., 0.3478, 0.6361, 0.5209],
          [0.6715, 0.7705, 0.3433,  ..., 0.4910, 0.5131, 0.4778],
          [0.5390, 0.5746, 0.4465,  ..., 0.6146, 0.4832, 0.4484],
          [0.5126, 0.5576, 0.7025,  ..., 0.4882, 0.4627, 0.6021]],

         [[0.6118, 0.4603, 0.4944,  ..., 0.4724, 0.5508, 0.4460],
          [0.4453, 0.6169, 0.4397,  ..., 0.4823, 0.5356, 0.5375],
          [0.6095, 0.5499, 0.4815,  ..., 0.4537, 0.5263, 0.3639],
          [0.4506, 0.6433, 0.4547,  ..., 0.6077, 0.5636, 0.4523]]],


        [[[0.4836, 0.3390, 0.4569,  ..., 0.5015, 0.6137, 0.5508],
          [0.4554, 0.6234, 0.4125,  ..., 0.4828, 0.4932, 0.4896],
          [0.5031, 0.3285, 0.4661,  ..., 0.5426, 0.4412, 0.5052],
          [0.5092, 0.3585, 0.3844,  ..., 0.6105, 0.4807, 0.5370]],

         [[0.5761, 0.6496, 0.4676,  ..., 0.5689, 0.5103, 0.4593],
          [0.4925, 0.4706, 0.4278,  ..., 0.6361, 0.4249, 0.3798],
          [0.4525, 0.7106, 0.4390,  ..., 0.4857, 0.4450, 0.5679],
          [0.5732, 0.5237, 0.3826,  ..., 0.4778, 0.5443, 0.4282]],

         [[0.6766, 0.4808, 0.6575,  ..., 0.5436, 0.5525, 0.5554],
          [0.3993, 0.5098, 0.3947,  ..., 0.4647, 0.5187, 0.3730],
          [0.3486, 0.3970, 0.4848,  ..., 0.6091, 0.4434, 0.4354],
          [0.4244, 0.4737, 0.5008,  ..., 0.6298, 0.4671, 0.6809]],

         ...,

         [[0.4583, 0.4776, 0.5598,  ..., 0.5660, 0.5627, 0.5260],
          [0.4723, 0.5479, 0.2720,  ..., 0.4856, 0.6270, 0.5053],
          [0.6637, 0.4426, 0.6081,  ..., 0.5249, 0.4780, 0.5346],
          [0.4610, 0.4596, 0.5287,  ..., 0.5217, 0.5304, 0.5085]],

         [[0.5595, 0.6893, 0.4083,  ..., 0.5784, 0.5030, 0.5115],
          [0.4828, 0.6451, 0.3789,  ..., 0.6289, 0.5455, 0.5448],
          [0.4988, 0.3576, 0.6142,  ..., 0.4858, 0.6270, 0.3157],
          [0.4325, 0.3007, 0.4834,  ..., 0.4889, 0.5351, 0.5595]],

         [[0.4381, 0.5525, 0.5238,  ..., 0.3905, 0.7090, 0.4598],
          [0.4608, 0.4177, 0.3702,  ..., 0.2524, 0.5511, 0.4571],
          [0.4668, 0.5063, 0.4849,  ..., 0.4158, 0.4116, 0.5055],
          [0.4765, 0.4489, 0.3905,  ..., 0.6192, 0.5039, 0.4330]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0050, -0.0090,  0.0030, -0.0010,  0.0030,  0.0050, -0.0030, -0.0050,
         0.0030,  0.0050], device='cuda:0')
selected experts tensor([1627, 1589, 1573, 1657, 1635, 1667, 1610, 1782, 1573, 1671],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6925, 0.4629, 0.4796,  ..., 0.5177, 0.4831, 0.5551],
          [0.5527, 0.4138, 0.4381,  ..., 0.5188, 0.4097, 0.6076],
          [0.4578, 0.3180, 0.5504,  ..., 0.4864, 0.6531, 0.5936],
          [0.5421, 0.4595, 0.5750,  ..., 0.3406, 0.4644, 0.4007]],

         [[0.6069, 0.3343, 0.6110,  ..., 0.5097, 0.4420, 0.5847],
          [0.4467, 0.5764, 0.4814,  ..., 0.4105, 0.4661, 0.4705],
          [0.5314, 0.4167, 0.4605,  ..., 0.4847, 0.5429, 0.5409],
          [0.4302, 0.4653, 0.4819,  ..., 0.5304, 0.3929, 0.3829]],

         [[0.4556, 0.5607, 0.4008,  ..., 0.4385, 0.4949, 0.4368],
          [0.3694, 0.5192, 0.5443,  ..., 0.4462, 0.3800, 0.4226],
          [0.5765, 0.4258, 0.4829,  ..., 0.5754, 0.4319, 0.6145],
          [0.4799, 0.4508, 0.6540,  ..., 0.4237, 0.6147, 0.5387]],

         ...,

         [[0.3557, 0.5159, 0.3540,  ..., 0.4999, 0.3204, 0.5307],
          [0.4163, 0.4510, 0.3994,  ..., 0.5128, 0.6495, 0.5303],
          [0.6925, 0.3973, 0.5121,  ..., 0.6113, 0.4238, 0.5295],
          [0.5331, 0.5236, 0.4319,  ..., 0.5293, 0.4149, 0.5407]],

         [[0.4336, 0.5554, 0.4167,  ..., 0.4114, 0.5465, 0.5804],
          [0.5174, 0.5583, 0.4059,  ..., 0.6174, 0.5699, 0.5809],
          [0.3620, 0.5117, 0.6897,  ..., 0.4486, 0.4506, 0.4951],
          [0.4375, 0.3673, 0.5189,  ..., 0.5887, 0.5138, 0.5671]],

         [[0.6069, 0.4782, 0.5955,  ..., 0.5797, 0.6049, 0.4624],
          [0.5469, 0.5607, 0.5822,  ..., 0.4937, 0.3549, 0.5964],
          [0.3814, 0.6359, 0.4898,  ..., 0.4484, 0.5917, 0.4354],
          [0.3758, 0.4522, 0.5329,  ..., 0.5174, 0.4888, 0.4465]]],


        [[[0.4589, 0.4200, 0.3791,  ..., 0.4204, 0.3846, 0.4036],
          [0.5536, 0.6136, 0.5270,  ..., 0.5423, 0.5397, 0.4591],
          [0.4101, 0.4631, 0.4656,  ..., 0.4530, 0.5098, 0.4236],
          [0.6288, 0.4472, 0.6939,  ..., 0.5356, 0.4632, 0.4699]],

         [[0.6785, 0.6940, 0.5460,  ..., 0.5435, 0.4649, 0.5551],
          [0.5646, 0.4684, 0.4663,  ..., 0.3974, 0.4492, 0.5818],
          [0.4073, 0.6404, 0.4220,  ..., 0.5281, 0.5922, 0.3843],
          [0.5522, 0.7544, 0.5988,  ..., 0.6349, 0.3665, 0.4407]],

         [[0.6216, 0.5587, 0.4825,  ..., 0.4544, 0.4003, 0.3695],
          [0.4445, 0.3283, 0.6981,  ..., 0.3771, 0.5150, 0.5709],
          [0.4305, 0.5812, 0.5003,  ..., 0.4600, 0.4485, 0.5470],
          [0.3698, 0.6112, 0.4027,  ..., 0.4811, 0.4884, 0.4193]],

         ...,

         [[0.6369, 0.5418, 0.4125,  ..., 0.4986, 0.5227, 0.4941],
          [0.4741, 0.3458, 0.4873,  ..., 0.5291, 0.6063, 0.4610],
          [0.4197, 0.4646, 0.6105,  ..., 0.5234, 0.6049, 0.5115],
          [0.5841, 0.5209, 0.3782,  ..., 0.5110, 0.4714, 0.3094]],

         [[0.3342, 0.5736, 0.3154,  ..., 0.4323, 0.4807, 0.3853],
          [0.6037, 0.4134, 0.5822,  ..., 0.5163, 0.4540, 0.4140],
          [0.6143, 0.3440, 0.5864,  ..., 0.6493, 0.4625, 0.4441],
          [0.5746, 0.4215, 0.6965,  ..., 0.6618, 0.4253, 0.5443]],

         [[0.5661, 0.4788, 0.5465,  ..., 0.4568, 0.4910, 0.4839],
          [0.4073, 0.4708, 0.5217,  ..., 0.4682, 0.5687, 0.4414],
          [0.6768, 0.5299, 0.4557,  ..., 0.5711, 0.3656, 0.4972],
          [0.5902, 0.6241, 0.3426,  ..., 0.4809, 0.5480, 0.4780]]]],
       device='cuda:0')
tensor([[[[0.7025, 0.4649, 0.4716,  ..., 0.5117, 0.4751, 0.5631],
          [0.5627, 0.4158, 0.4301,  ..., 0.5128, 0.4017, 0.6156],
          [0.4678, 0.3200, 0.5424,  ..., 0.4804, 0.6451, 0.6016],
          [0.5521, 0.4615, 0.5670,  ..., 0.3346, 0.4564, 0.4087]],

         [[0.6169, 0.3363, 0.6030,  ..., 0.5037, 0.4340, 0.5927],
          [0.4567, 0.5784, 0.4734,  ..., 0.4045, 0.4581, 0.4785],
          [0.5414, 0.4187, 0.4525,  ..., 0.4787, 0.5349, 0.5489],
          [0.4402, 0.4673, 0.4739,  ..., 0.5244, 0.3849, 0.3909]],

         [[0.4656, 0.5627, 0.3928,  ..., 0.4325, 0.4869, 0.4448],
          [0.3794, 0.5212, 0.5363,  ..., 0.4402, 0.3720, 0.4306],
          [0.5865, 0.4278, 0.4749,  ..., 0.5694, 0.4239, 0.6225],
          [0.4899, 0.4528, 0.6460,  ..., 0.4177, 0.6067, 0.5467]],

         ...,

         [[0.3657, 0.5179, 0.3460,  ..., 0.4939, 0.3124, 0.5387],
          [0.4263, 0.4530, 0.3914,  ..., 0.5068, 0.6415, 0.5383],
          [0.7025, 0.3993, 0.5041,  ..., 0.6053, 0.4158, 0.5375],
          [0.5431, 0.5256, 0.4239,  ..., 0.5233, 0.4069, 0.5487]],

         [[0.4436, 0.5574, 0.4087,  ..., 0.4054, 0.5385, 0.5884],
          [0.5274, 0.5603, 0.3979,  ..., 0.6114, 0.5619, 0.5889],
          [0.3720, 0.5137, 0.6817,  ..., 0.4426, 0.4426, 0.5031],
          [0.4475, 0.3693, 0.5109,  ..., 0.5827, 0.5058, 0.5751]],

         [[0.6169, 0.4802, 0.5875,  ..., 0.5737, 0.5969, 0.4704],
          [0.5569, 0.5627, 0.5742,  ..., 0.4877, 0.3469, 0.6044],
          [0.3914, 0.6379, 0.4818,  ..., 0.4424, 0.5837, 0.4434],
          [0.3858, 0.4542, 0.5249,  ..., 0.5114, 0.4808, 0.4545]]],


        [[[0.4689, 0.4220, 0.3711,  ..., 0.4144, 0.3766, 0.4116],
          [0.5636, 0.6156, 0.5190,  ..., 0.5363, 0.5317, 0.4671],
          [0.4201, 0.4651, 0.4576,  ..., 0.4470, 0.5018, 0.4316],
          [0.6388, 0.4492, 0.6859,  ..., 0.5296, 0.4552, 0.4779]],

         [[0.6885, 0.6960, 0.5380,  ..., 0.5375, 0.4569, 0.5631],
          [0.5746, 0.4704, 0.4583,  ..., 0.3914, 0.4412, 0.5898],
          [0.4173, 0.6424, 0.4140,  ..., 0.5221, 0.5842, 0.3923],
          [0.5622, 0.7564, 0.5908,  ..., 0.6289, 0.3585, 0.4487]],

         [[0.6316, 0.5607, 0.4745,  ..., 0.4484, 0.3923, 0.3775],
          [0.4545, 0.3303, 0.6901,  ..., 0.3711, 0.5070, 0.5789],
          [0.4405, 0.5832, 0.4923,  ..., 0.4540, 0.4405, 0.5550],
          [0.3798, 0.6132, 0.3947,  ..., 0.4751, 0.4804, 0.4273]],

         ...,

         [[0.6469, 0.5438, 0.4045,  ..., 0.4926, 0.5147, 0.5021],
          [0.4841, 0.3478, 0.4793,  ..., 0.5231, 0.5983, 0.4690],
          [0.4297, 0.4666, 0.6025,  ..., 0.5174, 0.5969, 0.5195],
          [0.5941, 0.5229, 0.3702,  ..., 0.5050, 0.4634, 0.3174]],

         [[0.3442, 0.5756, 0.3074,  ..., 0.4263, 0.4727, 0.3933],
          [0.6137, 0.4154, 0.5742,  ..., 0.5103, 0.4460, 0.4220],
          [0.6243, 0.3460, 0.5784,  ..., 0.6433, 0.4545, 0.4521],
          [0.5846, 0.4235, 0.6885,  ..., 0.6558, 0.4173, 0.5523]],

         [[0.5761, 0.4808, 0.5385,  ..., 0.4508, 0.4830, 0.4919],
          [0.4173, 0.4728, 0.5137,  ..., 0.4622, 0.5607, 0.4494],
          [0.6868, 0.5319, 0.4477,  ..., 0.5651, 0.3576, 0.5052],
          [0.6002, 0.6261, 0.3346,  ..., 0.4749, 0.5400, 0.4860]]]],
       device='cuda:0', requires_grad=True)
tensor([-1.0000e-02, -2.0000e-03,  8.0000e-03, -2.3283e-10,  8.0000e-03,
         8.0000e-03, -4.0000e-03,  6.0000e-03,  8.0000e-03, -8.0000e-03],
       device='cuda:0')
selected experts tensor([1758, 1595, 1741, 1506, 1668, 1723, 1655, 1606, 1636, 1496],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5134, 0.4462, 0.4539,  ..., 0.4917, 0.3922, 0.5015],
          [0.5787, 0.5728, 0.6763,  ..., 0.3845, 0.4327, 0.5179],
          [0.5484, 0.5457, 0.4890,  ..., 0.3966, 0.5034, 0.5172],
          [0.3389, 0.5204, 0.4440,  ..., 0.6933, 0.4638, 0.5344]],

         [[0.4053, 0.5908, 0.4839,  ..., 0.3597, 0.6112, 0.5452],
          [0.6220, 0.5851, 0.4735,  ..., 0.5662, 0.4660, 0.4404],
          [0.5675, 0.4621, 0.4052,  ..., 0.5335, 0.4109, 0.5033],
          [0.5387, 0.4083, 0.3214,  ..., 0.4853, 0.4469, 0.4866]],

         [[0.3780, 0.5614, 0.5844,  ..., 0.5279, 0.4294, 0.5036],
          [0.4376, 0.5662, 0.5218,  ..., 0.5562, 0.4803, 0.7265],
          [0.3476, 0.5361, 0.5777,  ..., 0.6105, 0.6491, 0.5719],
          [0.4581, 0.4322, 0.5676,  ..., 0.4051, 0.3788, 0.4919]],

         ...,

         [[0.5835, 0.6398, 0.4834,  ..., 0.5172, 0.6168, 0.3779],
          [0.4614, 0.5633, 0.6037,  ..., 0.6409, 0.5440, 0.6177],
          [0.5778, 0.4472, 0.4462,  ..., 0.4241, 0.5469, 0.4899],
          [0.6048, 0.4416, 0.4370,  ..., 0.6908, 0.5403, 0.4066]],

         [[0.6809, 0.4730, 0.6624,  ..., 0.5383, 0.5568, 0.5604],
          [0.4058, 0.5019, 0.3987,  ..., 0.4587, 0.5225, 0.3770],
          [0.3546, 0.3890, 0.4889,  ..., 0.6031, 0.4474, 0.4389],
          [0.4304, 0.4658, 0.5049,  ..., 0.6238, 0.4713, 0.6849]],

         [[0.4223, 0.4569, 0.5503,  ..., 0.5724, 0.5381, 0.5169],
          [0.5624, 0.5657, 0.4750,  ..., 0.3444, 0.5471, 0.4611],
          [0.6330, 0.4545, 0.5158,  ..., 0.6891, 0.4445, 0.5762],
          [0.5691, 0.4598, 0.5715,  ..., 0.6012, 0.4019, 0.4563]]],


        [[[0.5464, 0.4766, 0.5111,  ..., 0.5463, 0.4797, 0.4970],
          [0.5435, 0.3998, 0.4413,  ..., 0.5212, 0.5786, 0.4015],
          [0.6006, 0.4876, 0.4918,  ..., 0.5937, 0.5393, 0.4653],
          [0.5854, 0.5152, 0.5915,  ..., 0.4895, 0.6265, 0.4532]],

         [[0.5811, 0.6425, 0.4716,  ..., 0.5624, 0.5158, 0.4633],
          [0.4985, 0.4627, 0.4318,  ..., 0.6301, 0.4289, 0.3843],
          [0.4585, 0.7034, 0.4430,  ..., 0.4796, 0.4493, 0.5719],
          [0.5792, 0.5157, 0.3871,  ..., 0.4719, 0.5486, 0.4322]],

         [[0.5284, 0.5378, 0.3395,  ..., 0.4051, 0.4066, 0.4536],
          [0.5537, 0.5556, 0.4445,  ..., 0.5354, 0.4616, 0.4132],
          [0.5072, 0.4756, 0.4418,  ..., 0.4487, 0.5715, 0.4927],
          [0.4973, 0.5491, 0.4961,  ..., 0.3985, 0.5420, 0.5786]],

         ...,

         [[0.5344, 0.5571, 0.5323,  ..., 0.4412, 0.6891, 0.4327],
          [0.4484, 0.6487, 0.6598,  ..., 0.3642, 0.3625, 0.4161],
          [0.5778, 0.4997, 0.3770,  ..., 0.6559, 0.5153, 0.4755],
          [0.4096, 0.4764, 0.7412,  ..., 0.5876, 0.5083, 0.5025]],

         [[0.4448, 0.3686, 0.3996,  ..., 0.6480, 0.4337, 0.6527],
          [0.5646, 0.4715, 0.4619,  ..., 0.4759, 0.4409, 0.5235],
          [0.7416, 0.3774, 0.6135,  ..., 0.5143, 0.5124, 0.3987],
          [0.4129, 0.3532, 0.3571,  ..., 0.3780, 0.5729, 0.5196]],

         [[0.5445, 0.5913, 0.4892,  ..., 0.5038, 0.6473, 0.4773],
          [0.5585, 0.4012, 0.4099,  ..., 0.3752, 0.6196, 0.4213],
          [0.4901, 0.3640, 0.4389,  ..., 0.4973, 0.5305, 0.4910],
          [0.4228, 0.5421, 0.6832,  ..., 0.3489, 0.6966, 0.4859]]]],
       device='cuda:0')
tensor([[[[0.5074, 0.4542, 0.4499,  ..., 0.4977, 0.3882, 0.4975],
          [0.5727, 0.5808, 0.6723,  ..., 0.3905, 0.4287, 0.5139],
          [0.5424, 0.5537, 0.4850,  ..., 0.4026, 0.4994, 0.5132],
          [0.3329, 0.5284, 0.4400,  ..., 0.6993, 0.4598, 0.5304]],

         [[0.3993, 0.5988, 0.4799,  ..., 0.3657, 0.6072, 0.5412],
          [0.6160, 0.5931, 0.4695,  ..., 0.5722, 0.4620, 0.4364],
          [0.5615, 0.4701, 0.4012,  ..., 0.5395, 0.4069, 0.4993],
          [0.5327, 0.4163, 0.3174,  ..., 0.4913, 0.4429, 0.4826]],

         [[0.3720, 0.5694, 0.5804,  ..., 0.5339, 0.4254, 0.4996],
          [0.4316, 0.5742, 0.5178,  ..., 0.5622, 0.4763, 0.7225],
          [0.3416, 0.5441, 0.5737,  ..., 0.6165, 0.6451, 0.5679],
          [0.4521, 0.4402, 0.5636,  ..., 0.4111, 0.3748, 0.4879]],

         ...,

         [[0.5775, 0.6478, 0.4794,  ..., 0.5232, 0.6128, 0.3739],
          [0.4554, 0.5713, 0.5997,  ..., 0.6469, 0.5400, 0.6137],
          [0.5718, 0.4552, 0.4422,  ..., 0.4301, 0.5429, 0.4859],
          [0.5988, 0.4496, 0.4330,  ..., 0.6968, 0.5363, 0.4026]],

         [[0.6749, 0.4810, 0.6584,  ..., 0.5443, 0.5528, 0.5564],
          [0.3998, 0.5099, 0.3947,  ..., 0.4647, 0.5185, 0.3730],
          [0.3486, 0.3970, 0.4849,  ..., 0.6091, 0.4434, 0.4349],
          [0.4244, 0.4738, 0.5009,  ..., 0.6298, 0.4673, 0.6809]],

         [[0.4163, 0.4649, 0.5463,  ..., 0.5784, 0.5341, 0.5129],
          [0.5564, 0.5737, 0.4710,  ..., 0.3504, 0.5431, 0.4571],
          [0.6270, 0.4625, 0.5118,  ..., 0.6951, 0.4405, 0.5722],
          [0.5631, 0.4678, 0.5675,  ..., 0.6072, 0.3979, 0.4523]]],


        [[[0.5404, 0.4846, 0.5071,  ..., 0.5523, 0.4757, 0.4930],
          [0.5375, 0.4078, 0.4373,  ..., 0.5272, 0.5746, 0.3975],
          [0.5946, 0.4956, 0.4878,  ..., 0.5997, 0.5353, 0.4613],
          [0.5794, 0.5232, 0.5875,  ..., 0.4955, 0.6225, 0.4492]],

         [[0.5751, 0.6505, 0.4676,  ..., 0.5684, 0.5118, 0.4593],
          [0.4925, 0.4707, 0.4278,  ..., 0.6361, 0.4249, 0.3803],
          [0.4525, 0.7114, 0.4390,  ..., 0.4856, 0.4453, 0.5679],
          [0.5732, 0.5237, 0.3831,  ..., 0.4779, 0.5446, 0.4282]],

         [[0.5224, 0.5458, 0.3355,  ..., 0.4111, 0.4026, 0.4496],
          [0.5477, 0.5636, 0.4405,  ..., 0.5414, 0.4576, 0.4092],
          [0.5012, 0.4836, 0.4378,  ..., 0.4547, 0.5675, 0.4887],
          [0.4913, 0.5571, 0.4921,  ..., 0.4045, 0.5380, 0.5746]],

         ...,

         [[0.5284, 0.5651, 0.5283,  ..., 0.4472, 0.6851, 0.4287],
          [0.4424, 0.6567, 0.6558,  ..., 0.3702, 0.3585, 0.4121],
          [0.5718, 0.5077, 0.3730,  ..., 0.6619, 0.5113, 0.4715],
          [0.4036, 0.4844, 0.7372,  ..., 0.5936, 0.5043, 0.4985]],

         [[0.4388, 0.3766, 0.3956,  ..., 0.6540, 0.4297, 0.6487],
          [0.5586, 0.4795, 0.4579,  ..., 0.4819, 0.4369, 0.5195],
          [0.7356, 0.3854, 0.6095,  ..., 0.5203, 0.5084, 0.3947],
          [0.4069, 0.3612, 0.3531,  ..., 0.3840, 0.5689, 0.5156]],

         [[0.5385, 0.5993, 0.4852,  ..., 0.5098, 0.6433, 0.4733],
          [0.5525, 0.4092, 0.4059,  ..., 0.3812, 0.6156, 0.4173],
          [0.4841, 0.3720, 0.4349,  ..., 0.5033, 0.5265, 0.4870],
          [0.4168, 0.5501, 0.6792,  ..., 0.3549, 0.6926, 0.4819]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0060, -0.0080,  0.0040, -0.0020,  0.0040,  0.0040, -0.0020, -0.0060,
         0.0040,  0.0040], device='cuda:0')
selected experts tensor([1798, 1424, 1435, 1860, 1650, 1635, 1878, 1588, 1678, 1438],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4901, 0.4641, 0.4993,  ..., 0.5715, 0.4241, 0.4881],
          [0.3934, 0.4637, 0.6048,  ..., 0.3401, 0.3935, 0.5396],
          [0.5049, 0.4850, 0.4578,  ..., 0.4169, 0.4232, 0.5280],
          [0.5540, 0.4994, 0.4620,  ..., 0.4707, 0.6666, 0.5601]],

         [[0.6259, 0.4751, 0.3825,  ..., 0.5242, 0.4793, 0.5747],
          [0.4823, 0.4424, 0.4931,  ..., 0.4160, 0.3449, 0.5111],
          [0.6054, 0.5142, 0.5609,  ..., 0.4763, 0.5926, 0.5700],
          [0.4935, 0.5235, 0.4412,  ..., 0.5588, 0.4989, 0.5044]],

         [[0.5755, 0.5223, 0.5407,  ..., 0.5109, 0.5330, 0.5794],
          [0.4813, 0.4734, 0.5185,  ..., 0.5240, 0.3244, 0.5827],
          [0.4372, 0.4482, 0.5523,  ..., 0.3365, 0.5512, 0.5297],
          [0.4329, 0.4501, 0.5770,  ..., 0.5706, 0.5451, 0.4835]],

         ...,

         [[0.6255, 0.4007, 0.5921,  ..., 0.3825, 0.4675, 0.5864],
          [0.3962, 0.4731, 0.4239,  ..., 0.4153, 0.4181, 0.5077],
          [0.4177, 0.4827, 0.5684,  ..., 0.5374, 0.5495, 0.6447],
          [0.5750, 0.4629, 0.4822,  ..., 0.4237, 0.6001, 0.4123]],

         [[0.4536, 0.4077, 0.5172,  ..., 0.4592, 0.4176, 0.4363],
          [0.4773, 0.5224, 0.6486,  ..., 0.4303, 0.3731, 0.4746],
          [0.4553, 0.4663, 0.5959,  ..., 0.4725, 0.3670, 0.4944],
          [0.6496, 0.4562, 0.5450,  ..., 0.5341, 0.5574, 0.5439]],

         [[0.4978, 0.4367, 0.5003,  ..., 0.5073, 0.4204, 0.5700],
          [0.5741, 0.3836, 0.4325,  ..., 0.4203, 0.4651, 0.5137],
          [0.5874, 0.6258, 0.6251,  ..., 0.3968, 0.4292, 0.3739],
          [0.5979, 0.3521, 0.3548,  ..., 0.5484, 0.4992, 0.4649]]],


        [[[0.5855, 0.4989, 0.4851,  ..., 0.5715, 0.3713, 0.4723],
          [0.4368, 0.3887, 0.4748,  ..., 0.3641, 0.3872, 0.5278],
          [0.4415, 0.5224, 0.5689,  ..., 0.4588, 0.3828, 0.5947],
          [0.5296, 0.4704, 0.4878,  ..., 0.5295, 0.5484, 0.5677]],

         [[0.6054, 0.5031, 0.4717,  ..., 0.5009, 0.4704, 0.4510],
          [0.5511, 0.4535, 0.5258,  ..., 0.4114, 0.3424, 0.5420],
          [0.4377, 0.5236, 0.6006,  ..., 0.6033, 0.6693, 0.4852],
          [0.5456, 0.5072, 0.4244,  ..., 0.6274, 0.4437, 0.5381]],

         [[0.6087, 0.4624, 0.5426,  ..., 0.5002, 0.3783, 0.4963],
          [0.4594, 0.4367, 0.5631,  ..., 0.2722, 0.4227, 0.5162],
          [0.4182, 0.4219, 0.5419,  ..., 0.4416, 0.4613, 0.6201],
          [0.4917, 0.3745, 0.5074,  ..., 0.5126, 0.6134, 0.6201]],

         ...,

         [[0.4543, 0.4569, 0.5445,  ..., 0.4751, 0.4750, 0.6308],
          [0.6017, 0.6698, 0.4443,  ..., 0.4885, 0.4311, 0.5113],
          [0.3621, 0.4465, 0.5520,  ..., 0.3185, 0.6229, 0.6499],
          [0.4914, 0.4031, 0.4540,  ..., 0.6361, 0.4914, 0.4923]],

         [[0.5606, 0.2767, 0.4739,  ..., 0.5720, 0.5091, 0.6593],
          [0.5908, 0.4700, 0.4054,  ..., 0.5846, 0.3731, 0.5515],
          [0.5652, 0.4410, 0.4649,  ..., 0.4992, 0.4890, 0.5271],
          [0.6351, 0.5158, 0.4371,  ..., 0.5312, 0.6015, 0.4923]],

         [[0.6125, 0.3425, 0.5339,  ..., 0.5048, 0.3399, 0.4701],
          [0.5932, 0.5840, 0.5698,  ..., 0.4157, 0.4423, 0.5386],
          [0.5652, 0.4762, 0.5722,  ..., 0.5489, 0.4315, 0.5167],
          [0.6700, 0.3637, 0.5621,  ..., 0.6431, 0.4052, 0.5324]]]],
       device='cuda:0')
tensor([[[[0.4811, 0.4571, 0.5003,  ..., 0.6025, 0.3891, 0.5131],
          [0.3844, 0.4567, 0.6058,  ..., 0.3711, 0.3585, 0.5646],
          [0.4959, 0.4780, 0.4588,  ..., 0.4479, 0.3882, 0.5530],
          [0.5450, 0.4924, 0.4630,  ..., 0.5017, 0.6316, 0.5851]],

         [[0.6169, 0.4681, 0.3835,  ..., 0.5552, 0.4443, 0.5997],
          [0.4733, 0.4354, 0.4941,  ..., 0.4470, 0.3099, 0.5361],
          [0.5964, 0.5072, 0.5619,  ..., 0.5073, 0.5576, 0.5950],
          [0.4845, 0.5165, 0.4422,  ..., 0.5898, 0.4639, 0.5294]],

         [[0.5665, 0.5153, 0.5417,  ..., 0.5419, 0.4980, 0.6044],
          [0.4723, 0.4664, 0.5195,  ..., 0.5550, 0.2894, 0.6077],
          [0.4282, 0.4412, 0.5533,  ..., 0.3675, 0.5162, 0.5547],
          [0.4239, 0.4431, 0.5780,  ..., 0.6016, 0.5101, 0.5085]],

         ...,

         [[0.6165, 0.3937, 0.5931,  ..., 0.4135, 0.4325, 0.6114],
          [0.3872, 0.4661, 0.4249,  ..., 0.4463, 0.3831, 0.5327],
          [0.4087, 0.4757, 0.5694,  ..., 0.5684, 0.5145, 0.6697],
          [0.5660, 0.4559, 0.4832,  ..., 0.4547, 0.5651, 0.4373]],

         [[0.4446, 0.4007, 0.5182,  ..., 0.4902, 0.3826, 0.4613],
          [0.4683, 0.5154, 0.6496,  ..., 0.4613, 0.3381, 0.4996],
          [0.4463, 0.4593, 0.5969,  ..., 0.5035, 0.3320, 0.5194],
          [0.6406, 0.4492, 0.5460,  ..., 0.5651, 0.5224, 0.5689]],

         [[0.4888, 0.4297, 0.5013,  ..., 0.5383, 0.3854, 0.5950],
          [0.5651, 0.3766, 0.4335,  ..., 0.4513, 0.4301, 0.5387],
          [0.5784, 0.6188, 0.6261,  ..., 0.4278, 0.3942, 0.3989],
          [0.5889, 0.3451, 0.3558,  ..., 0.5794, 0.4642, 0.4899]]],


        [[[0.5765, 0.4919, 0.4861,  ..., 0.6025, 0.3363, 0.4973],
          [0.4278, 0.3817, 0.4758,  ..., 0.3951, 0.3522, 0.5528],
          [0.4325, 0.5154, 0.5699,  ..., 0.4898, 0.3478, 0.6197],
          [0.5206, 0.4634, 0.4888,  ..., 0.5605, 0.5134, 0.5927]],

         [[0.5964, 0.4961, 0.4727,  ..., 0.5319, 0.4354, 0.4760],
          [0.5421, 0.4465, 0.5268,  ..., 0.4424, 0.3074, 0.5670],
          [0.4287, 0.5166, 0.6016,  ..., 0.6343, 0.6343, 0.5102],
          [0.5366, 0.5002, 0.4254,  ..., 0.6584, 0.4087, 0.5631]],

         [[0.5997, 0.4554, 0.5436,  ..., 0.5312, 0.3433, 0.5213],
          [0.4504, 0.4297, 0.5641,  ..., 0.3032, 0.3877, 0.5412],
          [0.4092, 0.4149, 0.5429,  ..., 0.4726, 0.4263, 0.6451],
          [0.4827, 0.3675, 0.5084,  ..., 0.5436, 0.5784, 0.6451]],

         ...,

         [[0.4453, 0.4499, 0.5455,  ..., 0.5061, 0.4400, 0.6558],
          [0.5927, 0.6628, 0.4453,  ..., 0.5195, 0.3961, 0.5363],
          [0.3531, 0.4395, 0.5530,  ..., 0.3495, 0.5879, 0.6749],
          [0.4824, 0.3961, 0.4550,  ..., 0.6671, 0.4564, 0.5173]],

         [[0.5516, 0.2697, 0.4749,  ..., 0.6030, 0.4741, 0.6843],
          [0.5818, 0.4630, 0.4064,  ..., 0.6156, 0.3381, 0.5765],
          [0.5562, 0.4340, 0.4659,  ..., 0.5302, 0.4540, 0.5521],
          [0.6261, 0.5088, 0.4381,  ..., 0.5622, 0.5665, 0.5173]],

         [[0.6035, 0.3355, 0.5349,  ..., 0.5358, 0.3049, 0.4951],
          [0.5842, 0.5770, 0.5708,  ..., 0.4467, 0.4073, 0.5636],
          [0.5562, 0.4692, 0.5732,  ..., 0.5799, 0.3965, 0.5417],
          [0.6610, 0.3567, 0.5631,  ..., 0.6741, 0.3702, 0.5574]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0090,  0.0070, -0.0010,  0.0010,  0.0050, -0.0230,  0.0150, -0.0310,
         0.0350, -0.0250], device='cuda:0')
selected experts tensor([1542, 1149, 1875, 1751, 1535, 1962, 1796, 1462, 1036, 2276],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.6175, 0.6426, 0.6010,  ..., 0.5649, 0.4987, 0.5466],
          [0.5987, 0.4033, 0.4256,  ..., 0.5845, 0.5215, 0.5983],
          [0.3907, 0.5277, 0.5311,  ..., 0.4524, 0.4592, 0.6195],
          [0.4683, 0.5275, 0.4738,  ..., 0.6413, 0.4983, 0.4992]],

         [[0.5666, 0.5332, 0.6236,  ..., 0.3568, 0.5408, 0.3872],
          [0.5407, 0.6145, 0.4588,  ..., 0.4961, 0.5529, 0.5649],
          [0.4485, 0.4445, 0.4204,  ..., 0.5298, 0.5076, 0.5840],
          [0.5825, 0.5737, 0.5285,  ..., 0.4664, 0.5423, 0.5124]],

         [[0.4069, 0.6561, 0.6655,  ..., 0.4192, 0.4972, 0.4812],
          [0.5796, 0.4938, 0.4228,  ..., 0.5122, 0.5758, 0.6068],
          [0.4784, 0.4061, 0.4504,  ..., 0.4756, 0.4677, 0.4949],
          [0.6310, 0.6256, 0.5834,  ..., 0.5608, 0.4520, 0.4405]],

         ...,

         [[0.5150, 0.5110, 0.5150,  ..., 0.5170, 0.5050, 0.5170],
          [0.5150, 0.5110, 0.5150,  ..., 0.5170, 0.5050, 0.5170],
          [0.5150, 0.5110, 0.5150,  ..., 0.5170, 0.5050, 0.5170],
          [0.5150, 0.5110, 0.5150,  ..., 0.5170, 0.5050, 0.5170]],

         [[0.5150, 0.5110, 0.5150,  ..., 0.5170, 0.5050, 0.5170],
          [0.5150, 0.5110, 0.5150,  ..., 0.5170, 0.5050, 0.5170],
          [0.5150, 0.5110, 0.5150,  ..., 0.5170, 0.5050, 0.5170],
          [0.5150, 0.5110, 0.5150,  ..., 0.5170, 0.5050, 0.5170]],

         [[0.5150, 0.5110, 0.5150,  ..., 0.5170, 0.5050, 0.5170],
          [0.5150, 0.5110, 0.5150,  ..., 0.5170, 0.5050, 0.5170],
          [0.5150, 0.5110, 0.5150,  ..., 0.5170, 0.5050, 0.5170],
          [0.5150, 0.5110, 0.5150,  ..., 0.5170, 0.5050, 0.5170]]],


        [[[0.4495, 0.5493, 0.6393,  ..., 0.4780, 0.5633, 0.5240],
          [0.4866, 0.4212, 0.3852,  ..., 0.6326, 0.5005, 0.5394],
          [0.3916, 0.4316, 0.5513,  ..., 0.4524, 0.4711, 0.3586],
          [0.5673, 0.6289, 0.5996,  ..., 0.4065, 0.4823, 0.4505]],

         [[0.4661, 0.5746, 0.6628,  ..., 0.4001, 0.4655, 0.4409],
          [0.5059, 0.5970, 0.4967,  ..., 0.6450, 0.4663, 0.3473],
          [0.5451, 0.5999, 0.4816,  ..., 0.4024, 0.6713, 0.3881],
          [0.5834, 0.6312, 0.5627,  ..., 0.3674, 0.4945, 0.6021]],

         [[0.5552, 0.4226, 0.5755,  ..., 0.5700, 0.6238, 0.5255],
          [0.6664, 0.4407, 0.4617,  ..., 0.5657, 0.5672, 0.5061],
          [0.5306, 0.5420, 0.5920,  ..., 0.4957, 0.5091, 0.5161],
          [0.4901, 0.5238, 0.5537,  ..., 0.5854, 0.4733, 0.5091]],

         ...,

         [[0.5150, 0.5110, 0.5150,  ..., 0.5170, 0.5050, 0.5170],
          [0.5150, 0.5110, 0.5150,  ..., 0.5170, 0.5050, 0.5170],
          [0.5150, 0.5110, 0.5150,  ..., 0.5170, 0.5050, 0.5170],
          [0.5150, 0.5110, 0.5150,  ..., 0.5170, 0.5050, 0.5170]],

         [[0.5150, 0.5110, 0.5150,  ..., 0.5170, 0.5050, 0.5170],
          [0.5150, 0.5110, 0.5150,  ..., 0.5170, 0.5050, 0.5170],
          [0.5150, 0.5110, 0.5150,  ..., 0.5170, 0.5050, 0.5170],
          [0.5150, 0.5110, 0.5150,  ..., 0.5170, 0.5050, 0.5170]],

         [[0.5150, 0.5110, 0.5150,  ..., 0.5170, 0.5050, 0.5170],
          [0.5150, 0.5110, 0.5150,  ..., 0.5170, 0.5050, 0.5170],
          [0.5150, 0.5110, 0.5150,  ..., 0.5170, 0.5050, 0.5170],
          [0.5150, 0.5110, 0.5150,  ..., 0.5170, 0.5050, 0.5170]]]],
       device='cuda:0')
tensor([[[[0.6025, 0.6316, 0.5860,  ..., 0.5479, 0.4937, 0.5296],
          [0.5837, 0.3923, 0.4106,  ..., 0.5675, 0.5165, 0.5813],
          [0.3757, 0.5167, 0.5161,  ..., 0.4354, 0.4542, 0.6025],
          [0.4533, 0.5165, 0.4588,  ..., 0.6243, 0.4933, 0.4822]],

         [[0.5516, 0.5222, 0.6086,  ..., 0.3398, 0.5358, 0.3702],
          [0.5257, 0.6035, 0.4438,  ..., 0.4791, 0.5479, 0.5479],
          [0.4335, 0.4335, 0.4054,  ..., 0.5128, 0.5026, 0.5670],
          [0.5675, 0.5627, 0.5135,  ..., 0.4494, 0.5373, 0.4954]],

         [[0.3919, 0.6451, 0.6505,  ..., 0.4022, 0.4922, 0.4642],
          [0.5646, 0.4828, 0.4078,  ..., 0.4952, 0.5708, 0.5898],
          [0.4634, 0.3951, 0.4354,  ..., 0.4586, 0.4627, 0.4779],
          [0.6160, 0.6146, 0.5684,  ..., 0.5438, 0.4470, 0.4235]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.4345, 0.5383, 0.6243,  ..., 0.4610, 0.5583, 0.5070],
          [0.4716, 0.4102, 0.3702,  ..., 0.6156, 0.4955, 0.5224],
          [0.3766, 0.4206, 0.5363,  ..., 0.4354, 0.4661, 0.3416],
          [0.5523, 0.6179, 0.5846,  ..., 0.3895, 0.4773, 0.4335]],

         [[0.4511, 0.5636, 0.6478,  ..., 0.3831, 0.4605, 0.4239],
          [0.4909, 0.5860, 0.4817,  ..., 0.6280, 0.4613, 0.3303],
          [0.5301, 0.5889, 0.4666,  ..., 0.3854, 0.6663, 0.3711],
          [0.5684, 0.6202, 0.5477,  ..., 0.3504, 0.4895, 0.5851]],

         [[0.5402, 0.4116, 0.5605,  ..., 0.5530, 0.6188, 0.5085],
          [0.6514, 0.4297, 0.4467,  ..., 0.5487, 0.5622, 0.4891],
          [0.5156, 0.5310, 0.5770,  ..., 0.4787, 0.5041, 0.4991],
          [0.4751, 0.5128, 0.5387,  ..., 0.5684, 0.4683, 0.4921]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([0.0150, 0.0110, 0.0150, 0.0150, 0.0150, 0.0070, 0.0110, 0.0170, 0.0050,
        0.0170], device='cuda:0')
selected experts tensor([1226, 1287, 1261,  386,  657, 1981, 1569, 1722, 1264,  935],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.3944, 0.4603, 0.5355,  ..., 0.5869, 0.5779, 0.4499],
          [0.4974, 0.3097, 0.4324,  ..., 0.4646, 0.5936, 0.4203],
          [0.4295, 0.5822, 0.6086,  ..., 0.6359, 0.5745, 0.5824],
          [0.4206, 0.4158, 0.4790,  ..., 0.6151, 0.5577, 0.5463]],

         [[0.4753, 0.5270, 0.4508,  ..., 0.4874, 0.4536, 0.4193],
          [0.4345, 0.4774, 0.5537,  ..., 0.5438, 0.6255, 0.6016],
          [0.4683, 0.5515, 0.4371,  ..., 0.5120, 0.4055, 0.5252],
          [0.5139, 0.4980, 0.6202,  ..., 0.5078, 0.5069, 0.5407]],

         [[0.4144, 0.4291, 0.3845,  ..., 0.5845, 0.4562, 0.5937],
          [0.6631, 0.4658, 0.3495,  ..., 0.3961, 0.5846, 0.6136],
          [0.4583, 0.3979, 0.4952,  ..., 0.5576, 0.6305, 0.4222],
          [0.3288, 0.4296, 0.6020,  ..., 0.4571, 0.6296, 0.5051]],

         ...,

         [[0.6359, 0.5426, 0.4106,  ..., 0.5002, 0.5243, 0.4955],
          [0.4731, 0.3468, 0.4865,  ..., 0.5301, 0.6068, 0.4622],
          [0.4187, 0.4656, 0.6100,  ..., 0.5246, 0.6064, 0.5126],
          [0.5831, 0.5219, 0.3772,  ..., 0.5119, 0.4727, 0.3096]],

         [[0.4120, 0.6095, 0.5240,  ..., 0.5341, 0.5827, 0.3733],
          [0.5736, 0.5669, 0.4758,  ..., 0.4458, 0.5396, 0.4458],
          [0.4619, 0.5163, 0.3736,  ..., 0.6212, 0.5955, 0.6523],
          [0.6377, 0.5416, 0.4919,  ..., 0.5128, 0.6541, 0.5133]],

         [[0.4610, 0.6233, 0.5072,  ..., 0.4958, 0.5485, 0.4448],
          [0.5831, 0.4964, 0.4371,  ..., 0.3868, 0.4734, 0.4715],
          [0.5593, 0.4795, 0.3727,  ..., 0.4928, 0.6397, 0.3942],
          [0.6004, 0.5175, 0.5864,  ..., 0.5392, 0.6059, 0.4740]]],


        [[[0.5108, 0.4679, 0.6067,  ..., 0.5578, 0.4620, 0.4315],
          [0.5362, 0.5227, 0.5457,  ..., 0.4859, 0.6424, 0.4504],
          [0.4566, 0.4916, 0.6503,  ..., 0.5309, 0.5630, 0.5467],
          [0.4895, 0.4196, 0.3781,  ..., 0.5869, 0.5979, 0.2744]],

         [[0.5646, 0.5836, 0.6759,  ..., 0.4508, 0.3729, 0.4865],
          [0.6956, 0.4030, 0.5864,  ..., 0.6091, 0.6727, 0.4782],
          [0.5488, 0.5609, 0.6449,  ..., 0.5578, 0.5603, 0.5695],
          [0.4601, 0.4590, 0.6262,  ..., 0.5276, 0.4835, 0.4320]],

         [[0.5113, 0.5194, 0.4157,  ..., 0.5092, 0.4301, 0.4477],
          [0.3762, 0.4569, 0.5716,  ..., 0.4367, 0.3122, 0.4422],
          [0.3368, 0.5414, 0.5156,  ..., 0.4952, 0.5401, 0.6567],
          [0.4972, 0.6441, 0.6793,  ..., 0.3781, 0.4737, 0.6435]],

         ...,

         [[0.5660, 0.5267, 0.5382,  ..., 0.5496, 0.4812, 0.4916],
          [0.4030, 0.5609, 0.4525,  ..., 0.5816, 0.4565, 0.4484],
          [0.5314, 0.4547, 0.6011,  ..., 0.6109, 0.4808, 0.5431],
          [0.5222, 0.5903, 0.5716,  ..., 0.5840, 0.3756, 0.5339]],

         [[0.5655, 0.4892, 0.6285,  ..., 0.4219, 0.4415, 0.4193],
          [0.5309, 0.5907, 0.5067,  ..., 0.4172, 0.4908, 0.4174],
          [0.5241, 0.5703, 0.5187,  ..., 0.5701, 0.5608, 0.3751],
          [0.5755, 0.5832, 0.6207,  ..., 0.3548, 0.3847, 0.4013]],

         [[0.6287, 0.4339, 0.5079,  ..., 0.4101, 0.3506, 0.4279],
          [0.4808, 0.6011, 0.4087,  ..., 0.4814, 0.4329, 0.4966],
          [0.5627, 0.4790, 0.5564,  ..., 0.4729, 0.5865, 0.4188],
          [0.5100, 0.5746, 0.4798,  ..., 0.3901, 0.5894, 0.4414]]]],
       device='cuda:0')
tensor([[[[0.4054, 0.4613, 0.5285,  ..., 0.5799, 0.5689, 0.4569],
          [0.5084, 0.3107, 0.4254,  ..., 0.4576, 0.5846, 0.4273],
          [0.4405, 0.5832, 0.6016,  ..., 0.6289, 0.5655, 0.5894],
          [0.4316, 0.4168, 0.4720,  ..., 0.6081, 0.5487, 0.5533]],

         [[0.4863, 0.5280, 0.4438,  ..., 0.4804, 0.4446, 0.4263],
          [0.4455, 0.4784, 0.5467,  ..., 0.5368, 0.6165, 0.6086],
          [0.4793, 0.5525, 0.4301,  ..., 0.5050, 0.3965, 0.5322],
          [0.5249, 0.4990, 0.6132,  ..., 0.5008, 0.4979, 0.5477]],

         [[0.4254, 0.4301, 0.3775,  ..., 0.5775, 0.4472, 0.6007],
          [0.6741, 0.4668, 0.3425,  ..., 0.3891, 0.5756, 0.6206],
          [0.4693, 0.3989, 0.4882,  ..., 0.5506, 0.6215, 0.4292],
          [0.3398, 0.4306, 0.5950,  ..., 0.4501, 0.6206, 0.5121]],

         ...,

         [[0.6469, 0.5436, 0.4036,  ..., 0.4932, 0.5153, 0.5025],
          [0.4841, 0.3478, 0.4795,  ..., 0.5231, 0.5978, 0.4692],
          [0.4297, 0.4666, 0.6030,  ..., 0.5176, 0.5974, 0.5196],
          [0.5941, 0.5229, 0.3702,  ..., 0.5049, 0.4637, 0.3166]],

         [[0.4230, 0.6105, 0.5170,  ..., 0.5271, 0.5737, 0.3803],
          [0.5846, 0.5679, 0.4688,  ..., 0.4388, 0.5306, 0.4528],
          [0.4729, 0.5173, 0.3666,  ..., 0.6142, 0.5865, 0.6593],
          [0.6487, 0.5426, 0.4849,  ..., 0.5058, 0.6451, 0.5203]],

         [[0.4720, 0.6243, 0.5002,  ..., 0.4888, 0.5395, 0.4518],
          [0.5941, 0.4974, 0.4301,  ..., 0.3798, 0.4644, 0.4785],
          [0.5703, 0.4805, 0.3657,  ..., 0.4858, 0.6307, 0.4012],
          [0.6114, 0.5185, 0.5794,  ..., 0.5322, 0.5969, 0.4810]]],


        [[[0.5218, 0.4689, 0.5997,  ..., 0.5508, 0.4530, 0.4385],
          [0.5472, 0.5237, 0.5387,  ..., 0.4789, 0.6334, 0.4574],
          [0.4676, 0.4926, 0.6433,  ..., 0.5239, 0.5540, 0.5537],
          [0.5005, 0.4206, 0.3711,  ..., 0.5799, 0.5889, 0.2814]],

         [[0.5756, 0.5846, 0.6689,  ..., 0.4438, 0.3639, 0.4935],
          [0.7066, 0.4040, 0.5794,  ..., 0.6021, 0.6637, 0.4852],
          [0.5598, 0.5619, 0.6379,  ..., 0.5508, 0.5513, 0.5765],
          [0.4711, 0.4600, 0.6192,  ..., 0.5206, 0.4745, 0.4390]],

         [[0.5223, 0.5204, 0.4087,  ..., 0.5022, 0.4211, 0.4547],
          [0.3872, 0.4579, 0.5646,  ..., 0.4297, 0.3032, 0.4492],
          [0.3478, 0.5424, 0.5086,  ..., 0.4882, 0.5311, 0.6637],
          [0.5082, 0.6451, 0.6723,  ..., 0.3711, 0.4647, 0.6505]],

         ...,

         [[0.5770, 0.5277, 0.5312,  ..., 0.5426, 0.4722, 0.4986],
          [0.4140, 0.5619, 0.4455,  ..., 0.5746, 0.4475, 0.4554],
          [0.5424, 0.4557, 0.5941,  ..., 0.6039, 0.4718, 0.5501],
          [0.5332, 0.5913, 0.5646,  ..., 0.5770, 0.3666, 0.5409]],

         [[0.5765, 0.4902, 0.6215,  ..., 0.4149, 0.4325, 0.4263],
          [0.5419, 0.5917, 0.4997,  ..., 0.4102, 0.4818, 0.4244],
          [0.5351, 0.5713, 0.5117,  ..., 0.5631, 0.5518, 0.3821],
          [0.5865, 0.5842, 0.6137,  ..., 0.3478, 0.3757, 0.4083]],

         [[0.6397, 0.4349, 0.5009,  ..., 0.4031, 0.3416, 0.4349],
          [0.4918, 0.6021, 0.4017,  ..., 0.4744, 0.4239, 0.5036],
          [0.5737, 0.4800, 0.5494,  ..., 0.4659, 0.5775, 0.4258],
          [0.5210, 0.5756, 0.4728,  ..., 0.3831, 0.5804, 0.4484]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0110, -0.0010,  0.0070,  0.0010,  0.0070,  0.0070, -0.0050,  0.0070,
         0.0090, -0.0070], device='cuda:0')
selected experts tensor([1574, 1579, 1732, 1565, 1696, 1683, 1802, 1555, 1731, 1467],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5763, 0.4732, 0.5179,  ..., 0.6595, 0.6517, 0.4428],
          [0.3698, 0.5176, 0.5571,  ..., 0.4862, 0.6856, 0.3761],
          [0.4854, 0.4564, 0.5310,  ..., 0.5155, 0.6097, 0.6000],
          [0.4488, 0.5385, 0.3536,  ..., 0.5933, 0.6093, 0.6057]],

         [[0.5044, 0.5048, 0.5573,  ..., 0.5434, 0.4526, 0.5882],
          [0.5600, 0.3224, 0.5753,  ..., 0.5792, 0.6684, 0.4699],
          [0.3945, 0.5072, 0.5287,  ..., 0.5957, 0.6667, 0.4860],
          [0.4152, 0.6048, 0.6581,  ..., 0.5985, 0.5415, 0.6572]],

         [[0.3848, 0.4241, 0.3671,  ..., 0.4037, 0.3949, 0.5260],
          [0.4967, 0.6132, 0.5546,  ..., 0.3235, 0.4640, 0.5834],
          [0.6038, 0.4017, 0.4488,  ..., 0.5555, 0.6382, 0.5665],
          [0.5149, 0.4506, 0.4285,  ..., 0.5246, 0.4198, 0.4957]],

         ...,

         [[0.5335, 0.5590, 0.5344,  ..., 0.4422, 0.6873, 0.4332],
          [0.4472, 0.6497, 0.6608,  ..., 0.3652, 0.3615, 0.4171],
          [0.5768, 0.5008, 0.3780,  ..., 0.6569, 0.5143, 0.4766],
          [0.4081, 0.4774, 0.7422,  ..., 0.5886, 0.5072, 0.5034]],

         [[0.4156, 0.5185, 0.5088,  ..., 0.5294, 0.4913, 0.6393],
          [0.5657, 0.5885, 0.5887,  ..., 0.5262, 0.5776, 0.5131],
          [0.3361, 0.6127, 0.4520,  ..., 0.5176, 0.5652, 0.5170],
          [0.5199, 0.4165, 0.4784,  ..., 0.4497, 0.5548, 0.6071]],

         [[0.4863, 0.5776, 0.4404,  ..., 0.4146, 0.4468, 0.5167],
          [0.5095, 0.5114, 0.5858,  ..., 0.5308, 0.5224, 0.3608],
          [0.6756, 0.3276, 0.4147,  ..., 0.4118, 0.3865, 0.3978],
          [0.5296, 0.5460, 0.5806,  ..., 0.5269, 0.6282, 0.3839]]],


        [[[0.4636, 0.4521, 0.3734,  ..., 0.5473, 0.4674, 0.4443],
          [0.4227, 0.4407, 0.6201,  ..., 0.5354, 0.5584, 0.5725],
          [0.4199, 0.4871, 0.6825,  ..., 0.4794, 0.4698, 0.5505],
          [0.6510, 0.5969, 0.4517,  ..., 0.3887, 0.5393, 0.4520]],

         [[0.3074, 0.4088, 0.5820,  ..., 0.7183, 0.3171, 0.6089],
          [0.6136, 0.5305, 0.6057,  ..., 0.6320, 0.3732, 0.5910],
          [0.4860, 0.4212, 0.4347,  ..., 0.6311, 0.4303, 0.6546],
          [0.5929, 0.4738, 0.5355,  ..., 0.4075, 0.3953, 0.5882]],

         [[0.3644, 0.3372, 0.5411,  ..., 0.4928, 0.6190, 0.4481],
          [0.5710, 0.5153, 0.5672,  ..., 0.5682, 0.4803, 0.5474],
          [0.6052, 0.5941, 0.4081,  ..., 0.5081, 0.4956, 0.6765],
          [0.5295, 0.6113, 0.5772,  ..., 0.4473, 0.4976, 0.5313]],

         ...,

         [[0.5045, 0.4713, 0.4532,  ..., 0.4071, 0.5966, 0.4869],
          [0.5597, 0.4431, 0.6330,  ..., 0.6266, 0.5036, 0.4294],
          [0.4949, 0.6345, 0.6164,  ..., 0.4132, 0.5565, 0.5277],
          [0.5768, 0.5014, 0.4718,  ..., 0.5820, 0.5705, 0.4375]],

         [[0.4867, 0.3079, 0.5820,  ..., 0.5754, 0.4565, 0.5112],
          [0.4337, 0.4692, 0.3353,  ..., 0.5313, 0.5170, 0.4619],
          [0.5563, 0.4844, 0.3848,  ..., 0.5379, 0.4108, 0.6103],
          [0.5068, 0.4676, 0.6293,  ..., 0.4871, 0.5560, 0.4587]],

         [[0.3563, 0.5264, 0.5944,  ..., 0.2474, 0.4751, 0.4256],
          [0.5452, 0.4564, 0.4968,  ..., 0.4550, 0.5171, 0.4237],
          [0.6052, 0.5496, 0.4578,  ..., 0.5509, 0.5471, 0.6210],
          [0.4789, 0.5691, 0.5563,  ..., 0.4570, 0.4146, 0.3335]]]],
       device='cuda:0')
tensor([[[[0.5713, 0.4802, 0.5129,  ..., 0.6645, 0.6487, 0.4378],
          [0.3648, 0.5246, 0.5521,  ..., 0.4912, 0.6826, 0.3711],
          [0.4804, 0.4634, 0.5260,  ..., 0.5205, 0.6067, 0.5950],
          [0.4438, 0.5455, 0.3486,  ..., 0.5983, 0.6063, 0.6007]],

         [[0.4994, 0.5118, 0.5523,  ..., 0.5484, 0.4496, 0.5832],
          [0.5550, 0.3294, 0.5703,  ..., 0.5842, 0.6654, 0.4649],
          [0.3895, 0.5142, 0.5237,  ..., 0.6007, 0.6637, 0.4810],
          [0.4102, 0.6118, 0.6531,  ..., 0.6035, 0.5385, 0.6522]],

         [[0.3798, 0.4311, 0.3621,  ..., 0.4087, 0.3919, 0.5210],
          [0.4917, 0.6202, 0.5496,  ..., 0.3285, 0.4610, 0.5784],
          [0.5988, 0.4087, 0.4438,  ..., 0.5605, 0.6352, 0.5615],
          [0.5099, 0.4576, 0.4235,  ..., 0.5296, 0.4168, 0.4907]],

         ...,

         [[0.5285, 0.5660, 0.5294,  ..., 0.4472, 0.6843, 0.4282],
          [0.4422, 0.6567, 0.6558,  ..., 0.3702, 0.3585, 0.4121],
          [0.5718, 0.5078, 0.3730,  ..., 0.6619, 0.5113, 0.4716],
          [0.4031, 0.4844, 0.7372,  ..., 0.5936, 0.5042, 0.4984]],

         [[0.4106, 0.5255, 0.5038,  ..., 0.5344, 0.4883, 0.6343],
          [0.5607, 0.5955, 0.5837,  ..., 0.5312, 0.5746, 0.5081],
          [0.3311, 0.6197, 0.4470,  ..., 0.5226, 0.5622, 0.5120],
          [0.5149, 0.4235, 0.4734,  ..., 0.4547, 0.5518, 0.6021]],

         [[0.4813, 0.5846, 0.4354,  ..., 0.4196, 0.4438, 0.5117],
          [0.5045, 0.5184, 0.5808,  ..., 0.5358, 0.5194, 0.3558],
          [0.6706, 0.3346, 0.4097,  ..., 0.4168, 0.3835, 0.3928],
          [0.5246, 0.5530, 0.5756,  ..., 0.5319, 0.6252, 0.3789]]],


        [[[0.4586, 0.4591, 0.3684,  ..., 0.5523, 0.4644, 0.4393],
          [0.4177, 0.4477, 0.6151,  ..., 0.5404, 0.5554, 0.5675],
          [0.4149, 0.4941, 0.6775,  ..., 0.4844, 0.4668, 0.5455],
          [0.6460, 0.6039, 0.4467,  ..., 0.3937, 0.5363, 0.4470]],

         [[0.3024, 0.4158, 0.5770,  ..., 0.7233, 0.3141, 0.6039],
          [0.6086, 0.5375, 0.6007,  ..., 0.6370, 0.3702, 0.5860],
          [0.4810, 0.4282, 0.4297,  ..., 0.6361, 0.4273, 0.6496],
          [0.5879, 0.4808, 0.5305,  ..., 0.4125, 0.3923, 0.5832]],

         [[0.3594, 0.3442, 0.5361,  ..., 0.4978, 0.6160, 0.4431],
          [0.5660, 0.5223, 0.5622,  ..., 0.5732, 0.4773, 0.5424],
          [0.6002, 0.6011, 0.4031,  ..., 0.5131, 0.4926, 0.6715],
          [0.5245, 0.6183, 0.5722,  ..., 0.4523, 0.4946, 0.5263]],

         ...,

         [[0.4995, 0.4783, 0.4482,  ..., 0.4121, 0.5936, 0.4819],
          [0.5547, 0.4501, 0.6280,  ..., 0.6316, 0.5006, 0.4244],
          [0.4899, 0.6415, 0.6114,  ..., 0.4182, 0.5535, 0.5227],
          [0.5718, 0.5084, 0.4668,  ..., 0.5870, 0.5675, 0.4325]],

         [[0.4817, 0.3149, 0.5770,  ..., 0.5804, 0.4535, 0.5062],
          [0.4287, 0.4762, 0.3303,  ..., 0.5363, 0.5140, 0.4569],
          [0.5513, 0.4914, 0.3798,  ..., 0.5429, 0.4078, 0.6053],
          [0.5018, 0.4746, 0.6243,  ..., 0.4921, 0.5530, 0.4537]],

         [[0.3513, 0.5334, 0.5894,  ..., 0.2524, 0.4721, 0.4206],
          [0.5402, 0.4634, 0.4918,  ..., 0.4600, 0.5141, 0.4187],
          [0.6002, 0.5566, 0.4528,  ..., 0.5559, 0.5441, 0.6160],
          [0.4739, 0.5761, 0.5513,  ..., 0.4620, 0.4116, 0.3285]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0050, -0.0070,  0.0050, -0.0030,  0.0030,  0.0050, -0.0030, -0.0050,
         0.0030,  0.0050], device='cuda:0')
selected experts tensor([1663, 1574, 1649, 1688, 1657, 1707, 1655, 1598, 1623, 1570],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4756, 0.5311, 0.4248,  ..., 0.5970, 0.3190, 0.3804],
          [0.4966, 0.3620, 0.5491,  ..., 0.4303, 0.4518, 0.5591],
          [0.4565, 0.6549, 0.5544,  ..., 0.6319, 0.4255, 0.5415],
          [0.4659, 0.5161, 0.3309,  ..., 0.6423, 0.4293, 0.5761]],

         [[0.5056, 0.5035, 0.4675,  ..., 0.4707, 0.4614, 0.4768],
          [0.4979, 0.4985, 0.4258,  ..., 0.3773, 0.3065, 0.5714],
          [0.4696, 0.5593, 0.5907,  ..., 0.5832, 0.4604, 0.5076],
          [0.4788, 0.4851, 0.3913,  ..., 0.6782, 0.5282, 0.5386]],

         [[0.5300, 0.4601, 0.5779,  ..., 0.5494, 0.4599, 0.5410],
          [0.4803, 0.3764, 0.5174,  ..., 0.3647, 0.3384, 0.5765],
          [0.4606, 0.4907, 0.5450,  ..., 0.3661, 0.3981, 0.5309],
          [0.4863, 0.3496, 0.4827,  ..., 0.4957, 0.6676, 0.6641]],

         ...,

         [[0.5179, 0.3540, 0.5807,  ..., 0.3213, 0.4269, 0.4545],
          [0.3775, 0.4153, 0.4602,  ..., 0.3920, 0.4753, 0.4843],
          [0.4766, 0.3271, 0.4734,  ..., 0.3439, 0.4447, 0.4224],
          [0.4514, 0.4003, 0.4801,  ..., 0.5494, 0.5084, 0.3865]],

         [[0.5688, 0.3280, 0.4702,  ..., 0.3939, 0.4126, 0.4348],
          [0.5126, 0.2894, 0.3838,  ..., 0.5307, 0.3400, 0.4244],
          [0.4344, 0.4714, 0.5248,  ..., 0.3736, 0.5131, 0.5251],
          [0.5861, 0.3674, 0.5353,  ..., 0.5622, 0.4902, 0.4989]],

         [[0.5832, 0.4888, 0.5764,  ..., 0.4608, 0.3909, 0.4171],
          [0.5837, 0.3629, 0.4835,  ..., 0.3949, 0.4214, 0.5520],
          [0.5235, 0.5625, 0.6547,  ..., 0.4605, 0.5777, 0.4348],
          [0.5226, 0.4372, 0.4129,  ..., 0.5865, 0.4638, 0.5643]]],


        [[[0.6361, 0.4918, 0.5534,  ..., 0.4458, 0.3909, 0.5023],
          [0.4952, 0.3755, 0.3820,  ..., 0.3992, 0.3619, 0.3790],
          [0.5666, 0.4775, 0.5493,  ..., 0.4148, 0.5598, 0.5553],
          [0.5114, 0.5458, 0.3913,  ..., 0.6593, 0.4614, 0.5482]],

         [[0.5979, 0.4494, 0.4325,  ..., 0.5196, 0.2628, 0.5207],
          [0.5899, 0.5010, 0.4063,  ..., 0.4187, 0.4071, 0.4572],
          [0.3982, 0.6217, 0.5674,  ..., 0.5394, 0.4963, 0.4803],
          [0.4703, 0.4353, 0.2739,  ..., 0.7292, 0.3697, 0.5515]],

         [[0.5789, 0.5309, 0.5897,  ..., 0.5196, 0.5183, 0.4852],
          [0.4126, 0.5668, 0.4941,  ..., 0.4296, 0.3829, 0.3875],
          [0.4606, 0.5993, 0.5631,  ..., 0.5961, 0.5878, 0.6010],
          [0.4873, 0.4727, 0.4697,  ..., 0.6432, 0.4108, 0.4991]],

         ...,

         [[0.6470, 0.4215, 0.6094,  ..., 0.5058, 0.4709, 0.4421],
          [0.6177, 0.4879, 0.4751,  ..., 0.4393, 0.4325, 0.3253],
          [0.5220, 0.6124, 0.5148,  ..., 0.5303, 0.4924, 0.3936],
          [0.5424, 0.4951, 0.4905,  ..., 0.5242, 0.4547, 0.3837]],

         [[0.4572, 0.3674, 0.5165,  ..., 0.4593, 0.5350, 0.6155],
          [0.4888, 0.3620, 0.5140,  ..., 0.4864, 0.3367, 0.3724],
          [0.5480, 0.4453, 0.5949,  ..., 0.4413, 0.6575, 0.4372],
          [0.4387, 0.5446, 0.4210,  ..., 0.5375, 0.5854, 0.4065]],

         [[0.5427, 0.3782, 0.5193,  ..., 0.4830, 0.3442, 0.4350],
          [0.6256, 0.4586, 0.5621,  ..., 0.4418, 0.4071, 0.5186],
          [0.5242, 0.4588, 0.6287,  ..., 0.4347, 0.3758, 0.4210],
          [0.5746, 0.4377, 0.5307,  ..., 0.4254, 0.4657, 0.6092]]]],
       device='cuda:0')
tensor([[[[0.4656, 0.5231, 0.4268,  ..., 0.6270, 0.2830, 0.4064],
          [0.4866, 0.3540, 0.5511,  ..., 0.4603, 0.4158, 0.5851],
          [0.4465, 0.6469, 0.5564,  ..., 0.6619, 0.3895, 0.5675],
          [0.4559, 0.5081, 0.3329,  ..., 0.6723, 0.3933, 0.6021]],

         [[0.4956, 0.4955, 0.4695,  ..., 0.5007, 0.4254, 0.5028],
          [0.4879, 0.4905, 0.4278,  ..., 0.4073, 0.2705, 0.5974],
          [0.4596, 0.5513, 0.5927,  ..., 0.6132, 0.4244, 0.5336],
          [0.4688, 0.4771, 0.3933,  ..., 0.7082, 0.4922, 0.5646]],

         [[0.5200, 0.4521, 0.5799,  ..., 0.5794, 0.4239, 0.5670],
          [0.4703, 0.3684, 0.5194,  ..., 0.3947, 0.3024, 0.6025],
          [0.4506, 0.4827, 0.5470,  ..., 0.3961, 0.3621, 0.5569],
          [0.4763, 0.3416, 0.4847,  ..., 0.5257, 0.6316, 0.6901]],

         ...,

         [[0.5079, 0.3460, 0.5827,  ..., 0.3513, 0.3909, 0.4805],
          [0.3675, 0.4073, 0.4622,  ..., 0.4220, 0.4393, 0.5103],
          [0.4666, 0.3191, 0.4754,  ..., 0.3739, 0.4087, 0.4484],
          [0.4414, 0.3923, 0.4821,  ..., 0.5794, 0.4724, 0.4125]],

         [[0.5588, 0.3200, 0.4722,  ..., 0.4239, 0.3766, 0.4608],
          [0.5026, 0.2814, 0.3858,  ..., 0.5607, 0.3040, 0.4504],
          [0.4244, 0.4634, 0.5268,  ..., 0.4036, 0.4771, 0.5511],
          [0.5761, 0.3594, 0.5373,  ..., 0.5922, 0.4542, 0.5249]],

         [[0.5732, 0.4808, 0.5784,  ..., 0.4908, 0.3549, 0.4431],
          [0.5737, 0.3549, 0.4855,  ..., 0.4249, 0.3854, 0.5780],
          [0.5135, 0.5545, 0.6567,  ..., 0.4905, 0.5417, 0.4608],
          [0.5126, 0.4292, 0.4149,  ..., 0.6165, 0.4278, 0.5903]]],


        [[[0.6261, 0.4838, 0.5554,  ..., 0.4758, 0.3549, 0.5283],
          [0.4852, 0.3675, 0.3840,  ..., 0.4292, 0.3259, 0.4050],
          [0.5566, 0.4695, 0.5513,  ..., 0.4448, 0.5238, 0.5813],
          [0.5014, 0.5378, 0.3933,  ..., 0.6893, 0.4254, 0.5742]],

         [[0.5879, 0.4414, 0.4345,  ..., 0.5496, 0.2268, 0.5467],
          [0.5799, 0.4930, 0.4083,  ..., 0.4487, 0.3711, 0.4832],
          [0.3882, 0.6137, 0.5694,  ..., 0.5694, 0.4603, 0.5063],
          [0.4603, 0.4273, 0.2759,  ..., 0.7592, 0.3337, 0.5775]],

         [[0.5689, 0.5229, 0.5917,  ..., 0.5496, 0.4823, 0.5112],
          [0.4026, 0.5588, 0.4961,  ..., 0.4596, 0.3469, 0.4135],
          [0.4506, 0.5913, 0.5651,  ..., 0.6261, 0.5518, 0.6270],
          [0.4773, 0.4647, 0.4717,  ..., 0.6732, 0.3748, 0.5251]],

         ...,

         [[0.6370, 0.4135, 0.6114,  ..., 0.5358, 0.4349, 0.4681],
          [0.6077, 0.4799, 0.4771,  ..., 0.4693, 0.3965, 0.3513],
          [0.5120, 0.6044, 0.5168,  ..., 0.5603, 0.4564, 0.4196],
          [0.5324, 0.4871, 0.4925,  ..., 0.5542, 0.4187, 0.4097]],

         [[0.4472, 0.3594, 0.5185,  ..., 0.4893, 0.4990, 0.6415],
          [0.4788, 0.3540, 0.5160,  ..., 0.5164, 0.3007, 0.3984],
          [0.5380, 0.4373, 0.5969,  ..., 0.4713, 0.6215, 0.4632],
          [0.4287, 0.5366, 0.4230,  ..., 0.5675, 0.5494, 0.4325]],

         [[0.5327, 0.3702, 0.5213,  ..., 0.5130, 0.3082, 0.4610],
          [0.6156, 0.4506, 0.5641,  ..., 0.4718, 0.3711, 0.5446],
          [0.5142, 0.4508, 0.6307,  ..., 0.4647, 0.3398, 0.4470],
          [0.5646, 0.4297, 0.5327,  ..., 0.4554, 0.4297, 0.6352]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/expert_selection_callback.py:150: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
[batch=40/40]:
	 Train time/batch: 39
	 Train time/sample: 78
	 Train time/batch_in_epoch: 39
	 Train time/sample_in_epoch: 78
	 Train time/token: 79872
	 Train time/token_in_epoch: 79872
	 Train memory/current_allocated_mem: 1.1885
	 Train memory/current_active_mem: 1.1885
	 Train memory/current_inactive_mem: 0.3403
	 Train memory/current_reserved_mem: 3.8483
	 Train memory/peak_allocated_mem: 2.8024
	 Train memory/peak_active_mem: 2.8024
	 Train memory/peak_inactive_mem: 0.8598
	 Train memory/peak_reserved_mem: 3.8483
	 Train memory/alloc_retries: 0
	 Train trainer/device_train_microbatch_size: 10
	 Train loss/train/total: 0.0043
	 Train metrics/train/LanguageCrossEntropy: 8.8146
	 Train metrics/train/LanguagePerplexity: 6731.7827
	 Train metrics/train/TokenAccuracy: 0.2250
	 Train throughput/batches_per_sec: 0.1980
	 Train throughput/samples_per_sec: 0.3960
	 Train throughput/device/batches_per_sec: 0.1980
	 Train throughput/device/samples_per_sec: 0.3960
	 Train throughput/tokens_per_sec: 405.5084
	 Train throughput/device/tokens_per_sec: 405.5084
	 Train time/train: 0.0628
	 Train time/val: 0.0000
	 Train time/total: 0.0628
	 Train lr-DecoupledAdamW/group0: 0.0001
	 Train time/remaining_estimate: 0.0000
	 Train metrics/shannon_entropy: 10.4849
	 Train metrics/batch_shannon_entropy: <wandb.sdk.data_types.table.Table object at 0x7097b028cfb0>
	 Train metrics/seq_shannon_entropy: CustomChart(table=<wandb.sdk.data_types.table.Table object at 0x70955279a870>, spec=CustomChartSpec(spec_name='wandb/lineseries/v0', fields={'step': 'step', 'lineKey': 'lineKey', 'lineVal': 'lineVal'}, string_fields={'title': 'Per-Layer Shannon Entropy', 'xname': 'Block Index'}, key='', panel_type='Vega2', split_table=False))
	 Train metrics/exit_entropy: 0.6641
	 Train metrics/batch_exit_entropy: <wandb.sdk.data_types.table.Table object at 0x70984213f1a0>
	 Train metrics/seq_exit_entropy: CustomChart(table=<wandb.sdk.data_types.table.Table object at 0x709517382ed0>, spec=CustomChartSpec(spec_name='wandb/lineseries/v0', fields={'step': 'step', 'lineKey': 'lineKey', 'lineVal': 'lineVal'}, string_fields={'title': 'Per-Layer Exit Entropy', 'xname': 'Block Index'}, key='', panel_type='Vega2', split_table=False))
	 Train expert_selection/ffn_layer: <wandb.sdk.data_types.image.Image object at 0x70950f6a2e40>
	 Train expert_selection/attn_o_layer: <wandb.sdk.data_types.image.Image object at 0x709836b2b290>
	 Train expert_selection/attn_v_layer: <wandb.sdk.data_types.image.Image object at 0x70950f394680>
	 Train l2_norm/moment/model.transformer.router: 0.0000
	 Train l2_norm/param/model.transformer.router: 0.3645
	 Train l2_norm/update/model.transformer.router: 0.0007
	 Train l2_norm/grad/model.transformer.router: 0.0000
	 Train l2_norm/moment/model.transformer.tau: 0.0000
	 Train l2_norm/param/model.transformer.tau: 1.0007
	 Train l2_norm/update/model.transformer.tau: 0.0000
	 Train l2_norm/grad/model.transformer.tau: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.attention.v: 0.0003
	 Train l2_norm/param/model.transformer.layers.0.attention.v: 20.4717
	 Train l2_norm/update/model.transformer.layers.0.attention.v: 0.0188
	 Train l2_norm/grad/model.transformer.layers.0.attention.v: 0.0007
	 Train l2_norm/moment/model.transformer.layers.0.attention.o: 0.0004
	 Train l2_norm/param/model.transformer.layers.0.attention.o: 22.6959
	 Train l2_norm/update/model.transformer.layers.0.attention.o: 0.0262
	 Train l2_norm/grad/model.transformer.layers.0.attention.o: 0.0007
	 Train l2_norm/moment/model.transformer.layers.0.attention.sel_v: 0.0001
	 Train l2_norm/param/model.transformer.layers.0.attention.sel_v: 2.2340
	 Train l2_norm/update/model.transformer.layers.0.attention.sel_v: 0.0019
	 Train l2_norm/grad/model.transformer.layers.0.attention.sel_v: 0.0002
	 Train l2_norm/moment/model.transformer.layers.0.attention.sel_o: 0.0001
	 Train l2_norm/param/model.transformer.layers.0.attention.sel_o: 2.2493
	 Train l2_norm/update/model.transformer.layers.0.attention.sel_o: 0.0017
	 Train l2_norm/grad/model.transformer.layers.0.attention.sel_o: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.attention.q.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attention.q.weight: 6.4702
	 Train l2_norm/update/model.transformer.layers.0.attention.q.weight: 0.0082
	 Train l2_norm/grad/model.transformer.layers.0.attention.q.weight: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.attention.k.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attention.k.weight: 6.4760
	 Train l2_norm/update/model.transformer.layers.0.attention.k.weight: 0.0082
	 Train l2_norm/grad/model.transformer.layers.0.attention.k.weight: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.ffn.keys: 0.0003
	 Train l2_norm/param/model.transformer.layers.0.ffn.keys: 14.9992
	 Train l2_norm/update/model.transformer.layers.0.ffn.keys: 0.0203
	 Train l2_norm/grad/model.transformer.layers.0.ffn.keys: 0.0005
	 Train l2_norm/moment/model.transformer.layers.0.ffn.values: 0.0007
	 Train l2_norm/param/model.transformer.layers.0.ffn.values: 7.1968
	 Train l2_norm/update/model.transformer.layers.0.ffn.values: 0.0197
	 Train l2_norm/grad/model.transformer.layers.0.ffn.values: 0.0012
	 Train l2_norm/moment/model.transformer.layers.0.ffn.expert_sel: 0.0001
	 Train l2_norm/param/model.transformer.layers.0.ffn.expert_sel: 4.7519
	 Train l2_norm/update/model.transformer.layers.0.ffn.expert_sel: 0.0065
	 Train l2_norm/grad/model.transformer.layers.0.ffn.expert_sel: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.attn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attn_pre.weight: 20.2974
	 Train l2_norm/update/model.transformer.layers.0.attn_pre.weight: 0.0004
	 Train l2_norm/grad/model.transformer.layers.0.attn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.attn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attn_post.weight: 20.2953
	 Train l2_norm/update/model.transformer.layers.0.attn_post.weight: 0.0005
	 Train l2_norm/grad/model.transformer.layers.0.attn_post.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.ffn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.ffn_pre.weight: 20.2977
	 Train l2_norm/update/model.transformer.layers.0.ffn_pre.weight: 0.0005
	 Train l2_norm/grad/model.transformer.layers.0.ffn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.ffn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.ffn_post.weight: 20.3035
	 Train l2_norm/update/model.transformer.layers.0.ffn_post.weight: 0.0007
	 Train l2_norm/grad/model.transformer.layers.0.ffn_post.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.v: 0.0001
	 Train l2_norm/param/model.transformer.layers.1.attention.v: 20.4834
	 Train l2_norm/update/model.transformer.layers.1.attention.v: 0.0150
	 Train l2_norm/grad/model.transformer.layers.1.attention.v: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.o: 0.0001
	 Train l2_norm/param/model.transformer.layers.1.attention.o: 22.6893
	 Train l2_norm/update/model.transformer.layers.1.attention.o: 0.0165
	 Train l2_norm/grad/model.transformer.layers.1.attention.o: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.sel_v: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.sel_v: 2.2356
	 Train l2_norm/update/model.transformer.layers.1.attention.sel_v: 0.0008
	 Train l2_norm/grad/model.transformer.layers.1.attention.sel_v: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.sel_o: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.sel_o: 2.2254
	 Train l2_norm/update/model.transformer.layers.1.attention.sel_o: 0.0013
	 Train l2_norm/grad/model.transformer.layers.1.attention.sel_o: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.q.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.q.weight: 6.4889
	 Train l2_norm/update/model.transformer.layers.1.attention.q.weight: 0.0032
	 Train l2_norm/grad/model.transformer.layers.1.attention.q.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.k.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.k.weight: 6.4778
	 Train l2_norm/update/model.transformer.layers.1.attention.k.weight: 0.0033
	 Train l2_norm/grad/model.transformer.layers.1.attention.k.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn.keys: 0.0001
	 Train l2_norm/param/model.transformer.layers.1.ffn.keys: 15.0072
	 Train l2_norm/update/model.transformer.layers.1.ffn.keys: 0.0142
	 Train l2_norm/grad/model.transformer.layers.1.ffn.keys: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn.values: 0.0003
	 Train l2_norm/param/model.transformer.layers.1.ffn.values: 7.1674
	 Train l2_norm/update/model.transformer.layers.1.ffn.values: 0.0155
	 Train l2_norm/grad/model.transformer.layers.1.ffn.values: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn.expert_sel: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.ffn.expert_sel: 4.7381
	 Train l2_norm/update/model.transformer.layers.1.ffn.expert_sel: 0.0044
	 Train l2_norm/grad/model.transformer.layers.1.ffn.expert_sel: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attn_pre.weight: 20.2972
	 Train l2_norm/update/model.transformer.layers.1.attn_pre.weight: 0.0003
	 Train l2_norm/grad/model.transformer.layers.1.attn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attn_post.weight: 20.2937
	 Train l2_norm/update/model.transformer.layers.1.attn_post.weight: 0.0005
	 Train l2_norm/grad/model.transformer.layers.1.attn_post.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.ffn_pre.weight: 20.2974
	 Train l2_norm/update/model.transformer.layers.1.ffn_pre.weight: 0.0003
	 Train l2_norm/grad/model.transformer.layers.1.ffn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.ffn_post.weight: 20.2976
	 Train l2_norm/update/model.transformer.layers.1.ffn_post.weight: 0.0004
	 Train l2_norm/grad/model.transformer.layers.1.ffn_post.weight: 0.0000
	 Train l2_norm/moment/model.embedding.weight: 0.0001
	 Train l2_norm/param/model.embedding.weight: 221.7539
	 Train l2_norm/update/model.embedding.weight: 0.0154
	 Train l2_norm/grad/model.embedding.weight: 0.0003
	 Train l2_norm/moment/model.lm_head.weight: 0.0006
	 Train l2_norm/param/model.lm_head.weight: 127.9523
	 Train l2_norm/update/model.lm_head.weight: 0.0643
	 Train l2_norm/grad/model.lm_head.weight: 0.0008
	 Train l2_norm/moment/model.lm_head.bias: 0.0000
	 Train l2_norm/param/model.lm_head.bias: 6.2882
	 Train l2_norm/update/model.lm_head.bias: 0.0049
	 Train l2_norm/grad/model.lm_head.bias: 0.0000
	 Train l2_norm/moment/model.out_norm.weight: 0.0000
	 Train l2_norm/param/model.out_norm.weight: 20.3061
	 Train l2_norm/update/model.out_norm.weight: 0.0009
	 Train l2_norm/grad/model.out_norm.weight: 0.0000
	 Train l2_norm/grad/global: 0.0018
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                         activations/average/_output.0 ‚ñà‚ñÅ
wandb:                          activations/average/model.embedding_output.0 ‚ñà‚ñÅ
wandb:                          activations/average/model.embedding_output.1 ‚ñÅ‚ñà
wandb:                             activations/average/model.lm_head_input.0 ‚ñÅ‚ñà
wandb:                            activations/average/model.lm_head_output.0 ‚ñà‚ñÅ
wandb:                            activations/average/model.lm_head_output.1 ‚ñà‚ñÅ
wandb:                            activations/average/model.out_norm_input.0 ‚ñÅ‚ñà
wandb:                           activations/average/model.out_norm_output.0 ‚ñÅ‚ñà
wandb:                           activations/average/model.out_norm_output.1 ‚ñÅ‚ñà
wandb:    activations/average/model.transformer.layers.0.attention.k_input.0 ‚ñà‚ñÅ
wandb:   activations/average/model.transformer.layers.0.attention.k_output.0 ‚ñà‚ñÅ
wandb:   activations/average/model.transformer.layers.0.attention.k_output.1 ‚ñà‚ñÅ
wandb:   activations/average/model.transformer.layers.0.attention.pe_input.0 ‚ñÅ‚ñà
wandb:   activations/average/model.transformer.layers.0.attention.pe_input.1 ‚ñà‚ñÅ
wandb:  activations/average/model.transformer.layers.0.attention.pe_output.0 ‚ñÅ‚ñà
wandb:  activations/average/model.transformer.layers.0.attention.pe_output.1 ‚ñà‚ñÅ
wandb:    activations/average/model.transformer.layers.0.attention.q_input.0 ‚ñà‚ñÅ
wandb:   activations/average/model.transformer.layers.0.attention.q_output.0 ‚ñÅ‚ñà
wandb:   activations/average/model.transformer.layers.0.attention.q_output.1 ‚ñà‚ñÅ
wandb:      activations/average/model.transformer.layers.0.attention_input.0 ‚ñà‚ñÅ
wandb:      activations/average/model.transformer.layers.0.attention_input.1 ‚ñà‚ñÅ
wandb:      activations/average/model.transformer.layers.0.attention_input.2 ‚ñà‚ñÅ
wandb:     activations/average/model.transformer.layers.0.attention_output.0 ‚ñÅ‚ñà
wandb:      activations/average/model.transformer.layers.0.attn_post_input.0 ‚ñÅ‚ñà
wandb:     activations/average/model.transformer.layers.0.attn_post_output.0 ‚ñÅ‚ñà
wandb:     activations/average/model.transformer.layers.0.attn_post_output.1 ‚ñà‚ñÅ
wandb:       activations/average/model.transformer.layers.0.attn_pre_input.0 ‚ñà‚ñÅ
wandb:      activations/average/model.transformer.layers.0.attn_pre_output.0 ‚ñà‚ñÅ
wandb:      activations/average/model.transformer.layers.0.attn_pre_output.1 ‚ñÅ‚ñà
wandb:            activations/average/model.transformer.layers.0.ffn_input.0 ‚ñÅ‚ñà
wandb:            activations/average/model.transformer.layers.0.ffn_input.1 ‚ñÅ‚ñà
wandb:           activations/average/model.transformer.layers.0.ffn_output.0 ‚ñÅ‚ñà
wandb:       activations/average/model.transformer.layers.0.ffn_post_input.0 ‚ñÅ‚ñà
wandb:      activations/average/model.transformer.layers.0.ffn_post_output.0 ‚ñà‚ñÅ
wandb:      activations/average/model.transformer.layers.0.ffn_post_output.1 ‚ñÅ‚ñà
wandb:        activations/average/model.transformer.layers.0.ffn_pre_input.0 ‚ñÅ‚ñà
wandb:       activations/average/model.transformer.layers.0.ffn_pre_output.0 ‚ñÅ‚ñà
wandb:       activations/average/model.transformer.layers.0.ffn_pre_output.1 ‚ñà‚ñÅ
wandb:                activations/average/model.transformer.layers.0_input.0 ‚ñÅ‚ñà
wandb:                activations/average/model.transformer.layers.0_input.2 ‚ñà‚ñÅ
wandb:                activations/average/model.transformer.layers.0_input.3 ‚ñà‚ñÅ
wandb:                activations/average/model.transformer.layers.0_input.4 ‚ñà‚ñÅ
wandb:                activations/average/model.transformer.layers.0_input.5 ‚ñÅ‚ñà
wandb:               activations/average/model.transformer.layers.0_output.0 ‚ñÅ‚ñà
wandb:               activations/average/model.transformer.layers.0_output.4 ‚ñà‚ñÅ
wandb:    activations/average/model.transformer.layers.1.attention.k_input.0 ‚ñÅ‚ñà
wandb:   activations/average/model.transformer.layers.1.attention.k_output.0 ‚ñà‚ñÅ
wandb:   activations/average/model.transformer.layers.1.attention.k_output.1 ‚ñà‚ñÅ
wandb:   activations/average/model.transformer.layers.1.attention.pe_input.0 ‚ñÅ‚ñà
wandb:   activations/average/model.transformer.layers.1.attention.pe_input.1 ‚ñà‚ñÅ
wandb:  activations/average/model.transformer.layers.1.attention.pe_output.0 ‚ñÅ‚ñà
wandb:  activations/average/model.transformer.layers.1.attention.pe_output.1 ‚ñà‚ñÅ
wandb:    activations/average/model.transformer.layers.1.attention.q_input.0 ‚ñÅ‚ñà
wandb:   activations/average/model.transformer.layers.1.attention.q_output.0 ‚ñÅ‚ñà
wandb:   activations/average/model.transformer.layers.1.attention.q_output.1 ‚ñà‚ñÅ
wandb:      activations/average/model.transformer.layers.1.attention_input.0 ‚ñÅ‚ñà
wandb:      activations/average/model.transformer.layers.1.attention_input.1 ‚ñÅ‚ñà
wandb:      activations/average/model.transformer.layers.1.attention_input.2 ‚ñÅ‚ñà
wandb:     activations/average/model.transformer.layers.1.attention_output.0 ‚ñÅ‚ñà
wandb:      activations/average/model.transformer.layers.1.attn_post_input.0 ‚ñÅ‚ñà
wandb:     activations/average/model.transformer.layers.1.attn_post_output.0 ‚ñÅ‚ñà
wandb:     activations/average/model.transformer.layers.1.attn_post_output.1 ‚ñÅ‚ñà
wandb:       activations/average/model.transformer.layers.1.attn_pre_input.0 ‚ñÅ‚ñà
wandb:      activations/average/model.transformer.layers.1.attn_pre_output.0 ‚ñÅ‚ñà
wandb:      activations/average/model.transformer.layers.1.attn_pre_output.1 ‚ñÅ‚ñà
wandb:            activations/average/model.transformer.layers.1.ffn_input.0 ‚ñÅ‚ñà
wandb:            activations/average/model.transformer.layers.1.ffn_input.1 ‚ñÅ‚ñà
wandb:           activations/average/model.transformer.layers.1.ffn_output.0 ‚ñÅ‚ñà
wandb:       activations/average/model.transformer.layers.1.ffn_post_input.0 ‚ñÅ‚ñà
wandb:      activations/average/model.transformer.layers.1.ffn_post_output.0 ‚ñÅ‚ñà
wandb:      activations/average/model.transformer.layers.1.ffn_post_output.1 ‚ñÅ‚ñà
wandb:        activations/average/model.transformer.layers.1.ffn_pre_input.0 ‚ñÅ‚ñà
wandb:       activations/average/model.transformer.layers.1.ffn_pre_output.0 ‚ñÅ‚ñà
wandb:       activations/average/model.transformer.layers.1.ffn_pre_output.1 ‚ñÅ‚ñà
wandb:                activations/average/model.transformer.layers.1_input.0 ‚ñÅ‚ñà
wandb:                activations/average/model.transformer.layers.1_input.2 ‚ñà‚ñÅ
wandb:                activations/average/model.transformer.layers.1_input.3 ‚ñà‚ñÅ
wandb:                activations/average/model.transformer.layers.1_input.4 ‚ñà‚ñÅ
wandb:                activations/average/model.transformer.layers.1_input.5 ‚ñÅ‚ñà
wandb:               activations/average/model.transformer.layers.1_output.0 ‚ñÅ‚ñà
wandb:               activations/average/model.transformer.layers.1_output.4 ‚ñà‚ñÅ
wandb:                         activations/average/model.transformer_input.0 ‚ñà‚ñÅ
wandb:                         activations/average/model.transformer_input.1 ‚ñà‚ñÅ
wandb:                        activations/average/model.transformer_output.0 ‚ñÅ‚ñà
wandb:                                    activations/average/model_output.0 ‚ñà‚ñÅ
wandb:                                        activations/kurtosis/_output.0 ‚ñÅ‚ñà
wandb:                         activations/kurtosis/model.embedding_output.0 ‚ñà‚ñÅ
wandb:                         activations/kurtosis/model.embedding_output.1 ‚ñÅ‚ñà
wandb:                            activations/kurtosis/model.lm_head_input.0 ‚ñà‚ñÅ
wandb:                           activations/kurtosis/model.lm_head_output.0 ‚ñÅ‚ñà
wandb:                           activations/kurtosis/model.lm_head_output.1 ‚ñÅ‚ñà
wandb:                           activations/kurtosis/model.out_norm_input.0 ‚ñà‚ñÅ
wandb:                          activations/kurtosis/model.out_norm_output.0 ‚ñà‚ñÅ
wandb:                          activations/kurtosis/model.out_norm_output.1 ‚ñà‚ñÅ
wandb:   activations/kurtosis/model.transformer.layers.0.attention.k_input.0 ‚ñÅ‚ñà
wandb:  activations/kurtosis/model.transformer.layers.0.attention.k_output.0 ‚ñÅ‚ñà
wandb:  activations/kurtosis/model.transformer.layers.0.attention.k_output.1 ‚ñÅ‚ñà
wandb:  activations/kurtosis/model.transformer.layers.0.attention.pe_input.0 ‚ñà‚ñÅ
wandb:  activations/kurtosis/model.transformer.layers.0.attention.pe_input.1 ‚ñÅ‚ñà
wandb: activations/kurtosis/model.transformer.layers.0.attention.pe_output.0 ‚ñà‚ñÅ
wandb: activations/kurtosis/model.transformer.layers.0.attention.pe_output.1 ‚ñÅ‚ñà
wandb:   activations/kurtosis/model.transformer.layers.0.attention.q_input.0 ‚ñÅ‚ñà
wandb:  activations/kurtosis/model.transformer.layers.0.attention.q_output.0 ‚ñà‚ñÅ
wandb:  activations/kurtosis/model.transformer.layers.0.attention.q_output.1 ‚ñà‚ñÅ
wandb:     activations/kurtosis/model.transformer.layers.0.attention_input.0 ‚ñÅ‚ñà
wandb:     activations/kurtosis/model.transformer.layers.0.attention_input.1 ‚ñÅ‚ñà
wandb:     activations/kurtosis/model.transformer.layers.0.attention_input.2 ‚ñÅ‚ñà
wandb:      activations/kurtosis/model.transformer.layers.0.attn_pre_input.0 ‚ñÅ‚ñà
wandb:     activations/kurtosis/model.transformer.layers.0.attn_pre_output.0 ‚ñà‚ñÅ
wandb:     activations/kurtosis/model.transformer.layers.0.attn_pre_output.1 ‚ñÅ‚ñà
wandb:           activations/kurtosis/model.transformer.layers.0.ffn_input.0 ‚ñà‚ñÅ
wandb:           activations/kurtosis/model.transformer.layers.0.ffn_input.1 ‚ñà‚ñÅ
wandb:          activations/kurtosis/model.transformer.layers.0.ffn_output.0 ‚ñà‚ñÅ
wandb:      activations/kurtosis/model.transformer.layers.0.ffn_post_input.0 ‚ñà‚ñÅ
wandb:     activations/kurtosis/model.transformer.layers.0.ffn_post_output.0 ‚ñà‚ñÅ
wandb:     activations/kurtosis/model.transformer.layers.0.ffn_post_output.1 ‚ñà‚ñÅ
wandb:       activations/kurtosis/model.transformer.layers.0.ffn_pre_input.0 ‚ñà‚ñÅ
wandb:      activations/kurtosis/model.transformer.layers.0.ffn_pre_output.0 ‚ñà‚ñÅ
wandb:      activations/kurtosis/model.transformer.layers.0.ffn_pre_output.1 ‚ñà‚ñÅ
wandb:               activations/kurtosis/model.transformer.layers.0_input.0 ‚ñà‚ñÅ
wandb:               activations/kurtosis/model.transformer.layers.0_input.2 ‚ñÅ‚ñà
wandb:               activations/kurtosis/model.transformer.layers.0_input.3 ‚ñÅ‚ñà
wandb:               activations/kurtosis/model.transformer.layers.0_input.4 ‚ñà‚ñÅ
wandb:              activations/kurtosis/model.transformer.layers.0_output.0 ‚ñà‚ñÅ
wandb:              activations/kurtosis/model.transformer.layers.0_output.4 ‚ñà‚ñÅ
wandb:   activations/kurtosis/model.transformer.layers.1.attention.k_input.0 ‚ñà‚ñÅ
wandb:  activations/kurtosis/model.transformer.layers.1.attention.k_output.0 ‚ñÅ‚ñà
wandb:  activations/kurtosis/model.transformer.layers.1.attention.k_output.1 ‚ñÅ‚ñà
wandb:  activations/kurtosis/model.transformer.layers.1.attention.pe_input.1 ‚ñà‚ñÅ
wandb: activations/kurtosis/model.transformer.layers.1.attention.pe_output.1 ‚ñÅ‚ñà
wandb:     activations/kurtosis/model.transformer.layers.1.attention_input.1 ‚ñà‚ñÅ
wandb:     activations/kurtosis/model.transformer.layers.1.attention_input.2 ‚ñà‚ñÅ
wandb:      activations/kurtosis/model.transformer.layers.1.attn_pre_input.0 ‚ñà‚ñÅ
wandb:     activations/kurtosis/model.transformer.layers.1.attn_pre_output.0 ‚ñà‚ñÅ
wandb:     activations/kurtosis/model.transformer.layers.1.attn_pre_output.1 ‚ñà‚ñÅ
wandb:       activations/kurtosis/model.transformer.layers.1.ffn_pre_input.0 ‚ñà‚ñÅ
wandb:      activations/kurtosis/model.transformer.layers.1.ffn_pre_output.0 ‚ñà‚ñÅ
wandb:      activations/kurtosis/model.transformer.layers.1.ffn_pre_output.1 ‚ñà‚ñÅ
wandb:               activations/kurtosis/model.transformer.layers.1_input.0 ‚ñà‚ñÅ
wandb:               activations/kurtosis/model.transformer.layers.1_input.2 ‚ñÅ‚ñà
wandb:               activations/kurtosis/model.transformer.layers.1_input.3 ‚ñÅ‚ñà
wandb:               activations/kurtosis/model.transformer.layers.1_input.4 ‚ñÅ‚ñà
wandb:              activations/kurtosis/model.transformer.layers.1_output.0 ‚ñà‚ñÅ
wandb:              activations/kurtosis/model.transformer.layers.1_output.4 ‚ñÅ‚ñà
wandb:                        activations/kurtosis/model.transformer_input.0 ‚ñÅ‚ñà
wandb:                        activations/kurtosis/model.transformer_input.1 ‚ñÅ‚ñà
wandb:                       activations/kurtosis/model.transformer_output.0 ‚ñà‚ñÅ
wandb:                                   activations/kurtosis/model_output.0 ‚ñÅ‚ñà
wandb:                                         activations/l2_norm/_output.0 ‚ñà‚ñÅ
wandb:                          activations/l2_norm/model.embedding_output.0 ‚ñÅ‚ñà
wandb:                          activations/l2_norm/model.embedding_output.1 ‚ñà‚ñÅ
wandb:                             activations/l2_norm/model.lm_head_input.0 ‚ñÅ‚ñà
wandb:                            activations/l2_norm/model.lm_head_output.0 ‚ñà‚ñÅ
wandb:                            activations/l2_norm/model.lm_head_output.1 ‚ñà‚ñÅ
wandb:                            activations/l2_norm/model.out_norm_input.0 ‚ñÅ‚ñà
wandb:                           activations/l2_norm/model.out_norm_output.0 ‚ñÅ‚ñà
wandb:                           activations/l2_norm/model.out_norm_output.1 ‚ñÅ‚ñà
wandb:    activations/l2_norm/model.transformer.layers.0.attention.k_input.0 ‚ñà‚ñÅ
wandb:   activations/l2_norm/model.transformer.layers.0.attention.k_output.0 ‚ñà‚ñÅ
wandb:   activations/l2_norm/model.transformer.layers.0.attention.k_output.1 ‚ñÅ‚ñà
wandb:   activations/l2_norm/model.transformer.layers.0.attention.pe_input.0 ‚ñÅ‚ñà
wandb:   activations/l2_norm/model.transformer.layers.0.attention.pe_input.1 ‚ñà‚ñÅ
wandb:  activations/l2_norm/model.transformer.layers.0.attention.pe_output.0 ‚ñÅ‚ñà
wandb:  activations/l2_norm/model.transformer.layers.0.attention.pe_output.1 ‚ñà‚ñÅ
wandb:    activations/l2_norm/model.transformer.layers.0.attention.q_input.0 ‚ñà‚ñÅ
wandb:   activations/l2_norm/model.transformer.layers.0.attention.q_output.0 ‚ñÅ‚ñà
wandb:   activations/l2_norm/model.transformer.layers.0.attention.q_output.1 ‚ñÅ‚ñà
wandb:      activations/l2_norm/model.transformer.layers.0.attention_input.0 ‚ñà‚ñÅ
wandb:      activations/l2_norm/model.transformer.layers.0.attention_input.1 ‚ñà‚ñÅ
wandb:      activations/l2_norm/model.transformer.layers.0.attention_input.2 ‚ñà‚ñÅ
wandb:     activations/l2_norm/model.transformer.layers.0.attention_output.0 ‚ñÅ‚ñà
wandb:      activations/l2_norm/model.transformer.layers.0.attn_post_input.0 ‚ñÅ‚ñà
wandb:     activations/l2_norm/model.transformer.layers.0.attn_post_output.0 ‚ñÅ‚ñà
wandb:     activations/l2_norm/model.transformer.layers.0.attn_post_output.1 ‚ñÅ‚ñà
wandb:       activations/l2_norm/model.transformer.layers.0.attn_pre_input.0 ‚ñà‚ñÅ
wandb:      activations/l2_norm/model.transformer.layers.0.attn_pre_output.0 ‚ñà‚ñÅ
wandb:      activations/l2_norm/model.transformer.layers.0.attn_pre_output.1 ‚ñà‚ñÅ
wandb:            activations/l2_norm/model.transformer.layers.0.ffn_input.0 ‚ñà‚ñÅ
wandb:            activations/l2_norm/model.transformer.layers.0.ffn_input.1 ‚ñà‚ñÅ
wandb:           activations/l2_norm/model.transformer.layers.0.ffn_output.0 ‚ñÅ‚ñà
wandb:       activations/l2_norm/model.transformer.layers.0.ffn_post_input.0 ‚ñÅ‚ñà
wandb:      activations/l2_norm/model.transformer.layers.0.ffn_post_output.0 ‚ñÅ‚ñà
wandb:      activations/l2_norm/model.transformer.layers.0.ffn_post_output.1 ‚ñÅ‚ñà
wandb:        activations/l2_norm/model.transformer.layers.0.ffn_pre_input.0 ‚ñÅ‚ñà
wandb:       activations/l2_norm/model.transformer.layers.0.ffn_pre_output.0 ‚ñà‚ñÅ
wandb:       activations/l2_norm/model.transformer.layers.0.ffn_pre_output.1 ‚ñà‚ñÅ
wandb:                activations/l2_norm/model.transformer.layers.0_input.0 ‚ñÅ‚ñà
wandb:                activations/l2_norm/model.transformer.layers.0_input.2 ‚ñà‚ñÅ
wandb:                activations/l2_norm/model.transformer.layers.0_input.3 ‚ñÅ‚ñà
wandb:                activations/l2_norm/model.transformer.layers.0_input.4 ‚ñà‚ñÅ
wandb:                activations/l2_norm/model.transformer.layers.0_input.5 ‚ñÅ‚ñà
wandb:               activations/l2_norm/model.transformer.layers.0_output.0 ‚ñÅ‚ñà
wandb:               activations/l2_norm/model.transformer.layers.0_output.4 ‚ñà‚ñÅ
wandb:    activations/l2_norm/model.transformer.layers.1.attention.k_input.0 ‚ñÅ‚ñà
wandb:   activations/l2_norm/model.transformer.layers.1.attention.k_output.0 ‚ñÅ‚ñà
wandb:   activations/l2_norm/model.transformer.layers.1.attention.k_output.1 ‚ñÅ‚ñà
wandb:   activations/l2_norm/model.transformer.layers.1.attention.pe_input.0 ‚ñà‚ñÅ
wandb:   activations/l2_norm/model.transformer.layers.1.attention.pe_input.1 ‚ñÅ‚ñà
wandb:  activations/l2_norm/model.transformer.layers.1.attention.pe_output.0 ‚ñà‚ñÅ
wandb:  activations/l2_norm/model.transformer.layers.1.attention.pe_output.1 ‚ñÅ‚ñà
wandb:    activations/l2_norm/model.transformer.layers.1.attention.q_input.0 ‚ñà‚ñÅ
wandb:   activations/l2_norm/model.transformer.layers.1.attention.q_output.0 ‚ñà‚ñÅ
wandb:   activations/l2_norm/model.transformer.layers.1.attention.q_output.1 ‚ñà‚ñÅ
wandb:      activations/l2_norm/model.transformer.layers.1.attention_input.0 ‚ñà‚ñÅ
wandb:      activations/l2_norm/model.transformer.layers.1.attention_input.1 ‚ñÅ‚ñà
wandb:      activations/l2_norm/model.transformer.layers.1.attention_input.2 ‚ñÅ‚ñà
wandb:     activations/l2_norm/model.transformer.layers.1.attention_output.0 ‚ñÅ‚ñà
wandb:      activations/l2_norm/model.transformer.layers.1.attn_post_input.0 ‚ñÅ‚ñà
wandb:     activations/l2_norm/model.transformer.layers.1.attn_post_output.0 ‚ñà‚ñÅ
wandb:     activations/l2_norm/model.transformer.layers.1.attn_post_output.1 ‚ñà‚ñÅ
wandb:       activations/l2_norm/model.transformer.layers.1.attn_pre_input.0 ‚ñÅ‚ñà
wandb:      activations/l2_norm/model.transformer.layers.1.attn_pre_output.0 ‚ñÅ‚ñà
wandb:      activations/l2_norm/model.transformer.layers.1.attn_pre_output.1 ‚ñÅ‚ñà
wandb:            activations/l2_norm/model.transformer.layers.1.ffn_input.0 ‚ñà‚ñÅ
wandb:            activations/l2_norm/model.transformer.layers.1.ffn_input.1 ‚ñà‚ñÅ
wandb:           activations/l2_norm/model.transformer.layers.1.ffn_output.0 ‚ñÅ‚ñà
wandb:       activations/l2_norm/model.transformer.layers.1.ffn_post_input.0 ‚ñÅ‚ñà
wandb:      activations/l2_norm/model.transformer.layers.1.ffn_post_output.0 ‚ñà‚ñÅ
wandb:      activations/l2_norm/model.transformer.layers.1.ffn_post_output.1 ‚ñà‚ñÅ
wandb:        activations/l2_norm/model.transformer.layers.1.ffn_pre_input.0 ‚ñÅ‚ñà
wandb:       activations/l2_norm/model.transformer.layers.1.ffn_pre_output.0 ‚ñÅ‚ñà
wandb:       activations/l2_norm/model.transformer.layers.1.ffn_pre_output.1 ‚ñà‚ñÅ
wandb:                activations/l2_norm/model.transformer.layers.1_input.0 ‚ñÅ‚ñà
wandb:                activations/l2_norm/model.transformer.layers.1_input.2 ‚ñà‚ñÅ
wandb:                activations/l2_norm/model.transformer.layers.1_input.3 ‚ñÅ‚ñà
wandb:                activations/l2_norm/model.transformer.layers.1_input.4 ‚ñà‚ñÅ
wandb:                activations/l2_norm/model.transformer.layers.1_input.5 ‚ñÅ‚ñà
wandb:               activations/l2_norm/model.transformer.layers.1_output.0 ‚ñÅ‚ñà
wandb:               activations/l2_norm/model.transformer.layers.1_output.4 ‚ñà‚ñÅ
wandb:                         activations/l2_norm/model.transformer_input.0 ‚ñà‚ñÅ
wandb:                         activations/l2_norm/model.transformer_input.1 ‚ñà‚ñÅ
wandb:                        activations/l2_norm/model.transformer_output.0 ‚ñÅ‚ñà
wandb:                                    activations/l2_norm/model_output.0 ‚ñà‚ñÅ
wandb:                                             activations/max/_output.0 ‚ñÅ‚ñà
wandb:                              activations/max/model.embedding_output.0 ‚ñà‚ñÅ
wandb:                              activations/max/model.embedding_output.1 ‚ñÅ‚ñà
wandb:                                 activations/max/model.lm_head_input.0 ‚ñà‚ñÅ
wandb:                                activations/max/model.lm_head_output.0 ‚ñÅ‚ñà
wandb:                                activations/max/model.lm_head_output.1 ‚ñÅ‚ñà
wandb:                                activations/max/model.out_norm_input.0 ‚ñÅ‚ñà
wandb:                               activations/max/model.out_norm_output.0 ‚ñà‚ñÅ
wandb:                               activations/max/model.out_norm_output.1 ‚ñà‚ñÅ
wandb:        activations/max/model.transformer.layers.0.attention.k_input.0 ‚ñÅ‚ñà
wandb:       activations/max/model.transformer.layers.0.attention.k_output.0 ‚ñÅ‚ñà
wandb:       activations/max/model.transformer.layers.0.attention.k_output.1 ‚ñÅ‚ñà
wandb:       activations/max/model.transformer.layers.0.attention.pe_input.0 ‚ñÅ‚ñà
wandb:       activations/max/model.transformer.layers.0.attention.pe_input.1 ‚ñÅ‚ñà
wandb:      activations/max/model.transformer.layers.0.attention.pe_output.0 ‚ñà‚ñÅ
wandb:      activations/max/model.transformer.layers.0.attention.pe_output.1 ‚ñà‚ñÅ
wandb:        activations/max/model.transformer.layers.0.attention.q_input.0 ‚ñÅ‚ñà
wandb:       activations/max/model.transformer.layers.0.attention.q_output.0 ‚ñÅ‚ñÅ
wandb:       activations/max/model.transformer.layers.0.attention.q_output.1 ‚ñÅ‚ñÅ
wandb:          activations/max/model.transformer.layers.0.attention_input.0 ‚ñÅ‚ñà
wandb:          activations/max/model.transformer.layers.0.attention_input.1 ‚ñÅ‚ñà
wandb:          activations/max/model.transformer.layers.0.attention_input.2 ‚ñÅ‚ñà
wandb:         activations/max/model.transformer.layers.0.attention_output.0 ‚ñÅ‚ñà
wandb:          activations/max/model.transformer.layers.0.attn_post_input.0 ‚ñÅ‚ñà
wandb:         activations/max/model.transformer.layers.0.attn_post_output.0 ‚ñÅ‚ñà
wandb:         activations/max/model.transformer.layers.0.attn_post_output.1 ‚ñÅ‚ñÅ
wandb:           activations/max/model.transformer.layers.0.attn_pre_input.0 ‚ñÅ‚ñà
wandb:          activations/max/model.transformer.layers.0.attn_pre_output.0 ‚ñà‚ñÅ
wandb:          activations/max/model.transformer.layers.0.attn_pre_output.1 ‚ñÅ‚ñà
wandb:                activations/max/model.transformer.layers.0.ffn_input.0 ‚ñÅ‚ñà
wandb:                activations/max/model.transformer.layers.0.ffn_input.1 ‚ñÅ‚ñà
wandb:               activations/max/model.transformer.layers.0.ffn_output.0 ‚ñÅ‚ñà
wandb:           activations/max/model.transformer.layers.0.ffn_post_input.0 ‚ñÅ‚ñà
wandb:          activations/max/model.transformer.layers.0.ffn_post_output.0 ‚ñÅ‚ñà
wandb:          activations/max/model.transformer.layers.0.ffn_post_output.1 ‚ñÅ‚ñà
wandb:            activations/max/model.transformer.layers.0.ffn_pre_input.0 ‚ñÅ‚ñà
wandb:           activations/max/model.transformer.layers.0.ffn_pre_output.0 ‚ñÅ‚ñà
wandb:           activations/max/model.transformer.layers.0.ffn_pre_output.1 ‚ñà‚ñÅ
wandb:                    activations/max/model.transformer.layers.0_input.0 ‚ñÅ‚ñà
wandb:                    activations/max/model.transformer.layers.0_input.2 ‚ñÅ‚ñà
wandb:                    activations/max/model.transformer.layers.0_input.3 ‚ñÅ‚ñà
wandb:                    activations/max/model.transformer.layers.0_input.4 ‚ñÅ‚ñÅ
wandb:                    activations/max/model.transformer.layers.0_input.5 ‚ñÅ‚ñà
wandb:                   activations/max/model.transformer.layers.0_output.0 ‚ñÅ‚ñà
wandb:                   activations/max/model.transformer.layers.0_output.4 ‚ñà‚ñÅ
wandb:        activations/max/model.transformer.layers.1.attention.k_input.0 ‚ñà‚ñÅ
wandb:       activations/max/model.transformer.layers.1.attention.k_output.0 ‚ñà‚ñÅ
wandb:       activations/max/model.transformer.layers.1.attention.k_output.1 ‚ñÅ‚ñà
wandb:       activations/max/model.transformer.layers.1.attention.pe_input.0 ‚ñà‚ñÅ
wandb:       activations/max/model.transformer.layers.1.attention.pe_input.1 ‚ñà‚ñÅ
wandb:      activations/max/model.transformer.layers.1.attention.pe_output.0 ‚ñà‚ñÅ
wandb:      activations/max/model.transformer.layers.1.attention.pe_output.1 ‚ñà‚ñÅ
wandb:        activations/max/model.transformer.layers.1.attention.q_input.0 ‚ñà‚ñÅ
wandb:       activations/max/model.transformer.layers.1.attention.q_output.0 ‚ñà‚ñÅ
wandb:       activations/max/model.transformer.layers.1.attention.q_output.1 ‚ñà‚ñÅ
wandb:          activations/max/model.transformer.layers.1.attention_input.0 ‚ñà‚ñÅ
wandb:          activations/max/model.transformer.layers.1.attention_input.1 ‚ñà‚ñÅ
wandb:          activations/max/model.transformer.layers.1.attention_input.2 ‚ñà‚ñÅ
wandb:         activations/max/model.transformer.layers.1.attention_output.0 ‚ñÅ‚ñà
wandb:          activations/max/model.transformer.layers.1.attn_post_input.0 ‚ñÅ‚ñà
wandb:         activations/max/model.transformer.layers.1.attn_post_output.0 ‚ñà‚ñÅ
wandb:         activations/max/model.transformer.layers.1.attn_post_output.1 ‚ñà‚ñÅ
wandb:           activations/max/model.transformer.layers.1.attn_pre_input.0 ‚ñÅ‚ñà
wandb:          activations/max/model.transformer.layers.1.attn_pre_output.0 ‚ñà‚ñÅ
wandb:          activations/max/model.transformer.layers.1.attn_pre_output.1 ‚ñà‚ñÅ
wandb:                activations/max/model.transformer.layers.1.ffn_input.0 ‚ñà‚ñÅ
wandb:                activations/max/model.transformer.layers.1.ffn_input.1 ‚ñà‚ñÅ
wandb:               activations/max/model.transformer.layers.1.ffn_output.0 ‚ñÅ‚ñà
wandb:           activations/max/model.transformer.layers.1.ffn_post_input.0 ‚ñÅ‚ñà
wandb:          activations/max/model.transformer.layers.1.ffn_post_output.0 ‚ñà‚ñÅ
wandb:          activations/max/model.transformer.layers.1.ffn_post_output.1 ‚ñà‚ñÅ
wandb:            activations/max/model.transformer.layers.1.ffn_pre_input.0 ‚ñÅ‚ñà
wandb:           activations/max/model.transformer.layers.1.ffn_pre_output.0 ‚ñÅ‚ñà
wandb:           activations/max/model.transformer.layers.1.ffn_pre_output.1 ‚ñà‚ñÅ
wandb:                    activations/max/model.transformer.layers.1_input.0 ‚ñÅ‚ñà
wandb:                    activations/max/model.transformer.layers.1_input.2 ‚ñÅ‚ñà
wandb:                    activations/max/model.transformer.layers.1_input.3 ‚ñÅ‚ñà
wandb:                    activations/max/model.transformer.layers.1_input.4 ‚ñà‚ñÅ
wandb:                    activations/max/model.transformer.layers.1_input.5 ‚ñÅ‚ñà
wandb:                   activations/max/model.transformer.layers.1_output.0 ‚ñÅ‚ñà
wandb:                   activations/max/model.transformer.layers.1_output.4 ‚ñÅ‚ñà
wandb:                             activations/max/model.transformer_input.0 ‚ñÅ‚ñà
wandb:                             activations/max/model.transformer_input.1 ‚ñÅ‚ñà
wandb:                            activations/max/model.transformer_output.0 ‚ñÅ‚ñà
wandb:                                        activations/max/model_output.0 ‚ñÅ‚ñà
wandb:                                                   l2_norm/grad/global ‚ñà‚ñÑ‚ñÅ‚ñÇ
wandb:                                   l2_norm/grad/model.embedding.weight ‚ñà‚ñÉ‚ñÅ‚ñÉ
wandb:                                       l2_norm/grad/model.lm_head.bias ‚ñà‚ñá‚ñÑ‚ñÅ
wandb:                                     l2_norm/grad/model.lm_head.weight ‚ñà‚ñÇ‚ñÅ‚ñÑ
wandb:                                    l2_norm/grad/model.out_norm.weight ‚ñÅ‚ñÅ‚ñÉ‚ñà
wandb:            l2_norm/grad/model.transformer.layers.0.attention.k.weight ‚ñà‚ñÉ‚ñÅ‚ñÉ
wandb:                   l2_norm/grad/model.transformer.layers.0.attention.o ‚ñà‚ñÑ‚ñÅ‚ñÉ
wandb:            l2_norm/grad/model.transformer.layers.0.attention.q.weight ‚ñà‚ñÉ‚ñÅ‚ñÇ
wandb:               l2_norm/grad/model.transformer.layers.0.attention.sel_o ‚ñà‚ñÉ‚ñÅ‚ñÜ
wandb:               l2_norm/grad/model.transformer.layers.0.attention.sel_v ‚ñá‚ñÉ‚ñÅ‚ñà
wandb:                   l2_norm/grad/model.transformer.layers.0.attention.v ‚ñà‚ñÉ‚ñÅ‚ñÇ
wandb:              l2_norm/grad/model.transformer.layers.0.attn_post.weight ‚ñà‚ñÑ‚ñÅ‚ñÖ
wandb:               l2_norm/grad/model.transformer.layers.0.attn_pre.weight ‚ñà‚ñÑ‚ñÅ‚ñÉ
wandb:                l2_norm/grad/model.transformer.layers.0.ffn.expert_sel ‚ñà‚ñÑ‚ñÅ‚ñÉ
wandb:                      l2_norm/grad/model.transformer.layers.0.ffn.keys ‚ñà‚ñÑ‚ñÅ‚ñÉ
wandb:                    l2_norm/grad/model.transformer.layers.0.ffn.values ‚ñà‚ñÑ‚ñÅ‚ñÉ
wandb:               l2_norm/grad/model.transformer.layers.0.ffn_post.weight ‚ñà‚ñÑ‚ñÅ‚ñÖ
wandb:                l2_norm/grad/model.transformer.layers.0.ffn_pre.weight ‚ñà‚ñÑ‚ñÅ‚ñÑ
wandb:            l2_norm/grad/model.transformer.layers.1.attention.k.weight ‚ñÅ‚ñà‚ñá‚ñÅ
wandb:                   l2_norm/grad/model.transformer.layers.1.attention.o ‚ñÅ‚ñà‚ñà‚ñÅ
wandb:            l2_norm/grad/model.transformer.layers.1.attention.q.weight ‚ñÅ‚ñà‚ñá‚ñÅ
wandb:               l2_norm/grad/model.transformer.layers.1.attention.sel_o ‚ñÅ‚ñà‚ñà‚ñÅ
wandb:               l2_norm/grad/model.transformer.layers.1.attention.sel_v ‚ñÅ‚ñà‚ñá‚ñÅ
wandb:                   l2_norm/grad/model.transformer.layers.1.attention.v ‚ñÅ‚ñà‚ñà‚ñÅ
wandb:              l2_norm/grad/model.transformer.layers.1.attn_post.weight ‚ñÅ‚ñÜ‚ñà‚ñÅ
wandb:               l2_norm/grad/model.transformer.layers.1.attn_pre.weight ‚ñÅ‚ñà‚ñà‚ñÅ
wandb:                l2_norm/grad/model.transformer.layers.1.ffn.expert_sel ‚ñÅ‚ñà‚ñá‚ñÅ
wandb:                      l2_norm/grad/model.transformer.layers.1.ffn.keys ‚ñÅ‚ñà‚ñá‚ñÅ
wandb:                    l2_norm/grad/model.transformer.layers.1.ffn.values ‚ñÅ‚ñà‚ñà‚ñÅ
wandb:               l2_norm/grad/model.transformer.layers.1.ffn_post.weight ‚ñÅ‚ñà‚ñà‚ñÅ
wandb:                l2_norm/grad/model.transformer.layers.1.ffn_pre.weight ‚ñÅ‚ñà‚ñà‚ñÅ
wandb:                                 l2_norm/grad/model.transformer.router ‚ñÅ‚ñÉ‚ñà‚ñÅ
wandb:                                    l2_norm/grad/model.transformer.tau ‚ñÅ‚ñÉ‚ñÉ‚ñà
wandb:                                 l2_norm/moment/model.embedding.weight ‚ñá‚ñà‚ñÑ‚ñÅ
wandb:                                     l2_norm/moment/model.lm_head.bias ‚ñÅ‚ñÜ‚ñà‚ñá
wandb:                                   l2_norm/moment/model.lm_head.weight ‚ñÅ‚ñÖ‚ñà‚ñà
wandb:                                  l2_norm/moment/model.out_norm.weight ‚ñÅ‚ñÇ‚ñÖ‚ñà
wandb:          l2_norm/moment/model.transformer.layers.0.attention.k.weight ‚ñÅ‚ñà‚ñÑ‚ñÅ
wandb:                 l2_norm/moment/model.transformer.layers.0.attention.o ‚ñÖ‚ñà‚ñÖ‚ñÅ
wandb:          l2_norm/moment/model.transformer.layers.0.attention.q.weight ‚ñÅ‚ñà‚ñÖ‚ñÅ
wandb:             l2_norm/moment/model.transformer.layers.0.attention.sel_o ‚ñÇ‚ñà‚ñÖ‚ñÅ
wandb:             l2_norm/moment/model.transformer.layers.0.attention.sel_v ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:                 l2_norm/moment/model.transformer.layers.0.attention.v ‚ñá‚ñà‚ñÑ‚ñÅ
wandb:            l2_norm/moment/model.transformer.layers.0.attn_post.weight ‚ñÉ‚ñà‚ñÑ‚ñÅ
wandb:             l2_norm/moment/model.transformer.layers.0.attn_pre.weight ‚ñÜ‚ñà‚ñÉ‚ñÅ
wandb:              l2_norm/moment/model.transformer.layers.0.ffn.expert_sel ‚ñÖ‚ñà‚ñÖ‚ñÅ
wandb:                    l2_norm/moment/model.transformer.layers.0.ffn.keys ‚ñÖ‚ñà‚ñÖ‚ñÅ
wandb:                  l2_norm/moment/model.transformer.layers.0.ffn.values ‚ñÜ‚ñà‚ñÑ‚ñÅ
wandb:             l2_norm/moment/model.transformer.layers.0.ffn_post.weight ‚ñÅ‚ñà‚ñÇ‚ñÉ
wandb:              l2_norm/moment/model.transformer.layers.0.ffn_pre.weight ‚ñÖ‚ñà‚ñÑ‚ñÅ
wandb:          l2_norm/moment/model.transformer.layers.1.attention.k.weight ‚ñÖ‚ñà‚ñÇ‚ñÅ
wandb:                 l2_norm/moment/model.transformer.layers.1.attention.o ‚ñÉ‚ñà‚ñÑ‚ñÅ
wandb:          l2_norm/moment/model.transformer.layers.1.attention.q.weight ‚ñÖ‚ñà‚ñÉ‚ñÅ
wandb:             l2_norm/moment/model.transformer.layers.1.attention.sel_o ‚ñÅ‚ñÜ‚ñà‚ñÖ
wandb:             l2_norm/moment/model.transformer.layers.1.attention.sel_v ‚ñÇ‚ñá‚ñà‚ñÅ
wandb:                 l2_norm/moment/model.transformer.layers.1.attention.v ‚ñÑ‚ñà‚ñÑ‚ñÅ
wandb:            l2_norm/moment/model.transformer.layers.1.attn_post.weight ‚ñÅ‚ñà‚ñá‚ñÜ
wandb:             l2_norm/moment/model.transformer.layers.1.attn_pre.weight ‚ñÉ‚ñà‚ñÑ‚ñÅ
wandb:              l2_norm/moment/model.transformer.layers.1.ffn.expert_sel ‚ñÉ‚ñà‚ñÑ‚ñÅ
wandb:                    l2_norm/moment/model.transformer.layers.1.ffn.keys ‚ñÉ‚ñà‚ñÑ‚ñÅ
wandb:                  l2_norm/moment/model.transformer.layers.1.ffn.values ‚ñÉ‚ñà‚ñÑ‚ñÅ
wandb:             l2_norm/moment/model.transformer.layers.1.ffn_post.weight ‚ñÇ‚ñà‚ñÑ‚ñÅ
wandb:              l2_norm/moment/model.transformer.layers.1.ffn_pre.weight ‚ñÉ‚ñà‚ñÖ‚ñÅ
wandb:                               l2_norm/moment/model.transformer.router ‚ñÅ‚ñÉ‚ñà‚ñà
wandb:                                  l2_norm/moment/model.transformer.tau ‚ñÅ‚ñÇ‚ñá‚ñà
wandb:                                  l2_norm/param/model.embedding.weight ‚ñà‚ñá‚ñÑ‚ñÅ
wandb:                                      l2_norm/param/model.lm_head.bias ‚ñà‚ñÖ‚ñÇ‚ñÅ
wandb:                                    l2_norm/param/model.lm_head.weight ‚ñà‚ñÜ‚ñÑ‚ñÅ
wandb:                                   l2_norm/param/model.out_norm.weight ‚ñÅ‚ñÅ‚ñÉ‚ñà
wandb:           l2_norm/param/model.transformer.layers.0.attention.k.weight ‚ñÅ‚ñÅ‚ñÉ‚ñà
wandb:                  l2_norm/param/model.transformer.layers.0.attention.o ‚ñÅ‚ñÅ‚ñÑ‚ñà
wandb:           l2_norm/param/model.transformer.layers.0.attention.q.weight ‚ñÅ‚ñÅ‚ñÉ‚ñà
wandb:              l2_norm/param/model.transformer.layers.0.attention.sel_o ‚ñÅ‚ñÑ‚ñà‚ñá
wandb:              l2_norm/param/model.transformer.layers.0.attention.sel_v ‚ñÅ‚ñà‚ñÜ‚ñÇ
wandb:                  l2_norm/param/model.transformer.layers.0.attention.v ‚ñÅ‚ñÉ‚ñà‚ñà
wandb:             l2_norm/param/model.transformer.layers.0.attn_post.weight ‚ñà‚ñà‚ñÜ‚ñÅ
wandb:              l2_norm/param/model.transformer.layers.0.attn_pre.weight ‚ñà‚ñÜ‚ñÑ‚ñÅ
wandb:               l2_norm/param/model.transformer.layers.0.ffn.expert_sel ‚ñÅ‚ñÅ‚ñÉ‚ñà
wandb:                     l2_norm/param/model.transformer.layers.0.ffn.keys ‚ñÅ‚ñÇ‚ñÑ‚ñà
wandb:                   l2_norm/param/model.transformer.layers.0.ffn.values ‚ñÅ‚ñÇ‚ñÑ‚ñà
wandb:              l2_norm/param/model.transformer.layers.0.ffn_post.weight ‚ñÅ‚ñÅ‚ñÉ‚ñà
wandb:               l2_norm/param/model.transformer.layers.0.ffn_pre.weight ‚ñà‚ñÜ‚ñÉ‚ñÅ
wandb:           l2_norm/param/model.transformer.layers.1.attention.k.weight ‚ñÅ‚ñÉ‚ñÖ‚ñà
wandb:                  l2_norm/param/model.transformer.layers.1.attention.o ‚ñÇ‚ñÅ‚ñÑ‚ñà
wandb:           l2_norm/param/model.transformer.layers.1.attention.q.weight ‚ñÅ‚ñÉ‚ñÖ‚ñà
wandb:              l2_norm/param/model.transformer.layers.1.attention.sel_o ‚ñÜ‚ñà‚ñÑ‚ñÅ
wandb:              l2_norm/param/model.transformer.layers.1.attention.sel_v ‚ñà‚ñá‚ñÑ‚ñÅ
wandb:                  l2_norm/param/model.transformer.layers.1.attention.v ‚ñÅ‚ñÇ‚ñÖ‚ñà
wandb:             l2_norm/param/model.transformer.layers.1.attn_post.weight ‚ñà‚ñá‚ñÖ‚ñÅ
wandb:              l2_norm/param/model.transformer.layers.1.attn_pre.weight ‚ñà‚ñá‚ñÖ‚ñÅ
wandb:               l2_norm/param/model.transformer.layers.1.ffn.expert_sel ‚ñà‚ñá‚ñÖ‚ñÅ
wandb:                     l2_norm/param/model.transformer.layers.1.ffn.keys ‚ñÅ‚ñÅ‚ñÑ‚ñà
wandb:                   l2_norm/param/model.transformer.layers.1.ffn.values ‚ñÅ‚ñÇ‚ñÖ‚ñà
wandb:              l2_norm/param/model.transformer.layers.1.ffn_post.weight ‚ñà‚ñà‚ñá‚ñÅ
wandb:               l2_norm/param/model.transformer.layers.1.ffn_pre.weight ‚ñà‚ñá‚ñÑ‚ñÅ
wandb:                                l2_norm/param/model.transformer.router ‚ñÅ‚ñÇ‚ñÑ‚ñà
wandb:                                   l2_norm/param/model.transformer.tau ‚ñÅ‚ñÇ‚ñÑ‚ñà
wandb:                                 l2_norm/update/model.embedding.weight ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:                                     l2_norm/update/model.lm_head.bias ‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:                                   l2_norm/update/model.lm_head.weight ‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:                                  l2_norm/update/model.out_norm.weight ‚ñÅ‚ñÇ‚ñÖ‚ñà
wandb:          l2_norm/update/model.transformer.layers.0.attention.k.weight ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:                 l2_norm/update/model.transformer.layers.0.attention.o ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:          l2_norm/update/model.transformer.layers.0.attention.q.weight ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:             l2_norm/update/model.transformer.layers.0.attention.sel_o ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:             l2_norm/update/model.transformer.layers.0.attention.sel_v ‚ñÅ‚ñÉ‚ñÖ‚ñà
wandb:                 l2_norm/update/model.transformer.layers.0.attention.v ‚ñÅ‚ñÖ‚ñÜ‚ñà
wandb:            l2_norm/update/model.transformer.layers.0.attn_post.weight ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:             l2_norm/update/model.transformer.layers.0.attn_pre.weight ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:              l2_norm/update/model.transformer.layers.0.ffn.expert_sel ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:                    l2_norm/update/model.transformer.layers.0.ffn.keys ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:                  l2_norm/update/model.transformer.layers.0.ffn.values ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:             l2_norm/update/model.transformer.layers.0.ffn_post.weight ‚ñÅ‚ñÉ‚ñÖ‚ñà
wandb:              l2_norm/update/model.transformer.layers.0.ffn_pre.weight ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:          l2_norm/update/model.transformer.layers.1.attention.k.weight ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:                 l2_norm/update/model.transformer.layers.1.attention.o ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:          l2_norm/update/model.transformer.layers.1.attention.q.weight ‚ñÅ‚ñÖ‚ñÜ‚ñà
wandb:             l2_norm/update/model.transformer.layers.1.attention.sel_o ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:             l2_norm/update/model.transformer.layers.1.attention.sel_v ‚ñÅ‚ñÖ‚ñà‚ñá
wandb:                 l2_norm/update/model.transformer.layers.1.attention.v ‚ñÅ‚ñÖ‚ñá‚ñà
wandb:            l2_norm/update/model.transformer.layers.1.attn_post.weight ‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:             l2_norm/update/model.transformer.layers.1.attn_pre.weight ‚ñÅ‚ñÖ‚ñá‚ñà
wandb:              l2_norm/update/model.transformer.layers.1.ffn.expert_sel ‚ñÅ‚ñÖ‚ñá‚ñà
wandb:                    l2_norm/update/model.transformer.layers.1.ffn.keys ‚ñÅ‚ñÖ‚ñá‚ñà
wandb:                  l2_norm/update/model.transformer.layers.1.ffn.values ‚ñÅ‚ñÖ‚ñá‚ñà
wandb:             l2_norm/update/model.transformer.layers.1.ffn_post.weight ‚ñÅ‚ñÖ‚ñá‚ñà
wandb:              l2_norm/update/model.transformer.layers.1.ffn_pre.weight ‚ñÅ‚ñÑ‚ñá‚ñà
wandb:                               l2_norm/update/model.transformer.router ‚ñÅ‚ñÑ‚ñá‚ñà
wandb:                                  l2_norm/update/model.transformer.tau ‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:                                                      loss/train/total ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ
wandb:                                              lr-DecoupledAdamW/group0 ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                                                  memory/alloc_retries ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             memory/current_active_mem ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                          memory/current_allocated_mem ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                           memory/current_inactive_mem ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                           memory/current_reserved_mem ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                memory/peak_active_mem ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                             memory/peak_allocated_mem ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                              memory/peak_inactive_mem ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                              memory/peak_reserved_mem ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                  metrics/exit_entropy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                               metrics/shannon_entropy ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÅ
wandb:                                    metrics/train/LanguageCrossEntropy ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ
wandb:                                      metrics/train/LanguagePerplexity ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                           metrics/train/TokenAccuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà
wandb:                                            throughput/batches_per_sec ‚ñÅ‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ
wandb:                                     throughput/device/batches_per_sec ‚ñÅ‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ
wandb:                                     throughput/device/samples_per_sec ‚ñÅ‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ
wandb:                                      throughput/device/tokens_per_sec ‚ñÅ‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ
wandb:                                            throughput/samples_per_sec ‚ñÅ‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ
wandb:                                             throughput/tokens_per_sec ‚ñÅ‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ
wandb:                                                            time/batch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:                                                   time/batch_in_epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:                                                            time/epoch ‚ñÅ‚ñÅ
wandb:                                               time/remaining_estimate ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                           time/sample ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:                                                  time/sample_in_epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:                                                            time/token ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:                                                   time/token_in_epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:                                                            time/total ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                                                            time/train ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                                                              time/val ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                  trainer/device_train_microbatch_size ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                                         activations/average/_output.0 -0.01721
wandb:                          activations/average/model.embedding_output.0 -0.00019
wandb:                          activations/average/model.embedding_output.1 -5e-05
wandb:                             activations/average/model.lm_head_input.0 0.00408
wandb:                            activations/average/model.lm_head_output.0 -0.01758
wandb:                            activations/average/model.lm_head_output.1 -0.01672
wandb:                            activations/average/model.out_norm_input.0 0.00129
wandb:                           activations/average/model.out_norm_output.0 0.00672
wandb:                           activations/average/model.out_norm_output.1 0.00144
wandb:    activations/average/model.transformer.layers.0.attention.k_input.0 -0.00241
wandb:   activations/average/model.transformer.layers.0.attention.k_output.0 -0.0032
wandb:   activations/average/model.transformer.layers.0.attention.k_output.1 -0.00252
wandb:   activations/average/model.transformer.layers.0.attention.pe_input.0 -0.00052
wandb:   activations/average/model.transformer.layers.0.attention.pe_input.1 -0.00182
wandb:  activations/average/model.transformer.layers.0.attention.pe_output.0 0.00014
wandb:  activations/average/model.transformer.layers.0.attention.pe_output.1 -0.0008
wandb:    activations/average/model.transformer.layers.0.attention.q_input.0 -0.00241
wandb:   activations/average/model.transformer.layers.0.attention.q_output.0 -0.00141
wandb:   activations/average/model.transformer.layers.0.attention.q_output.1 -0.00057
wandb:      activations/average/model.transformer.layers.0.attention_input.0 -0.00241
wandb:      activations/average/model.transformer.layers.0.attention_input.1 -0.00241
wandb:      activations/average/model.transformer.layers.0.attention_input.2 -0.00241
wandb:     activations/average/model.transformer.layers.0.attention_output.0 0.00017
wandb:      activations/average/model.transformer.layers.0.attn_post_input.0 0.00017
wandb:     activations/average/model.transformer.layers.0.attn_post_output.0 0.02124
wandb:     activations/average/model.transformer.layers.0.attn_post_output.1 0.01086
wandb:       activations/average/model.transformer.layers.0.attn_pre_input.0 -0.00012
wandb:      activations/average/model.transformer.layers.0.attn_pre_output.0 -0.00388
wandb:      activations/average/model.transformer.layers.0.attn_pre_output.1 -0.00093
wandb:            activations/average/model.transformer.layers.0.ffn_input.0 0.01609
wandb:            activations/average/model.transformer.layers.0.ffn_input.1 0.01609
wandb:           activations/average/model.transformer.layers.0.ffn_output.0 -0.00067
wandb:       activations/average/model.transformer.layers.0.ffn_post_input.0 -0.00067
wandb:      activations/average/model.transformer.layers.0.ffn_post_output.0 -0.01709
wandb:      activations/average/model.transformer.layers.0.ffn_post_output.1 -0.01111
wandb:        activations/average/model.transformer.layers.0.ffn_pre_input.0 0.00388
wandb:       activations/average/model.transformer.layers.0.ffn_pre_output.0 0.02111
wandb:       activations/average/model.transformer.layers.0.ffn_pre_output.1 0.01107
wandb:                activations/average/model.transformer.layers.0_input.0 0.00129
wandb:                activations/average/model.transformer.layers.0_input.2 -0.00012
wandb:                activations/average/model.transformer.layers.0_input.3 -0.00069
wandb:                activations/average/model.transformer.layers.0_input.4 1.47278
wandb:                activations/average/model.transformer.layers.0_input.5 1.00015
wandb:               activations/average/model.transformer.layers.0_output.0 0.00129
wandb:               activations/average/model.transformer.layers.0_output.4 0.48438
wandb:    activations/average/model.transformer.layers.1.attention.k_input.0 0.00079
wandb:   activations/average/model.transformer.layers.1.attention.k_output.0 0.01056
wandb:   activations/average/model.transformer.layers.1.attention.k_output.1 0.01624
wandb:   activations/average/model.transformer.layers.1.attention.pe_input.0 0.0
wandb:   activations/average/model.transformer.layers.1.attention.pe_input.1 0.00424
wandb:  activations/average/model.transformer.layers.1.attention.pe_output.0 0.00254
wandb:  activations/average/model.transformer.layers.1.attention.pe_output.1 0.00266
wandb:    activations/average/model.transformer.layers.1.attention.q_input.0 0.00083
wandb:   activations/average/model.transformer.layers.1.attention.q_output.0 -0.00769
wandb:   activations/average/model.transformer.layers.1.attention.q_output.1 -0.00659
wandb:      activations/average/model.transformer.layers.1.attention_input.0 0.00083
wandb:      activations/average/model.transformer.layers.1.attention_input.1 0.00079
wandb:      activations/average/model.transformer.layers.1.attention_input.2 0.00079
wandb:     activations/average/model.transformer.layers.1.attention_output.0 0.0004
wandb:      activations/average/model.transformer.layers.1.attn_post_input.0 0.0004
wandb:     activations/average/model.transformer.layers.1.attn_post_output.0 0.01807
wandb:     activations/average/model.transformer.layers.1.attn_post_output.1 0.01355
wandb:       activations/average/model.transformer.layers.1.attn_pre_input.0 0.00019
wandb:      activations/average/model.transformer.layers.1.attn_pre_output.0 0.00203
wandb:      activations/average/model.transformer.layers.1.attn_pre_output.1 -0.00045
wandb:            activations/average/model.transformer.layers.1.ffn_input.0 0.01085
wandb:            activations/average/model.transformer.layers.1.ffn_input.1 0.01085
wandb:           activations/average/model.transformer.layers.1.ffn_output.0 -0.00029
wandb:       activations/average/model.transformer.layers.1.ffn_post_input.0 -0.00029
wandb:      activations/average/model.transformer.layers.1.ffn_post_output.0 -0.0072
wandb:      activations/average/model.transformer.layers.1.ffn_post_output.1 -0.00476
wandb:        activations/average/model.transformer.layers.1.ffn_pre_input.0 0.00329
wandb:       activations/average/model.transformer.layers.1.ffn_pre_output.0 0.01516
wandb:       activations/average/model.transformer.layers.1.ffn_pre_output.1 0.00645
wandb:                activations/average/model.transformer.layers.1_input.0 0.00019
wandb:                activations/average/model.transformer.layers.1_input.2 -0.00012
wandb:                activations/average/model.transformer.layers.1_input.3 -0.00069
wandb:                activations/average/model.transformer.layers.1_input.4 0.98884
wandb:                activations/average/model.transformer.layers.1_input.5 1.00015
wandb:               activations/average/model.transformer.layers.1_output.0 0.00129
wandb:               activations/average/model.transformer.layers.1_output.4 0.48828
wandb:                         activations/average/model.transformer_input.0 -0.00012
wandb:                         activations/average/model.transformer_input.1 -0.00012
wandb:                        activations/average/model.transformer_output.0 0.00129
wandb:                                    activations/average/model_output.0 -0.01721
wandb:                                        activations/kurtosis/_output.0 3.07615
wandb:                         activations/kurtosis/model.embedding_output.0 2.98779
wandb:                         activations/kurtosis/model.embedding_output.1 2.98511
wandb:                            activations/kurtosis/model.lm_head_input.0 2.89802
wandb:                           activations/kurtosis/model.lm_head_output.0 3.09536
wandb:                           activations/kurtosis/model.lm_head_output.1 3.05694
wandb:                           activations/kurtosis/model.out_norm_input.0 2.89773
wandb:                          activations/kurtosis/model.out_norm_output.0 2.90611
wandb:                          activations/kurtosis/model.out_norm_output.1 2.88992
wandb:   activations/kurtosis/model.transformer.layers.0.attention.k_input.0 2.98645
wandb:  activations/kurtosis/model.transformer.layers.0.attention.k_output.0 3.00626
wandb:  activations/kurtosis/model.transformer.layers.0.attention.k_output.1 2.99345
wandb:  activations/kurtosis/model.transformer.layers.0.attention.pe_input.0 2.87676
wandb:  activations/kurtosis/model.transformer.layers.0.attention.pe_input.1 2.88654
wandb: activations/kurtosis/model.transformer.layers.0.attention.pe_output.0 2.84627
wandb: activations/kurtosis/model.transformer.layers.0.attention.pe_output.1 2.86492
wandb:   activations/kurtosis/model.transformer.layers.0.attention.q_input.0 2.98645
wandb:  activations/kurtosis/model.transformer.layers.0.attention.q_output.0 3.01497
wandb:  activations/kurtosis/model.transformer.layers.0.attention.q_output.1 2.99021
wandb:     activations/kurtosis/model.transformer.layers.0.attention_input.0 2.98645
wandb:     activations/kurtosis/model.transformer.layers.0.attention_input.1 2.98645
wandb:     activations/kurtosis/model.transformer.layers.0.attention_input.2 2.98645
wandb:    activations/kurtosis/model.transformer.layers.0.attention_output.0 nan
wandb:     activations/kurtosis/model.transformer.layers.0.attn_post_input.0 nan
wandb:    activations/kurtosis/model.transformer.layers.0.attn_post_output.0 nan
wandb:    activations/kurtosis/model.transformer.layers.0.attn_post_output.1 nan
wandb:      activations/kurtosis/model.transformer.layers.0.attn_pre_input.0 2.98645
wandb:     activations/kurtosis/model.transformer.layers.0.attn_pre_output.0 2.98779
wandb:     activations/kurtosis/model.transformer.layers.0.attn_pre_output.1 2.9851
wandb:           activations/kurtosis/model.transformer.layers.0.ffn_input.0 2.95791
wandb:           activations/kurtosis/model.transformer.layers.0.ffn_input.1 2.95791
wandb:          activations/kurtosis/model.transformer.layers.0.ffn_output.0 2.92727
wandb:      activations/kurtosis/model.transformer.layers.0.ffn_post_input.0 2.92727
wandb:     activations/kurtosis/model.transformer.layers.0.ffn_post_output.0 2.92057
wandb:     activations/kurtosis/model.transformer.layers.0.ffn_post_output.1 2.93488
wandb:       activations/kurtosis/model.transformer.layers.0.ffn_pre_input.0 2.9579
wandb:      activations/kurtosis/model.transformer.layers.0.ffn_pre_output.0 2.94798
wandb:      activations/kurtosis/model.transformer.layers.0.ffn_pre_output.1 2.96784
wandb:               activations/kurtosis/model.transformer.layers.0_input.0 2.89773
wandb:               activations/kurtosis/model.transformer.layers.0_input.2 2.98645
wandb:               activations/kurtosis/model.transformer.layers.0_input.3 3.3545
wandb:               activations/kurtosis/model.transformer.layers.0_input.4 2.90183
wandb:               activations/kurtosis/model.transformer.layers.0_input.5 nan
wandb:              activations/kurtosis/model.transformer.layers.0_output.0 2.89773
wandb:              activations/kurtosis/model.transformer.layers.0_output.4 2.58113
wandb:   activations/kurtosis/model.transformer.layers.1.attention.k_input.0 2.91128
wandb:  activations/kurtosis/model.transformer.layers.1.attention.k_output.0 3.03099
wandb:  activations/kurtosis/model.transformer.layers.1.attention.k_output.1 3.02875
wandb:  activations/kurtosis/model.transformer.layers.1.attention.pe_input.0 nan
wandb:  activations/kurtosis/model.transformer.layers.1.attention.pe_input.1 2.87694
wandb: activations/kurtosis/model.transformer.layers.1.attention.pe_output.0 nan
wandb: activations/kurtosis/model.transformer.layers.1.attention.pe_output.1 2.88707
wandb:   activations/kurtosis/model.transformer.layers.1.attention.q_input.0 nan
wandb:  activations/kurtosis/model.transformer.layers.1.attention.q_output.0 nan
wandb:  activations/kurtosis/model.transformer.layers.1.attention.q_output.1 nan
wandb:     activations/kurtosis/model.transformer.layers.1.attention_input.0 nan
wandb:     activations/kurtosis/model.transformer.layers.1.attention_input.1 2.91128
wandb:     activations/kurtosis/model.transformer.layers.1.attention_input.2 2.91128
wandb:    activations/kurtosis/model.transformer.layers.1.attention_output.0 nan
wandb:     activations/kurtosis/model.transformer.layers.1.attn_post_input.0 nan
wandb:    activations/kurtosis/model.transformer.layers.1.attn_post_output.0 nan
wandb:    activations/kurtosis/model.transformer.layers.1.attn_post_output.1 nan
wandb:      activations/kurtosis/model.transformer.layers.1.attn_pre_input.0 2.91117
wandb:     activations/kurtosis/model.transformer.layers.1.attn_pre_output.0 2.90861
wandb:     activations/kurtosis/model.transformer.layers.1.attn_pre_output.1 2.91395
wandb:           activations/kurtosis/model.transformer.layers.1.ffn_input.0 nan
wandb:           activations/kurtosis/model.transformer.layers.1.ffn_input.1 nan
wandb:          activations/kurtosis/model.transformer.layers.1.ffn_output.0 nan
wandb:      activations/kurtosis/model.transformer.layers.1.ffn_post_input.0 nan
wandb:     activations/kurtosis/model.transformer.layers.1.ffn_post_output.0 nan
wandb:     activations/kurtosis/model.transformer.layers.1.ffn_post_output.1 nan
wandb:       activations/kurtosis/model.transformer.layers.1.ffn_pre_input.0 2.93434
wandb:      activations/kurtosis/model.transformer.layers.1.ffn_pre_output.0 2.92588
wandb:      activations/kurtosis/model.transformer.layers.1.ffn_pre_output.1 2.94293
wandb:               activations/kurtosis/model.transformer.layers.1_input.0 2.91117
wandb:               activations/kurtosis/model.transformer.layers.1_input.2 2.98645
wandb:               activations/kurtosis/model.transformer.layers.1_input.3 3.3545
wandb:               activations/kurtosis/model.transformer.layers.1_input.4 2.92102
wandb:               activations/kurtosis/model.transformer.layers.1_input.5 nan
wandb:              activations/kurtosis/model.transformer.layers.1_output.0 2.89773
wandb:              activations/kurtosis/model.transformer.layers.1_output.4 2.99981
wandb:                        activations/kurtosis/model.transformer_input.0 2.98645
wandb:                        activations/kurtosis/model.transformer_input.1 2.98645
wandb:                       activations/kurtosis/model.transformer_output.0 2.89773
wandb:                                   activations/kurtosis/model_output.0 3.07615
wandb:                                         activations/l2_norm/_output.0 127.64046
wandb:                          activations/l2_norm/model.embedding_output.0 1.00339
wandb:                          activations/l2_norm/model.embedding_output.1 1.00019
wandb:                             activations/l2_norm/model.lm_head_input.0 20.29739
wandb:                            activations/l2_norm/model.lm_head_output.0 127.60361
wandb:                            activations/l2_norm/model.lm_head_output.1 127.6773
wandb:                            activations/l2_norm/model.out_norm_input.0 6.0097
wandb:                           activations/l2_norm/model.out_norm_output.0 20.29785
wandb:                           activations/l2_norm/model.out_norm_output.1 20.29692
wandb:    activations/l2_norm/model.transformer.layers.0.attention.k_input.0 20.25594
wandb:   activations/l2_norm/model.transformer.layers.0.attention.k_output.0 6.47455
wandb:   activations/l2_norm/model.transformer.layers.0.attention.k_output.1 6.47044
wandb:   activations/l2_norm/model.transformer.layers.0.attention.pe_input.0 0.75568
wandb:   activations/l2_norm/model.transformer.layers.0.attention.pe_input.1 0.7551
wandb:  activations/l2_norm/model.transformer.layers.0.attention.pe_output.0 0.75571
wandb:  activations/l2_norm/model.transformer.layers.0.attention.pe_output.1 0.75513
wandb:    activations/l2_norm/model.transformer.layers.0.attention.q_input.0 20.25594
wandb:   activations/l2_norm/model.transformer.layers.0.attention.q_output.0 6.4491
wandb:   activations/l2_norm/model.transformer.layers.0.attention.q_output.1 6.46206
wandb:      activations/l2_norm/model.transformer.layers.0.attention_input.0 20.25594
wandb:      activations/l2_norm/model.transformer.layers.0.attention_input.1 20.25594
wandb:      activations/l2_norm/model.transformer.layers.0.attention_input.2 20.25594
wandb:     activations/l2_norm/model.transformer.layers.0.attention_output.0 0.20462
wandb:      activations/l2_norm/model.transformer.layers.0.attn_post_input.0 0.20462
wandb:     activations/l2_norm/model.transformer.layers.0.attn_post_output.0 19.28828
wandb:     activations/l2_norm/model.transformer.layers.0.attn_post_output.1 19.18838
wandb:       activations/l2_norm/model.transformer.layers.0.attn_pre_input.0 1.00179
wandb:      activations/l2_norm/model.transformer.layers.0.attn_pre_output.0 20.25607
wandb:      activations/l2_norm/model.transformer.layers.0.attn_pre_output.1 20.25581
wandb:            activations/l2_norm/model.transformer.layers.0.ffn_input.0 20.29595
wandb:            activations/l2_norm/model.transformer.layers.0.ffn_input.1 20.29595
wandb:           activations/l2_norm/model.transformer.layers.0.ffn_output.0 0.97983
wandb:       activations/l2_norm/model.transformer.layers.0.ffn_post_input.0 0.97983
wandb:      activations/l2_norm/model.transformer.layers.0.ffn_post_output.0 20.25636
wandb:      activations/l2_norm/model.transformer.layers.0.ffn_post_output.1 20.25605
wandb:        activations/l2_norm/model.transformer.layers.0.ffn_pre_input.0 4.91531
wandb:       activations/l2_norm/model.transformer.layers.0.ffn_pre_output.0 20.29593
wandb:       activations/l2_norm/model.transformer.layers.0.ffn_pre_output.1 20.29597
wandb:                activations/l2_norm/model.transformer.layers.0_input.0 6.0097
wandb:                activations/l2_norm/model.transformer.layers.0_input.2 1.00179
wandb:                activations/l2_norm/model.transformer.layers.0_input.3 0.36359
wandb:                activations/l2_norm/model.transformer.layers.0_input.4 47.14312
wandb:                activations/l2_norm/model.transformer.layers.0_input.5 1.00015
wandb:               activations/l2_norm/model.transformer.layers.0_output.0 6.0097
wandb:               activations/l2_norm/model.transformer.layers.0_output.4 15.5008
wandb:    activations/l2_norm/model.transformer.layers.1.attention.k_input.0 20.29615
wandb:   activations/l2_norm/model.transformer.layers.1.attention.k_output.0 6.50538
wandb:   activations/l2_norm/model.transformer.layers.1.attention.k_output.1 6.48759
wandb:   activations/l2_norm/model.transformer.layers.1.attention.pe_input.0 0.56345
wandb:   activations/l2_norm/model.transformer.layers.1.attention.pe_input.1 0.75168
wandb:  activations/l2_norm/model.transformer.layers.1.attention.pe_output.0 0.56347
wandb:  activations/l2_norm/model.transformer.layers.1.attention.pe_output.1 0.7517
wandb:    activations/l2_norm/model.transformer.layers.1.attention.q_input.0 15.20228
wandb:   activations/l2_norm/model.transformer.layers.1.attention.q_output.0 4.93975
wandb:   activations/l2_norm/model.transformer.layers.1.attention.q_output.1 4.59242
wandb:      activations/l2_norm/model.transformer.layers.1.attention_input.0 15.20228
wandb:      activations/l2_norm/model.transformer.layers.1.attention_input.1 20.29615
wandb:      activations/l2_norm/model.transformer.layers.1.attention_input.2 20.29615
wandb:     activations/l2_norm/model.transformer.layers.1.attention_output.0 0.38568
wandb:      activations/l2_norm/model.transformer.layers.1.attn_post_input.0 0.38568
wandb:     activations/l2_norm/model.transformer.layers.1.attn_post_output.0 15.67033
wandb:     activations/l2_norm/model.transformer.layers.1.attn_post_output.1 14.45715
wandb:       activations/l2_norm/model.transformer.layers.1.attn_pre_input.0 4.90728
wandb:      activations/l2_norm/model.transformer.layers.1.attn_pre_output.0 20.29617
wandb:      activations/l2_norm/model.transformer.layers.1.attn_pre_output.1 20.29612
wandb:            activations/l2_norm/model.transformer.layers.1.ffn_input.0 15.20246
wandb:            activations/l2_norm/model.transformer.layers.1.ffn_input.1 15.20246
wandb:           activations/l2_norm/model.transformer.layers.1.ffn_output.0 0.75623
wandb:       activations/l2_norm/model.transformer.layers.1.ffn_post_input.0 0.75623
wandb:      activations/l2_norm/model.transformer.layers.1.ffn_post_output.0 15.7876
wandb:      activations/l2_norm/model.transformer.layers.1.ffn_post_output.1 14.55992
wandb:        activations/l2_norm/model.transformer.layers.1.ffn_pre_input.0 5.72532
wandb:       activations/l2_norm/model.transformer.layers.1.ffn_pre_output.0 20.29653
wandb:       activations/l2_norm/model.transformer.layers.1.ffn_pre_output.1 20.29599
wandb:                activations/l2_norm/model.transformer.layers.1_input.0 4.90728
wandb:                activations/l2_norm/model.transformer.layers.1_input.2 1.00179
wandb:                activations/l2_norm/model.transformer.layers.1_input.3 0.36359
wandb:                activations/l2_norm/model.transformer.layers.1_input.4 31.64809
wandb:                activations/l2_norm/model.transformer.layers.1_input.5 1.00015
wandb:               activations/l2_norm/model.transformer.layers.1_output.0 6.0097
wandb:               activations/l2_norm/model.transformer.layers.1_output.4 15.66478
wandb:                         activations/l2_norm/model.transformer_input.0 1.00179
wandb:                         activations/l2_norm/model.transformer_input.1 1.00179
wandb:                        activations/l2_norm/model.transformer_output.0 6.0097
wandb:                                    activations/l2_norm/model_output.0 127.64046
wandb:                                             activations/max/_output.0 4
wandb:                              activations/max/model.embedding_output.0 0.14661
wandb:                              activations/max/model.embedding_output.1 0.1468
wandb:                                 activations/max/model.lm_head_input.0 2.87735
wandb:                                activations/max/model.lm_head_output.0 4.1875
wandb:                                activations/max/model.lm_head_output.1 3.82812
wandb:                                activations/max/model.out_norm_input.0 0.85179
wandb:                               activations/max/model.out_norm_output.0 2.86871
wandb:                               activations/max/model.out_norm_output.1 2.886
wandb:        activations/max/model.transformer.layers.0.attention.k_input.0 2.96687
wandb:       activations/max/model.transformer.layers.0.attention.k_output.0 1.03906
wandb:       activations/max/model.transformer.layers.0.attention.k_output.1 1.03906
wandb:       activations/max/model.transformer.layers.0.attention.pe_input.0 0.25561
wandb:       activations/max/model.transformer.layers.0.attention.pe_input.1 0.25549
wandb:      activations/max/model.transformer.layers.0.attention.pe_output.0 0.2567
wandb:      activations/max/model.transformer.layers.0.attention.pe_output.1 0.25485
wandb:        activations/max/model.transformer.layers.0.attention.q_input.0 2.96687
wandb:       activations/max/model.transformer.layers.0.attention.q_output.0 1.01562
wandb:       activations/max/model.transformer.layers.0.attention.q_output.1 1.01562
wandb:          activations/max/model.transformer.layers.0.attention_input.0 2.96687
wandb:          activations/max/model.transformer.layers.0.attention_input.1 2.96687
wandb:          activations/max/model.transformer.layers.0.attention_input.2 2.96687
wandb:         activations/max/model.transformer.layers.0.attention_output.0 0.03052
wandb:          activations/max/model.transformer.layers.0.attn_post_input.0 0.03052
wandb:         activations/max/model.transformer.layers.0.attn_post_output.0 2.89062
wandb:         activations/max/model.transformer.layers.0.attn_post_output.1 2.84375
wandb:           activations/max/model.transformer.layers.0.attn_pre_input.0 0.14671
wandb:          activations/max/model.transformer.layers.0.attn_pre_output.0 2.96019
wandb:          activations/max/model.transformer.layers.0.attn_pre_output.1 2.97356
wandb:                activations/max/model.transformer.layers.0.ffn_input.0 3.0048
wandb:                activations/max/model.transformer.layers.0.ffn_input.1 3.0048
wandb:               activations/max/model.transformer.layers.0.ffn_output.0 0.13965
wandb:           activations/max/model.transformer.layers.0.ffn_post_input.0 0.13965
wandb:          activations/max/model.transformer.layers.0.ffn_post_output.0 2.875
wandb:          activations/max/model.transformer.layers.0.ffn_post_output.1 2.90625
wandb:            activations/max/model.transformer.layers.0.ffn_pre_input.0 0.72746
wandb:           activations/max/model.transformer.layers.0.ffn_pre_output.0 3.0282
wandb:           activations/max/model.transformer.layers.0.ffn_pre_output.1 2.98139
wandb:                    activations/max/model.transformer.layers.0_input.0 0.85179
wandb:                    activations/max/model.transformer.layers.0_input.2 0.14671
wandb:                    activations/max/model.transformer.layers.0_input.3 0.05695
wandb:                    activations/max/model.transformer.layers.0_input.4 1.60742
wandb:                    activations/max/model.transformer.layers.0_input.5 1.00015
wandb:                   activations/max/model.transformer.layers.0_output.0 0.85179
wandb:                   activations/max/model.transformer.layers.0_output.4 0.55078
wandb:        activations/max/model.transformer.layers.1.attention.k_input.0 2.91303
wandb:       activations/max/model.transformer.layers.1.attention.k_output.0 1.0625
wandb:       activations/max/model.transformer.layers.1.attention.k_output.1 1.07812
wandb:       activations/max/model.transformer.layers.1.attention.pe_input.0 0.1904
wandb:       activations/max/model.transformer.layers.1.attention.pe_input.1 0.26657
wandb:      activations/max/model.transformer.layers.1.attention.pe_output.0 0.19233
wandb:      activations/max/model.transformer.layers.1.attention.pe_output.1 0.25998
wandb:        activations/max/model.transformer.layers.1.attention.q_input.0 2.19328
wandb:       activations/max/model.transformer.layers.1.attention.q_output.0 0.76562
wandb:       activations/max/model.transformer.layers.1.attention.q_output.1 0.71094
wandb:          activations/max/model.transformer.layers.1.attention_input.0 2.19328
wandb:          activations/max/model.transformer.layers.1.attention_input.1 2.91303
wandb:          activations/max/model.transformer.layers.1.attention_input.2 2.91303
wandb:         activations/max/model.transformer.layers.1.attention_output.0 0.05786
wandb:          activations/max/model.transformer.layers.1.attn_post_input.0 0.05786
wandb:         activations/max/model.transformer.layers.1.attn_post_output.0 2.32812
wandb:         activations/max/model.transformer.layers.1.attn_post_output.1 2.1875
wandb:           activations/max/model.transformer.layers.1.attn_pre_input.0 0.70438
wandb:          activations/max/model.transformer.layers.1.attn_pre_output.0 2.92227
wandb:          activations/max/model.transformer.layers.1.attn_pre_output.1 2.90379
wandb:                activations/max/model.transformer.layers.1.ffn_input.0 2.2615
wandb:                activations/max/model.transformer.layers.1.ffn_input.1 2.2615
wandb:               activations/max/model.transformer.layers.1.ffn_output.0 0.11328
wandb:           activations/max/model.transformer.layers.1.ffn_post_input.0 0.11328
wandb:          activations/max/model.transformer.layers.1.ffn_post_output.0 2.34375
wandb:          activations/max/model.transformer.layers.1.ffn_post_output.1 2.1875
wandb:            activations/max/model.transformer.layers.1.ffn_pre_input.0 0.84441
wandb:           activations/max/model.transformer.layers.1.ffn_pre_output.0 3.00893
wandb:           activations/max/model.transformer.layers.1.ffn_pre_output.1 2.95349
wandb:                    activations/max/model.transformer.layers.1_input.0 0.70438
wandb:                    activations/max/model.transformer.layers.1_input.2 0.14671
wandb:                    activations/max/model.transformer.layers.1_input.3 0.05695
wandb:                    activations/max/model.transformer.layers.1_input.4 1.05664
wandb:                    activations/max/model.transformer.layers.1_input.5 1.00015
wandb:                   activations/max/model.transformer.layers.1_output.0 0.85179
wandb:                   activations/max/model.transformer.layers.1_output.4 0.55078
wandb:                             activations/max/model.transformer_input.0 0.14671
wandb:                             activations/max/model.transformer_input.1 0.14671
wandb:                            activations/max/model.transformer_output.0 0.85179
wandb:                                        activations/max/model_output.0 4
wandb:                                                   l2_norm/grad/global 0.00182
wandb:                                   l2_norm/grad/model.embedding.weight 0.00026
wandb:                                       l2_norm/grad/model.lm_head.bias 4e-05
wandb:                                     l2_norm/grad/model.lm_head.weight 0.00081
wandb:                                    l2_norm/grad/model.out_norm.weight 5e-05
wandb:            l2_norm/grad/model.transformer.layers.0.attention.k.weight 6e-05
wandb:                   l2_norm/grad/model.transformer.layers.0.attention.o 0.00067
wandb:            l2_norm/grad/model.transformer.layers.0.attention.q.weight 6e-05
wandb:               l2_norm/grad/model.transformer.layers.0.attention.sel_o 0.00013
wandb:               l2_norm/grad/model.transformer.layers.0.attention.sel_v 0.00016
wandb:                   l2_norm/grad/model.transformer.layers.0.attention.v 0.00066
wandb:              l2_norm/grad/model.transformer.layers.0.attn_post.weight 2e-05
wandb:               l2_norm/grad/model.transformer.layers.0.attn_pre.weight 1e-05
wandb:                l2_norm/grad/model.transformer.layers.0.ffn.expert_sel 9e-05
wandb:                      l2_norm/grad/model.transformer.layers.0.ffn.keys 0.00051
wandb:                    l2_norm/grad/model.transformer.layers.0.ffn.values 0.00117
wandb:               l2_norm/grad/model.transformer.layers.0.ffn_post.weight 1e-05
wandb:                l2_norm/grad/model.transformer.layers.0.ffn_pre.weight 1e-05
wandb:            l2_norm/grad/model.transformer.layers.1.attention.k.weight 0
wandb:                   l2_norm/grad/model.transformer.layers.1.attention.o 0
wandb:            l2_norm/grad/model.transformer.layers.1.attention.q.weight 0
wandb:               l2_norm/grad/model.transformer.layers.1.attention.sel_o 0
wandb:               l2_norm/grad/model.transformer.layers.1.attention.sel_v 0
wandb:                   l2_norm/grad/model.transformer.layers.1.attention.v 0
wandb:              l2_norm/grad/model.transformer.layers.1.attn_post.weight 0
wandb:               l2_norm/grad/model.transformer.layers.1.attn_pre.weight 0
wandb:                l2_norm/grad/model.transformer.layers.1.ffn.expert_sel 0
wandb:                      l2_norm/grad/model.transformer.layers.1.ffn.keys 0
wandb:                    l2_norm/grad/model.transformer.layers.1.ffn.values 0
wandb:               l2_norm/grad/model.transformer.layers.1.ffn_post.weight 0
wandb:                l2_norm/grad/model.transformer.layers.1.ffn_pre.weight 0
wandb:                                 l2_norm/grad/model.transformer.router 1e-05
wandb:                                    l2_norm/grad/model.transformer.tau 3e-05
wandb:                                 l2_norm/moment/model.embedding.weight 0.00013
wandb:                                     l2_norm/moment/model.lm_head.bias 4e-05
wandb:                                   l2_norm/moment/model.lm_head.weight 0.00065
wandb:                                  l2_norm/moment/model.out_norm.weight 3e-05
wandb:          l2_norm/moment/model.transformer.layers.0.attention.k.weight 4e-05
wandb:                 l2_norm/moment/model.transformer.layers.0.attention.o 0.00041
wandb:          l2_norm/moment/model.transformer.layers.0.attention.q.weight 4e-05
wandb:             l2_norm/moment/model.transformer.layers.0.attention.sel_o 7e-05
wandb:             l2_norm/moment/model.transformer.layers.0.attention.sel_v 0.0001
wandb:                 l2_norm/moment/model.transformer.layers.0.attention.v 0.00028
wandb:            l2_norm/moment/model.transformer.layers.0.attn_post.weight 1e-05
wandb:             l2_norm/moment/model.transformer.layers.0.attn_pre.weight 1e-05
wandb:              l2_norm/moment/model.transformer.layers.0.ffn.expert_sel 5e-05
wandb:                    l2_norm/moment/model.transformer.layers.0.ffn.keys 0.0003
wandb:                  l2_norm/moment/model.transformer.layers.0.ffn.values 0.00066
wandb:             l2_norm/moment/model.transformer.layers.0.ffn_post.weight 1e-05
wandb:              l2_norm/moment/model.transformer.layers.0.ffn_pre.weight 1e-05
wandb:          l2_norm/moment/model.transformer.layers.1.attention.k.weight 0.0
wandb:                 l2_norm/moment/model.transformer.layers.1.attention.o 0.00013
wandb:          l2_norm/moment/model.transformer.layers.1.attention.q.weight 0.0
wandb:             l2_norm/moment/model.transformer.layers.1.attention.sel_o 3e-05
wandb:             l2_norm/moment/model.transformer.layers.1.attention.sel_v 1e-05
wandb:                 l2_norm/moment/model.transformer.layers.1.attention.v 0.00012
wandb:            l2_norm/moment/model.transformer.layers.1.attn_post.weight 0.0
wandb:             l2_norm/moment/model.transformer.layers.1.attn_pre.weight 0.0
wandb:              l2_norm/moment/model.transformer.layers.1.ffn.expert_sel 2e-05
wandb:                    l2_norm/moment/model.transformer.layers.1.ffn.keys 0.00011
wandb:                  l2_norm/moment/model.transformer.layers.1.ffn.values 0.00028
wandb:             l2_norm/moment/model.transformer.layers.1.ffn_post.weight 0.0
wandb:              l2_norm/moment/model.transformer.layers.1.ffn_pre.weight 0.0
wandb:                               l2_norm/moment/model.transformer.router 3e-05
wandb:                                  l2_norm/moment/model.transformer.tau 1e-05
wandb:                                  l2_norm/param/model.embedding.weight 221.75391
wandb:                                      l2_norm/param/model.lm_head.bias 6.28824
wandb:                                    l2_norm/param/model.lm_head.weight 127.95229
wandb:                                   l2_norm/param/model.out_norm.weight 20.30611
wandb:           l2_norm/param/model.transformer.layers.0.attention.k.weight 6.47599
wandb:                  l2_norm/param/model.transformer.layers.0.attention.o 22.69595
wandb:           l2_norm/param/model.transformer.layers.0.attention.q.weight 6.47017
wandb:              l2_norm/param/model.transformer.layers.0.attention.sel_o 2.24934
wandb:              l2_norm/param/model.transformer.layers.0.attention.sel_v 2.23399
wandb:                  l2_norm/param/model.transformer.layers.0.attention.v 20.4717
wandb:             l2_norm/param/model.transformer.layers.0.attn_post.weight 20.29532
wandb:              l2_norm/param/model.transformer.layers.0.attn_pre.weight 20.29737
wandb:               l2_norm/param/model.transformer.layers.0.ffn.expert_sel 4.75194
wandb:                     l2_norm/param/model.transformer.layers.0.ffn.keys 14.99918
wandb:                   l2_norm/param/model.transformer.layers.0.ffn.values 7.19676
wandb:              l2_norm/param/model.transformer.layers.0.ffn_post.weight 20.30353
wandb:               l2_norm/param/model.transformer.layers.0.ffn_pre.weight 20.29767
wandb:           l2_norm/param/model.transformer.layers.1.attention.k.weight 6.47779
wandb:                  l2_norm/param/model.transformer.layers.1.attention.o 22.68928
wandb:           l2_norm/param/model.transformer.layers.1.attention.q.weight 6.48895
wandb:              l2_norm/param/model.transformer.layers.1.attention.sel_o 2.22536
wandb:              l2_norm/param/model.transformer.layers.1.attention.sel_v 2.23561
wandb:                  l2_norm/param/model.transformer.layers.1.attention.v 20.48335
wandb:             l2_norm/param/model.transformer.layers.1.attn_post.weight 20.29367
wandb:              l2_norm/param/model.transformer.layers.1.attn_pre.weight 20.29719
wandb:               l2_norm/param/model.transformer.layers.1.ffn.expert_sel 4.73809
wandb:                     l2_norm/param/model.transformer.layers.1.ffn.keys 15.00718
wandb:                   l2_norm/param/model.transformer.layers.1.ffn.values 7.16745
wandb:              l2_norm/param/model.transformer.layers.1.ffn_post.weight 20.29758
wandb:               l2_norm/param/model.transformer.layers.1.ffn_pre.weight 20.29736
wandb:                                l2_norm/param/model.transformer.router 0.36446
wandb:                                   l2_norm/param/model.transformer.tau 1.0007
wandb:                                 l2_norm/update/model.embedding.weight 0.01544
wandb:                                     l2_norm/update/model.lm_head.bias 0.00486
wandb:                                   l2_norm/update/model.lm_head.weight 0.06427
wandb:                                  l2_norm/update/model.out_norm.weight 0.00091
wandb:          l2_norm/update/model.transformer.layers.0.attention.k.weight 0.00817
wandb:                 l2_norm/update/model.transformer.layers.0.attention.o 0.02616
wandb:          l2_norm/update/model.transformer.layers.0.attention.q.weight 0.00819
wandb:             l2_norm/update/model.transformer.layers.0.attention.sel_o 0.00169
wandb:             l2_norm/update/model.transformer.layers.0.attention.sel_v 0.00186
wandb:                 l2_norm/update/model.transformer.layers.0.attention.v 0.01878
wandb:            l2_norm/update/model.transformer.layers.0.attn_post.weight 0.00053
wandb:             l2_norm/update/model.transformer.layers.0.attn_pre.weight 0.00043
wandb:              l2_norm/update/model.transformer.layers.0.ffn.expert_sel 0.00652
wandb:                    l2_norm/update/model.transformer.layers.0.ffn.keys 0.02032
wandb:                  l2_norm/update/model.transformer.layers.0.ffn.values 0.01968
wandb:             l2_norm/update/model.transformer.layers.0.ffn_post.weight 0.00066
wandb:              l2_norm/update/model.transformer.layers.0.ffn_pre.weight 0.00049
wandb:          l2_norm/update/model.transformer.layers.1.attention.k.weight 0.00327
wandb:                 l2_norm/update/model.transformer.layers.1.attention.o 0.01655
wandb:          l2_norm/update/model.transformer.layers.1.attention.q.weight 0.00324
wandb:             l2_norm/update/model.transformer.layers.1.attention.sel_o 0.00127
wandb:             l2_norm/update/model.transformer.layers.1.attention.sel_v 0.0008
wandb:                 l2_norm/update/model.transformer.layers.1.attention.v 0.01497
wandb:            l2_norm/update/model.transformer.layers.1.attn_post.weight 0.00046
wandb:             l2_norm/update/model.transformer.layers.1.attn_pre.weight 0.00029
wandb:              l2_norm/update/model.transformer.layers.1.ffn.expert_sel 0.0044
wandb:                    l2_norm/update/model.transformer.layers.1.ffn.keys 0.01422
wandb:                  l2_norm/update/model.transformer.layers.1.ffn.values 0.01554
wandb:             l2_norm/update/model.transformer.layers.1.ffn_post.weight 0.00038
wandb:              l2_norm/update/model.transformer.layers.1.ffn_pre.weight 0.00034
wandb:                               l2_norm/update/model.transformer.router 0.00065
wandb:                                  l2_norm/update/model.transformer.tau 5e-05
wandb:                                                      loss/train/total 0.0043
wandb:                                              lr-DecoupledAdamW/group0 5e-05
wandb:                                                  memory/alloc_retries 0
wandb:                                             memory/current_active_mem 1.1885
wandb:                                          memory/current_allocated_mem 1.1885
wandb:                                           memory/current_inactive_mem 0.34033
wandb:                                           memory/current_reserved_mem 3.8483
wandb:                                                memory/peak_active_mem 2.8024
wandb:                                             memory/peak_allocated_mem 2.8024
wandb:                                              memory/peak_inactive_mem 0.85983
wandb:                                              memory/peak_reserved_mem 3.8483
wandb:                                                  metrics/exit_entropy 0.66406
wandb:                                               metrics/shannon_entropy 10.4849
wandb:                                    metrics/train/LanguageCrossEntropy 8.8146
wandb:                                      metrics/train/LanguagePerplexity 6731.78271
wandb:                                           metrics/train/TokenAccuracy 0.225
wandb:                                            throughput/batches_per_sec 0.198
wandb:                                     throughput/device/batches_per_sec 0.198
wandb:                                     throughput/device/samples_per_sec 0.396
wandb:                                      throughput/device/tokens_per_sec 405.50844
wandb:                                            throughput/samples_per_sec 0.396
wandb:                                             throughput/tokens_per_sec 405.50844
wandb:                                                            time/batch 40
wandb:                                                   time/batch_in_epoch 40
wandb:                                                            time/epoch 0
wandb:                                               time/remaining_estimate 0
wandb:                                                           time/sample 80
wandb:                                                  time/sample_in_epoch 80
wandb:                                                            time/token 81920
wandb:                                                   time/token_in_epoch 81920
wandb:                                                            time/total 0.06283
wandb:                                                            time/train 0.06283
wandb:                                                              time/val 0
wandb:                                  trainer/device_train_microbatch_size 10
wandb: 
wandb: üöÄ View run ttkunurg_29jul25_t5dwfglh_scale_add=True_prot_emb=True_d_model=412_n_layers=16_n_heads=4_n_experts_ffn=180_n_experts_attn=10_ff_expert_size=10_dropout=0.0 at: https://wandb.ai/camlsys/dyna/runs/9c43iczg
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/camlsys/dyna
wandb: Synced 5 W&B file(s), 0 media file(s), 320 artifact file(s) and 280 other file(s)
wandb: Find logs at: ./wandb/run-20250729_180722-9c43iczg/logs

tensor([ 1.0000e-02,  8.0000e-03, -2.0000e-03, -6.9849e-10,  6.0000e-03,
        -2.4000e-02,  1.4000e-02, -3.0000e-02,  3.6000e-02, -2.6000e-02],
       device='cuda:0')
selected experts tensor([1783, 1329, 1618, 1662, 1556, 2004, 1894, 1805,  922, 1811],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180],
          [0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180],
          [0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180],
          [0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180]],

         [[0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180],
          [0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180],
          [0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180],
          [0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180]],

         [[0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180],
          [0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180],
          [0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180],
          [0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180]],

         ...,

         [[0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180],
          [0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180],
          [0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180],
          [0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180]],

         [[0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180],
          [0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180],
          [0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180],
          [0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180]],

         [[0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180],
          [0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180],
          [0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180],
          [0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180]]],


        [[[0.6700, 0.6269, 0.5273,  ..., 0.4751, 0.5159, 0.5570],
          [0.5557, 0.5596, 0.4008,  ..., 0.6153, 0.4604, 0.4800],
          [0.3629, 0.4392, 0.5353,  ..., 0.4741, 0.5896, 0.3622],
          [0.5337, 0.6596, 0.3680,  ..., 0.5659, 0.4346, 0.5274]],

         [[0.5930, 0.4696, 0.6501,  ..., 0.4182, 0.5872, 0.4136],
          [0.4995, 0.5192, 0.4936,  ..., 0.6412, 0.5107, 0.4192],
          [0.4088, 0.4635, 0.4562,  ..., 0.5114, 0.5618, 0.3801],
          [0.5137, 0.6320, 0.4767,  ..., 0.5403, 0.4657, 0.5374]],

         [[0.6035, 0.4979, 0.6081,  ..., 0.4438, 0.6182, 0.4659],
          [0.4845, 0.4974, 0.4204,  ..., 0.6857, 0.4421, 0.4249],
          [0.4046, 0.5608, 0.5501,  ..., 0.4210, 0.4964, 0.3287],
          [0.5287, 0.6242, 0.5540,  ..., 0.5779, 0.4645, 0.5165]],

         ...,

         [[0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180],
          [0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180],
          [0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180],
          [0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180]],

         [[0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180],
          [0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180],
          [0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180],
          [0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180]],

         [[0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180],
          [0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180],
          [0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180],
          [0.5160, 0.5100, 0.5140,  ..., 0.5160, 0.5040, 0.5180]]]],
       device='cuda:0')
tensor([[[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.6540, 0.6169, 0.5133,  ..., 0.4591, 0.5119, 0.5390],
          [0.5397, 0.5496, 0.3868,  ..., 0.5993, 0.4564, 0.4620],
          [0.3469, 0.4292, 0.5213,  ..., 0.4581, 0.5856, 0.3442],
          [0.5177, 0.6496, 0.3540,  ..., 0.5499, 0.4306, 0.5094]],

         [[0.5770, 0.4596, 0.6361,  ..., 0.4022, 0.5832, 0.3956],
          [0.4835, 0.5092, 0.4796,  ..., 0.6252, 0.5067, 0.4012],
          [0.3928, 0.4535, 0.4422,  ..., 0.4954, 0.5578, 0.3621],
          [0.4977, 0.6220, 0.4627,  ..., 0.5243, 0.4617, 0.5194]],

         [[0.5875, 0.4879, 0.5941,  ..., 0.4278, 0.6142, 0.4479],
          [0.4685, 0.4874, 0.4064,  ..., 0.6697, 0.4381, 0.4069],
          [0.3886, 0.5508, 0.5361,  ..., 0.4050, 0.4924, 0.3107],
          [0.5127, 0.6142, 0.5400,  ..., 0.5619, 0.4605, 0.4985]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0160, 0.0100, 0.0140, 0.0160, 0.0160, 0.0060, 0.0100, 0.0160, 0.0040,
        0.0180], device='cuda:0')
selected experts tensor([5722,  687,  640,  263,  376,  970,  820,  874,  630, 5402],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
