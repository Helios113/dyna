/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/model/cvmm.py:402: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("mylib::cvmm_triton", cvmm_triton)
[2025-07-29 16:56:29,905][streaming.base.dataset][INFO] - Because `predownload` was not specified, it will default to 8*batch_size if batch_size is not None, otherwise 64.
[2025-07-29 16:56:30,097][composer.utils.reproducibility][INFO] - Setting seed to 1797637623
[2025-07-29 16:56:30,126][composer.trainer.trainer][INFO] - Run name: 1753804590-mustard-sloth
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: preslav-aleksandrov (camlsys). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.7
wandb: Run data is saved locally in /nfs-share/pa511/code_bases/dyna_project/dyna/wandb/run-20250729_165630-mnxj6fh8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 87cbuqjw_29jul25_c33chxzl_scale_add=True_prot_emb=True_d_model=412_n_layers=16_n_heads=4_n_experts_ffn=180_n_experts_attn=10_ff_expert_size=10_dropout=0.0
wandb: ‚≠êÔ∏è View project at https://wandb.ai/camlsys/dyna
wandb: üöÄ View run at https://wandb.ai/camlsys/dyna/runs/mnxj6fh8
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/callbacks/speed_monitor.py:290: UserWarning: gpu_flop count not found for nvidia h100 nvl with precision=amp_bf16 so MFU cannot be calculated and reported. gpu_flops_available can be manually overridden by setting gpu_flops_available in SpeedMonitor or nvidia h100 nvl can be added to GPU_AVAILABLE_FLOPS in composer/callbacks/speed_monitor.py
  self.gpu_flops_available = get_gpu_flops_available(state)
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/trainer/trainer.py:1556: UserWarning: Specifying `eval_interval=100ba` without an `eval_dataloader` has no effect. If trying to run an evaluator, make sure `eval_dataloader` is specified. Otherwise, set `eval_interval` to 0 or default value 1.
  warnings.warn(
[2025-07-29 16:56:31,616][composer.trainer.trainer][INFO] - Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
[2025-07-29 16:56:31,617][composer.trainer.trainer][INFO] - Setting seed to 1797637623
[2025-07-29 16:56:31,617][composer.utils.reproducibility][INFO] - Setting seed to 1797637623
[2025-07-29 16:56:31,617][composer.trainer.trainer][INFO] - Using precision Precision.AMP_BF16
******************************
Config:
composer_commit_hash: None
composer_version: 0.31.0
node_name: unknown because NODENAME environment variable not set
num_gpus_per_node: 1
num_nodes: 1
rank_zero_seed: 1797637623
time/remaining_estimate_unit: hours

******************************
[2025-07-29 16:56:31,681][streaming.base.dataset][INFO] - Because `num_canonical_nodes` was not specified, and `shuffle_algo` is py1e, it will default to be equal to physical nodes.
[2025-07-29 16:56:31,681][streaming.base.dataset][INFO] - Because `shuffle_block_size` was not specified, it will default to max(4_000_000 // num_canonical_nodes, 1 << 18) if num_canonical_nodes is not None, otherwise 262144.
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4140, 0.3895, 0.4173,  ..., 0.5950, 0.5266, 0.4845],
          [0.4397, 0.6142, 0.5718,  ..., 0.5242, 0.3817, 0.3882],
          [0.5159, 0.4438, 0.4721,  ..., 0.5071, 0.5703, 0.5358],
          [0.4499, 0.4388, 0.3720,  ..., 0.5056, 0.3504, 0.5559]],

         [[0.4140, 0.3895, 0.4173,  ..., 0.5950, 0.5266, 0.4845],
          [0.4397, 0.6142, 0.5718,  ..., 0.5242, 0.3817, 0.3882],
          [0.5159, 0.4438, 0.4721,  ..., 0.5071, 0.5703, 0.5358],
          [0.4499, 0.4388, 0.3720,  ..., 0.5056, 0.3504, 0.5559]],

         [[0.6100, 0.5007, 0.5780,  ..., 0.5808, 0.5039, 0.4158],
          [0.4705, 0.6188, 0.6514,  ..., 0.3693, 0.4865, 0.5243],
          [0.4045, 0.5631, 0.5332,  ..., 0.4730, 0.6352, 0.5827],
          [0.4373, 0.3407, 0.4861,  ..., 0.4630, 0.3947, 0.4743]],

         ...,

         [[0.5064, 0.5375, 0.6522,  ..., 0.4986, 0.4301, 0.3090],
          [0.5409, 0.5215, 0.6325,  ..., 0.3504, 0.5319, 0.6397],
          [0.5111, 0.5586, 0.4875,  ..., 0.5213, 0.6593, 0.4158],
          [0.5056, 0.4972, 0.3821,  ..., 0.5433, 0.5889, 0.5922]],

         [[0.6021, 0.4871, 0.4567,  ..., 0.5547, 0.2846, 0.4031],
          [0.4695, 0.4220, 0.3157,  ..., 0.5167, 0.4187, 0.3381],
          [0.5699, 0.4059, 0.4933,  ..., 0.4064, 0.4453, 0.4926],
          [0.3183, 0.4078, 0.5619,  ..., 0.4545, 0.4017, 0.3329]],

         [[0.4955, 0.4830, 0.4340,  ..., 0.4794, 0.5246, 0.4926],
          [0.4036, 0.4441, 0.6610,  ..., 0.5842, 0.5050, 0.5670],
          [0.5770, 0.4064, 0.5211,  ..., 0.4681, 0.5559, 0.5005],
          [0.5993, 0.4173, 0.3831,  ..., 0.2934, 0.5205, 0.4523]]],


        [[[0.4924, 0.5472, 0.4301,  ..., 0.3693, 0.4407, 0.5055],
          [0.5341, 0.4316, 0.6352,  ..., 0.5641, 0.4225, 0.5832],
          [0.5482, 0.5290, 0.2983,  ..., 0.5978, 0.4808, 0.6058],
          [0.4600, 0.4779, 0.6567,  ..., 0.4741, 0.3942, 0.4381]],

         [[0.5291, 0.4321, 0.4022,  ..., 0.4581, 0.4489, 0.5383],
          [0.5789, 0.5761, 0.7050,  ..., 0.5860, 0.5518, 0.3407],
          [0.5284, 0.4598, 0.5574,  ..., 0.3989, 0.6976, 0.5007],
          [0.4681, 0.4752, 0.3785,  ..., 0.5670, 0.5402, 0.4547]],

         [[0.4789, 0.5246, 0.4349,  ..., 0.5450, 0.5775, 0.4530],
          [0.5349, 0.5109, 0.4206,  ..., 0.4287, 0.5964, 0.4373],
          [0.4845, 0.4778, 0.3775,  ..., 0.4637, 0.4335, 0.6067],
          [0.4559, 0.5784, 0.6100,  ..., 0.4369, 0.5404, 0.4902]],

         ...,

         [[0.6160, 0.4944, 0.6114,  ..., 0.3442, 0.4254, 0.5339],
          [0.4297, 0.4889, 0.5660,  ..., 0.4833, 0.4373, 0.4297],
          [0.4135, 0.3285, 0.4325,  ..., 0.4419, 0.5299, 0.4826],
          [0.5223, 0.4921, 0.3882,  ..., 0.4069, 0.4438, 0.4769]],

         [[0.5349, 0.4225, 0.5256,  ..., 0.4673, 0.5378, 0.4523],
          [0.3372, 0.5780, 0.3648,  ..., 0.4301, 0.4540, 0.4373],
          [0.6628, 0.4158, 0.6243,  ..., 0.4586, 0.5460, 0.5441],
          [0.4168, 0.5586, 0.4603,  ..., 0.3831, 0.4912, 0.3469]],

         [[0.5327, 0.4901, 0.3748,  ..., 0.4463, 0.4966, 0.5183],
          [0.5289, 0.4492, 0.5046,  ..., 0.4787, 0.6049, 0.5884],
          [0.3558, 0.6876, 0.5467,  ..., 0.4511, 0.4335, 0.4349],
          [0.5216, 0.4634, 0.4373,  ..., 0.5513, 0.5964, 0.4902]]]],
       device='cuda:0')
tensor([[[[0.4140, 0.3895, 0.4173,  ..., 0.5950, 0.5266, 0.4845],
          [0.4397, 0.6142, 0.5718,  ..., 0.5242, 0.3817, 0.3882],
          [0.5159, 0.4438, 0.4721,  ..., 0.5071, 0.5703, 0.5358],
          [0.4499, 0.4388, 0.3720,  ..., 0.5056, 0.3504, 0.5559]],

         [[0.4140, 0.3895, 0.4173,  ..., 0.5950, 0.5266, 0.4845],
          [0.4397, 0.6142, 0.5718,  ..., 0.5242, 0.3817, 0.3882],
          [0.5159, 0.4438, 0.4721,  ..., 0.5071, 0.5703, 0.5358],
          [0.4499, 0.4388, 0.3720,  ..., 0.5056, 0.3504, 0.5559]],

         [[0.6100, 0.5007, 0.5780,  ..., 0.5808, 0.5039, 0.4158],
          [0.4705, 0.6188, 0.6514,  ..., 0.3693, 0.4865, 0.5243],
          [0.4045, 0.5631, 0.5332,  ..., 0.4730, 0.6352, 0.5827],
          [0.4373, 0.3407, 0.4861,  ..., 0.4630, 0.3947, 0.4743]],

         ...,

         [[0.5064, 0.5375, 0.6522,  ..., 0.4986, 0.4301, 0.3090],
          [0.5409, 0.5215, 0.6325,  ..., 0.3504, 0.5319, 0.6397],
          [0.5111, 0.5586, 0.4875,  ..., 0.5213, 0.6593, 0.4158],
          [0.5056, 0.4972, 0.3821,  ..., 0.5433, 0.5889, 0.5922]],

         [[0.6021, 0.4871, 0.4567,  ..., 0.5547, 0.2846, 0.4031],
          [0.4695, 0.4220, 0.3157,  ..., 0.5167, 0.4187, 0.3381],
          [0.5699, 0.4059, 0.4933,  ..., 0.4064, 0.4453, 0.4926],
          [0.3183, 0.4078, 0.5619,  ..., 0.4545, 0.4017, 0.3329]],

         [[0.4955, 0.4830, 0.4340,  ..., 0.4794, 0.5246, 0.4926],
          [0.4036, 0.4441, 0.6610,  ..., 0.5842, 0.5050, 0.5670],
          [0.5770, 0.4064, 0.5211,  ..., 0.4681, 0.5559, 0.5005],
          [0.5993, 0.4173, 0.3831,  ..., 0.2934, 0.5205, 0.4523]]],


        [[[0.4924, 0.5472, 0.4301,  ..., 0.3693, 0.4407, 0.5055],
          [0.5341, 0.4316, 0.6352,  ..., 0.5641, 0.4225, 0.5832],
          [0.5482, 0.5290, 0.2983,  ..., 0.5978, 0.4808, 0.6058],
          [0.4600, 0.4779, 0.6567,  ..., 0.4741, 0.3942, 0.4381]],

         [[0.5291, 0.4321, 0.4022,  ..., 0.4581, 0.4489, 0.5383],
          [0.5789, 0.5761, 0.7050,  ..., 0.5860, 0.5518, 0.3407],
          [0.5284, 0.4598, 0.5574,  ..., 0.3989, 0.6976, 0.5007],
          [0.4681, 0.4752, 0.3785,  ..., 0.5670, 0.5402, 0.4547]],

         [[0.4789, 0.5246, 0.4349,  ..., 0.5450, 0.5775, 0.4530],
          [0.5349, 0.5109, 0.4206,  ..., 0.4287, 0.5964, 0.4373],
          [0.4845, 0.4778, 0.3775,  ..., 0.4637, 0.4335, 0.6067],
          [0.4559, 0.5784, 0.6100,  ..., 0.4369, 0.5404, 0.4902]],

         ...,

         [[0.6160, 0.4944, 0.6114,  ..., 0.3442, 0.4254, 0.5339],
          [0.4297, 0.4889, 0.5660,  ..., 0.4833, 0.4373, 0.4297],
          [0.4135, 0.3285, 0.4325,  ..., 0.4419, 0.5299, 0.4826],
          [0.5223, 0.4921, 0.3882,  ..., 0.4069, 0.4438, 0.4769]],

         [[0.5349, 0.4225, 0.5256,  ..., 0.4673, 0.5378, 0.4523],
          [0.3372, 0.5780, 0.3648,  ..., 0.4301, 0.4540, 0.4373],
          [0.6628, 0.4158, 0.6243,  ..., 0.4586, 0.5460, 0.5441],
          [0.4168, 0.5586, 0.4603,  ..., 0.3831, 0.4912, 0.3469]],

         [[0.5327, 0.4901, 0.3748,  ..., 0.4463, 0.4966, 0.5183],
          [0.5289, 0.4492, 0.5046,  ..., 0.4787, 0.6049, 0.5884],
          [0.3558, 0.6876, 0.5467,  ..., 0.4511, 0.4335, 0.4349],
          [0.5216, 0.4634, 0.4373,  ..., 0.5513, 0.5964, 0.4902]]]],
       device='cuda:0', requires_grad=True)
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
selected experts tensor([1737, 1637, 1800, 1642, 1784, 1802, 1423, 1473, 1652, 1434],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6496, 0.6058, 0.5361,  ..., 0.4756, 0.5327, 0.4516],
          [0.5032, 0.6697, 0.5250,  ..., 0.5239, 0.6298, 0.4121],
          [0.6039, 0.4707, 0.3390,  ..., 0.3355, 0.5832, 0.3451],
          [0.5178, 0.4431, 0.3998,  ..., 0.7186, 0.5898, 0.4579]],

         [[0.6496, 0.6058, 0.5361,  ..., 0.4756, 0.5327, 0.4516],
          [0.5032, 0.6697, 0.5250,  ..., 0.5239, 0.6298, 0.4121],
          [0.6039, 0.4707, 0.3390,  ..., 0.3355, 0.5832, 0.3451],
          [0.5178, 0.4431, 0.3998,  ..., 0.7186, 0.5898, 0.4579]],

         [[0.4615, 0.5057, 0.4858,  ..., 0.3115, 0.5665, 0.3919],
          [0.4448, 0.4206, 0.3702,  ..., 0.4273, 0.3648, 0.4967],
          [0.4059, 0.6049, 0.5115,  ..., 0.5081, 0.4649, 0.5183],
          [0.4732, 0.6225, 0.6792,  ..., 0.4431, 0.4824, 0.5148]],

         ...,

         [[0.4934, 0.5358, 0.5165,  ..., 0.5789, 0.4908, 0.5477],
          [0.4211, 0.6220, 0.6095,  ..., 0.5492, 0.4479, 0.3900],
          [0.6334, 0.5174, 0.4158,  ..., 0.5842, 0.4707, 0.5431],
          [0.4125, 0.4412, 0.5285,  ..., 0.5174, 0.4402, 0.6179]],

         [[0.6202, 0.4465, 0.5259,  ..., 0.4158, 0.4810, 0.3311],
          [0.5297, 0.5106, 0.5703,  ..., 0.5508, 0.4779, 0.5823],
          [0.5285, 0.5363, 0.4410,  ..., 0.6289, 0.6834, 0.4064],
          [0.4598, 0.5032, 0.4899,  ..., 0.4441, 0.4705, 0.4173]],

         [[0.5285, 0.4896, 0.4664,  ..., 0.6298, 0.5927, 0.5832],
          [0.6505, 0.5274, 0.4373,  ..., 0.5528, 0.4431, 0.4239],
          [0.5588, 0.4983, 0.4637,  ..., 0.6220, 0.5670, 0.4545],
          [0.5528, 0.5600, 0.3684,  ..., 0.5946, 0.5727, 0.3234]]],


        [[[0.2822, 0.4581, 0.3337,  ..., 0.4701, 0.4036, 0.5988],
          [0.4130, 0.5506, 0.5837,  ..., 0.4632, 0.5179, 0.4879],
          [0.5025, 0.5775, 0.4710,  ..., 0.5368, 0.5157, 0.4830],
          [0.5409, 0.4943, 0.4823,  ..., 0.2613, 0.4419, 0.6243]],

         [[0.5066, 0.4834, 0.5799,  ..., 0.5436, 0.4528, 0.3739],
          [0.5125, 0.5353, 0.3702,  ..., 0.6211, 0.4381, 0.4395],
          [0.4785, 0.5037, 0.4812,  ..., 0.6063, 0.5173, 0.4754],
          [0.6007, 0.4140, 0.4306,  ..., 0.5627, 0.4916, 0.4144]],

         [[0.5003, 0.4206, 0.3868,  ..., 0.3416, 0.5631, 0.3909],
          [0.5250, 0.5115, 0.5784,  ..., 0.4681, 0.3460, 0.4788],
          [0.5832, 0.3984, 0.4345,  ..., 0.4745, 0.5198, 0.4824],
          [0.6783, 0.5903, 0.3442,  ..., 0.3886, 0.3174, 0.4958]],

         ...,

         [[0.5737, 0.5380, 0.5168,  ..., 0.5557, 0.3970, 0.5631],
          [0.4808, 0.6316, 0.5012,  ..., 0.4683, 0.5931, 0.4664],
          [0.3621, 0.5794, 0.4866,  ..., 0.5339, 0.5404, 0.5334],
          [0.5260, 0.6072, 0.5889,  ..., 0.4681, 0.4706, 0.4523]],

         [[0.4216, 0.6567, 0.5252,  ..., 0.5317, 0.4659, 0.5504],
          [0.4431, 0.5993, 0.4504,  ..., 0.4470, 0.6202, 0.4297],
          [0.4125, 0.5453, 0.4797,  ..., 0.4974, 0.5012, 0.5746],
          [0.5190, 0.4978, 0.4116,  ..., 0.5552, 0.4850, 0.6415]],

         [[0.5368, 0.4492, 0.4306,  ..., 0.5070, 0.4603, 0.6549],
          [0.5040, 0.5196, 0.4552,  ..., 0.5436, 0.5188, 0.5308],
          [0.5351, 0.6859, 0.4992,  ..., 0.5018, 0.2465, 0.4707],
          [0.4971, 0.5142, 0.4130,  ..., 0.4036, 0.5823, 0.4817]]]],
       device='cuda:0')/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/activation_monitor.py:211: UserWarning: Value passed to ActivationMonitor is not a torch.Tensor, skipping.
  warnings.warn(

tensor([[[[0.6496, 0.6058, 0.5361,  ..., 0.4756, 0.5327, 0.4516],
          [0.5032, 0.6697, 0.5250,  ..., 0.5239, 0.6298, 0.4121],
          [0.6039, 0.4707, 0.3390,  ..., 0.3355, 0.5832, 0.3451],
          [0.5178, 0.4431, 0.3998,  ..., 0.7186, 0.5898, 0.4579]],

         [[0.6496, 0.6058, 0.5361,  ..., 0.4756, 0.5327, 0.4516],
          [0.5032, 0.6697, 0.5250,  ..., 0.5239, 0.6298, 0.4121],
          [0.6039, 0.4707, 0.3390,  ..., 0.3355, 0.5832, 0.3451],
          [0.5178, 0.4431, 0.3998,  ..., 0.7186, 0.5898, 0.4579]],

         [[0.4615, 0.5057, 0.4858,  ..., 0.3115, 0.5665, 0.3919],
          [0.4448, 0.4206, 0.3702,  ..., 0.4273, 0.3648, 0.4967],
          [0.4059, 0.6049, 0.5115,  ..., 0.5081, 0.4649, 0.5183],
          [0.4732, 0.6225, 0.6792,  ..., 0.4431, 0.4824, 0.5148]],

         ...,

         [[0.4934, 0.5358, 0.5165,  ..., 0.5789, 0.4908, 0.5477],
          [0.4211, 0.6220, 0.6095,  ..., 0.5492, 0.4479, 0.3900],
          [0.6334, 0.5174, 0.4158,  ..., 0.5842, 0.4707, 0.5431],
          [0.4125, 0.4412, 0.5285,  ..., 0.5174, 0.4402, 0.6179]],

         [[0.6202, 0.4465, 0.5259,  ..., 0.4158, 0.4810, 0.3311],
          [0.5297, 0.5106, 0.5703,  ..., 0.5508, 0.4779, 0.5823],
          [0.5285, 0.5363, 0.4410,  ..., 0.6289, 0.6834, 0.4064],
          [0.4598, 0.5032, 0.4899,  ..., 0.4441, 0.4705, 0.4173]],

         [[0.5285, 0.4896, 0.4664,  ..., 0.6298, 0.5927, 0.5832],
          [0.6505, 0.5274, 0.4373,  ..., 0.5528, 0.4431, 0.4239],
          [0.5588, 0.4983, 0.4637,  ..., 0.6220, 0.5670, 0.4545],
          [0.5528, 0.5600, 0.3684,  ..., 0.5946, 0.5727, 0.3234]]],


        [[[0.2822, 0.4581, 0.3337,  ..., 0.4701, 0.4036, 0.5988],
          [0.4130, 0.5506, 0.5837,  ..., 0.4632, 0.5179, 0.4879],
          [0.5025, 0.5775, 0.4710,  ..., 0.5368, 0.5157, 0.4830],
          [0.5409, 0.4943, 0.4823,  ..., 0.2613, 0.4419, 0.6243]],

         [[0.5066, 0.4834, 0.5799,  ..., 0.5436, 0.4528, 0.3739],
          [0.5125, 0.5353, 0.3702,  ..., 0.6211, 0.4381, 0.4395],
          [0.4785, 0.5037, 0.4812,  ..., 0.6063, 0.5173, 0.4754],
          [0.6007, 0.4140, 0.4306,  ..., 0.5627, 0.4916, 0.4144]],

         [[0.5003, 0.4206, 0.3868,  ..., 0.3416, 0.5631, 0.3909],
          [0.5250, 0.5115, 0.5784,  ..., 0.4681, 0.3460, 0.4788],
          [0.5832, 0.3984, 0.4345,  ..., 0.4745, 0.5198, 0.4824],
          [0.6783, 0.5903, 0.3442,  ..., 0.3886, 0.3174, 0.4958]],

         ...,

         [[0.5737, 0.5380, 0.5168,  ..., 0.5557, 0.3970, 0.5631],
          [0.4808, 0.6316, 0.5012,  ..., 0.4683, 0.5931, 0.4664],
          [0.3621, 0.5794, 0.4866,  ..., 0.5339, 0.5404, 0.5334],
          [0.5260, 0.6072, 0.5889,  ..., 0.4681, 0.4706, 0.4523]],

         [[0.4216, 0.6567, 0.5252,  ..., 0.5317, 0.4659, 0.5504],
          [0.4431, 0.5993, 0.4504,  ..., 0.4470, 0.6202, 0.4297],
          [0.4125, 0.5453, 0.4797,  ..., 0.4974, 0.5012, 0.5746],
          [0.5190, 0.4978, 0.4116,  ..., 0.5552, 0.4850, 0.6415]],

         [[0.5368, 0.4492, 0.4306,  ..., 0.5070, 0.4603, 0.6549],
          [0.5040, 0.5196, 0.4552,  ..., 0.5436, 0.5188, 0.5308],
          [0.5351, 0.6859, 0.4992,  ..., 0.5018, 0.2465, 0.4707],
          [0.4971, 0.5142, 0.4130,  ..., 0.4036, 0.5823, 0.4817]]]],
       device='cuda:0', requires_grad=True)
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
selected experts tensor([1765, 1744, 1528, 1315, 1518, 1525, 2008, 1793, 1484, 1704],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6334, 0.5358, 0.4467,  ..., 0.4799, 0.5535, 0.5615],
          [0.5083, 0.4793, 0.5670,  ..., 0.6442, 0.6077, 0.3961],
          [0.6053, 0.4045, 0.5143,  ..., 0.4521, 0.7106, 0.3849],
          [0.5072, 0.4644, 0.5737,  ..., 0.4971, 0.6053, 0.5665]],

         [[0.6334, 0.5363, 0.4472,  ..., 0.4807, 0.5525, 0.5631],
          [0.5081, 0.4797, 0.5679,  ..., 0.6433, 0.6058, 0.3956],
          [0.6067, 0.4036, 0.5137,  ..., 0.4508, 0.7090, 0.3844],
          [0.5072, 0.4644, 0.5708,  ..., 0.4966, 0.6058, 0.5660]],

         [[0.5870, 0.6174, 0.5727,  ..., 0.4211, 0.4720, 0.6558],
          [0.5183, 0.6469, 0.4125,  ..., 0.6188, 0.5034, 0.4829],
          [0.5780, 0.4765, 0.5421,  ..., 0.4877, 0.3460, 0.4201],
          [0.5276, 0.5751, 0.5237,  ..., 0.6025, 0.5042, 0.6109]],

         ...,

         [[0.6002, 0.4489, 0.4547,  ..., 0.4867, 0.5917, 0.5542],
          [0.3947, 0.6469, 0.5684,  ..., 0.5109, 0.5279, 0.4465],
          [0.4766, 0.4729, 0.4610,  ..., 0.5598, 0.4639, 0.4211],
          [0.5212, 0.5260, 0.5482,  ..., 0.4727, 0.4829, 0.4804]],

         [[0.6645, 0.4378, 0.4523,  ..., 0.3639, 0.5038, 0.4521],
          [0.3372, 0.4755, 0.4925,  ..., 0.5029, 0.5109, 0.4784],
          [0.5038, 0.3868, 0.6118,  ..., 0.3858, 0.4871, 0.5199],
          [0.4969, 0.5983, 0.5651,  ..., 0.5903, 0.3868, 0.5016]],

         [[0.6183, 0.4467, 0.4853,  ..., 0.5946, 0.4979, 0.6522],
          [0.4431, 0.4872, 0.5941,  ..., 0.5684, 0.5050, 0.5780],
          [0.4748, 0.3780, 0.5268,  ..., 0.4791, 0.4450, 0.4705],
          [0.4728, 0.4990, 0.5865,  ..., 0.6053, 0.5746, 0.5349]]],


        [[[0.5988, 0.5694, 0.4642,  ..., 0.5194, 0.4354, 0.5770],
          [0.6252, 0.5097, 0.3956,  ..., 0.6689, 0.5694, 0.4121],
          [0.6307, 0.4482, 0.5670,  ..., 0.4201, 0.5417, 0.3868],
          [0.6021, 0.5061, 0.5894,  ..., 0.4898, 0.5135, 0.5074]],

         [[0.7066, 0.3451, 0.3891,  ..., 0.3065, 0.5722, 0.6531],
          [0.5146, 0.4813, 0.5251,  ..., 0.5823, 0.5146, 0.3933],
          [0.5412, 0.4402, 0.5274,  ..., 0.4311, 0.5794, 0.4007],
          [0.6280, 0.5511, 0.5913,  ..., 0.5583, 0.5429, 0.4858]],

         [[0.6370, 0.4045, 0.4974,  ..., 0.6370, 0.3984, 0.5025],
          [0.5165, 0.4871, 0.5397,  ..., 0.5373, 0.4699, 0.5118],
          [0.4610, 0.5015, 0.5101,  ..., 0.3909, 0.2910, 0.4402],
          [0.6876, 0.7225, 0.5699,  ..., 0.6252, 0.5055, 0.5550]],

         ...,

         [[0.6156, 0.5879, 0.6243,  ..., 0.4533, 0.5489, 0.6039],
          [0.4349, 0.4069, 0.5140,  ..., 0.3780, 0.3702, 0.4173],
          [0.5823, 0.4634, 0.5004,  ..., 0.4901, 0.5282, 0.3594],
          [0.5229, 0.5395, 0.5675,  ..., 0.4378, 0.5492, 0.4436]],

         [[0.5946, 0.5504, 0.5324,  ..., 0.5334, 0.4862, 0.4827],
          [0.5051, 0.4889, 0.4092,  ..., 0.4608, 0.3363, 0.4893],
          [0.5737, 0.4364, 0.4516,  ..., 0.5506, 0.4381, 0.3630],
          [0.4583, 0.5879, 0.4649,  ..., 0.4972, 0.5545, 0.5329]],

         [[0.5290, 0.5315, 0.5670,  ..., 0.4201, 0.6406, 0.4898],
          [0.3984, 0.4991, 0.5284,  ..., 0.6252, 0.5302, 0.3684],
          [0.5392, 0.4627, 0.6016,  ..., 0.5250, 0.6225, 0.4916],
          [0.3166, 0.6132, 0.5417,  ..., 0.4885, 0.3657, 0.5571]]]],
       device='cuda:0')
tensor([[[[0.6334, 0.5358, 0.4467,  ..., 0.4799, 0.5535, 0.5615],
          [0.5083, 0.4793, 0.5670,  ..., 0.6442, 0.6077, 0.3961],
          [0.6053, 0.4045, 0.5143,  ..., 0.4521, 0.7106, 0.3849],
          [0.5072, 0.4644, 0.5737,  ..., 0.4971, 0.6053, 0.5665]],

         [[0.6334, 0.5363, 0.4472,  ..., 0.4807, 0.5525, 0.5631],
          [0.5081, 0.4797, 0.5679,  ..., 0.6433, 0.6058, 0.3956],
          [0.6067, 0.4036, 0.5137,  ..., 0.4508, 0.7090, 0.3844],
          [0.5072, 0.4644, 0.5708,  ..., 0.4966, 0.6058, 0.5660]],

         [[0.5870, 0.6174, 0.5727,  ..., 0.4211, 0.4720, 0.6558],
          [0.5183, 0.6469, 0.4125,  ..., 0.6188, 0.5034, 0.4829],
          [0.5780, 0.4765, 0.5421,  ..., 0.4877, 0.3460, 0.4201],
          [0.5276, 0.5751, 0.5237,  ..., 0.6025, 0.5042, 0.6109]],

         ...,

         [[0.6002, 0.4489, 0.4547,  ..., 0.4867, 0.5917, 0.5542],
          [0.3947, 0.6469, 0.5684,  ..., 0.5109, 0.5279, 0.4465],
          [0.4766, 0.4729, 0.4610,  ..., 0.5598, 0.4639, 0.4211],
          [0.5212, 0.5260, 0.5482,  ..., 0.4727, 0.4829, 0.4804]],

         [[0.6645, 0.4378, 0.4523,  ..., 0.3639, 0.5038, 0.4521],
          [0.3372, 0.4755, 0.4925,  ..., 0.5029, 0.5109, 0.4784],
          [0.5038, 0.3868, 0.6118,  ..., 0.3858, 0.4871, 0.5199],
          [0.4969, 0.5983, 0.5651,  ..., 0.5903, 0.3868, 0.5016]],

         [[0.6183, 0.4467, 0.4853,  ..., 0.5946, 0.4979, 0.6522],
          [0.4431, 0.4872, 0.5941,  ..., 0.5684, 0.5050, 0.5780],
          [0.4748, 0.3780, 0.5268,  ..., 0.4791, 0.4450, 0.4705],
          [0.4728, 0.4990, 0.5865,  ..., 0.6053, 0.5746, 0.5349]]],


        [[[0.5988, 0.5694, 0.4642,  ..., 0.5194, 0.4354, 0.5770],
          [0.6252, 0.5097, 0.3956,  ..., 0.6689, 0.5694, 0.4121],
          [0.6307, 0.4482, 0.5670,  ..., 0.4201, 0.5417, 0.3868],
          [0.6021, 0.5061, 0.5894,  ..., 0.4898, 0.5135, 0.5074]],

         [[0.7066, 0.3451, 0.3891,  ..., 0.3065, 0.5722, 0.6531],
          [0.5146, 0.4813, 0.5251,  ..., 0.5823, 0.5146, 0.3933],
          [0.5412, 0.4402, 0.5274,  ..., 0.4311, 0.5794, 0.4007],
          [0.6280, 0.5511, 0.5913,  ..., 0.5583, 0.5429, 0.4858]],

         [[0.6370, 0.4045, 0.4974,  ..., 0.6370, 0.3984, 0.5025],
          [0.5165, 0.4871, 0.5397,  ..., 0.5373, 0.4699, 0.5118],
          [0.4610, 0.5015, 0.5101,  ..., 0.3909, 0.2910, 0.4402],
          [0.6876, 0.7225, 0.5699,  ..., 0.6252, 0.5055, 0.5550]],

         ...,

         [[0.6156, 0.5879, 0.6243,  ..., 0.4533, 0.5489, 0.6039],
          [0.4349, 0.4069, 0.5140,  ..., 0.3780, 0.3702, 0.4173],
          [0.5823, 0.4634, 0.5004,  ..., 0.4901, 0.5282, 0.3594],
          [0.5229, 0.5395, 0.5675,  ..., 0.4378, 0.5492, 0.4436]],

         [[0.5946, 0.5504, 0.5324,  ..., 0.5334, 0.4862, 0.4827],
          [0.5051, 0.4889, 0.4092,  ..., 0.4608, 0.3363, 0.4893],
          [0.5737, 0.4364, 0.4516,  ..., 0.5506, 0.4381, 0.3630],
          [0.4583, 0.5879, 0.4649,  ..., 0.4972, 0.5545, 0.5329]],

         [[0.5290, 0.5315, 0.5670,  ..., 0.4201, 0.6406, 0.4898],
          [0.3984, 0.4991, 0.5284,  ..., 0.6252, 0.5302, 0.3684],
          [0.5392, 0.4627, 0.6016,  ..., 0.5250, 0.6225, 0.4916],
          [0.3166, 0.6132, 0.5417,  ..., 0.4885, 0.3657, 0.5571]]]],
       device='cuda:0', requires_grad=True)[batch=1/40]:
	 Train time/epoch: 0
	 Train time/batch: 0
	 Train time/sample: 0
	 Train time/batch_in_epoch: 0
	 Train time/sample_in_epoch: 0
	 Train time/token: 0
	 Train time/token_in_epoch: 0
	 Train memory/current_allocated_mem: 0.6924
	 Train memory/current_active_mem: 0.6924
	 Train memory/current_inactive_mem: 0.3227
	 Train memory/current_reserved_mem: 2.9318
	 Train memory/peak_allocated_mem: 2.1003
	 Train memory/peak_active_mem: 2.1003
	 Train memory/peak_inactive_mem: 0.4002
	 Train memory/peak_reserved_mem: 2.9318
	 Train memory/alloc_retries: 0
	 Train trainer/device_train_microbatch_size: 10
	 Train loss/train/total: 0.0054
	 Train metrics/train/LanguageCrossEntropy: 10.9853
	 Train metrics/train/LanguagePerplexity: 58999.2539
	 Train metrics/train/TokenAccuracy: 0.0000
	 Train time/train: 0.0055
	 Train time/val: 0.0000
	 Train time/total: 0.0055
	 Train lr-DecoupledAdamW/group0: 0.0000
	 Train metrics/shannon_entropy: 10.6355
	 Train metrics/batch_shannon_entropy: <wandb.sdk.data_types.table.Table object at 0x77b707caac90>
	 Train metrics/seq_shannon_entropy: CustomChart(table=<wandb.sdk.data_types.table.Table object at 0x77b9a1aff2c0>, spec=CustomChartSpec(spec_name='wandb/lineseries/v0', fields={'step': 'step', 'lineKey': 'lineKey', 'lineVal': 'lineVal'}, string_fields={'title': 'Per-Layer Shannon Entropy', 'xname': 'Block Index'}, key='', panel_type='Vega2', split_table=False))
	 Train metrics/exit_entropy: 0.6641
	 Train metrics/batch_exit_entropy: <wandb.sdk.data_types.table.Table object at 0x77b9a1b2f8f0>
	 Train metrics/seq_exit_entropy: CustomChart(table=<wandb.sdk.data_types.table.Table object at 0x77b9a1ba77d0>, spec=CustomChartSpec(spec_name='wandb/lineseries/v0', fields={'step': 'step', 'lineKey': 'lineKey', 'lineVal': 'lineVal'}, string_fields={'title': 'Per-Layer Exit Entropy', 'xname': 'Block Index'}, key='', panel_type='Vega2', split_table=False))
	 Train expert_selection/ffn_layer: <wandb.sdk.data_types.image.Image object at 0x77b9a15dbb30>
	 Train expert_selection/attn_o_layer: <wandb.sdk.data_types.image.Image object at 0x77b9a1809280>
	 Train expert_selection/attn_v_layer: <wandb.sdk.data_types.image.Image object at 0x77b7001c7e00>
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
selected experts tensor([2393, 1625, 2185, 1398, 2180,  704, 2481, 1537, 1050,  831],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4930, 0.5012, 0.6035,  ..., 0.4610, 0.6361, 0.5974],
          [0.5028, 0.4839, 0.6424,  ..., 0.4695, 0.4158, 0.4886],
          [0.3858, 0.5732, 0.6072,  ..., 0.4622, 0.4168, 0.4634],
          [0.4545, 0.4470, 0.5718,  ..., 0.6334, 0.5846, 0.4196]],

         [[0.4292, 0.4785, 0.3407,  ..., 0.5095, 0.6086, 0.4325],
          [0.3993, 0.6316, 0.4301,  ..., 0.4688, 0.5756, 0.6234],
          [0.5794, 0.6123, 0.4716,  ..., 0.5139, 0.4340, 0.4982],
          [0.4805, 0.5742, 0.4610,  ..., 0.5508, 0.5356, 0.4335]],

         [[0.4434, 0.4489, 0.4045,  ..., 0.4182, 0.4130, 0.6487],
          [0.5718, 0.2838, 0.6261,  ..., 0.5675, 0.5433, 0.5017],
          [0.4713, 0.4369, 0.4673,  ..., 0.2798, 0.3844, 0.4083],
          [0.4596, 0.5020, 0.5077,  ..., 0.6325, 0.6151, 0.4851]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5660, 0.3914, 0.6091,  ..., 0.4278, 0.3486, 0.5513],
          [0.3311, 0.4453, 0.5072,  ..., 0.3657, 0.3124, 0.4904],
          [0.4795, 0.4012, 0.4978,  ..., 0.4895, 0.5889, 0.5794],
          [0.4605, 0.5506, 0.5727,  ..., 0.4220, 0.5600, 0.3540]],

         [[0.5513, 0.5007, 0.4467,  ..., 0.3016, 0.2494, 0.5356],
          [0.4349, 0.4012, 0.5026,  ..., 0.5108, 0.4340, 0.3576],
          [0.5908, 0.6522, 0.4995,  ..., 0.3141, 0.4465, 0.3900],
          [0.3132, 0.5636, 0.4772,  ..., 0.5689, 0.5818, 0.4494]],

         [[0.5525, 0.4475, 0.3639,  ..., 0.3183, 0.4230, 0.5804],
          [0.4489, 0.5098, 0.4412,  ..., 0.6030, 0.3989, 0.5007],
          [0.6100, 0.4811, 0.4521,  ..., 0.4316, 0.5351, 0.6025],
          [0.5061, 0.4651, 0.4843,  ..., 0.4613, 0.5689, 0.4429]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0')
tensor([[[[0.4930, 0.5012, 0.6035,  ..., 0.4610, 0.6361, 0.5974],
          [0.5028, 0.4839, 0.6424,  ..., 0.4695, 0.4158, 0.4886],
          [0.3858, 0.5732, 0.6072,  ..., 0.4622, 0.4168, 0.4634],
          [0.4545, 0.4470, 0.5718,  ..., 0.6334, 0.5846, 0.4196]],

         [[0.4292, 0.4785, 0.3407,  ..., 0.5095, 0.6086, 0.4325],
          [0.3993, 0.6316, 0.4301,  ..., 0.4688, 0.5756, 0.6234],
          [0.5794, 0.6123, 0.4716,  ..., 0.5139, 0.4340, 0.4982],
          [0.4805, 0.5742, 0.4610,  ..., 0.5508, 0.5356, 0.4335]],

         [[0.4434, 0.4489, 0.4045,  ..., 0.4182, 0.4130, 0.6487],
          [0.5718, 0.2838, 0.6261,  ..., 0.5675, 0.5433, 0.5017],
          [0.4713, 0.4369, 0.4673,  ..., 0.2798, 0.3844, 0.4083],
          [0.4596, 0.5020, 0.5077,  ..., 0.6325, 0.6151, 0.4851]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5660, 0.3914, 0.6091,  ..., 0.4278, 0.3486, 0.5513],
          [0.3311, 0.4453, 0.5072,  ..., 0.3657, 0.3124, 0.4904],
          [0.4795, 0.4012, 0.4978,  ..., 0.4895, 0.5889, 0.5794],
          [0.4605, 0.5506, 0.5727,  ..., 0.4220, 0.5600, 0.3540]],

         [[0.5513, 0.5007, 0.4467,  ..., 0.3016, 0.2494, 0.5356],
          [0.4349, 0.4012, 0.5026,  ..., 0.5108, 0.4340, 0.3576],
          [0.5908, 0.6522, 0.4995,  ..., 0.3141, 0.4465, 0.3900],
          [0.3132, 0.5636, 0.4772,  ..., 0.5689, 0.5818, 0.4494]],

         [[0.5525, 0.4475, 0.3639,  ..., 0.3183, 0.4230, 0.5804],
          [0.4489, 0.5098, 0.4412,  ..., 0.6030, 0.3989, 0.5007],
          [0.6100, 0.4811, 0.4521,  ..., 0.4316, 0.5351, 0.6025],
          [0.5061, 0.4651, 0.4843,  ..., 0.4613, 0.5689, 0.4429]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
selected experts tensor([3087, 2665, 1451, 1296, 2572,  869,  721, 1086, 1147, 1490],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1765., 1744., 1528., 1315., 1518., 1525., 2008., 1793., 1484., 1704.],
        [3087., 2665., 1451., 1296., 2572.,  869.,  721., 1086., 1147., 1490.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.3747, 0.3676, 0.3770,  ..., 0.5497, 0.5151, 0.7220],
          [0.7120, 0.6317, 0.6378,  ..., 0.5388, 0.6251, 0.4107],
          [0.5552, 0.4717, 0.5091,  ..., 0.6407, 0.2940, 0.4359],
          [0.4564, 0.5641, 0.3629,  ..., 0.4465, 0.5542, 0.4678]],

         [[0.3701, 0.4728, 0.5765,  ..., 0.4943, 0.5836, 0.5244],
          [0.5736, 0.5600, 0.6423,  ..., 0.3905, 0.4359, 0.5550],
          [0.3853, 0.5275, 0.3485,  ..., 0.4545, 0.5888, 0.5756],
          [0.5118, 0.4477, 0.3593,  ..., 0.3269, 0.4296, 0.4652]],

         [[0.5515, 0.4826, 0.5146,  ..., 0.5448, 0.5865, 0.7068],
          [0.5779, 0.5393, 0.4397,  ..., 0.5016, 0.3406, 0.5718],
          [0.4925, 0.5588, 0.4903,  ..., 0.5521, 0.4206, 0.5584],
          [0.2529, 0.6082, 0.5273,  ..., 0.3850, 0.4258, 0.3818]],

         ...,

         [[0.5450, 0.4789, 0.4363,  ..., 0.5718, 0.5307, 0.6207],
          [0.5012, 0.3841, 0.4258,  ..., 0.4543, 0.5679, 0.3892],
          [0.4854, 0.5213, 0.4746,  ..., 0.5646, 0.3647, 0.4088],
          [0.4513, 0.4504, 0.5813,  ..., 0.4795, 0.4969, 0.4564]],

         [[0.3293, 0.5419, 0.5191,  ..., 0.6063, 0.5779, 0.5785],
          [0.6574, 0.5790, 0.4363,  ..., 0.4183, 0.4532, 0.4150],
          [0.4282, 0.5298, 0.5542,  ..., 0.4340, 0.3932, 0.5115],
          [0.5014, 0.6138, 0.4651,  ..., 0.4531, 0.4867, 0.4627]],

         [[0.4515, 0.5456, 0.3816,  ..., 0.5732, 0.6192, 0.3261],
          [0.5275, 0.5927, 0.5317,  ..., 0.3577, 0.3674, 0.5147],
          [0.6369, 0.4379, 0.4234,  ..., 0.5163, 0.5324, 0.4637],
          [0.5717, 0.6398, 0.3494,  ..., 0.4235, 0.6333, 0.6225]]],


        [[[0.4996, 0.6193, 0.5368,  ..., 0.4872, 0.4542, 0.4939],
          [0.6108, 0.5960, 0.6011,  ..., 0.5713, 0.4012, 0.5160],
          [0.3566, 0.3443, 0.6468,  ..., 0.4211, 0.5091, 0.4400],
          [0.6405, 0.3577, 0.5888,  ..., 0.5574, 0.4016, 0.5932]],

         [[0.5151, 0.5656, 0.5698,  ..., 0.4983, 0.4901, 0.4804],
          [0.4624, 0.6077, 0.4520,  ..., 0.3971, 0.4813, 0.4297],
          [0.4474, 0.5045, 0.4177,  ..., 0.4393, 0.5888, 0.4908],
          [0.4395, 0.4079, 0.5377,  ..., 0.5617, 0.6260, 0.3952]],

         [[0.3336, 0.4625, 0.5609,  ..., 0.4422, 0.4077, 0.4775],
          [0.2217, 0.6087, 0.5423,  ..., 0.4311, 0.4938, 0.4862],
          [0.5218, 0.4230, 0.5100,  ..., 0.4829, 0.6090, 0.4807],
          [0.5212, 0.5361, 0.5329,  ..., 0.6541, 0.5460, 0.4889]],

         ...,

         [[0.5983, 0.4531, 0.4450,  ..., 0.5325, 0.4886, 0.4795],
          [0.4756, 0.4311, 0.4467,  ..., 0.4959, 0.5760, 0.5492],
          [0.4943, 0.5694, 0.6233,  ..., 0.5475, 0.4026, 0.5061],
          [0.5131, 0.5904, 0.4607,  ..., 0.4211, 0.5019, 0.4164]],

         [[0.4412, 0.5429, 0.5556,  ..., 0.4875, 0.6090, 0.5017],
          [0.3825, 0.4731, 0.4730,  ..., 0.6647, 0.5679, 0.4971],
          [0.3638, 0.4772, 0.5765,  ..., 0.5492, 0.5091, 0.4623],
          [0.4805, 0.5941, 0.3784,  ..., 0.4407, 0.3494, 0.5165]],

         [[0.4096, 0.5443, 0.4603,  ..., 0.3694, 0.4750, 0.5266],
          [0.4755, 0.5487, 0.4864,  ..., 0.5150, 0.5641, 0.4960],
          [0.5295, 0.5380, 0.4708,  ..., 0.5151, 0.5813, 0.5823],
          [0.4947, 0.5388, 0.4958,  ..., 0.5152, 0.6118, 0.5436]]]],
       device='cuda:0')
tensor([[[[0.3757, 0.3666, 0.3780,  ..., 0.5487, 0.5161, 0.7210],
          [0.7130, 0.6307, 0.6388,  ..., 0.5378, 0.6261, 0.4097],
          [0.5562, 0.4707, 0.5101,  ..., 0.6397, 0.2950, 0.4349],
          [0.4574, 0.5631, 0.3639,  ..., 0.4455, 0.5552, 0.4668]],

         [[0.3711, 0.4718, 0.5775,  ..., 0.4933, 0.5846, 0.5234],
          [0.5746, 0.5590, 0.6433,  ..., 0.3895, 0.4369, 0.5540],
          [0.3863, 0.5265, 0.3495,  ..., 0.4535, 0.5898, 0.5746],
          [0.5128, 0.4467, 0.3603,  ..., 0.3259, 0.4306, 0.4642]],

         [[0.5525, 0.4816, 0.5156,  ..., 0.5438, 0.5875, 0.7058],
          [0.5789, 0.5383, 0.4407,  ..., 0.5006, 0.3416, 0.5708],
          [0.4935, 0.5578, 0.4913,  ..., 0.5511, 0.4216, 0.5574],
          [0.2539, 0.6072, 0.5283,  ..., 0.3840, 0.4268, 0.3808]],

         ...,

         [[0.5460, 0.4779, 0.4373,  ..., 0.5708, 0.5317, 0.6197],
          [0.5022, 0.3831, 0.4268,  ..., 0.4533, 0.5689, 0.3882],
          [0.4864, 0.5203, 0.4756,  ..., 0.5636, 0.3657, 0.4078],
          [0.4523, 0.4494, 0.5823,  ..., 0.4785, 0.4979, 0.4554]],

         [[0.3303, 0.5409, 0.5201,  ..., 0.6053, 0.5789, 0.5775],
          [0.6584, 0.5780, 0.4373,  ..., 0.4173, 0.4542, 0.4140],
          [0.4292, 0.5288, 0.5552,  ..., 0.4330, 0.3942, 0.5105],
          [0.5024, 0.6128, 0.4661,  ..., 0.4521, 0.4877, 0.4617]],

         [[0.4525, 0.5446, 0.3826,  ..., 0.5722, 0.6202, 0.3251],
          [0.5285, 0.5917, 0.5327,  ..., 0.3567, 0.3684, 0.5137],
          [0.6379, 0.4369, 0.4244,  ..., 0.5153, 0.5334, 0.4627],
          [0.5727, 0.6388, 0.3504,  ..., 0.4225, 0.6343, 0.6215]]],


        [[[0.5006, 0.6183, 0.5378,  ..., 0.4862, 0.4552, 0.4929],
          [0.6118, 0.5950, 0.6021,  ..., 0.5703, 0.4022, 0.5150],
          [0.3576, 0.3433, 0.6478,  ..., 0.4201, 0.5101, 0.4390],
          [0.6415, 0.3567, 0.5898,  ..., 0.5564, 0.4026, 0.5922]],

         [[0.5161, 0.5646, 0.5708,  ..., 0.4973, 0.4911, 0.4794],
          [0.4634, 0.6067, 0.4530,  ..., 0.3961, 0.4823, 0.4287],
          [0.4484, 0.5035, 0.4187,  ..., 0.4383, 0.5898, 0.4898],
          [0.4405, 0.4069, 0.5387,  ..., 0.5607, 0.6270, 0.3942]],

         [[0.3346, 0.4615, 0.5619,  ..., 0.4412, 0.4087, 0.4765],
          [0.2227, 0.6077, 0.5433,  ..., 0.4301, 0.4948, 0.4852],
          [0.5228, 0.4220, 0.5110,  ..., 0.4819, 0.6100, 0.4797],
          [0.5222, 0.5351, 0.5339,  ..., 0.6531, 0.5470, 0.4879]],

         ...,

         [[0.5993, 0.4521, 0.4460,  ..., 0.5315, 0.4896, 0.4785],
          [0.4766, 0.4301, 0.4477,  ..., 0.4949, 0.5770, 0.5482],
          [0.4953, 0.5684, 0.6243,  ..., 0.5465, 0.4036, 0.5051],
          [0.5141, 0.5894, 0.4617,  ..., 0.4201, 0.5029, 0.4154]],

         [[0.4422, 0.5419, 0.5566,  ..., 0.4865, 0.6100, 0.5007],
          [0.3835, 0.4721, 0.4740,  ..., 0.6637, 0.5689, 0.4961],
          [0.3648, 0.4762, 0.5775,  ..., 0.5482, 0.5101, 0.4613],
          [0.4815, 0.5931, 0.3794,  ..., 0.4397, 0.3504, 0.5155]],

         [[0.4106, 0.5433, 0.4613,  ..., 0.3684, 0.4760, 0.5256],
          [0.4765, 0.5477, 0.4874,  ..., 0.5140, 0.5651, 0.4950],
          [0.5305, 0.5370, 0.4718,  ..., 0.5141, 0.5823, 0.5813],
          [0.4957, 0.5378, 0.4968,  ..., 0.5142, 0.6128, 0.5426]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0010,  0.0010, -0.0010, -0.0010, -0.0010, -0.0010,  0.0010,  0.0010,
        -0.0010,  0.0010], device='cuda:0')
selected experts tensor([1634, 1654, 1785, 1636, 1734, 1737, 1513, 1611, 1524, 1556],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4773, 0.4745, 0.4916,  ..., 0.6565, 0.4843, 0.3890],
          [0.6122, 0.3241, 0.5908,  ..., 0.5992, 0.4074, 0.4753],
          [0.5307, 0.6048, 0.5646,  ..., 0.4730, 0.4618, 0.6315],
          [0.4486, 0.4172, 0.5651,  ..., 0.4073, 0.5956, 0.4708]],

         [[0.5075, 0.5474, 0.4528,  ..., 0.3512, 0.4693, 0.4424],
          [0.6215, 0.5002, 0.4964,  ..., 0.4315, 0.5203, 0.4918],
          [0.4040, 0.5052, 0.3452,  ..., 0.5163, 0.4164, 0.4903],
          [0.5755, 0.4842, 0.4036,  ..., 0.4680, 0.7397, 0.3476]],

         [[0.3765, 0.5300, 0.5090,  ..., 0.6574, 0.4097, 0.5124],
          [0.5020, 0.3941, 0.3813,  ..., 0.5121, 0.5852, 0.5746],
          [0.4921, 0.4021, 0.4728,  ..., 0.5580, 0.4608, 0.5945],
          [0.5251, 0.5855, 0.5258,  ..., 0.4697, 0.4581, 0.5822]],

         ...,

         [[0.4583, 0.5940, 0.3694,  ..., 0.6270, 0.4632, 0.4850],
          [0.5390, 0.4557, 0.5951,  ..., 0.5717, 0.5861, 0.5846],
          [0.4644, 0.4707, 0.5172,  ..., 0.3965, 0.6035, 0.5931],
          [0.5319, 0.3811, 0.3184,  ..., 0.6210, 0.5670, 0.4943]],

         [[0.4325, 0.7246, 0.5354,  ..., 0.6635, 0.5436, 0.5945],
          [0.5741, 0.4186, 0.4899,  ..., 0.5600, 0.4221, 0.6020],
          [0.4482, 0.4368, 0.4925,  ..., 0.4134, 0.4307, 0.5855],
          [0.4002, 0.7104, 0.4615,  ..., 0.4934, 0.5586, 0.5206]],

         [[0.5329, 0.5133, 0.5317,  ..., 0.7362, 0.5540, 0.4059],
          [0.5860, 0.5564, 0.5015,  ..., 0.7096, 0.5273, 0.5631],
          [0.4424, 0.5511, 0.5390,  ..., 0.4693, 0.5728, 0.6233],
          [0.4537, 0.4130, 0.4316,  ..., 0.4923, 0.6335, 0.4733]]],


        [[[0.4063, 0.6468, 0.5489,  ..., 0.4679, 0.5092, 0.4844],
          [0.4678, 0.3275, 0.3676,  ..., 0.6387, 0.7321, 0.4287],
          [0.4924, 0.4390, 0.6179,  ..., 0.6178, 0.4909, 0.4390],
          [0.5051, 0.6504, 0.5480,  ..., 0.4530, 0.6031, 0.6875]],

         [[0.4770, 0.4990, 0.4745,  ..., 0.5266, 0.5151, 0.5537],
          [0.5089, 0.6187, 0.4240,  ..., 0.6113, 0.4355, 0.6306],
          [0.5292, 0.4320, 0.5870,  ..., 0.5322, 0.5294, 0.2820],
          [0.4751, 0.7080, 0.5322,  ..., 0.5945, 0.4922, 0.4049]],

         [[0.4320, 0.3602, 0.4674,  ..., 0.4234, 0.5856, 0.4790],
          [0.4460, 0.6071, 0.3261,  ..., 0.4148, 0.5412, 0.4054],
          [0.6182, 0.6866, 0.5870,  ..., 0.4153, 0.4369, 0.5218],
          [0.6297, 0.4414, 0.4492,  ..., 0.4040, 0.5040, 0.5051]],

         ...,

         [[0.4671, 0.4489, 0.5804,  ..., 0.6090, 0.3933, 0.6405],
          [0.5893, 0.4615, 0.5699,  ..., 0.3701, 0.5259, 0.4744],
          [0.3406, 0.5474, 0.4691,  ..., 0.4301, 0.6124, 0.6450],
          [0.5392, 0.6081, 0.6335,  ..., 0.3139, 0.6920, 0.5827]],

         [[0.5104, 0.6351, 0.3822,  ..., 0.5722, 0.4451, 0.5351],
          [0.4134, 0.4870, 0.4364,  ..., 0.5571, 0.4364, 0.3181],
          [0.5351, 0.5093, 0.5074,  ..., 0.4395, 0.6353, 0.2876],
          [0.5428, 0.4186, 0.6073,  ..., 0.3899, 0.3758, 0.4876]],

         [[0.5377, 0.6164, 0.3287,  ..., 0.5326, 0.7108, 0.4883],
          [0.4751, 0.5774, 0.6179,  ..., 0.5636, 0.3818, 0.6468],
          [0.4400, 0.4694, 0.5608,  ..., 0.5438, 0.5309, 0.5061],
          [0.3885, 0.4248, 0.6742,  ..., 0.4380, 0.5625, 0.3997]]]],
       device='cuda:0')
tensor([[[[0.4783, 0.4755, 0.4906,  ..., 0.6575, 0.4833, 0.3900],
          [0.6132, 0.3251, 0.5898,  ..., 0.6002, 0.4064, 0.4763],
          [0.5317, 0.6058, 0.5636,  ..., 0.4740, 0.4608, 0.6325],
          [0.4496, 0.4182, 0.5641,  ..., 0.4083, 0.5946, 0.4718]],

         [[0.5085, 0.5484, 0.4518,  ..., 0.3522, 0.4683, 0.4434],
          [0.6225, 0.5012, 0.4954,  ..., 0.4325, 0.5193, 0.4928],
          [0.4050, 0.5062, 0.3442,  ..., 0.5173, 0.4154, 0.4913],
          [0.5765, 0.4852, 0.4026,  ..., 0.4690, 0.7387, 0.3486]],

         [[0.3775, 0.5310, 0.5080,  ..., 0.6584, 0.4087, 0.5134],
          [0.5030, 0.3951, 0.3803,  ..., 0.5131, 0.5842, 0.5756],
          [0.4931, 0.4031, 0.4718,  ..., 0.5590, 0.4598, 0.5955],
          [0.5261, 0.5865, 0.5248,  ..., 0.4707, 0.4571, 0.5832]],

         ...,

         [[0.4593, 0.5950, 0.3684,  ..., 0.6280, 0.4622, 0.4860],
          [0.5400, 0.4567, 0.5941,  ..., 0.5727, 0.5851, 0.5856],
          [0.4654, 0.4717, 0.5162,  ..., 0.3975, 0.6025, 0.5941],
          [0.5329, 0.3821, 0.3174,  ..., 0.6220, 0.5660, 0.4953]],

         [[0.4335, 0.7256, 0.5344,  ..., 0.6645, 0.5426, 0.5955],
          [0.5751, 0.4196, 0.4889,  ..., 0.5610, 0.4211, 0.6030],
          [0.4492, 0.4378, 0.4915,  ..., 0.4144, 0.4297, 0.5865],
          [0.4012, 0.7114, 0.4605,  ..., 0.4944, 0.5576, 0.5216]],

         [[0.5339, 0.5143, 0.5307,  ..., 0.7372, 0.5530, 0.4069],
          [0.5870, 0.5574, 0.5005,  ..., 0.7106, 0.5263, 0.5641],
          [0.4434, 0.5521, 0.5380,  ..., 0.4703, 0.5718, 0.6243],
          [0.4547, 0.4140, 0.4306,  ..., 0.4933, 0.6325, 0.4743]]],


        [[[0.4073, 0.6478, 0.5479,  ..., 0.4689, 0.5082, 0.4854],
          [0.4688, 0.3285, 0.3666,  ..., 0.6397, 0.7311, 0.4297],
          [0.4934, 0.4400, 0.6169,  ..., 0.6188, 0.4899, 0.4400],
          [0.5061, 0.6514, 0.5470,  ..., 0.4540, 0.6021, 0.6885]],

         [[0.4780, 0.5000, 0.4735,  ..., 0.5276, 0.5141, 0.5547],
          [0.5099, 0.6197, 0.4230,  ..., 0.6123, 0.4345, 0.6316],
          [0.5302, 0.4330, 0.5860,  ..., 0.5332, 0.5284, 0.2830],
          [0.4761, 0.7090, 0.5312,  ..., 0.5955, 0.4912, 0.4059]],

         [[0.4330, 0.3612, 0.4664,  ..., 0.4244, 0.5846, 0.4800],
          [0.4470, 0.6081, 0.3251,  ..., 0.4158, 0.5402, 0.4064],
          [0.6192, 0.6876, 0.5860,  ..., 0.4163, 0.4359, 0.5228],
          [0.6307, 0.4424, 0.4482,  ..., 0.4050, 0.5030, 0.5061]],

         ...,

         [[0.4681, 0.4499, 0.5794,  ..., 0.6100, 0.3923, 0.6415],
          [0.5903, 0.4625, 0.5689,  ..., 0.3711, 0.5249, 0.4754],
          [0.3416, 0.5484, 0.4681,  ..., 0.4311, 0.6114, 0.6460],
          [0.5402, 0.6091, 0.6325,  ..., 0.3149, 0.6910, 0.5837]],

         [[0.5114, 0.6361, 0.3812,  ..., 0.5732, 0.4441, 0.5361],
          [0.4144, 0.4880, 0.4354,  ..., 0.5581, 0.4354, 0.3191],
          [0.5361, 0.5103, 0.5064,  ..., 0.4405, 0.6343, 0.2886],
          [0.5438, 0.4196, 0.6063,  ..., 0.3909, 0.3748, 0.4886]],

         [[0.5387, 0.6174, 0.3277,  ..., 0.5336, 0.7098, 0.4893],
          [0.4761, 0.5784, 0.6169,  ..., 0.5646, 0.3808, 0.6478],
          [0.4410, 0.4704, 0.5598,  ..., 0.5448, 0.5299, 0.5071],
          [0.3895, 0.4258, 0.6732,  ..., 0.4390, 0.5615, 0.4007]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0010, -0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0010, -0.0010,
         0.0010, -0.0010], device='cuda:0')
selected experts tensor([1726, 1797, 1569, 1341, 1774, 1437, 2058, 1634, 1458, 1590],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6574, 0.4183, 0.5722,  ..., 0.4121, 0.5202, 0.4931],
          [0.5004, 0.6179, 0.4073,  ..., 0.5880, 0.5894, 0.3532],
          [0.5409, 0.5823, 0.5566,  ..., 0.4499, 0.5294, 0.3957],
          [0.5489, 0.6317, 0.5755,  ..., 0.4623, 0.4432, 0.6119]],

         [[0.6369, 0.4838, 0.5046,  ..., 0.4811, 0.3878, 0.6230],
          [0.5850, 0.6026, 0.4082,  ..., 0.6380, 0.5242, 0.4405],
          [0.5978, 0.4821, 0.5423,  ..., 0.5514, 0.5106, 0.4815],
          [0.7331, 0.5533, 0.6378,  ..., 0.4893, 0.4990, 0.3767]],

         [[0.5940, 0.5283, 0.6048,  ..., 0.5101, 0.3957, 0.5334],
          [0.5865, 0.6550, 0.3432,  ..., 0.5286, 0.4283, 0.4591],
          [0.4588, 0.4586, 0.5513,  ..., 0.5723, 0.3905, 0.4088],
          [0.5983, 0.5385, 0.5467,  ..., 0.4598, 0.5017, 0.3795]],

         ...,

         [[0.5576, 0.5380, 0.6441,  ..., 0.5523, 0.5267, 0.4574],
          [0.4191, 0.4516, 0.5277,  ..., 0.5641, 0.3532, 0.4676],
          [0.4486, 0.3767, 0.6722,  ..., 0.6425, 0.5567, 0.5286],
          [0.4385, 0.6230, 0.5189,  ..., 0.5216, 0.5380, 0.4911]],

         [[0.4853, 0.4412, 0.5283,  ..., 0.4851, 0.5572, 0.5766],
          [0.3821, 0.5312, 0.4248,  ..., 0.5852, 0.4093, 0.3961],
          [0.5132, 0.5314, 0.5987,  ..., 0.4922, 0.4770, 0.4516],
          [0.4397, 0.5359, 0.5789,  ..., 0.5463, 0.4470, 0.5156]],

         [[0.6875, 0.4788, 0.5869,  ..., 0.3730, 0.6506, 0.3622],
          [0.4412, 0.6073, 0.4632,  ..., 0.4591, 0.4836, 0.5254],
          [0.4972, 0.3176, 0.5460,  ..., 0.5136, 0.4943, 0.3479],
          [0.4419, 0.5325, 0.6756,  ..., 0.5023, 0.5193, 0.4861]]],


        [[[0.6192, 0.4273, 0.5689,  ..., 0.3864, 0.5122, 0.6380],
          [0.6182, 0.5979, 0.3557,  ..., 0.6568, 0.5927, 0.4235],
          [0.4921, 0.3896, 0.5626,  ..., 0.4335, 0.4858, 0.5605],
          [0.5433, 0.5132, 0.6270,  ..., 0.4720, 0.4940, 0.4853]],

         [[0.5727, 0.6161, 0.6029,  ..., 0.3905, 0.3613, 0.4121],
          [0.6297, 0.6407, 0.5409,  ..., 0.5276, 0.2848, 0.4555],
          [0.5423, 0.4878, 0.6095,  ..., 0.4088, 0.6105, 0.5451],
          [0.6592, 0.3470, 0.4752,  ..., 0.5047, 0.4630, 0.4264]],

         [[0.5997, 0.5380, 0.5160,  ..., 0.5283, 0.4699, 0.5405],
          [0.5590, 0.5562, 0.4511,  ..., 0.5552, 0.3915, 0.4940],
          [0.6342, 0.4424, 0.4561,  ..., 0.5670, 0.5098, 0.3631],
          [0.5751, 0.6317, 0.6369,  ..., 0.5847, 0.5516, 0.5337]],

         ...,

         [[0.6875, 0.5485, 0.4975,  ..., 0.4659, 0.5523, 0.4903],
          [0.3485, 0.4618, 0.4863,  ..., 0.5567, 0.5899, 0.4046],
          [0.4779, 0.3391, 0.6297,  ..., 0.4456, 0.4302, 0.5304],
          [0.5612, 0.6082, 0.5936,  ..., 0.4249, 0.3980, 0.4678]],

         [[0.5669, 0.5694, 0.5665,  ..., 0.3218, 0.5613, 0.5164],
          [0.5674, 0.6389, 0.4496,  ..., 0.5086, 0.5993, 0.5622],
          [0.5230, 0.4899, 0.4282,  ..., 0.5197, 0.5661, 0.3943],
          [0.3862, 0.5065, 0.5552,  ..., 0.5547, 0.3408, 0.3227]],

         [[0.6941, 0.4088, 0.5414,  ..., 0.4345, 0.5122, 0.4601],
          [0.4861, 0.4811, 0.4864,  ..., 0.6371, 0.4997, 0.5077],
          [0.6414, 0.4584, 0.6201,  ..., 0.4321, 0.3850, 0.4615],
          [0.5385, 0.4601, 0.5655,  ..., 0.5651, 0.3142, 0.4610]]]],
       device='cuda:0')
tensor([[[[0.6584, 0.4173, 0.5732,  ..., 0.4111, 0.5192, 0.4921],
          [0.5014, 0.6169, 0.4083,  ..., 0.5870, 0.5884, 0.3522],
          [0.5419, 0.5813, 0.5576,  ..., 0.4489, 0.5284, 0.3947],
          [0.5499, 0.6307, 0.5765,  ..., 0.4613, 0.4422, 0.6109]],

         [[0.6379, 0.4828, 0.5056,  ..., 0.4801, 0.3868, 0.6220],
          [0.5860, 0.6016, 0.4092,  ..., 0.6370, 0.5232, 0.4395],
          [0.5988, 0.4811, 0.5433,  ..., 0.5504, 0.5096, 0.4805],
          [0.7341, 0.5523, 0.6388,  ..., 0.4883, 0.4980, 0.3757]],

         [[0.5950, 0.5273, 0.6058,  ..., 0.5091, 0.3947, 0.5324],
          [0.5875, 0.6540, 0.3442,  ..., 0.5276, 0.4273, 0.4581],
          [0.4598, 0.4576, 0.5523,  ..., 0.5713, 0.3895, 0.4078],
          [0.5993, 0.5375, 0.5477,  ..., 0.4588, 0.5007, 0.3785]],

         ...,

         [[0.5586, 0.5370, 0.6451,  ..., 0.5513, 0.5257, 0.4564],
          [0.4201, 0.4506, 0.5287,  ..., 0.5631, 0.3522, 0.4666],
          [0.4496, 0.3757, 0.6732,  ..., 0.6415, 0.5557, 0.5276],
          [0.4395, 0.6220, 0.5199,  ..., 0.5206, 0.5370, 0.4901]],

         [[0.4863, 0.4402, 0.5293,  ..., 0.4841, 0.5562, 0.5756],
          [0.3831, 0.5302, 0.4258,  ..., 0.5842, 0.4083, 0.3951],
          [0.5142, 0.5304, 0.5997,  ..., 0.4912, 0.4760, 0.4506],
          [0.4407, 0.5349, 0.5799,  ..., 0.5453, 0.4460, 0.5146]],

         [[0.6885, 0.4778, 0.5879,  ..., 0.3720, 0.6496, 0.3612],
          [0.4422, 0.6063, 0.4642,  ..., 0.4581, 0.4826, 0.5244],
          [0.4982, 0.3166, 0.5470,  ..., 0.5126, 0.4933, 0.3469],
          [0.4429, 0.5315, 0.6766,  ..., 0.5013, 0.5183, 0.4851]]],


        [[[0.6202, 0.4263, 0.5699,  ..., 0.3854, 0.5112, 0.6370],
          [0.6192, 0.5969, 0.3567,  ..., 0.6558, 0.5917, 0.4225],
          [0.4931, 0.3886, 0.5636,  ..., 0.4325, 0.4848, 0.5595],
          [0.5443, 0.5122, 0.6280,  ..., 0.4710, 0.4930, 0.4843]],

         [[0.5737, 0.6151, 0.6039,  ..., 0.3895, 0.3603, 0.4111],
          [0.6307, 0.6397, 0.5419,  ..., 0.5266, 0.2838, 0.4545],
          [0.5433, 0.4868, 0.6105,  ..., 0.4078, 0.6095, 0.5441],
          [0.6602, 0.3460, 0.4762,  ..., 0.5037, 0.4620, 0.4254]],

         [[0.6007, 0.5370, 0.5170,  ..., 0.5273, 0.4689, 0.5395],
          [0.5600, 0.5552, 0.4521,  ..., 0.5542, 0.3905, 0.4930],
          [0.6352, 0.4414, 0.4571,  ..., 0.5660, 0.5088, 0.3621],
          [0.5761, 0.6307, 0.6379,  ..., 0.5837, 0.5506, 0.5327]],

         ...,

         [[0.6885, 0.5475, 0.4985,  ..., 0.4649, 0.5513, 0.4893],
          [0.3495, 0.4608, 0.4873,  ..., 0.5557, 0.5889, 0.4036],
          [0.4789, 0.3381, 0.6307,  ..., 0.4446, 0.4292, 0.5294],
          [0.5622, 0.6072, 0.5946,  ..., 0.4239, 0.3970, 0.4668]],

         [[0.5679, 0.5684, 0.5675,  ..., 0.3208, 0.5603, 0.5154],
          [0.5684, 0.6379, 0.4506,  ..., 0.5076, 0.5983, 0.5612],
          [0.5240, 0.4889, 0.4292,  ..., 0.5187, 0.5651, 0.3933],
          [0.3872, 0.5055, 0.5562,  ..., 0.5537, 0.3398, 0.3217]],

         [[0.6951, 0.4078, 0.5424,  ..., 0.4335, 0.5112, 0.4591],
          [0.4871, 0.4801, 0.4874,  ..., 0.6361, 0.4987, 0.5067],
          [0.6424, 0.4574, 0.6211,  ..., 0.4311, 0.3840, 0.4605],
          [0.5395, 0.4591, 0.5665,  ..., 0.5641, 0.3132, 0.4600]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0010,  0.0010, -0.0010,  0.0010, -0.0010,  0.0010, -0.0010,  0.0010,
         0.0010,  0.0010], device='cuda:0')
selected experts tensor([2566, 1835, 2371, 1243, 1685,  786, 2402, 1700, 1005,  791],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010],
          [0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010],
          [0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010],
          [0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010]],

         [[0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010],
          [0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010],
          [0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010],
          [0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010]],

         [[0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010],
          [0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010],
          [0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010],
          [0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010]],

         ...,

         [[0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010],
          [0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010],
          [0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010],
          [0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010]],

         [[0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010],
          [0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010],
          [0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010],
          [0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010]],

         [[0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010],
          [0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010],
          [0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010],
          [0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010]]],


        [[[0.4201, 0.5205, 0.3541,  ..., 0.3938, 0.4961, 0.4292],
          [0.3611, 0.4206, 0.4708,  ..., 0.5265, 0.5244, 0.6308],
          [0.6233, 0.5770, 0.4240,  ..., 0.4417, 0.3595, 0.5044],
          [0.4983, 0.4063, 0.5591,  ..., 0.5984, 0.5361, 0.4302]],

         [[0.5621, 0.4044, 0.4550,  ..., 0.3622, 0.4340, 0.3915],
          [0.4819, 0.3548, 0.5627,  ..., 0.4798, 0.4659, 0.4463],
          [0.5210, 0.5760, 0.4400,  ..., 0.3676, 0.4197, 0.4434],
          [0.4929, 0.6315, 0.5412,  ..., 0.6994, 0.4926, 0.4759]],

         [[0.4896, 0.5428, 0.4145,  ..., 0.4952, 0.4535, 0.4579],
          [0.4843, 0.4287, 0.4715,  ..., 0.3703, 0.6524, 0.5193],
          [0.5423, 0.4234, 0.5397,  ..., 0.4444, 0.4187, 0.6110],
          [0.5779, 0.4929, 0.5699,  ..., 0.4627, 0.5368, 0.4552]],

         ...,

         [[0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010],
          [0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010],
          [0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010],
          [0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010]],

         [[0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010],
          [0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010],
          [0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010],
          [0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010]],

         [[0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010],
          [0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010],
          [0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010],
          [0.4990, 0.4990, 0.5010,  ..., 0.5010, 0.5010, 0.5010]]]],
       device='cuda:0')/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([[[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.4211, 0.5215, 0.3531,  ..., 0.3928, 0.4951, 0.4282],
          [0.3621, 0.4216, 0.4698,  ..., 0.5255, 0.5234, 0.6298],
          [0.6243, 0.5780, 0.4230,  ..., 0.4407, 0.3585, 0.5034],
          [0.4993, 0.4073, 0.5581,  ..., 0.5974, 0.5351, 0.4292]],

         [[0.5631, 0.4054, 0.4540,  ..., 0.3612, 0.4330, 0.3905],
          [0.4829, 0.3558, 0.5617,  ..., 0.4788, 0.4649, 0.4453],
          [0.5220, 0.5770, 0.4390,  ..., 0.3666, 0.4187, 0.4424],
          [0.4939, 0.6325, 0.5402,  ..., 0.6984, 0.4916, 0.4749]],

         [[0.4906, 0.5438, 0.4135,  ..., 0.4942, 0.4525, 0.4569],
          [0.4853, 0.4297, 0.4705,  ..., 0.3693, 0.6514, 0.5183],
          [0.5433, 0.4244, 0.5387,  ..., 0.4434, 0.4177, 0.6100],
          [0.5789, 0.4939, 0.5689,  ..., 0.4617, 0.5358, 0.4542]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0010, -0.0010,  0.0010,  0.0010, -0.0010,  0.0010,  0.0010,  0.0010,
         0.0010,  0.0010], device='cuda:0')
selected experts tensor([ 539,  571, 5768, 5633, 1128,  531,  316,  624,  485,  789],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1726., 1797., 1569., 1341., 1774., 1437., 2058., 1634., 1458., 1590.],
        [ 539.,  571., 5768., 5633., 1128.,  531.,  316.,  624.,  485.,  789.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4045, 0.5064, 0.4807,  ..., 0.6345, 0.6035, 0.5814],
          [0.4012, 0.6215, 0.4522,  ..., 0.6189, 0.4637, 0.4360],
          [0.4567, 0.2983, 0.5064,  ..., 0.6318, 0.3657, 0.3768],
          [0.6100, 0.5066, 0.3619,  ..., 0.2834, 0.5201, 0.4269]],

         [[0.5308, 0.6637, 0.4627,  ..., 0.6083, 0.5134, 0.5190],
          [0.5224, 0.4373, 0.6404,  ..., 0.5422, 0.5646, 0.4150],
          [0.4830, 0.4588, 0.6323,  ..., 0.6863, 0.5684, 0.5519],
          [0.5098, 0.5908, 0.5650,  ..., 0.3851, 0.6261, 0.6507]],

         [[0.5266, 0.4078, 0.5683,  ..., 0.5354, 0.5176, 0.6245],
          [0.6334, 0.5229, 0.3538,  ..., 0.6665, 0.5228, 0.4618],
          [0.4385, 0.6307, 0.5855,  ..., 0.6078, 0.4201, 0.3837],
          [0.6379, 0.4463, 0.4532,  ..., 0.5327, 0.4373, 0.5310]],

         ...,

         [[0.3693, 0.5079, 0.4798,  ..., 0.4613, 0.5302, 0.3731],
          [0.6270, 0.2950, 0.5812,  ..., 0.5499, 0.5011, 0.5885],
          [0.6011, 0.6169, 0.4392,  ..., 0.6453, 0.5708, 0.4894],
          [0.5983, 0.5761, 0.6323,  ..., 0.5112, 0.3191, 0.5124]],

         [[0.3831, 0.5489, 0.5784,  ..., 0.3524, 0.4149, 0.4183],
          [0.6234, 0.6007, 0.5106,  ..., 0.5776, 0.5324, 0.5560],
          [0.4967, 0.7106, 0.5611,  ..., 0.5135, 0.5533, 0.5013],
          [0.5884, 0.5446, 0.4551,  ..., 0.4065, 0.5799, 0.5804]],

         [[0.4666, 0.5463, 0.4503,  ..., 0.5102, 0.6514, 0.5176],
          [0.4910, 0.5562, 0.5996,  ..., 0.3878, 0.5118, 0.4288],
          [0.5931, 0.2838, 0.4520,  ..., 0.4973, 0.4443, 0.4906],
          [0.4513, 0.4951, 0.3547,  ..., 0.6829, 0.4022, 0.3864]]],


        [[[0.4045, 0.4564, 0.6080,  ..., 0.5795, 0.6540, 0.4434],
          [0.5412, 0.5194, 0.4277,  ..., 0.4755, 0.5114, 0.4935],
          [0.6442, 0.4789, 0.4368,  ..., 0.4855, 0.5550, 0.3943],
          [0.5636, 0.5139, 0.6964,  ..., 0.4606, 0.4424, 0.6735]],

         [[0.5227, 0.5177, 0.4438,  ..., 0.3542, 0.5069, 0.5738],
          [0.6016, 0.5407, 0.5878,  ..., 0.3750, 0.5279, 0.6863],
          [0.5460, 0.3863, 0.4566,  ..., 0.4245, 0.4822, 0.5548],
          [0.4506, 0.5631, 0.6440,  ..., 0.5490, 0.3993, 0.5282]],

         [[0.5196, 0.5076, 0.3950,  ..., 0.5376, 0.4349, 0.5220],
          [0.4972, 0.4196, 0.5926,  ..., 0.5685, 0.5655, 0.6871],
          [0.6252, 0.4130, 0.5124,  ..., 0.3668, 0.4121, 0.4536],
          [0.4282, 0.4855, 0.4496,  ..., 0.6489, 0.4130, 0.4906]],

         ...,

         [[0.4335, 0.4664, 0.3054,  ..., 0.4326, 0.6316, 0.4577],
          [0.5931, 0.4487, 0.5659,  ..., 0.5456, 0.4431, 0.4278],
          [0.5075, 0.4826, 0.5822,  ..., 0.5828, 0.5492, 0.4972],
          [0.5913, 0.5615, 0.5683,  ..., 0.4659, 0.5950, 0.5615]],

         [[0.4528, 0.5518, 0.4749,  ..., 0.5833, 0.6160, 0.4584],
          [0.3648, 0.5165, 0.3231,  ..., 0.3915, 0.6234, 0.5531],
          [0.4537, 0.5610, 0.6250,  ..., 0.4865, 0.6307, 0.3471],
          [0.4567, 0.5356, 0.4947,  ..., 0.3814, 0.4900, 0.3943]],

         [[0.4656, 0.3798, 0.5836,  ..., 0.4723, 0.4410, 0.3883],
          [0.3951, 0.4722, 0.6195,  ..., 0.5970, 0.4031, 0.4991],
          [0.5477, 0.4340, 0.4196,  ..., 0.5441, 0.6645, 0.4446],
          [0.5044, 0.3666, 0.4162,  ..., 0.5356, 0.3666, 0.4906]]]],
       device='cuda:0')
tensor([[[[0.4045, 0.5064, 0.4827,  ..., 0.6325, 0.6035, 0.5794],
          [0.4012, 0.6215, 0.4542,  ..., 0.6169, 0.4637, 0.4340],
          [0.4567, 0.2983, 0.5084,  ..., 0.6298, 0.3657, 0.3748],
          [0.6100, 0.5066, 0.3639,  ..., 0.2814, 0.5201, 0.4249]],

         [[0.5308, 0.6637, 0.4647,  ..., 0.6063, 0.5134, 0.5170],
          [0.5224, 0.4373, 0.6424,  ..., 0.5402, 0.5646, 0.4130],
          [0.4830, 0.4588, 0.6343,  ..., 0.6843, 0.5684, 0.5499],
          [0.5098, 0.5908, 0.5670,  ..., 0.3831, 0.6261, 0.6487]],

         [[0.5266, 0.4078, 0.5703,  ..., 0.5334, 0.5176, 0.6225],
          [0.6334, 0.5229, 0.3558,  ..., 0.6645, 0.5228, 0.4598],
          [0.4385, 0.6307, 0.5875,  ..., 0.6058, 0.4201, 0.3817],
          [0.6379, 0.4463, 0.4552,  ..., 0.5307, 0.4373, 0.5290]],

         ...,

         [[0.3693, 0.5079, 0.4818,  ..., 0.4593, 0.5302, 0.3711],
          [0.6270, 0.2950, 0.5832,  ..., 0.5479, 0.5011, 0.5865],
          [0.6011, 0.6169, 0.4412,  ..., 0.6433, 0.5708, 0.4874],
          [0.5983, 0.5761, 0.6343,  ..., 0.5092, 0.3191, 0.5104]],

         [[0.3831, 0.5489, 0.5804,  ..., 0.3504, 0.4149, 0.4163],
          [0.6234, 0.6007, 0.5126,  ..., 0.5756, 0.5324, 0.5540],
          [0.4967, 0.7106, 0.5631,  ..., 0.5115, 0.5533, 0.4993],
          [0.5884, 0.5446, 0.4571,  ..., 0.4045, 0.5799, 0.5784]],

         [[0.4666, 0.5463, 0.4523,  ..., 0.5082, 0.6514, 0.5156],
          [0.4910, 0.5562, 0.6016,  ..., 0.3858, 0.5118, 0.4268],
          [0.5931, 0.2838, 0.4540,  ..., 0.4953, 0.4443, 0.4886],
          [0.4513, 0.4951, 0.3567,  ..., 0.6809, 0.4022, 0.3844]]],


        [[[0.4045, 0.4564, 0.6100,  ..., 0.5775, 0.6540, 0.4414],
          [0.5412, 0.5194, 0.4297,  ..., 0.4735, 0.5114, 0.4915],
          [0.6442, 0.4789, 0.4388,  ..., 0.4835, 0.5550, 0.3923],
          [0.5636, 0.5139, 0.6984,  ..., 0.4586, 0.4424, 0.6715]],

         [[0.5227, 0.5177, 0.4458,  ..., 0.3522, 0.5069, 0.5718],
          [0.6016, 0.5407, 0.5898,  ..., 0.3730, 0.5279, 0.6843],
          [0.5460, 0.3863, 0.4586,  ..., 0.4225, 0.4822, 0.5528],
          [0.4506, 0.5631, 0.6460,  ..., 0.5470, 0.3993, 0.5262]],

         [[0.5196, 0.5076, 0.3970,  ..., 0.5356, 0.4349, 0.5200],
          [0.4972, 0.4196, 0.5946,  ..., 0.5665, 0.5655, 0.6851],
          [0.6252, 0.4130, 0.5144,  ..., 0.3648, 0.4121, 0.4516],
          [0.4282, 0.4855, 0.4516,  ..., 0.6469, 0.4130, 0.4886]],

         ...,

         [[0.4335, 0.4664, 0.3074,  ..., 0.4306, 0.6316, 0.4557],
          [0.5931, 0.4487, 0.5679,  ..., 0.5436, 0.4431, 0.4258],
          [0.5075, 0.4826, 0.5842,  ..., 0.5808, 0.5492, 0.4952],
          [0.5913, 0.5615, 0.5703,  ..., 0.4639, 0.5950, 0.5595]],

         [[0.4528, 0.5518, 0.4769,  ..., 0.5813, 0.6160, 0.4564],
          [0.3648, 0.5165, 0.3251,  ..., 0.3895, 0.6234, 0.5511],
          [0.4537, 0.5610, 0.6270,  ..., 0.4845, 0.6307, 0.3451],
          [0.4567, 0.5356, 0.4967,  ..., 0.3794, 0.4900, 0.3923]],

         [[0.4656, 0.3798, 0.5856,  ..., 0.4703, 0.4410, 0.3863],
          [0.3951, 0.4722, 0.6215,  ..., 0.5950, 0.4031, 0.4971],
          [0.5477, 0.4340, 0.4216,  ..., 0.5421, 0.6645, 0.4426],
          [0.5044, 0.3666, 0.4182,  ..., 0.5336, 0.3666, 0.4886]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0000,  0.0000, -0.0020,  0.0000, -0.0020, -0.0020,  0.0020,  0.0020,
         0.0000,  0.0020], device='cuda:0')
selected experts tensor([1698, 1603, 1807, 1542, 1681, 1637, 1517, 1654, 1630, 1615],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4612, 0.6015, 0.4122,  ..., 0.4412, 0.3837, 0.6619],
          [0.5611, 0.6729, 0.4131,  ..., 0.5262, 0.4867, 0.5312],
          [0.3728, 0.4580, 0.4245,  ..., 0.6025, 0.5356, 0.3381],
          [0.4115, 0.3574, 0.5388,  ..., 0.6146, 0.5565, 0.4791]],

         [[0.3592, 0.5304, 0.3795,  ..., 0.6132, 0.4893, 0.5997],
          [0.4639, 0.3889, 0.5122,  ..., 0.5257, 0.5088, 0.4664],
          [0.4101, 0.4011, 0.4625,  ..., 0.4453, 0.3498, 0.3320],
          [0.4835, 0.5963, 0.6041,  ..., 0.3821, 0.5059, 0.3684]],

         [[0.5324, 0.4464, 0.4122,  ..., 0.4273, 0.4667, 0.5053],
          [0.5145, 0.5176, 0.5728,  ..., 0.6160, 0.5553, 0.4634],
          [0.4481, 0.5195, 0.4555,  ..., 0.2393, 0.4463, 0.4369],
          [0.3788, 0.5148, 0.4360,  ..., 0.4824, 0.5395, 0.4407]],

         ...,

         [[0.4016, 0.5139, 0.4930,  ..., 0.4482, 0.5970, 0.4904],
          [0.4172, 0.5611, 0.4360,  ..., 0.4651, 0.3934, 0.4273],
          [0.4922, 0.6015, 0.5252,  ..., 0.5062, 0.4405, 0.5392],
          [0.5546, 0.4428, 0.5570,  ..., 0.3831, 0.4389, 0.4111]],

         [[0.5631, 0.6085, 0.4326,  ..., 0.4574, 0.5028, 0.7058],
          [0.4970, 0.6323, 0.5118,  ..., 0.5889, 0.5499, 0.4630],
          [0.5669, 0.6634, 0.4620,  ..., 0.5903, 0.5041, 0.4059],
          [0.5702, 0.5212, 0.5125,  ..., 0.4373, 0.2970, 0.4941]],

         [[0.4262, 0.6205, 0.5441,  ..., 0.5164, 0.6208, 0.6424],
          [0.6377, 0.4930, 0.6417,  ..., 0.3919, 0.5690, 0.7786],
          [0.3317, 0.5185, 0.5610,  ..., 0.4754, 0.5265, 0.4763],
          [0.4956, 0.4344, 0.3777,  ..., 0.4947, 0.6560, 0.4201]]],


        [[[0.5472, 0.4387, 0.5625,  ..., 0.4168, 0.4393, 0.3998],
          [0.4622, 0.5268, 0.3194,  ..., 0.5291, 0.4794, 0.4694],
          [0.4258, 0.3079, 0.6129,  ..., 0.4477, 0.4821, 0.5083],
          [0.5944, 0.4205, 0.5928,  ..., 0.4040, 0.4169, 0.6451]],

         [[0.5443, 0.4493, 0.6300,  ..., 0.4540, 0.4659, 0.4479],
          [0.4162, 0.4883, 0.6752,  ..., 0.4760, 0.4925, 0.3711],
          [0.4358, 0.4262, 0.5373,  ..., 0.5206, 0.5800, 0.4059],
          [0.5321, 0.5350, 0.4013,  ..., 0.5019, 0.4618, 0.5346]],

         [[0.5926, 0.3475, 0.5269,  ..., 0.4620, 0.4107, 0.5341],
          [0.4353, 0.4846, 0.4051,  ..., 0.3780, 0.3297, 0.6049],
          [0.3610, 0.3760, 0.4434,  ..., 0.4311, 0.4093, 0.4064],
          [0.3746, 0.5033, 0.6665,  ..., 0.4916, 0.5263, 0.6442]],

         ...,

         [[0.4248, 0.3815, 0.3713,  ..., 0.5590, 0.7158, 0.4154],
          [0.5664, 0.4989, 0.4974,  ..., 0.4078, 0.5064, 0.3451],
          [0.3792, 0.4365, 0.6050,  ..., 0.5922, 0.5287, 0.3630],
          [0.3913, 0.3746, 0.4616,  ..., 0.4121, 0.3686, 0.5029]],

         [[0.4753, 0.5110, 0.4478,  ..., 0.4733, 0.7086, 0.3495],
          [0.3908, 0.4597, 0.3750,  ..., 0.5093, 0.6820, 0.4828],
          [0.5693, 0.4505, 0.4480,  ..., 0.5908, 0.4625, 0.5477],
          [0.4148, 0.4775, 0.4379,  ..., 0.6424, 0.3851, 0.5162]],

         [[0.5750, 0.6814, 0.4221,  ..., 0.3630, 0.4652, 0.5983],
          [0.3592, 0.4910, 0.4719,  ..., 0.5475, 0.6471, 0.5358],
          [0.4110, 0.3710, 0.4793,  ..., 0.5732, 0.4495, 0.4751],
          [0.5316, 0.4349, 0.5847,  ..., 0.5562, 0.4389, 0.5518]]]],
       device='cuda:0')
tensor([[[[0.4632, 0.6035, 0.4102,  ..., 0.4412, 0.3817, 0.6619],
          [0.5631, 0.6749, 0.4111,  ..., 0.5262, 0.4847, 0.5312],
          [0.3748, 0.4600, 0.4225,  ..., 0.6025, 0.5336, 0.3381],
          [0.4135, 0.3594, 0.5368,  ..., 0.6146, 0.5545, 0.4791]],

         [[0.3612, 0.5324, 0.3775,  ..., 0.6132, 0.4873, 0.5997],
          [0.4659, 0.3909, 0.5102,  ..., 0.5257, 0.5068, 0.4664],
          [0.4121, 0.4031, 0.4605,  ..., 0.4453, 0.3478, 0.3320],
          [0.4855, 0.5983, 0.6021,  ..., 0.3821, 0.5039, 0.3684]],

         [[0.5344, 0.4484, 0.4102,  ..., 0.4273, 0.4647, 0.5053],
          [0.5165, 0.5196, 0.5708,  ..., 0.6160, 0.5533, 0.4634],
          [0.4501, 0.5215, 0.4535,  ..., 0.2393, 0.4443, 0.4369],
          [0.3808, 0.5168, 0.4340,  ..., 0.4824, 0.5375, 0.4407]],

         ...,

         [[0.4036, 0.5159, 0.4910,  ..., 0.4482, 0.5950, 0.4904],
          [0.4192, 0.5631, 0.4340,  ..., 0.4651, 0.3914, 0.4273],
          [0.4942, 0.6035, 0.5232,  ..., 0.5062, 0.4385, 0.5392],
          [0.5566, 0.4448, 0.5550,  ..., 0.3831, 0.4369, 0.4111]],

         [[0.5651, 0.6105, 0.4306,  ..., 0.4574, 0.5008, 0.7058],
          [0.4990, 0.6343, 0.5098,  ..., 0.5889, 0.5479, 0.4630],
          [0.5689, 0.6654, 0.4600,  ..., 0.5903, 0.5021, 0.4059],
          [0.5722, 0.5232, 0.5105,  ..., 0.4373, 0.2950, 0.4941]],

         [[0.4282, 0.6225, 0.5421,  ..., 0.5164, 0.6188, 0.6424],
          [0.6397, 0.4950, 0.6397,  ..., 0.3919, 0.5670, 0.7786],
          [0.3337, 0.5205, 0.5590,  ..., 0.4754, 0.5245, 0.4763],
          [0.4976, 0.4364, 0.3757,  ..., 0.4947, 0.6540, 0.4201]]],


        [[[0.5492, 0.4407, 0.5605,  ..., 0.4168, 0.4373, 0.3998],
          [0.4642, 0.5288, 0.3174,  ..., 0.5291, 0.4774, 0.4694],
          [0.4278, 0.3099, 0.6109,  ..., 0.4477, 0.4801, 0.5083],
          [0.5964, 0.4225, 0.5908,  ..., 0.4040, 0.4149, 0.6451]],

         [[0.5463, 0.4513, 0.6280,  ..., 0.4540, 0.4639, 0.4479],
          [0.4182, 0.4903, 0.6732,  ..., 0.4760, 0.4905, 0.3711],
          [0.4378, 0.4282, 0.5353,  ..., 0.5206, 0.5780, 0.4059],
          [0.5341, 0.5370, 0.3993,  ..., 0.5019, 0.4598, 0.5346]],

         [[0.5946, 0.3495, 0.5249,  ..., 0.4620, 0.4087, 0.5341],
          [0.4373, 0.4866, 0.4031,  ..., 0.3780, 0.3277, 0.6049],
          [0.3630, 0.3780, 0.4414,  ..., 0.4311, 0.4073, 0.4064],
          [0.3766, 0.5053, 0.6645,  ..., 0.4916, 0.5243, 0.6442]],

         ...,

         [[0.4268, 0.3835, 0.3693,  ..., 0.5590, 0.7138, 0.4154],
          [0.5684, 0.5009, 0.4954,  ..., 0.4078, 0.5044, 0.3451],
          [0.3812, 0.4385, 0.6030,  ..., 0.5922, 0.5267, 0.3630],
          [0.3933, 0.3766, 0.4596,  ..., 0.4121, 0.3666, 0.5029]],

         [[0.4773, 0.5130, 0.4458,  ..., 0.4733, 0.7066, 0.3495],
          [0.3928, 0.4617, 0.3730,  ..., 0.5093, 0.6800, 0.4828],
          [0.5713, 0.4525, 0.4460,  ..., 0.5908, 0.4605, 0.5477],
          [0.4168, 0.4795, 0.4359,  ..., 0.6424, 0.3831, 0.5162]],

         [[0.5770, 0.6834, 0.4201,  ..., 0.3630, 0.4632, 0.5983],
          [0.3612, 0.4930, 0.4699,  ..., 0.5475, 0.6451, 0.5358],
          [0.4130, 0.3730, 0.4773,  ..., 0.5732, 0.4475, 0.4751],
          [0.5336, 0.4369, 0.5827,  ..., 0.5562, 0.4369, 0.5518]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0020, -0.0020,  0.0020,  0.0020,  0.0000,  0.0020, -0.0020,  0.0000,
         0.0020,  0.0000], device='cuda:0')
selected experts tensor([1674, 1661, 1658, 1398, 1572, 1559, 1883, 1868, 1614, 1497],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4305, 0.6095, 0.6359,  ..., 0.4869, 0.3127, 0.4490],
          [0.5631, 0.6397, 0.4901,  ..., 0.4557, 0.3271, 0.4637],
          [0.6149, 0.4982, 0.5358,  ..., 0.5250, 0.6083, 0.3418],
          [0.5973, 0.4381, 0.6010,  ..., 0.4249, 0.5085, 0.4393]],

         [[0.4176, 0.4664, 0.4856,  ..., 0.4557, 0.5804, 0.5207],
          [0.4857, 0.4761, 0.3992,  ..., 0.6215, 0.4197, 0.4056],
          [0.4272, 0.5751, 0.5774,  ..., 0.5344, 0.3713, 0.3915],
          [0.5192, 0.4552, 0.6755,  ..., 0.4896, 0.3759, 0.5495]],

         [[0.6061, 0.4615, 0.4751,  ..., 0.4758, 0.5890, 0.5728],
          [0.4181, 0.4216, 0.4467,  ..., 0.6478, 0.5876, 0.3623],
          [0.4777, 0.5187, 0.5057,  ..., 0.4975, 0.4991, 0.4509],
          [0.5590, 0.4064, 0.6136,  ..., 0.5636, 0.4288, 0.6820]],

         ...,

         [[0.4305, 0.4523, 0.5888,  ..., 0.5012, 0.5690, 0.4560],
          [0.4884, 0.4537, 0.5179,  ..., 0.5311, 0.3851, 0.5785],
          [0.7911, 0.5804, 0.6538,  ..., 0.4830, 0.5307, 0.5441],
          [0.5935, 0.5302, 0.5702,  ..., 0.5636, 0.4197, 0.6846]],

         [[0.5902, 0.3522, 0.5285,  ..., 0.2983, 0.3976, 0.5507],
          [0.3792, 0.6100, 0.6651,  ..., 0.5129, 0.3383, 0.4461],
          [0.5229, 0.5078, 0.4286,  ..., 0.5851, 0.5608, 0.3892],
          [0.6033, 0.5540, 0.5384,  ..., 0.5851, 0.5146, 0.5210]],

         [[0.6075, 0.5204, 0.6582,  ..., 0.4649, 0.4393, 0.4955],
          [0.2716, 0.5198, 0.4447,  ..., 0.4858, 0.6263, 0.5495],
          [0.4176, 0.4407, 0.4815,  ..., 0.3970, 0.5847, 0.5444],
          [0.2778, 0.3863, 0.5409,  ..., 0.5324, 0.4207, 0.3515]]],


        [[[0.5826, 0.5045, 0.5650,  ..., 0.4116, 0.4512, 0.6863],
          [0.4238, 0.5215, 0.4812,  ..., 0.6114, 0.6055, 0.4037],
          [0.5447, 0.4140, 0.5698,  ..., 0.5346, 0.5536, 0.4240],
          [0.4721, 0.3975, 0.6440,  ..., 0.6406, 0.4259, 0.6363]],

         [[0.5028, 0.3993, 0.6386,  ..., 0.5679, 0.5352, 0.6803],
          [0.4459, 0.4620, 0.4610,  ..., 0.5244, 0.5862, 0.4753],
          [0.4756, 0.4947, 0.4938,  ..., 0.5813, 0.4708, 0.4596],
          [0.5004, 0.5559, 0.6214,  ..., 0.6343, 0.4060, 0.4188]],

         [[0.5650, 0.5024, 0.6940,  ..., 0.5170, 0.4122, 0.5129],
          [0.4738, 0.5084, 0.4634,  ..., 0.4882, 0.4630, 0.5138],
          [0.5869, 0.4689, 0.5168,  ..., 0.5732, 0.4345, 0.5121],
          [0.5712, 0.6397, 0.5968,  ..., 0.5936, 0.5579, 0.5059]],

         ...,

         [[0.6314, 0.5492, 0.6149,  ..., 0.4168, 0.3119, 0.3995],
          [0.5883, 0.5397, 0.5172,  ..., 0.5463, 0.4832, 0.4712],
          [0.4532, 0.3872, 0.5350,  ..., 0.3868, 0.4160, 0.3943],
          [0.5360, 0.3877, 0.5940,  ..., 0.4723, 0.3650, 0.4650]],

         [[0.5755, 0.5501, 0.5411,  ..., 0.4756, 0.4606, 0.5036],
          [0.4951, 0.4717, 0.5270,  ..., 0.2830, 0.4915, 0.4710],
          [0.4399, 0.4489, 0.4695,  ..., 0.4173, 0.5570, 0.5339],
          [0.5568, 0.5267, 0.4866,  ..., 0.4426, 0.3939, 0.4948]],

         [[0.6712, 0.4754, 0.6573,  ..., 0.5421, 0.4972, 0.5871],
          [0.5488, 0.5765, 0.6449,  ..., 0.5265, 0.5236, 0.3418],
          [0.5287, 0.4381, 0.4394,  ..., 0.4755, 0.4548, 0.3578],
          [0.5498, 0.6118, 0.6431,  ..., 0.5808, 0.4369, 0.4393]]]],
       device='cuda:0')
tensor([[[[0.4325, 0.6095, 0.6379,  ..., 0.4869, 0.3107, 0.4470],
          [0.5651, 0.6397, 0.4921,  ..., 0.4557, 0.3251, 0.4617],
          [0.6169, 0.4982, 0.5378,  ..., 0.5250, 0.6063, 0.3398],
          [0.5993, 0.4381, 0.6030,  ..., 0.4249, 0.5065, 0.4373]],

         [[0.4196, 0.4664, 0.4876,  ..., 0.4557, 0.5784, 0.5187],
          [0.4877, 0.4761, 0.4012,  ..., 0.6215, 0.4177, 0.4036],
          [0.4292, 0.5751, 0.5794,  ..., 0.5344, 0.3693, 0.3895],
          [0.5212, 0.4552, 0.6775,  ..., 0.4896, 0.3739, 0.5475]],

         [[0.6081, 0.4615, 0.4771,  ..., 0.4758, 0.5870, 0.5708],
          [0.4201, 0.4216, 0.4487,  ..., 0.6478, 0.5856, 0.3603],
          [0.4797, 0.5187, 0.5077,  ..., 0.4975, 0.4971, 0.4489],
          [0.5610, 0.4064, 0.6156,  ..., 0.5636, 0.4268, 0.6800]],

         ...,

         [[0.4325, 0.4523, 0.5908,  ..., 0.5012, 0.5670, 0.4540],
          [0.4904, 0.4537, 0.5199,  ..., 0.5311, 0.3831, 0.5765],
          [0.7931, 0.5804, 0.6558,  ..., 0.4830, 0.5287, 0.5421],
          [0.5955, 0.5302, 0.5722,  ..., 0.5636, 0.4177, 0.6826]],

         [[0.5922, 0.3522, 0.5305,  ..., 0.2983, 0.3956, 0.5487],
          [0.3812, 0.6100, 0.6671,  ..., 0.5129, 0.3363, 0.4441],
          [0.5249, 0.5078, 0.4306,  ..., 0.5851, 0.5588, 0.3872],
          [0.6053, 0.5540, 0.5404,  ..., 0.5851, 0.5126, 0.5190]],

         [[0.6095, 0.5204, 0.6602,  ..., 0.4649, 0.4373, 0.4935],
          [0.2736, 0.5198, 0.4467,  ..., 0.4858, 0.6243, 0.5475],
          [0.4196, 0.4407, 0.4835,  ..., 0.3970, 0.5827, 0.5424],
          [0.2798, 0.3863, 0.5429,  ..., 0.5324, 0.4187, 0.3495]]],


        [[[0.5846, 0.5045, 0.5670,  ..., 0.4116, 0.4492, 0.6843],
          [0.4258, 0.5215, 0.4832,  ..., 0.6114, 0.6035, 0.4017],
          [0.5467, 0.4140, 0.5718,  ..., 0.5346, 0.5516, 0.4220],
          [0.4741, 0.3975, 0.6460,  ..., 0.6406, 0.4239, 0.6343]],

         [[0.5048, 0.3993, 0.6406,  ..., 0.5679, 0.5332, 0.6783],
          [0.4479, 0.4620, 0.4630,  ..., 0.5244, 0.5842, 0.4733],
          [0.4776, 0.4947, 0.4958,  ..., 0.5813, 0.4688, 0.4576],
          [0.5024, 0.5559, 0.6234,  ..., 0.6343, 0.4040, 0.4168]],

         [[0.5670, 0.5024, 0.6960,  ..., 0.5170, 0.4102, 0.5109],
          [0.4758, 0.5084, 0.4654,  ..., 0.4882, 0.4610, 0.5118],
          [0.5889, 0.4689, 0.5188,  ..., 0.5732, 0.4325, 0.5101],
          [0.5732, 0.6397, 0.5988,  ..., 0.5936, 0.5559, 0.5039]],

         ...,

         [[0.6334, 0.5492, 0.6169,  ..., 0.4168, 0.3099, 0.3975],
          [0.5903, 0.5397, 0.5192,  ..., 0.5463, 0.4812, 0.4692],
          [0.4552, 0.3872, 0.5370,  ..., 0.3868, 0.4140, 0.3923],
          [0.5380, 0.3877, 0.5960,  ..., 0.4723, 0.3630, 0.4630]],

         [[0.5775, 0.5501, 0.5431,  ..., 0.4756, 0.4586, 0.5016],
          [0.4971, 0.4717, 0.5290,  ..., 0.2830, 0.4895, 0.4690],
          [0.4419, 0.4489, 0.4715,  ..., 0.4173, 0.5550, 0.5319],
          [0.5588, 0.5267, 0.4886,  ..., 0.4426, 0.3919, 0.4928]],

         [[0.6732, 0.4754, 0.6593,  ..., 0.5421, 0.4952, 0.5851],
          [0.5508, 0.5765, 0.6469,  ..., 0.5265, 0.5216, 0.3398],
          [0.5307, 0.4381, 0.4414,  ..., 0.4755, 0.4528, 0.3558],
          [0.5518, 0.6118, 0.6451,  ..., 0.5808, 0.4349, 0.4373]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0020,  0.0000, -0.0020,  0.0020, -0.0020,  0.0020, -0.0020,  0.0000,
         0.0020,  0.0020], device='cuda:0')
selected experts tensor([2189, 1840, 2443, 1497, 1832,  839, 2284, 1629, 1043,  788],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4230, 0.4728, 0.3702,  ..., 0.4749, 0.5124, 0.5890],
          [0.4630, 0.3951, 0.3975,  ..., 0.4360, 0.5326, 0.5319],
          [0.5908, 0.5756, 0.4552,  ..., 0.4312, 0.5369, 0.3271],
          [0.4201, 0.5378, 0.6469,  ..., 0.5314, 0.4502, 0.4991]],

         [[0.5931, 0.4149, 0.4654,  ..., 0.4654, 0.3713, 0.5914],
          [0.5610, 0.3739, 0.4078,  ..., 0.3809, 0.5852, 0.5113],
          [0.5823, 0.4244, 0.4706,  ..., 0.4259, 0.4350, 0.6507],
          [0.4244, 0.4530, 0.6030,  ..., 0.4974, 0.5076, 0.4336]],

         [[0.5641, 0.5133, 0.5383,  ..., 0.4916, 0.4331, 0.5942],
          [0.3720, 0.3923, 0.5098,  ..., 0.4369, 0.5862, 0.4207],
          [0.5492, 0.5395, 0.4813,  ..., 0.4977, 0.4943, 0.5331],
          [0.4966, 0.4711, 0.5404,  ..., 0.5876, 0.5294, 0.4216]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020]]],


        [[[0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020],
          [0.5000, 0.5000, 0.5000,  ..., 0.5020, 0.5020, 0.5020]]]],
       device='cuda:0')
tensor([[[[0.4230, 0.4728, 0.3702,  ..., 0.4729, 0.5104, 0.5870],
          [0.4630, 0.3951, 0.3975,  ..., 0.4340, 0.5306, 0.5299],
          [0.5908, 0.5756, 0.4552,  ..., 0.4292, 0.5349, 0.3251],
          [0.4201, 0.5378, 0.6469,  ..., 0.5294, 0.4482, 0.4971]],

         [[0.5931, 0.4149, 0.4654,  ..., 0.4634, 0.3693, 0.5894],
          [0.5610, 0.3739, 0.4078,  ..., 0.3789, 0.5832, 0.5093],
          [0.5823, 0.4244, 0.4706,  ..., 0.4239, 0.4330, 0.6487],
          [0.4244, 0.4530, 0.6030,  ..., 0.4954, 0.5056, 0.4316]],

         [[0.5641, 0.5133, 0.5383,  ..., 0.4896, 0.4311, 0.5922],
          [0.3720, 0.3923, 0.5098,  ..., 0.4349, 0.5842, 0.4187],
          [0.5492, 0.5395, 0.4813,  ..., 0.4957, 0.4923, 0.5311],
          [0.4966, 0.4711, 0.5404,  ..., 0.5856, 0.5274, 0.4196]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0020, 0.0020, 0.0020, 0.0020,
        0.0020], device='cuda:0')
selected experts tensor([ 492,  555,  598,  531, 1392, 5487, 5406,  661,  492,  770],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1674., 1661., 1658., 1398., 1572., 1559., 1883., 1868., 1614., 1497.],
        [ 492.,  555.,  598.,  531., 1392., 5487., 5406.,  661.,  492.,  770.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5832, 0.4523, 0.5165,  ..., 0.4302, 0.5487, 0.5666],
          [0.3904, 0.4407, 0.5090,  ..., 0.5670, 0.5974, 0.4435],
          [0.3539, 0.4557, 0.6121,  ..., 0.4939, 0.3859, 0.5637],
          [0.6146, 0.4981, 0.5621,  ..., 0.4744, 0.5115, 0.4991]],

         [[0.5314, 0.6398, 0.3555,  ..., 0.3758, 0.6207, 0.5396],
          [0.5167, 0.6068, 0.5649,  ..., 0.5349, 0.4283, 0.4609],
          [0.4837, 0.5177, 0.5009,  ..., 0.5089, 0.5042, 0.4724],
          [0.4450, 0.5804, 0.5669,  ..., 0.3640, 0.6049, 0.5709]],

         [[0.4812, 0.5067, 0.5035,  ..., 0.3952, 0.6479, 0.4960],
          [0.7088, 0.4221, 0.5892,  ..., 0.3532, 0.5804, 0.5046],
          [0.5770, 0.5596, 0.5916,  ..., 0.2920, 0.4013, 0.5961],
          [0.2908, 0.3975, 0.6075,  ..., 0.5247, 0.5908, 0.4127]],

         ...,

         [[0.5353, 0.3901, 0.4928,  ..., 0.4288, 0.5329, 0.5862],
          [0.5390, 0.5234, 0.4478,  ..., 0.6198, 0.4613, 0.4613],
          [0.6122, 0.4083, 0.4595,  ..., 0.6225, 0.4922, 0.4587],
          [0.5515, 0.4187, 0.4392,  ..., 0.5828, 0.4487, 0.3624]],

         [[0.4383, 0.4288, 0.4324,  ..., 0.5485, 0.5317, 0.4880],
          [0.3834, 0.2432, 0.5565,  ..., 0.5315, 0.6049, 0.5301],
          [0.5617, 0.5866, 0.4095,  ..., 0.5395, 0.4810, 0.5334],
          [0.5416, 0.5368, 0.3865,  ..., 0.4803, 0.5299, 0.5308]],

         [[0.5491, 0.5007, 0.4205,  ..., 0.4245, 0.4458, 0.5150],
          [0.3327, 0.5118, 0.4072,  ..., 0.5059, 0.5329, 0.4346],
          [0.3232, 0.5603, 0.4152,  ..., 0.5564, 0.5956, 0.5290],
          [0.5784, 0.5136, 0.4824,  ..., 0.6179, 0.4934, 0.4075]]],


        [[[0.5201, 0.4528, 0.4319,  ..., 0.3919, 0.5053, 0.5757],
          [0.4368, 0.4572, 0.4977,  ..., 0.5051, 0.5685, 0.5279],
          [0.5322, 0.4764, 0.6693,  ..., 0.6035, 0.6638, 0.4207],
          [0.4634, 0.4535, 0.4248,  ..., 0.3999, 0.4759, 0.4954]],

         [[0.5074, 0.5794, 0.4001,  ..., 0.7148, 0.4487, 0.5613],
          [0.4530, 0.5129, 0.5001,  ..., 0.6559, 0.4726, 0.5705],
          [0.4917, 0.5081, 0.4924,  ..., 0.4249, 0.4383, 0.4284],
          [0.6136, 0.7011, 0.4551,  ..., 0.4937, 0.6470, 0.6236]],

         [[0.5151, 0.4990, 0.4726,  ..., 0.5255, 0.5254, 0.4906],
          [0.3692, 0.5177, 0.5721,  ..., 0.4742, 0.4983, 0.4604],
          [0.6053, 0.4579, 0.4339,  ..., 0.5376, 0.5356, 0.5285],
          [0.4520, 0.5322, 0.5539,  ..., 0.4453, 0.4379, 0.4906]],

         ...,

         [[0.6441, 0.4420, 0.6037,  ..., 0.6515, 0.6045, 0.5905],
          [0.4697, 0.5842, 0.5017,  ..., 0.4926, 0.3514, 0.4864],
          [0.5798, 0.6344, 0.5039,  ..., 0.6059, 0.4569, 0.5075],
          [0.4445, 0.5090, 0.4699,  ..., 0.5081, 0.3901, 0.3534]],

         [[0.5154, 0.5167, 0.5089,  ..., 0.3864, 0.5993, 0.5223],
          [0.5660, 0.4976, 0.3935,  ..., 0.6142, 0.4637, 0.4379],
          [0.4939, 0.5434, 0.6098,  ..., 0.5622, 0.4666, 0.4795],
          [0.5163, 0.5927, 0.3805,  ..., 0.4929, 0.4432, 0.5909]],

         [[0.4789, 0.6216, 0.5328,  ..., 0.5378, 0.5528, 0.4312],
          [0.5224, 0.5266, 0.5241,  ..., 0.4417, 0.4871, 0.4136],
          [0.3006, 0.5913, 0.5635,  ..., 0.5680, 0.3924, 0.5324],
          [0.4282, 0.5397, 0.5759,  ..., 0.6101, 0.5904, 0.6158]]]],
       device='cuda:0')
tensor([[[[0.5842, 0.4513, 0.5195,  ..., 0.4292, 0.5477, 0.5636],
          [0.3914, 0.4397, 0.5120,  ..., 0.5660, 0.5964, 0.4405],
          [0.3549, 0.4547, 0.6151,  ..., 0.4929, 0.3849, 0.5607],
          [0.6156, 0.4971, 0.5651,  ..., 0.4734, 0.5105, 0.4961]],

         [[0.5324, 0.6388, 0.3585,  ..., 0.3748, 0.6197, 0.5366],
          [0.5177, 0.6058, 0.5679,  ..., 0.5339, 0.4273, 0.4579],
          [0.4847, 0.5167, 0.5039,  ..., 0.5079, 0.5032, 0.4694],
          [0.4460, 0.5794, 0.5699,  ..., 0.3630, 0.6039, 0.5679]],

         [[0.4822, 0.5057, 0.5065,  ..., 0.3942, 0.6469, 0.4930],
          [0.7098, 0.4211, 0.5922,  ..., 0.3522, 0.5794, 0.5016],
          [0.5780, 0.5586, 0.5946,  ..., 0.2910, 0.4003, 0.5931],
          [0.2918, 0.3965, 0.6105,  ..., 0.5237, 0.5898, 0.4097]],

         ...,

         [[0.5363, 0.3891, 0.4958,  ..., 0.4278, 0.5319, 0.5832],
          [0.5400, 0.5224, 0.4508,  ..., 0.6188, 0.4603, 0.4583],
          [0.6132, 0.4073, 0.4625,  ..., 0.6215, 0.4912, 0.4557],
          [0.5525, 0.4177, 0.4422,  ..., 0.5818, 0.4477, 0.3594]],

         [[0.4393, 0.4278, 0.4354,  ..., 0.5475, 0.5307, 0.4850],
          [0.3844, 0.2422, 0.5595,  ..., 0.5305, 0.6039, 0.5271],
          [0.5627, 0.5856, 0.4125,  ..., 0.5385, 0.4800, 0.5304],
          [0.5426, 0.5358, 0.3895,  ..., 0.4793, 0.5289, 0.5278]],

         [[0.5501, 0.4997, 0.4235,  ..., 0.4235, 0.4448, 0.5120],
          [0.3337, 0.5108, 0.4102,  ..., 0.5049, 0.5319, 0.4316],
          [0.3242, 0.5593, 0.4182,  ..., 0.5554, 0.5946, 0.5260],
          [0.5794, 0.5126, 0.4854,  ..., 0.6169, 0.4924, 0.4045]]],


        [[[0.5211, 0.4518, 0.4349,  ..., 0.3909, 0.5043, 0.5727],
          [0.4378, 0.4562, 0.5007,  ..., 0.5041, 0.5675, 0.5249],
          [0.5332, 0.4754, 0.6723,  ..., 0.6025, 0.6628, 0.4177],
          [0.4644, 0.4525, 0.4278,  ..., 0.3989, 0.4749, 0.4924]],

         [[0.5084, 0.5784, 0.4031,  ..., 0.7138, 0.4477, 0.5583],
          [0.4540, 0.5119, 0.5031,  ..., 0.6549, 0.4716, 0.5675],
          [0.4927, 0.5071, 0.4954,  ..., 0.4239, 0.4373, 0.4254],
          [0.6146, 0.7001, 0.4581,  ..., 0.4927, 0.6460, 0.6206]],

         [[0.5161, 0.4980, 0.4756,  ..., 0.5245, 0.5244, 0.4876],
          [0.3702, 0.5167, 0.5751,  ..., 0.4732, 0.4973, 0.4574],
          [0.6063, 0.4569, 0.4369,  ..., 0.5366, 0.5346, 0.5255],
          [0.4530, 0.5312, 0.5569,  ..., 0.4443, 0.4369, 0.4876]],

         ...,

         [[0.6451, 0.4410, 0.6067,  ..., 0.6505, 0.6035, 0.5875],
          [0.4707, 0.5832, 0.5047,  ..., 0.4916, 0.3504, 0.4834],
          [0.5808, 0.6334, 0.5069,  ..., 0.6049, 0.4559, 0.5045],
          [0.4455, 0.5080, 0.4729,  ..., 0.5071, 0.3891, 0.3504]],

         [[0.5164, 0.5157, 0.5119,  ..., 0.3854, 0.5983, 0.5193],
          [0.5670, 0.4966, 0.3965,  ..., 0.6132, 0.4627, 0.4349],
          [0.4949, 0.5424, 0.6128,  ..., 0.5612, 0.4656, 0.4765],
          [0.5173, 0.5917, 0.3835,  ..., 0.4919, 0.4422, 0.5879]],

         [[0.4799, 0.6206, 0.5358,  ..., 0.5368, 0.5518, 0.4282],
          [0.5234, 0.5256, 0.5271,  ..., 0.4407, 0.4861, 0.4106],
          [0.3016, 0.5903, 0.5665,  ..., 0.5670, 0.3914, 0.5294],
          [0.4292, 0.5387, 0.5789,  ..., 0.6091, 0.5894, 0.6128]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0010,  0.0010, -0.0030,  0.0010, -0.0030, -0.0010,  0.0030,  0.0010,
         0.0010,  0.0030], device='cuda:0')
selected experts tensor([1687, 1679, 1744, 1674, 1693, 1670, 1513, 1562, 1718, 1444],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.3884, 0.6528, 0.3568,  ..., 0.5850, 0.4952, 0.5608],
          [0.5411, 0.5151, 0.5670,  ..., 0.4201, 0.4756, 0.5120],
          [0.5645, 0.4372, 0.4102,  ..., 0.5056, 0.4798, 0.4815],
          [0.5649, 0.5731, 0.4278,  ..., 0.3353, 0.3944, 0.4088]],

         [[0.4699, 0.4885, 0.3864,  ..., 0.3173, 0.4454, 0.4836],
          [0.4679, 0.5517, 0.5178,  ..., 0.5146, 0.6245, 0.4681],
          [0.4171, 0.5678, 0.4798,  ..., 0.6085, 0.4728, 0.6280],
          [0.5294, 0.5241, 0.5407,  ..., 0.5158, 0.5331, 0.4240]],

         [[0.5934, 0.5958, 0.3767,  ..., 0.5813, 0.4430, 0.4966],
          [0.6545, 0.5248, 0.5230,  ..., 0.5641, 0.5252, 0.5531],
          [0.5716, 0.3810, 0.4463,  ..., 0.4282, 0.4056, 0.4983],
          [0.4792, 0.4138, 0.4557,  ..., 0.3014, 0.5442, 0.4608]],

         ...,

         [[0.4699, 0.5731, 0.4772,  ..., 0.4547, 0.5041, 0.4008],
          [0.4914, 0.4912, 0.5651,  ..., 0.5698, 0.5895, 0.5569],
          [0.5360, 0.6554, 0.5304,  ..., 0.4895, 0.4645, 0.4661],
          [0.6070, 0.5213, 0.4569,  ..., 0.6242, 0.3981, 0.4678]],

         [[0.6259, 0.5272, 0.4388,  ..., 0.4773, 0.4643, 0.5400],
          [0.4900, 0.4437, 0.6362,  ..., 0.4960, 0.5410, 0.6179],
          [0.5585, 0.4408, 0.5402,  ..., 0.4809, 0.5268, 0.4335],
          [0.6102, 0.3736, 0.4958,  ..., 0.4658, 0.4512, 0.5694]],

         [[0.5060, 0.5025, 0.4589,  ..., 0.5621, 0.4103, 0.3631],
          [0.5080, 0.3546, 0.5344,  ..., 0.5375, 0.5584, 0.6742],
          [0.5367, 0.5683, 0.4041,  ..., 0.6076, 0.4803, 0.5339],
          [0.6421, 0.3856, 0.5141,  ..., 0.3844, 0.6130, 0.5395]]],


        [[[0.4835, 0.5314, 0.5852,  ..., 0.4063, 0.4108, 0.3059],
          [0.5019, 0.5099, 0.4083,  ..., 0.5484, 0.5625, 0.4400],
          [0.4956, 0.5560, 0.4919,  ..., 0.5803, 0.4980, 0.5946],
          [0.5577, 0.3963, 0.3827,  ..., 0.4646, 0.5187, 0.4591]],

         [[0.4966, 0.4827, 0.6138,  ..., 0.5211, 0.5637, 0.6119],
          [0.4926, 0.5425, 0.5277,  ..., 0.5232, 0.6373, 0.5927],
          [0.5243, 0.5257, 0.5509,  ..., 0.4513, 0.5141, 0.4613],
          [0.4515, 0.3824, 0.5378,  ..., 0.5770, 0.5251, 0.3790]],

         [[0.4324, 0.3528, 0.4497,  ..., 0.2788, 0.5066, 0.5908],
          [0.4876, 0.6158, 0.4121,  ..., 0.5593, 0.6499, 0.4359],
          [0.5688, 0.4387, 0.4572,  ..., 0.5108, 0.4992, 0.6559],
          [0.4573, 0.5193, 0.4671,  ..., 0.4612, 0.5444, 0.3586]],

         ...,

         [[0.4464, 0.4534, 0.3269,  ..., 0.6099, 0.5985, 0.6371],
          [0.5887, 0.4257, 0.3799,  ..., 0.3904, 0.6218, 0.5567],
          [0.5338, 0.6394, 0.5109,  ..., 0.4898, 0.4817, 0.4216],
          [0.4885, 0.4452, 0.5842,  ..., 0.3530, 0.4863, 0.6479]],

         [[0.5510, 0.4445, 0.4630,  ..., 0.4784, 0.5791, 0.5166],
          [0.6093, 0.5958, 0.5118,  ..., 0.5978, 0.4141, 0.4758],
          [0.4491, 0.5051, 0.3595,  ..., 0.4229, 0.5531, 0.3505],
          [0.4951, 0.4575, 0.5675,  ..., 0.4624, 0.4935, 0.4917]],

         [[0.4858, 0.4138, 0.5390,  ..., 0.4823, 0.4689, 0.5842],
          [0.4973, 0.4430, 0.6802,  ..., 0.5898, 0.3888, 0.4688],
          [0.5031, 0.5070, 0.6647,  ..., 0.4287, 0.5724, 0.3443],
          [0.4624, 0.3973, 0.5295,  ..., 0.3521, 0.4955, 0.6063]]]],
       device='cuda:0')
tensor([[[[0.3914, 0.6558, 0.3558,  ..., 0.5860, 0.4922, 0.5598],
          [0.5441, 0.5181, 0.5660,  ..., 0.4211, 0.4726, 0.5110],
          [0.5675, 0.4402, 0.4092,  ..., 0.5066, 0.4768, 0.4805],
          [0.5679, 0.5761, 0.4268,  ..., 0.3363, 0.3914, 0.4078]],

         [[0.4729, 0.4915, 0.3854,  ..., 0.3183, 0.4424, 0.4826],
          [0.4709, 0.5547, 0.5168,  ..., 0.5156, 0.6215, 0.4671],
          [0.4201, 0.5708, 0.4788,  ..., 0.6095, 0.4698, 0.6270],
          [0.5324, 0.5271, 0.5397,  ..., 0.5168, 0.5301, 0.4230]],

         [[0.5964, 0.5988, 0.3757,  ..., 0.5823, 0.4400, 0.4956],
          [0.6575, 0.5278, 0.5220,  ..., 0.5651, 0.5222, 0.5521],
          [0.5746, 0.3840, 0.4453,  ..., 0.4292, 0.4026, 0.4973],
          [0.4822, 0.4168, 0.4547,  ..., 0.3024, 0.5412, 0.4598]],

         ...,

         [[0.4729, 0.5761, 0.4762,  ..., 0.4557, 0.5011, 0.3998],
          [0.4944, 0.4942, 0.5641,  ..., 0.5708, 0.5865, 0.5559],
          [0.5390, 0.6584, 0.5294,  ..., 0.4905, 0.4615, 0.4651],
          [0.6100, 0.5243, 0.4559,  ..., 0.6252, 0.3951, 0.4668]],

         [[0.6289, 0.5302, 0.4378,  ..., 0.4783, 0.4613, 0.5390],
          [0.4930, 0.4467, 0.6352,  ..., 0.4970, 0.5380, 0.6169],
          [0.5615, 0.4438, 0.5392,  ..., 0.4819, 0.5238, 0.4325],
          [0.6132, 0.3766, 0.4948,  ..., 0.4668, 0.4482, 0.5684]],

         [[0.5090, 0.5055, 0.4579,  ..., 0.5631, 0.4073, 0.3621],
          [0.5110, 0.3576, 0.5334,  ..., 0.5385, 0.5554, 0.6732],
          [0.5397, 0.5713, 0.4031,  ..., 0.6086, 0.4773, 0.5329],
          [0.6451, 0.3886, 0.5131,  ..., 0.3854, 0.6100, 0.5385]]],


        [[[0.4865, 0.5344, 0.5842,  ..., 0.4073, 0.4078, 0.3049],
          [0.5049, 0.5129, 0.4073,  ..., 0.5494, 0.5595, 0.4390],
          [0.4986, 0.5590, 0.4909,  ..., 0.5813, 0.4950, 0.5936],
          [0.5607, 0.3993, 0.3817,  ..., 0.4656, 0.5157, 0.4581]],

         [[0.4996, 0.4857, 0.6128,  ..., 0.5221, 0.5607, 0.6109],
          [0.4956, 0.5455, 0.5267,  ..., 0.5242, 0.6343, 0.5917],
          [0.5273, 0.5287, 0.5499,  ..., 0.4523, 0.5111, 0.4603],
          [0.4545, 0.3854, 0.5368,  ..., 0.5780, 0.5221, 0.3780]],

         [[0.4354, 0.3558, 0.4487,  ..., 0.2798, 0.5036, 0.5898],
          [0.4906, 0.6188, 0.4111,  ..., 0.5603, 0.6469, 0.4349],
          [0.5718, 0.4417, 0.4562,  ..., 0.5118, 0.4962, 0.6549],
          [0.4603, 0.5223, 0.4661,  ..., 0.4622, 0.5414, 0.3576]],

         ...,

         [[0.4494, 0.4564, 0.3259,  ..., 0.6109, 0.5955, 0.6361],
          [0.5917, 0.4287, 0.3789,  ..., 0.3914, 0.6188, 0.5557],
          [0.5368, 0.6424, 0.5099,  ..., 0.4908, 0.4787, 0.4206],
          [0.4915, 0.4482, 0.5832,  ..., 0.3540, 0.4833, 0.6469]],

         [[0.5540, 0.4475, 0.4620,  ..., 0.4794, 0.5761, 0.5156],
          [0.6123, 0.5988, 0.5108,  ..., 0.5988, 0.4111, 0.4748],
          [0.4521, 0.5081, 0.3585,  ..., 0.4239, 0.5501, 0.3495],
          [0.4981, 0.4605, 0.5665,  ..., 0.4634, 0.4905, 0.4907]],

         [[0.4888, 0.4168, 0.5380,  ..., 0.4833, 0.4659, 0.5832],
          [0.5003, 0.4460, 0.6792,  ..., 0.5908, 0.3858, 0.4678],
          [0.5061, 0.5100, 0.6637,  ..., 0.4297, 0.5694, 0.3433],
          [0.4654, 0.4003, 0.5285,  ..., 0.3531, 0.4925, 0.6053]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0030, -0.0030,  0.0010,  0.0030,  0.0010,  0.0030, -0.0030, -0.0010,
         0.0030,  0.0010], device='cuda:0')
selected experts tensor([1718, 1582, 1624, 1389, 1617, 1456, 1928, 1670, 1635, 1765],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4694, 0.4404, 0.6277,  ..., 0.5065, 0.5306, 0.5113],
          [0.4010, 0.5094, 0.5377,  ..., 0.6664, 0.5417, 0.3815],
          [0.4595, 0.5472, 0.4396,  ..., 0.4740, 0.3420, 0.5776],
          [0.4921, 0.5631, 0.5236,  ..., 0.5675, 0.4630, 0.5334]],

         [[0.5078, 0.3629, 0.5640,  ..., 0.4764, 0.4898, 0.4594],
          [0.5673, 0.6713, 0.3736,  ..., 0.5656, 0.5150, 0.4799],
          [0.5892, 0.5373, 0.5225,  ..., 0.5378, 0.5410, 0.4341],
          [0.4964, 0.6039, 0.5816,  ..., 0.4615, 0.4824, 0.5223]],

         [[0.4735, 0.5450, 0.5507,  ..., 0.4064, 0.4835, 0.5242],
          [0.4478, 0.5161, 0.5090,  ..., 0.5615, 0.4849, 0.2980],
          [0.6457, 0.4938, 0.6693,  ..., 0.4350, 0.5026, 0.4621],
          [0.7036, 0.6477, 0.5462,  ..., 0.5494, 0.5323, 0.4829]],

         ...,

         [[0.5498, 0.4569, 0.6938,  ..., 0.3845, 0.6018, 0.5115],
          [0.5659, 0.5135, 0.5726,  ..., 0.4489, 0.4655, 0.4250],
          [0.5168, 0.4773, 0.5267,  ..., 0.4126, 0.4265, 0.3851],
          [0.5428, 0.5404, 0.4992,  ..., 0.5747, 0.3324, 0.5642]],

         [[0.7164, 0.4371, 0.5292,  ..., 0.4644, 0.5197, 0.4260],
          [0.4343, 0.5950, 0.4691,  ..., 0.5417, 0.5357, 0.5548],
          [0.5158, 0.5110, 0.5906,  ..., 0.5665, 0.4862, 0.3912],
          [0.4754, 0.6182, 0.6813,  ..., 0.5752, 0.5538, 0.3411]],

         [[0.6149, 0.5072, 0.6779,  ..., 0.6362, 0.4662, 0.5118],
          [0.4758, 0.5200, 0.3968,  ..., 0.5169, 0.5379, 0.4331],
          [0.5333, 0.4581, 0.5205,  ..., 0.4818, 0.5483, 0.3981],
          [0.5292, 0.5099, 0.5017,  ..., 0.5211, 0.4742, 0.5122]]],


        [[[0.5840, 0.4571, 0.6394,  ..., 0.4900, 0.4447, 0.6111],
          [0.5062, 0.5774, 0.3875,  ..., 0.5956, 0.5357, 0.4955],
          [0.5802, 0.4909, 0.5788,  ..., 0.4828, 0.5031, 0.4080],
          [0.5563, 0.5655, 0.6047,  ..., 0.5756, 0.3588, 0.4331]],

         [[0.5067, 0.6020, 0.5793,  ..., 0.3400, 0.4799, 0.5971],
          [0.6268, 0.7088, 0.3954,  ..., 0.5314, 0.5666, 0.4099],
          [0.5464, 0.5015, 0.5425,  ..., 0.4245, 0.5309, 0.3411],
          [0.5005, 0.4016, 0.4720,  ..., 0.6147, 0.3995, 0.5990]],

         [[0.6079, 0.5339, 0.5524,  ..., 0.5670, 0.5011, 0.4430],
          [0.5292, 0.5212, 0.5263,  ..., 0.5171, 0.3778, 0.4534],
          [0.5830, 0.5940, 0.5529,  ..., 0.5410, 0.5442, 0.2940],
          [0.6580, 0.4978, 0.6093,  ..., 0.5613, 0.5115, 0.5919]],

         ...,

         [[0.5807, 0.3923, 0.3325,  ..., 0.4781, 0.4921, 0.4341],
          [0.3893, 0.5021, 0.5948,  ..., 0.6105, 0.5933, 0.3760],
          [0.4209, 0.5373, 0.4445,  ..., 0.4131, 0.4979, 0.4831],
          [0.2994, 0.4984, 0.5745,  ..., 0.4355, 0.3376, 0.5748]],

         [[0.5683, 0.4822, 0.6492,  ..., 0.6007, 0.5947, 0.4375],
          [0.5084, 0.4992, 0.7012,  ..., 0.6953, 0.5567, 0.3333],
          [0.4551, 0.3441, 0.5201,  ..., 0.4761, 0.4916, 0.5191],
          [0.4384, 0.4234, 0.4590,  ..., 0.4659, 0.3902, 0.5341]],

         [[0.5683, 0.5167, 0.6153,  ..., 0.5814, 0.5876, 0.4630],
          [0.4095, 0.6739, 0.5389,  ..., 0.5856, 0.5649, 0.4037],
          [0.5981, 0.4559, 0.4979,  ..., 0.5395, 0.5872, 0.5752],
          [0.5163, 0.6765, 0.6484,  ..., 0.5780, 0.2828, 0.4703]]]],
       device='cuda:0')
tensor([[[[0.4724, 0.4414, 0.6307,  ..., 0.5055, 0.5276, 0.5083],
          [0.4040, 0.5104, 0.5407,  ..., 0.6654, 0.5387, 0.3785],
          [0.4625, 0.5482, 0.4426,  ..., 0.4730, 0.3390, 0.5746],
          [0.4951, 0.5641, 0.5266,  ..., 0.5665, 0.4600, 0.5304]],

         [[0.5108, 0.3639, 0.5670,  ..., 0.4754, 0.4868, 0.4564],
          [0.5703, 0.6723, 0.3766,  ..., 0.5646, 0.5120, 0.4769],
          [0.5922, 0.5383, 0.5255,  ..., 0.5368, 0.5380, 0.4311],
          [0.4994, 0.6049, 0.5846,  ..., 0.4605, 0.4794, 0.5193]],

         [[0.4765, 0.5460, 0.5537,  ..., 0.4054, 0.4805, 0.5212],
          [0.4508, 0.5171, 0.5120,  ..., 0.5605, 0.4819, 0.2950],
          [0.6487, 0.4948, 0.6723,  ..., 0.4340, 0.4996, 0.4591],
          [0.7066, 0.6487, 0.5492,  ..., 0.5484, 0.5293, 0.4799]],

         ...,

         [[0.5528, 0.4579, 0.6968,  ..., 0.3835, 0.5988, 0.5085],
          [0.5689, 0.5145, 0.5756,  ..., 0.4479, 0.4625, 0.4220],
          [0.5198, 0.4783, 0.5297,  ..., 0.4116, 0.4235, 0.3821],
          [0.5458, 0.5414, 0.5022,  ..., 0.5737, 0.3294, 0.5612]],

         [[0.7194, 0.4381, 0.5322,  ..., 0.4634, 0.5167, 0.4230],
          [0.4373, 0.5960, 0.4721,  ..., 0.5407, 0.5327, 0.5518],
          [0.5188, 0.5120, 0.5936,  ..., 0.5655, 0.4832, 0.3882],
          [0.4784, 0.6192, 0.6843,  ..., 0.5742, 0.5508, 0.3381]],

         [[0.6179, 0.5082, 0.6809,  ..., 0.6352, 0.4632, 0.5088],
          [0.4788, 0.5210, 0.3998,  ..., 0.5159, 0.5349, 0.4301],
          [0.5363, 0.4591, 0.5235,  ..., 0.4808, 0.5453, 0.3951],
          [0.5322, 0.5109, 0.5047,  ..., 0.5201, 0.4712, 0.5092]]],


        [[[0.5870, 0.4581, 0.6424,  ..., 0.4890, 0.4417, 0.6081],
          [0.5092, 0.5784, 0.3905,  ..., 0.5946, 0.5327, 0.4925],
          [0.5832, 0.4919, 0.5818,  ..., 0.4818, 0.5001, 0.4050],
          [0.5593, 0.5665, 0.6077,  ..., 0.5746, 0.3558, 0.4301]],

         [[0.5097, 0.6030, 0.5823,  ..., 0.3390, 0.4769, 0.5941],
          [0.6298, 0.7098, 0.3984,  ..., 0.5304, 0.5636, 0.4069],
          [0.5494, 0.5025, 0.5455,  ..., 0.4235, 0.5279, 0.3381],
          [0.5035, 0.4026, 0.4750,  ..., 0.6137, 0.3965, 0.5960]],

         [[0.6109, 0.5349, 0.5554,  ..., 0.5660, 0.4981, 0.4400],
          [0.5322, 0.5222, 0.5293,  ..., 0.5161, 0.3748, 0.4504],
          [0.5860, 0.5950, 0.5559,  ..., 0.5400, 0.5412, 0.2910],
          [0.6610, 0.4988, 0.6123,  ..., 0.5603, 0.5085, 0.5889]],

         ...,

         [[0.5837, 0.3933, 0.3355,  ..., 0.4771, 0.4891, 0.4311],
          [0.3923, 0.5031, 0.5978,  ..., 0.6095, 0.5903, 0.3730],
          [0.4239, 0.5383, 0.4475,  ..., 0.4121, 0.4949, 0.4801],
          [0.3024, 0.4994, 0.5775,  ..., 0.4345, 0.3346, 0.5718]],

         [[0.5713, 0.4832, 0.6522,  ..., 0.5997, 0.5917, 0.4345],
          [0.5114, 0.5002, 0.7042,  ..., 0.6943, 0.5537, 0.3303],
          [0.4581, 0.3451, 0.5231,  ..., 0.4751, 0.4886, 0.5161],
          [0.4414, 0.4244, 0.4620,  ..., 0.4649, 0.3872, 0.5311]],

         [[0.5713, 0.5177, 0.6183,  ..., 0.5804, 0.5846, 0.4600],
          [0.4125, 0.6749, 0.5419,  ..., 0.5846, 0.5619, 0.4007],
          [0.6011, 0.4569, 0.5009,  ..., 0.5385, 0.5842, 0.5722],
          [0.5193, 0.6775, 0.6514,  ..., 0.5770, 0.2798, 0.4673]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([-0.0030, -0.0010, -0.0030,  0.0030, -0.0030,  0.0030, -0.0030,  0.0010,
         0.0030,  0.0030], device='cuda:0')
selected experts tensor([2127, 1825, 2137, 1620, 1670,  834, 2354, 1649, 1225,  943],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030]],

         [[0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030]],

         [[0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030]],

         ...,

         [[0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030]],

         [[0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030]],

         [[0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030]]],


        [[[0.6063, 0.3915, 0.4230,  ..., 0.3977, 0.3963, 0.6561],
          [0.5351, 0.3435, 0.3799,  ..., 0.4652, 0.4070, 0.3907],
          [0.4708, 0.5104, 0.3873,  ..., 0.5890, 0.5601, 0.3552],
          [0.3749, 0.4850, 0.6353,  ..., 0.4188, 0.5413, 0.3669]],

         [[0.4475, 0.4572, 0.3532,  ..., 0.5719, 0.3104, 0.6310],
          [0.3827, 0.4369, 0.4642,  ..., 0.4825, 0.5616, 0.5724],
          [0.5063, 0.4564, 0.4288,  ..., 0.5045, 0.5606, 0.5613],
          [0.6895, 0.5287, 0.5303,  ..., 0.5164, 0.4303, 0.4647]],

         [[0.4572, 0.5598, 0.4187,  ..., 0.4317, 0.4122, 0.5427],
          [0.4750, 0.3461, 0.6253,  ..., 0.4317, 0.5800, 0.5202],
          [0.5266, 0.4978, 0.5451,  ..., 0.4000, 0.4864, 0.5652],
          [0.5665, 0.4827, 0.4956,  ..., 0.5403, 0.6093, 0.6083]],

         ...,

         [[0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030]],

         [[0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030]],

         [[0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5010, 0.5010,  ..., 0.5030, 0.5030, 0.5030]]]],
       device='cuda:0')
tensor([[[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.6053, 0.3905, 0.4220,  ..., 0.3947, 0.3933, 0.6531],
          [0.5341, 0.3425, 0.3789,  ..., 0.4622, 0.4040, 0.3877],
          [0.4698, 0.5094, 0.3863,  ..., 0.5860, 0.5571, 0.3522],
          [0.3739, 0.4840, 0.6343,  ..., 0.4158, 0.5383, 0.3639]],

         [[0.4465, 0.4562, 0.3522,  ..., 0.5689, 0.3074, 0.6280],
          [0.3817, 0.4359, 0.4632,  ..., 0.4795, 0.5586, 0.5694],
          [0.5053, 0.4554, 0.4278,  ..., 0.5015, 0.5576, 0.5583],
          [0.6885, 0.5277, 0.5293,  ..., 0.5134, 0.4273, 0.4617]],

         [[0.4562, 0.5588, 0.4177,  ..., 0.4287, 0.4092, 0.5397],
          [0.4740, 0.3451, 0.6243,  ..., 0.4287, 0.5770, 0.5172],
          [0.5256, 0.4968, 0.5441,  ..., 0.3970, 0.4834, 0.5622],
          [0.5655, 0.4817, 0.4946,  ..., 0.5373, 0.6063, 0.6053]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0030, 0.0030,
        0.0030], device='cuda:0')
selected experts tensor([ 622,  423,  878,  343, 1102,  431,  397, 3560, 3744,  788],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
attn_o for batch tensor([[1718., 1582., 1624., 1389., 1617., 1456., 1928., 1670., 1635., 1765.],
        [ 622.,  423.,  878.,  343., 1102.,  431.,  397., 3560., 3744.,  788.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5141, 0.5641, 0.5668,  ..., 0.4994, 0.4912, 0.4834],
          [0.4612, 0.6072, 0.4490,  ..., 0.3985, 0.4822, 0.4332],
          [0.4464, 0.5035, 0.4147,  ..., 0.4403, 0.5898, 0.4938],
          [0.4385, 0.4069, 0.5347,  ..., 0.5627, 0.6270, 0.3982]],

         [[0.4658, 0.5271, 0.4005,  ..., 0.6569, 0.4571, 0.4933],
          [0.5570, 0.3928, 0.4468,  ..., 0.5942, 0.6549, 0.5286],
          [0.4840, 0.5465, 0.3635,  ..., 0.5635, 0.4938, 0.5986],
          [0.4930, 0.4712, 0.4238,  ..., 0.4676, 0.5950, 0.3351]],

         [[0.5331, 0.6379, 0.4937,  ..., 0.4321, 0.4882, 0.4510],
          [0.5252, 0.4707, 0.5240,  ..., 0.4748, 0.5955, 0.4539],
          [0.4731, 0.3993, 0.6175,  ..., 0.4548, 0.7033, 0.4927],
          [0.6089, 0.5375, 0.6366,  ..., 0.4178, 0.5263, 0.4500]],

         ...,

         [[0.5803, 0.6343, 0.5333,  ..., 0.4757, 0.5363, 0.5139],
          [0.4508, 0.4121, 0.5560,  ..., 0.4065, 0.5641, 0.5230],
          [0.5864, 0.4671, 0.4190,  ..., 0.4326, 0.4249, 0.5524],
          [0.5607, 0.4751, 0.5377,  ..., 0.5123, 0.5184, 0.3903]],

         [[0.4210, 0.5174, 0.5444,  ..., 0.4312, 0.5595, 0.5614],
          [0.5360, 0.5898, 0.4664,  ..., 0.4567, 0.4249, 0.6037],
          [0.5779, 0.5511, 0.6366,  ..., 0.4941, 0.4064, 0.6265],
          [0.4782, 0.5341, 0.5995,  ..., 0.6630, 0.3603, 0.3770]],

         [[0.4759, 0.4818, 0.4784,  ..., 0.4389, 0.3416, 0.5758],
          [0.3664, 0.3200, 0.4439,  ..., 0.4524, 0.5429, 0.3518],
          [0.5542, 0.5038, 0.4379,  ..., 0.4927, 0.3398, 0.5291],
          [0.4428, 0.4703, 0.4824,  ..., 0.5490, 0.4656, 0.6242]]],


        [[[0.4989, 0.5050, 0.5114,  ..., 0.5244, 0.4254, 0.4556],
          [0.3137, 0.4239, 0.4319,  ..., 0.3906, 0.5283, 0.5396],
          [0.5041, 0.5550, 0.4333,  ..., 0.4939, 0.5276, 0.5466],
          [0.6214, 0.4588, 0.5882,  ..., 0.5514, 0.3844, 0.4471]],

         [[0.4072, 0.4083, 0.4100,  ..., 0.5545, 0.4649, 0.4524],
          [0.7441, 0.4685, 0.5625,  ..., 0.5012, 0.5564, 0.5655],
          [0.4390, 0.4630, 0.4886,  ..., 0.4654, 0.4692, 0.6589],
          [0.4972, 0.4964, 0.3907,  ..., 0.6069, 0.4258, 0.5640]],

         [[0.5722, 0.4455, 0.5802,  ..., 0.4492, 0.6202, 0.3122],
          [0.5513, 0.4795, 0.4553,  ..., 0.3331, 0.4499, 0.5308],
          [0.5440, 0.3329, 0.2927,  ..., 0.6435, 0.4467, 0.5452],
          [0.4394, 0.5641, 0.6267,  ..., 0.5265, 0.4562, 0.5221]],

         ...,

         [[0.4082, 0.4206, 0.4720,  ..., 0.4183, 0.4173, 0.6571],
          [0.5793, 0.5142, 0.4095,  ..., 0.3677, 0.4465, 0.6274],
          [0.5123, 0.4832, 0.6718,  ..., 0.4867, 0.4211, 0.4279],
          [0.6599, 0.4402, 0.5345,  ..., 0.4365, 0.4919, 0.3509]],

         [[0.3834, 0.5675, 0.5692,  ..., 0.3177, 0.5703, 0.5943],
          [0.4423, 0.4970, 0.6402,  ..., 0.4369, 0.5492, 0.5386],
          [0.5262, 0.3657, 0.5277,  ..., 0.4652, 0.5964, 0.5141],
          [0.5435, 0.5827, 0.6769,  ..., 0.4909, 0.4740, 0.3884]],

         [[0.6223, 0.4716, 0.4507,  ..., 0.5728, 0.5083, 0.6958],
          [0.4619, 0.6220, 0.4729,  ..., 0.4231, 0.4780, 0.6320],
          [0.5859, 0.5174, 0.4329,  ..., 0.4674, 0.4092, 0.5313],
          [0.4802, 0.5336, 0.5267,  ..., 0.5680, 0.6044, 0.4180]]]],
       device='cuda:0')
tensor([[[[0.5161, 0.5641, 0.5708,  ..., 0.4974, 0.4912, 0.4794],
          [0.4632, 0.6072, 0.4530,  ..., 0.3965, 0.4822, 0.4292],
          [0.4484, 0.5035, 0.4187,  ..., 0.4383, 0.5898, 0.4898],
          [0.4405, 0.4069, 0.5387,  ..., 0.5607, 0.6270, 0.3942]],

         [[0.4678, 0.5271, 0.4045,  ..., 0.6549, 0.4571, 0.4893],
          [0.5590, 0.3928, 0.4508,  ..., 0.5922, 0.6549, 0.5246],
          [0.4860, 0.5465, 0.3675,  ..., 0.5615, 0.4938, 0.5946],
          [0.4950, 0.4712, 0.4278,  ..., 0.4656, 0.5950, 0.3311]],

         [[0.5351, 0.6379, 0.4977,  ..., 0.4301, 0.4882, 0.4470],
          [0.5272, 0.4707, 0.5280,  ..., 0.4728, 0.5955, 0.4499],
          [0.4751, 0.3993, 0.6215,  ..., 0.4528, 0.7033, 0.4887],
          [0.6109, 0.5375, 0.6406,  ..., 0.4158, 0.5263, 0.4460]],

         ...,

         [[0.5823, 0.6343, 0.5373,  ..., 0.4737, 0.5363, 0.5099],
          [0.4528, 0.4121, 0.5600,  ..., 0.4045, 0.5641, 0.5190],
          [0.5884, 0.4671, 0.4230,  ..., 0.4306, 0.4249, 0.5484],
          [0.5627, 0.4751, 0.5417,  ..., 0.5103, 0.5184, 0.3863]],

         [[0.4230, 0.5174, 0.5484,  ..., 0.4292, 0.5595, 0.5574],
          [0.5380, 0.5898, 0.4704,  ..., 0.4547, 0.4249, 0.5997],
          [0.5799, 0.5511, 0.6406,  ..., 0.4921, 0.4064, 0.6225],
          [0.4802, 0.5341, 0.6035,  ..., 0.6610, 0.3603, 0.3730]],

         [[0.4779, 0.4818, 0.4824,  ..., 0.4369, 0.3416, 0.5718],
          [0.3684, 0.3200, 0.4479,  ..., 0.4504, 0.5429, 0.3478],
          [0.5562, 0.5038, 0.4419,  ..., 0.4907, 0.3398, 0.5251],
          [0.4448, 0.4703, 0.4864,  ..., 0.5470, 0.4656, 0.6202]]],


        [[[0.5009, 0.5050, 0.5154,  ..., 0.5224, 0.4254, 0.4516],
          [0.3157, 0.4239, 0.4359,  ..., 0.3886, 0.5283, 0.5356],
          [0.5061, 0.5550, 0.4373,  ..., 0.4919, 0.5276, 0.5426],
          [0.6234, 0.4588, 0.5922,  ..., 0.5494, 0.3844, 0.4431]],

         [[0.4092, 0.4083, 0.4140,  ..., 0.5525, 0.4649, 0.4484],
          [0.7461, 0.4685, 0.5665,  ..., 0.4992, 0.5564, 0.5615],
          [0.4410, 0.4630, 0.4926,  ..., 0.4634, 0.4692, 0.6549],
          [0.4992, 0.4964, 0.3947,  ..., 0.6049, 0.4258, 0.5600]],

         [[0.5742, 0.4455, 0.5842,  ..., 0.4472, 0.6202, 0.3082],
          [0.5533, 0.4795, 0.4593,  ..., 0.3311, 0.4499, 0.5268],
          [0.5460, 0.3329, 0.2967,  ..., 0.6415, 0.4467, 0.5412],
          [0.4414, 0.5641, 0.6307,  ..., 0.5245, 0.4562, 0.5181]],

         ...,

         [[0.4102, 0.4206, 0.4760,  ..., 0.4163, 0.4173, 0.6531],
          [0.5813, 0.5142, 0.4135,  ..., 0.3657, 0.4465, 0.6234],
          [0.5143, 0.4832, 0.6758,  ..., 0.4847, 0.4211, 0.4239],
          [0.6619, 0.4402, 0.5385,  ..., 0.4345, 0.4919, 0.3469]],

         [[0.3854, 0.5675, 0.5732,  ..., 0.3157, 0.5703, 0.5903],
          [0.4443, 0.4970, 0.6442,  ..., 0.4349, 0.5492, 0.5346],
          [0.5282, 0.3657, 0.5317,  ..., 0.4632, 0.5964, 0.5101],
          [0.5455, 0.5827, 0.6809,  ..., 0.4889, 0.4740, 0.3844]],

         [[0.6243, 0.4716, 0.4547,  ..., 0.5708, 0.5083, 0.6918],
          [0.4639, 0.6220, 0.4769,  ..., 0.4211, 0.4780, 0.6280],
          [0.5879, 0.5174, 0.4369,  ..., 0.4654, 0.4092, 0.5273],
          [0.4822, 0.5336, 0.5307,  ..., 0.5660, 0.6044, 0.4140]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0020,  0.0000, -0.0040,  0.0000, -0.0040, -0.0020,  0.0040,  0.0020,
         0.0000,  0.0040], device='cuda:0')
selected experts tensor([1660, 1537, 1679, 1568, 1689, 1734, 1631, 1544, 1612, 1730],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4740, 0.4981, 0.4757,  ..., 0.5258, 0.5182, 0.5547],
          [0.5059, 0.6177, 0.4250,  ..., 0.6103, 0.4385, 0.6316],
          [0.5262, 0.4310, 0.5880,  ..., 0.5309, 0.5324, 0.2830],
          [0.4721, 0.7070, 0.5332,  ..., 0.5935, 0.4952, 0.4059]],

         [[0.3888, 0.4510, 0.5238,  ..., 0.5409, 0.4741, 0.6086],
          [0.5272, 0.5522, 0.4809,  ..., 0.4948, 0.2807, 0.4588],
          [0.5119, 0.3889, 0.4870,  ..., 0.3655, 0.6061, 0.5380],
          [0.4660, 0.5949, 0.6212,  ..., 0.6140, 0.5882, 0.4622]],

         [[0.4827, 0.4602, 0.4577,  ..., 0.6172, 0.3438, 0.5903],
          [0.5110, 0.5056, 0.5852,  ..., 0.4349, 0.4986, 0.5339],
          [0.6366, 0.4547, 0.4345,  ..., 0.4950, 0.6755, 0.4843],
          [0.5209, 0.5172, 0.4519,  ..., 0.5042, 0.5498, 0.6025]],

         ...,

         [[0.6321, 0.5893, 0.4326,  ..., 0.6555, 0.4418, 0.4482],
          [0.5783, 0.4262, 0.5819,  ..., 0.4310, 0.6703, 0.4807],
          [0.5697, 0.4544, 0.3418,  ..., 0.3538, 0.3959, 0.6817],
          [0.5488, 0.5280, 0.6390,  ..., 0.5000, 0.4701, 0.5278]],

         [[0.4656, 0.4554, 0.5352,  ..., 0.4435, 0.4963, 0.3720],
          [0.2558, 0.4788, 0.5001,  ..., 0.4831, 0.6292, 0.4744],
          [0.4676, 0.5256, 0.5980,  ..., 0.5018, 0.4745, 0.4373],
          [0.5687, 0.5817, 0.5342,  ..., 0.5302, 0.4999, 0.5595]],

         [[0.3865, 0.5139, 0.5776,  ..., 0.6458, 0.5891, 0.5414],
          [0.4757, 0.4981, 0.5309,  ..., 0.4479, 0.3797, 0.4962],
          [0.6171, 0.5049, 0.3119,  ..., 0.5963, 0.3395, 0.3803],
          [0.3323, 0.4423, 0.4178,  ..., 0.5136, 0.4298, 0.4383]]],


        [[[0.5287, 0.4143, 0.4964,  ..., 0.5223, 0.3884, 0.6011],
          [0.4660, 0.3978, 0.3245,  ..., 0.5611, 0.5743, 0.3979],
          [0.5389, 0.4841, 0.5502,  ..., 0.3728, 0.5342, 0.5732],
          [0.6777, 0.4394, 0.4827,  ..., 0.5534, 0.5981, 0.5020]],

         [[0.5721, 0.4588, 0.6199,  ..., 0.5163, 0.4874, 0.6926],
          [0.5225, 0.5100, 0.5560,  ..., 0.4799, 0.5331, 0.4762],
          [0.4645, 0.5592, 0.7013,  ..., 0.5235, 0.5486, 0.5417],
          [0.4478, 0.5921, 0.5824,  ..., 0.5130, 0.5510, 0.4487]],

         [[0.5268, 0.6200, 0.4771,  ..., 0.3769, 0.5314, 0.5117],
          [0.3358, 0.6200, 0.4307,  ..., 0.4770, 0.4729, 0.4634],
          [0.5204, 0.6024, 0.5766,  ..., 0.3903, 0.3377, 0.4465],
          [0.5471, 0.4661, 0.4037,  ..., 0.6529, 0.5264, 0.4003]],

         ...,

         [[0.4795, 0.5423, 0.5704,  ..., 0.6856, 0.5929, 0.5149],
          [0.4985, 0.5459, 0.5144,  ..., 0.4510, 0.4881, 0.5537],
          [0.6579, 0.5760, 0.6143,  ..., 0.5503, 0.4990, 0.4528],
          [0.6152, 0.4262, 0.6008,  ..., 0.4612, 0.4981, 0.3947]],

         [[0.6777, 0.5996, 0.6630,  ..., 0.5505, 0.3889, 0.4438],
          [0.3092, 0.6368, 0.5895,  ..., 0.4802, 0.5896, 0.4523],
          [0.4109, 0.5788, 0.6309,  ..., 0.4802, 0.5645, 0.5334],
          [0.5271, 0.5003, 0.6022,  ..., 0.6590, 0.5614, 0.3657]],

         [[0.4266, 0.4636, 0.4825,  ..., 0.5537, 0.5202, 0.5622],
          [0.4536, 0.3493, 0.6059,  ..., 0.3945, 0.5575, 0.5612],
          [0.4950, 0.6066, 0.5733,  ..., 0.4452, 0.4983, 0.4554],
          [0.5572, 0.5273, 0.6761,  ..., 0.4686, 0.5274, 0.5116]]]],
       device='cuda:0')
tensor([[[[0.4780, 0.5001, 0.4737,  ..., 0.5278, 0.5142, 0.5547],
          [0.5099, 0.6197, 0.4230,  ..., 0.6123, 0.4345, 0.6316],
          [0.5302, 0.4330, 0.5860,  ..., 0.5329, 0.5284, 0.2830],
          [0.4761, 0.7090, 0.5312,  ..., 0.5955, 0.4912, 0.4059]],

         [[0.3928, 0.4530, 0.5218,  ..., 0.5429, 0.4701, 0.6086],
          [0.5312, 0.5542, 0.4789,  ..., 0.4968, 0.2767, 0.4588],
          [0.5159, 0.3909, 0.4850,  ..., 0.3675, 0.6021, 0.5380],
          [0.4700, 0.5969, 0.6192,  ..., 0.6160, 0.5842, 0.4622]],

         [[0.4867, 0.4622, 0.4557,  ..., 0.6192, 0.3398, 0.5903],
          [0.5150, 0.5076, 0.5832,  ..., 0.4369, 0.4946, 0.5339],
          [0.6406, 0.4567, 0.4325,  ..., 0.4970, 0.6715, 0.4843],
          [0.5249, 0.5192, 0.4499,  ..., 0.5062, 0.5458, 0.6025]],

         ...,

         [[0.6361, 0.5913, 0.4306,  ..., 0.6575, 0.4378, 0.4482],
          [0.5823, 0.4282, 0.5799,  ..., 0.4330, 0.6663, 0.4807],
          [0.5737, 0.4564, 0.3398,  ..., 0.3558, 0.3919, 0.6817],
          [0.5528, 0.5300, 0.6370,  ..., 0.5020, 0.4661, 0.5278]],

         [[0.4696, 0.4574, 0.5332,  ..., 0.4455, 0.4923, 0.3720],
          [0.2598, 0.4808, 0.4981,  ..., 0.4851, 0.6252, 0.4744],
          [0.4716, 0.5276, 0.5960,  ..., 0.5038, 0.4705, 0.4373],
          [0.5727, 0.5837, 0.5322,  ..., 0.5322, 0.4959, 0.5595]],

         [[0.3905, 0.5159, 0.5756,  ..., 0.6478, 0.5851, 0.5414],
          [0.4797, 0.5001, 0.5289,  ..., 0.4499, 0.3757, 0.4962],
          [0.6211, 0.5069, 0.3099,  ..., 0.5983, 0.3355, 0.3803],
          [0.3363, 0.4443, 0.4158,  ..., 0.5156, 0.4258, 0.4383]]],


        [[[0.5327, 0.4163, 0.4944,  ..., 0.5243, 0.3844, 0.6011],
          [0.4700, 0.3998, 0.3225,  ..., 0.5631, 0.5703, 0.3979],
          [0.5429, 0.4861, 0.5482,  ..., 0.3748, 0.5302, 0.5732],
          [0.6817, 0.4414, 0.4807,  ..., 0.5554, 0.5941, 0.5020]],

         [[0.5761, 0.4608, 0.6179,  ..., 0.5183, 0.4834, 0.6926],
          [0.5265, 0.5120, 0.5540,  ..., 0.4819, 0.5291, 0.4762],
          [0.4685, 0.5612, 0.6993,  ..., 0.5255, 0.5446, 0.5417],
          [0.4518, 0.5941, 0.5804,  ..., 0.5150, 0.5470, 0.4487]],

         [[0.5308, 0.6220, 0.4751,  ..., 0.3789, 0.5274, 0.5117],
          [0.3398, 0.6220, 0.4287,  ..., 0.4790, 0.4689, 0.4634],
          [0.5244, 0.6044, 0.5746,  ..., 0.3923, 0.3337, 0.4465],
          [0.5511, 0.4681, 0.4017,  ..., 0.6549, 0.5224, 0.4003]],

         ...,

         [[0.4835, 0.5443, 0.5684,  ..., 0.6876, 0.5889, 0.5149],
          [0.5025, 0.5479, 0.5124,  ..., 0.4530, 0.4841, 0.5537],
          [0.6619, 0.5780, 0.6123,  ..., 0.5523, 0.4950, 0.4528],
          [0.6192, 0.4282, 0.5988,  ..., 0.4632, 0.4941, 0.3947]],

         [[0.6817, 0.6016, 0.6610,  ..., 0.5525, 0.3849, 0.4438],
          [0.3132, 0.6388, 0.5875,  ..., 0.4822, 0.5856, 0.4523],
          [0.4149, 0.5808, 0.6289,  ..., 0.4822, 0.5605, 0.5334],
          [0.5311, 0.5023, 0.6002,  ..., 0.6610, 0.5574, 0.3657]],

         [[0.4306, 0.4656, 0.4805,  ..., 0.5557, 0.5162, 0.5622],
          [0.4576, 0.3513, 0.6039,  ..., 0.3965, 0.5535, 0.5612],
          [0.4990, 0.6086, 0.5713,  ..., 0.4472, 0.4943, 0.4554],
          [0.5612, 0.5293, 0.6741,  ..., 0.4706, 0.5234, 0.5116]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0040, -0.0020,  0.0020,  0.0040,  0.0020,  0.0040, -0.0040, -0.0020,
         0.0040,  0.0000], device='cuda:0')
selected experts tensor([1546, 1611, 1599, 1354, 1744, 1579, 2012, 1685, 1615, 1639],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5183, 0.6395, 0.5768,  ..., 0.3294, 0.4109, 0.4043],
          [0.6258, 0.5698, 0.5180,  ..., 0.5482, 0.2784, 0.4313],
          [0.5591, 0.5532, 0.6357,  ..., 0.4026, 0.6806, 0.5313],
          [0.7301, 0.4229, 0.4442,  ..., 0.4477, 0.5359, 0.4889]],

         [[0.4142, 0.4661, 0.5192,  ..., 0.4007, 0.3908, 0.4573],
          [0.7082, 0.6314, 0.4442,  ..., 0.5489, 0.3661, 0.4870],
          [0.4350, 0.5331, 0.5764,  ..., 0.4799, 0.5748, 0.3571],
          [0.6384, 0.5253, 0.3536,  ..., 0.4986, 0.4294, 0.6347]],

         [[0.4233, 0.5195, 0.5981,  ..., 0.5870, 0.4798, 0.5208],
          [0.5924, 0.5935, 0.4887,  ..., 0.5008, 0.3317, 0.5275],
          [0.3893, 0.5755, 0.5136,  ..., 0.3844, 0.4696, 0.3733],
          [0.5625, 0.5537, 0.4725,  ..., 0.4946, 0.4548, 0.5824]],

         ...,

         [[0.6836, 0.3257, 0.6465,  ..., 0.5794, 0.5810, 0.2737],
          [0.5044, 0.4296, 0.4114,  ..., 0.6496, 0.5805, 0.4936],
          [0.3953, 0.4743, 0.5783,  ..., 0.5129, 0.3963, 0.6191],
          [0.6055, 0.4880, 0.5882,  ..., 0.5020, 0.4565, 0.6883]],

         [[0.6366, 0.4291, 0.3219,  ..., 0.4059, 0.4123, 0.5782],
          [0.5615, 0.5450, 0.3991,  ..., 0.6160, 0.4944, 0.6500],
          [0.6143, 0.4612, 0.4881,  ..., 0.5846, 0.3616, 0.6177],
          [0.4856, 0.4387, 0.6348,  ..., 0.5756, 0.3589, 0.5369]],

         [[0.3644, 0.4765, 0.4839,  ..., 0.4862, 0.4156, 0.6338],
          [0.5189, 0.4914, 0.6051,  ..., 0.4239, 0.5507, 0.5330],
          [0.5510, 0.4559, 0.5811,  ..., 0.5460, 0.5471, 0.4847],
          [0.4389, 0.6071, 0.4905,  ..., 0.3951, 0.5133, 0.3056]]],


        [[[0.5611, 0.6599, 0.5792,  ..., 0.5636, 0.4927, 0.5155],
          [0.5420, 0.5197, 0.5251,  ..., 0.5110, 0.4137, 0.3197],
          [0.5244, 0.6163, 0.5873,  ..., 0.4282, 0.5695, 0.4800],
          [0.5725, 0.5595, 0.6185,  ..., 0.4927, 0.4786, 0.5027]],

         [[0.6321, 0.5428, 0.4233,  ..., 0.5315, 0.4922, 0.6500],
          [0.5391, 0.6923, 0.4005,  ..., 0.5542, 0.5333, 0.4660],
          [0.3907, 0.4723, 0.6194,  ..., 0.3303, 0.5145, 0.3421],
          [0.5437, 0.3922, 0.4563,  ..., 0.5846, 0.3760, 0.5515]],

         [[0.4784, 0.5443, 0.5401,  ..., 0.5011, 0.5323, 0.5695],
          [0.5215, 0.6085, 0.3860,  ..., 0.5000, 0.5339, 0.3661],
          [0.5835, 0.5065, 0.5452,  ..., 0.4547, 0.4351, 0.3553],
          [0.4512, 0.4752, 0.5596,  ..., 0.6325, 0.5084, 0.7106]],

         ...,

         [[0.5692, 0.5954, 0.5070,  ..., 0.4426, 0.5503, 0.4421],
          [0.4338, 0.5350, 0.4570,  ..., 0.5903, 0.5844, 0.5133],
          [0.5323, 0.4110, 0.5522,  ..., 0.3858, 0.4459, 0.3968],
          [0.4894, 0.5855, 0.5995,  ..., 0.4069, 0.4823, 0.4721]],

         [[0.4853, 0.4229, 0.4066,  ..., 0.3989, 0.4898, 0.4457],
          [0.4338, 0.5655, 0.3545,  ..., 0.4651, 0.4640, 0.4653],
          [0.4867, 0.5457, 0.5437,  ..., 0.3794, 0.4803, 0.5384],
          [0.6249, 0.5982, 0.4712,  ..., 0.4973, 0.4308, 0.4132]],

         [[0.4609, 0.6564, 0.5471,  ..., 0.4431, 0.5169, 0.4640],
          [0.3581, 0.5635, 0.4435,  ..., 0.5617, 0.5306, 0.5449],
          [0.6041, 0.4096, 0.5355,  ..., 0.4678, 0.4867, 0.4222],
          [0.5106, 0.5530, 0.5265,  ..., 0.4761, 0.5213, 0.3922]]]],
       device='cuda:0')
tensor([[[[0.5223, 0.6415, 0.5808,  ..., 0.3294, 0.4069, 0.4003],
          [0.6298, 0.5718, 0.5220,  ..., 0.5482, 0.2744, 0.4273],
          [0.5631, 0.5552, 0.6397,  ..., 0.4026, 0.6766, 0.5273],
          [0.7341, 0.4249, 0.4482,  ..., 0.4477, 0.5319, 0.4849]],

         [[0.4182, 0.4681, 0.5232,  ..., 0.4007, 0.3868, 0.4533],
          [0.7122, 0.6334, 0.4482,  ..., 0.5489, 0.3621, 0.4830],
          [0.4390, 0.5351, 0.5804,  ..., 0.4799, 0.5708, 0.3531],
          [0.6424, 0.5273, 0.3576,  ..., 0.4986, 0.4254, 0.6307]],

         [[0.4273, 0.5215, 0.6021,  ..., 0.5870, 0.4758, 0.5168],
          [0.5964, 0.5955, 0.4927,  ..., 0.5008, 0.3277, 0.5235],
          [0.3933, 0.5775, 0.5176,  ..., 0.3844, 0.4656, 0.3693],
          [0.5665, 0.5557, 0.4765,  ..., 0.4946, 0.4508, 0.5784]],

         ...,

         [[0.6876, 0.3277, 0.6505,  ..., 0.5794, 0.5770, 0.2697],
          [0.5084, 0.4316, 0.4154,  ..., 0.6496, 0.5765, 0.4896],
          [0.3993, 0.4763, 0.5823,  ..., 0.5129, 0.3923, 0.6151],
          [0.6095, 0.4900, 0.5922,  ..., 0.5020, 0.4525, 0.6843]],

         [[0.6406, 0.4311, 0.3259,  ..., 0.4059, 0.4083, 0.5742],
          [0.5655, 0.5470, 0.4031,  ..., 0.6160, 0.4904, 0.6460],
          [0.6183, 0.4632, 0.4921,  ..., 0.5846, 0.3576, 0.6137],
          [0.4896, 0.4407, 0.6388,  ..., 0.5756, 0.3549, 0.5329]],

         [[0.3684, 0.4785, 0.4879,  ..., 0.4862, 0.4116, 0.6298],
          [0.5229, 0.4934, 0.6091,  ..., 0.4239, 0.5467, 0.5290],
          [0.5550, 0.4579, 0.5851,  ..., 0.5460, 0.5431, 0.4807],
          [0.4429, 0.6091, 0.4945,  ..., 0.3951, 0.5093, 0.3016]]],


        [[[0.5651, 0.6619, 0.5832,  ..., 0.5636, 0.4887, 0.5115],
          [0.5460, 0.5217, 0.5291,  ..., 0.5110, 0.4097, 0.3157],
          [0.5284, 0.6183, 0.5913,  ..., 0.4282, 0.5655, 0.4760],
          [0.5765, 0.5615, 0.6225,  ..., 0.4927, 0.4746, 0.4987]],

         [[0.6361, 0.5448, 0.4273,  ..., 0.5315, 0.4882, 0.6460],
          [0.5431, 0.6943, 0.4045,  ..., 0.5542, 0.5293, 0.4620],
          [0.3947, 0.4743, 0.6234,  ..., 0.3303, 0.5105, 0.3381],
          [0.5477, 0.3942, 0.4603,  ..., 0.5846, 0.3720, 0.5475]],

         [[0.4824, 0.5463, 0.5441,  ..., 0.5011, 0.5283, 0.5655],
          [0.5255, 0.6105, 0.3900,  ..., 0.5000, 0.5299, 0.3621],
          [0.5875, 0.5085, 0.5492,  ..., 0.4547, 0.4311, 0.3513],
          [0.4552, 0.4772, 0.5636,  ..., 0.6325, 0.5044, 0.7066]],

         ...,

         [[0.5732, 0.5974, 0.5110,  ..., 0.4426, 0.5463, 0.4381],
          [0.4378, 0.5370, 0.4610,  ..., 0.5903, 0.5804, 0.5093],
          [0.5363, 0.4130, 0.5562,  ..., 0.3858, 0.4419, 0.3928],
          [0.4934, 0.5875, 0.6035,  ..., 0.4069, 0.4783, 0.4681]],

         [[0.4893, 0.4249, 0.4106,  ..., 0.3989, 0.4858, 0.4417],
          [0.4378, 0.5675, 0.3585,  ..., 0.4651, 0.4600, 0.4613],
          [0.4907, 0.5477, 0.5477,  ..., 0.3794, 0.4763, 0.5344],
          [0.6289, 0.6002, 0.4752,  ..., 0.4973, 0.4268, 0.4092]],

         [[0.4649, 0.6584, 0.5511,  ..., 0.4431, 0.5129, 0.4600],
          [0.3621, 0.5655, 0.4475,  ..., 0.5617, 0.5266, 0.5409],
          [0.6081, 0.4116, 0.5395,  ..., 0.4678, 0.4827, 0.4182],
          [0.5146, 0.5550, 0.5305,  ..., 0.4761, 0.5173, 0.3882]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0040, -0.0020, -0.0040,  0.0040, -0.0040,  0.0040, -0.0040,  0.0000,
         0.0040,  0.0040], device='cuda:0')
selected experts tensor([1949, 1704, 2478, 1593, 1914,  820, 2054, 1698, 1208,  966],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6078, 0.4885, 0.3795,  ..., 0.6263, 0.4684, 0.3625],
          [0.4596, 0.3480, 0.4245,  ..., 0.5201, 0.5994, 0.5777],
          [0.6203, 0.3995, 0.3800,  ..., 0.5212, 0.3169, 0.6000],
          [0.5804, 0.3999, 0.5504,  ..., 0.5918, 0.5321, 0.4764]],

         [[0.6125, 0.3962, 0.2922,  ..., 0.4835, 0.3818, 0.4132],
          [0.4650, 0.5009, 0.4543,  ..., 0.5152, 0.3957, 0.4985],
          [0.6254, 0.4899, 0.3587,  ..., 0.5032, 0.3186, 0.6392],
          [0.4437, 0.3632, 0.5253,  ..., 0.5153, 0.5671, 0.3553]],

         [[0.5627, 0.3957, 0.4405,  ..., 0.4326, 0.3186, 0.6274],
          [0.4964, 0.4985, 0.4985,  ..., 0.4103, 0.4859, 0.4365],
          [0.5014, 0.5174, 0.6345,  ..., 0.4747, 0.5747, 0.6103],
          [0.5085, 0.6471, 0.4909,  ..., 0.5545, 0.4321, 0.3589]],

         ...,

         [[0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040]],

         [[0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040]],

         [[0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040]]],


        [[[0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040]],

         [[0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040]],

         [[0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040]],

         ...,

         [[0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040]],

         [[0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040]],

         [[0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5020, 0.5020, 0.5040]]]],
       device='cuda:0')/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([[[[0.6058, 0.4865, 0.3775,  ..., 0.6243, 0.4664, 0.3585],
          [0.4576, 0.3460, 0.4225,  ..., 0.5181, 0.5974, 0.5737],
          [0.6183, 0.3975, 0.3780,  ..., 0.5192, 0.3149, 0.5960],
          [0.5784, 0.3979, 0.5484,  ..., 0.5898, 0.5301, 0.4724]],

         [[0.6105, 0.3942, 0.2902,  ..., 0.4815, 0.3798, 0.4092],
          [0.4630, 0.4989, 0.4523,  ..., 0.5132, 0.3937, 0.4945],
          [0.6234, 0.4879, 0.3567,  ..., 0.5012, 0.3166, 0.6352],
          [0.4417, 0.3612, 0.5233,  ..., 0.5133, 0.5651, 0.3513]],

         [[0.5607, 0.3937, 0.4385,  ..., 0.4306, 0.3166, 0.6234],
          [0.4944, 0.4965, 0.4965,  ..., 0.4083, 0.4839, 0.4325],
          [0.4994, 0.5154, 0.6325,  ..., 0.4727, 0.5727, 0.6063],
          [0.5065, 0.6451, 0.4889,  ..., 0.5525, 0.4301, 0.3549]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,
        0.0040], device='cuda:0')
selected experts tensor([5667,  511,  869,  538, 1400,  441,  521,  543,  346, 5548],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1546., 1611., 1599., 1354., 1744., 1579., 2012., 1685., 1615., 1639.],
        [5667.,  511.,  869.,  538., 1400.,  441.,  521.,  543.,  346., 5548.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5328, 0.5130, 0.3707,  ..., 0.5145, 0.5875, 0.6693],
          [0.5148, 0.4754, 0.4738,  ..., 0.4365, 0.4116, 0.5604],
          [0.4653, 0.6585, 0.6284,  ..., 0.5240, 0.4187, 0.5531],
          [0.5793, 0.5223, 0.4295,  ..., 0.4099, 0.5504, 0.4575]],

         [[0.6061, 0.4586, 0.4156,  ..., 0.5661, 0.5205, 0.5589],
          [0.5229, 0.5273, 0.5867,  ..., 0.5671, 0.5024, 0.5729],
          [0.3430, 0.5380, 0.5881,  ..., 0.5261, 0.6244, 0.4788],
          [0.4334, 0.4458, 0.5451,  ..., 0.5064, 0.5066, 0.6355]],

         [[0.4972, 0.6138, 0.5181,  ..., 0.5045, 0.4140, 0.3967],
          [0.5143, 0.5390, 0.4794,  ..., 0.4997, 0.4625, 0.4750],
          [0.4719, 0.4988, 0.3822,  ..., 0.2868, 0.4572, 0.5224],
          [0.5986, 0.4531, 0.5725,  ..., 0.6753, 0.5448, 0.3741]],

         ...,

         [[0.4200, 0.5186, 0.5437,  ..., 0.4322, 0.5605, 0.5604],
          [0.5350, 0.5908, 0.4654,  ..., 0.4577, 0.4259, 0.6027],
          [0.5769, 0.5521, 0.6356,  ..., 0.4951, 0.4074, 0.6255],
          [0.4772, 0.5351, 0.5985,  ..., 0.6640, 0.3613, 0.3760]],

         [[0.4209, 0.6262, 0.4019,  ..., 0.5136, 0.4374, 0.5666],
          [0.5683, 0.4036, 0.6115,  ..., 0.5393, 0.5941, 0.4852],
          [0.4435, 0.6407, 0.4646,  ..., 0.4672, 0.5511, 0.6771],
          [0.5021, 0.4936, 0.4232,  ..., 0.3884, 0.5880, 0.3732]],

         [[0.3204, 0.5125, 0.4066,  ..., 0.4925, 0.5128, 0.4442],
          [0.5420, 0.5880, 0.5067,  ..., 0.5364, 0.4569, 0.5971],
          [0.5534, 0.5400, 0.3235,  ..., 0.4966, 0.4249, 0.5114],
          [0.4770, 0.6271, 0.4052,  ..., 0.4728, 0.6059, 0.4555]]],


        [[[0.4200, 0.5186, 0.5437,  ..., 0.4322, 0.5605, 0.5604],
          [0.5350, 0.5908, 0.4654,  ..., 0.4577, 0.4259, 0.6027],
          [0.5769, 0.5521, 0.6356,  ..., 0.4951, 0.4074, 0.6255],
          [0.4772, 0.5351, 0.5985,  ..., 0.6640, 0.3613, 0.3760]],

         [[0.5399, 0.5077, 0.5274,  ..., 0.6337, 0.3577, 0.4957],
          [0.5821, 0.4953, 0.3227,  ..., 0.5083, 0.5552, 0.4921],
          [0.3119, 0.5114, 0.5446,  ..., 0.4817, 0.4937, 0.5572],
          [0.6126, 0.4991, 0.5820,  ..., 0.5611, 0.6461, 0.6605]],

         [[0.5764, 0.5385, 0.4104,  ..., 0.5089, 0.6054, 0.4804],
          [0.3368, 0.6119, 0.5296,  ..., 0.3916, 0.5521, 0.4978],
          [0.4539, 0.4948, 0.4811,  ..., 0.5146, 0.5567, 0.5565],
          [0.6340, 0.4756, 0.5239,  ..., 0.6300, 0.6077, 0.5867]],

         ...,

         [[0.3945, 0.3435, 0.5896,  ..., 0.4868, 0.6524, 0.5064],
          [0.5859, 0.5675, 0.5403,  ..., 0.4613, 0.4485, 0.4099],
          [0.5252, 0.4598, 0.3445,  ..., 0.5762, 0.4514, 0.7080],
          [0.5625, 0.6953, 0.5408,  ..., 0.4500, 0.4714, 0.4231]],

         [[0.5769, 0.4659, 0.5320,  ..., 0.4892, 0.4311, 0.4672],
          [0.5625, 0.6416, 0.4463,  ..., 0.5724, 0.4674, 0.6083],
          [0.4329, 0.4264, 0.5082,  ..., 0.6167, 0.4369, 0.4080],
          [0.4585, 0.6776, 0.4609,  ..., 0.5480, 0.3896, 0.4821]],

         [[0.5034, 0.5380, 0.6472,  ..., 0.5017, 0.4311, 0.3120],
          [0.5382, 0.5225, 0.6275,  ..., 0.3534, 0.5329, 0.6427],
          [0.5080, 0.5593, 0.4825,  ..., 0.5243, 0.6603, 0.4188],
          [0.5027, 0.4982, 0.3771,  ..., 0.5463, 0.5899, 0.5952]]]],
       device='cuda:0')
tensor([[[[0.5358, 0.5120, 0.3757,  ..., 0.5115, 0.5865, 0.6663],
          [0.5178, 0.4744, 0.4788,  ..., 0.4335, 0.4106, 0.5574],
          [0.4683, 0.6575, 0.6334,  ..., 0.5210, 0.4177, 0.5501],
          [0.5823, 0.5213, 0.4345,  ..., 0.4069, 0.5494, 0.4545]],

         [[0.6091, 0.4576, 0.4206,  ..., 0.5631, 0.5195, 0.5559],
          [0.5259, 0.5263, 0.5917,  ..., 0.5641, 0.5014, 0.5699],
          [0.3460, 0.5370, 0.5931,  ..., 0.5231, 0.6234, 0.4758],
          [0.4364, 0.4448, 0.5501,  ..., 0.5034, 0.5056, 0.6325]],

         [[0.5002, 0.6128, 0.5231,  ..., 0.5015, 0.4130, 0.3937],
          [0.5173, 0.5380, 0.4844,  ..., 0.4967, 0.4615, 0.4720],
          [0.4749, 0.4978, 0.3872,  ..., 0.2838, 0.4562, 0.5194],
          [0.6016, 0.4521, 0.5775,  ..., 0.6723, 0.5438, 0.3711]],

         ...,

         [[0.4230, 0.5176, 0.5487,  ..., 0.4292, 0.5595, 0.5574],
          [0.5380, 0.5898, 0.4704,  ..., 0.4547, 0.4249, 0.5997],
          [0.5799, 0.5511, 0.6406,  ..., 0.4921, 0.4064, 0.6225],
          [0.4802, 0.5341, 0.6035,  ..., 0.6610, 0.3603, 0.3730]],

         [[0.4239, 0.6252, 0.4069,  ..., 0.5106, 0.4364, 0.5636],
          [0.5713, 0.4026, 0.6165,  ..., 0.5363, 0.5931, 0.4822],
          [0.4465, 0.6397, 0.4696,  ..., 0.4642, 0.5501, 0.6741],
          [0.5051, 0.4926, 0.4282,  ..., 0.3854, 0.5870, 0.3702]],

         [[0.3234, 0.5115, 0.4116,  ..., 0.4895, 0.5118, 0.4412],
          [0.5450, 0.5870, 0.5117,  ..., 0.5334, 0.4559, 0.5941],
          [0.5564, 0.5390, 0.3285,  ..., 0.4936, 0.4239, 0.5084],
          [0.4800, 0.6261, 0.4102,  ..., 0.4698, 0.6049, 0.4525]]],


        [[[0.4230, 0.5176, 0.5487,  ..., 0.4292, 0.5595, 0.5574],
          [0.5380, 0.5898, 0.4704,  ..., 0.4547, 0.4249, 0.5997],
          [0.5799, 0.5511, 0.6406,  ..., 0.4921, 0.4064, 0.6225],
          [0.4802, 0.5341, 0.6035,  ..., 0.6610, 0.3603, 0.3730]],

         [[0.5429, 0.5067, 0.5324,  ..., 0.6307, 0.3567, 0.4927],
          [0.5851, 0.4943, 0.3277,  ..., 0.5053, 0.5542, 0.4891],
          [0.3149, 0.5104, 0.5496,  ..., 0.4787, 0.4927, 0.5542],
          [0.6156, 0.4981, 0.5870,  ..., 0.5581, 0.6451, 0.6575]],

         [[0.5794, 0.5375, 0.4154,  ..., 0.5059, 0.6044, 0.4774],
          [0.3398, 0.6109, 0.5346,  ..., 0.3886, 0.5511, 0.4948],
          [0.4569, 0.4938, 0.4861,  ..., 0.5116, 0.5557, 0.5535],
          [0.6370, 0.4746, 0.5289,  ..., 0.6270, 0.6067, 0.5837]],

         ...,

         [[0.3975, 0.3425, 0.5946,  ..., 0.4838, 0.6514, 0.5034],
          [0.5889, 0.5665, 0.5453,  ..., 0.4583, 0.4475, 0.4069],
          [0.5282, 0.4588, 0.3495,  ..., 0.5732, 0.4504, 0.7050],
          [0.5655, 0.6943, 0.5458,  ..., 0.4470, 0.4704, 0.4201]],

         [[0.5799, 0.4649, 0.5370,  ..., 0.4862, 0.4301, 0.4642],
          [0.5655, 0.6406, 0.4513,  ..., 0.5694, 0.4664, 0.6053],
          [0.4359, 0.4254, 0.5132,  ..., 0.6137, 0.4359, 0.4050],
          [0.4615, 0.6766, 0.4659,  ..., 0.5450, 0.3886, 0.4791]],

         [[0.5064, 0.5370, 0.6522,  ..., 0.4987, 0.4301, 0.3090],
          [0.5412, 0.5215, 0.6325,  ..., 0.3504, 0.5319, 0.6397],
          [0.5110, 0.5583, 0.4875,  ..., 0.5213, 0.6593, 0.4158],
          [0.5057, 0.4972, 0.3821,  ..., 0.5433, 0.5889, 0.5922]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0030,  0.0010, -0.0050,  0.0010, -0.0050, -0.0030,  0.0050,  0.0030,
         0.0010,  0.0030], device='cuda:0')
selected experts tensor([1560, 1700, 1643, 1605, 1585, 1787, 1617, 1637, 1695, 1555],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.3627, 0.5936, 0.5077,  ..., 0.5897, 0.6402, 0.6841],
          [0.5539, 0.5474, 0.6023,  ..., 0.4466, 0.6210, 0.6807],
          [0.4676, 0.6387, 0.5278,  ..., 0.3921, 0.4242, 0.4848],
          [0.5577, 0.3701, 0.5553,  ..., 0.5379, 0.4190, 0.4339]],

         [[0.3035, 0.5498, 0.4589,  ..., 0.4953, 0.6850, 0.4979],
          [0.4733, 0.5798, 0.5805,  ..., 0.4983, 0.6555, 0.5164],
          [0.5048, 0.4779, 0.5246,  ..., 0.6112, 0.5915, 0.4450],
          [0.3519, 0.5631, 0.5786,  ..., 0.4143, 0.6196, 0.4445]],

         [[0.4947, 0.5423, 0.4379,  ..., 0.6846, 0.4986, 0.5210],
          [0.5089, 0.3415, 0.5324,  ..., 0.4224, 0.4677, 0.5201],
          [0.6615, 0.4186, 0.4425,  ..., 0.4291, 0.2778, 0.4392],
          [0.5731, 0.4272, 0.5201,  ..., 0.4076, 0.4823, 0.4363]],

         ...,

         [[0.4666, 0.4564, 0.5364,  ..., 0.4425, 0.4973, 0.3710],
          [0.2568, 0.4798, 0.5011,  ..., 0.4821, 0.6302, 0.4734],
          [0.4687, 0.5266, 0.5990,  ..., 0.5008, 0.4755, 0.4363],
          [0.5697, 0.5827, 0.5352,  ..., 0.5292, 0.5009, 0.5585]],

         [[0.6995, 0.3816, 0.6209,  ..., 0.6268, 0.3413, 0.5440],
          [0.4143, 0.3858, 0.5587,  ..., 0.5697, 0.4561, 0.4646],
          [0.4738, 0.5552, 0.3824,  ..., 0.4999, 0.6339, 0.6057],
          [0.4355, 0.4044, 0.3912,  ..., 0.3992, 0.6024, 0.3656]],

         [[0.5546, 0.6169, 0.6346,  ..., 0.6231, 0.6420, 0.5755],
          [0.4708, 0.4637, 0.7063,  ..., 0.4430, 0.4332, 0.4460],
          [0.3996, 0.5860, 0.5417,  ..., 0.4262, 0.4889, 0.3779],
          [0.3996, 0.5016, 0.5541,  ..., 0.5331, 0.6935, 0.5448]]],


        [[[0.4666, 0.4564, 0.5364,  ..., 0.4425, 0.4973, 0.3710],
          [0.2568, 0.4798, 0.5011,  ..., 0.4821, 0.6302, 0.4734],
          [0.4687, 0.5266, 0.5990,  ..., 0.5008, 0.4755, 0.4363],
          [0.5697, 0.5827, 0.5352,  ..., 0.5292, 0.5009, 0.5585]],

         [[0.5759, 0.5305, 0.4517,  ..., 0.4722, 0.4034, 0.4306],
          [0.4809, 0.4012, 0.6300,  ..., 0.5640, 0.3626, 0.5322],
          [0.5129, 0.3432, 0.5226,  ..., 0.4561, 0.4641, 0.5028],
          [0.4039, 0.6705, 0.6544,  ..., 0.3992, 0.4971, 0.4813]],

         [[0.5621, 0.3951, 0.5444,  ..., 0.4986, 0.4428, 0.4448],
          [0.5529, 0.7152, 0.5264,  ..., 0.4091, 0.5527, 0.5537],
          [0.4034, 0.5319, 0.5167,  ..., 0.4152, 0.4218, 0.5964],
          [0.3333, 0.4841, 0.5202,  ..., 0.4425, 0.4884, 0.4021]],

         ...,

         [[0.4339, 0.5158, 0.5476,  ..., 0.5883, 0.5820, 0.4557],
          [0.4209, 0.6025, 0.5928,  ..., 0.5314, 0.3955, 0.6053],
          [0.4993, 0.4063, 0.4379,  ..., 0.5635, 0.4104, 0.5118],
          [0.4324, 0.4988, 0.5767,  ..., 0.3842, 0.4371, 0.3638]],

         [[0.3931, 0.3584, 0.4713,  ..., 0.4955, 0.6357, 0.5205],
          [0.6028, 0.7437, 0.4207,  ..., 0.5101, 0.4133, 0.4330],
          [0.5230, 0.5741, 0.5317,  ..., 0.6181, 0.5300, 0.4708],
          [0.3782, 0.5428, 0.6255,  ..., 0.4243, 0.5321, 0.3674]],

         [[0.4903, 0.5346, 0.5197,  ..., 0.5759, 0.4957, 0.5467],
          [0.4181, 0.6205, 0.6125,  ..., 0.5462, 0.4529, 0.3885],
          [0.6304, 0.5164, 0.4188,  ..., 0.5812, 0.4757, 0.5421],
          [0.4095, 0.4402, 0.5315,  ..., 0.5144, 0.4452, 0.6169]]]],
       device='cuda:0')
tensor([[[[0.3657, 0.5946, 0.5047,  ..., 0.5927, 0.6352, 0.6851],
          [0.5569, 0.5484, 0.5993,  ..., 0.4496, 0.6160, 0.6817],
          [0.4706, 0.6397, 0.5248,  ..., 0.3951, 0.4192, 0.4858],
          [0.5607, 0.3711, 0.5523,  ..., 0.5409, 0.4140, 0.4349]],

         [[0.3065, 0.5508, 0.4559,  ..., 0.4983, 0.6800, 0.4989],
          [0.4763, 0.5808, 0.5775,  ..., 0.5013, 0.6505, 0.5174],
          [0.5078, 0.4789, 0.5216,  ..., 0.6142, 0.5865, 0.4460],
          [0.3549, 0.5641, 0.5756,  ..., 0.4173, 0.6146, 0.4455]],

         [[0.4977, 0.5433, 0.4349,  ..., 0.6876, 0.4936, 0.5220],
          [0.5119, 0.3425, 0.5294,  ..., 0.4254, 0.4627, 0.5211],
          [0.6645, 0.4196, 0.4395,  ..., 0.4321, 0.2728, 0.4402],
          [0.5761, 0.4282, 0.5171,  ..., 0.4106, 0.4773, 0.4373]],

         ...,

         [[0.4696, 0.4574, 0.5334,  ..., 0.4455, 0.4923, 0.3720],
          [0.2598, 0.4808, 0.4981,  ..., 0.4851, 0.6252, 0.4744],
          [0.4717, 0.5276, 0.5960,  ..., 0.5038, 0.4705, 0.4373],
          [0.5727, 0.5837, 0.5322,  ..., 0.5322, 0.4959, 0.5595]],

         [[0.7025, 0.3826, 0.6179,  ..., 0.6298, 0.3363, 0.5450],
          [0.4173, 0.3868, 0.5557,  ..., 0.5727, 0.4511, 0.4656],
          [0.4768, 0.5562, 0.3794,  ..., 0.5029, 0.6289, 0.6067],
          [0.4385, 0.4054, 0.3882,  ..., 0.4022, 0.5974, 0.3666]],

         [[0.5576, 0.6179, 0.6316,  ..., 0.6261, 0.6370, 0.5765],
          [0.4738, 0.4647, 0.7033,  ..., 0.4460, 0.4282, 0.4470],
          [0.4026, 0.5870, 0.5387,  ..., 0.4292, 0.4839, 0.3789],
          [0.4026, 0.5026, 0.5511,  ..., 0.5361, 0.6885, 0.5458]]],


        [[[0.4696, 0.4574, 0.5334,  ..., 0.4455, 0.4923, 0.3720],
          [0.2598, 0.4808, 0.4981,  ..., 0.4851, 0.6252, 0.4744],
          [0.4717, 0.5276, 0.5960,  ..., 0.5038, 0.4705, 0.4373],
          [0.5727, 0.5837, 0.5322,  ..., 0.5322, 0.4959, 0.5595]],

         [[0.5789, 0.5315, 0.4487,  ..., 0.4752, 0.3984, 0.4316],
          [0.4839, 0.4022, 0.6270,  ..., 0.5670, 0.3576, 0.5332],
          [0.5159, 0.3442, 0.5196,  ..., 0.4591, 0.4591, 0.5038],
          [0.4069, 0.6715, 0.6514,  ..., 0.4022, 0.4921, 0.4823]],

         [[0.5651, 0.3961, 0.5414,  ..., 0.5016, 0.4378, 0.4458],
          [0.5559, 0.7162, 0.5234,  ..., 0.4121, 0.5477, 0.5547],
          [0.4064, 0.5329, 0.5137,  ..., 0.4182, 0.4168, 0.5974],
          [0.3363, 0.4851, 0.5172,  ..., 0.4455, 0.4834, 0.4031]],

         ...,

         [[0.4369, 0.5168, 0.5446,  ..., 0.5913, 0.5770, 0.4567],
          [0.4239, 0.6035, 0.5898,  ..., 0.5344, 0.3905, 0.6063],
          [0.5023, 0.4073, 0.4349,  ..., 0.5665, 0.4054, 0.5128],
          [0.4354, 0.4998, 0.5737,  ..., 0.3872, 0.4321, 0.3648]],

         [[0.3961, 0.3594, 0.4683,  ..., 0.4985, 0.6307, 0.5215],
          [0.6058, 0.7447, 0.4177,  ..., 0.5131, 0.4083, 0.4340],
          [0.5260, 0.5751, 0.5287,  ..., 0.6211, 0.5250, 0.4718],
          [0.3812, 0.5438, 0.6225,  ..., 0.4273, 0.5271, 0.3684]],

         [[0.4933, 0.5356, 0.5167,  ..., 0.5789, 0.4907, 0.5477],
          [0.4211, 0.6215, 0.6095,  ..., 0.5492, 0.4479, 0.3895],
          [0.6334, 0.5174, 0.4158,  ..., 0.5842, 0.4707, 0.5431],
          [0.4125, 0.4412, 0.5285,  ..., 0.5174, 0.4402, 0.6179]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0030, -0.0010,  0.0030,  0.0050,  0.0010,  0.0050, -0.0050, -0.0030,
         0.0050, -0.0010], device='cuda:0')
selected experts tensor([1600, 1614, 1612, 1339, 1788, 1572, 1865, 1603, 1739, 1652],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5013, 0.5302, 0.6055,  ..., 0.5083, 0.4675, 0.4935],
          [0.6682, 0.5901, 0.4931,  ..., 0.4349, 0.3734, 0.4208],
          [0.4928, 0.3912, 0.4256,  ..., 0.5556, 0.4774, 0.3689],
          [0.5877, 0.6745, 0.5451,  ..., 0.5836, 0.5792, 0.5602]],

         [[0.5159, 0.5069, 0.6725,  ..., 0.5380, 0.4801, 0.6420],
          [0.4480, 0.6195, 0.4906,  ..., 0.7064, 0.6302, 0.5386],
          [0.5682, 0.5323, 0.5172,  ..., 0.5641, 0.4988, 0.4237],
          [0.4517, 0.6340, 0.6525,  ..., 0.5655, 0.5068, 0.4970]],

         [[0.6193, 0.4442, 0.5267,  ..., 0.5860, 0.5777, 0.4571],
          [0.5672, 0.4825, 0.3967,  ..., 0.5597, 0.4232, 0.4733],
          [0.4019, 0.4243, 0.5782,  ..., 0.4455, 0.4067, 0.4806],
          [0.5881, 0.6075, 0.4987,  ..., 0.5794, 0.6057, 0.4886]],

         ...,

         [[0.6793, 0.4838, 0.4314,  ..., 0.4779, 0.3894, 0.4636],
          [0.5957, 0.4685, 0.5359,  ..., 0.3937, 0.5403, 0.7220],
          [0.5015, 0.4824, 0.4921,  ..., 0.5436, 0.4445, 0.4433],
          [0.5649, 0.5925, 0.5269,  ..., 0.4742, 0.2809, 0.5614]],

         [[0.3753, 0.4976, 0.4529,  ..., 0.4177, 0.6005, 0.6976],
          [0.4722, 0.6448, 0.6124,  ..., 0.5641, 0.4807, 0.4133],
          [0.5668, 0.4015, 0.5815,  ..., 0.4258, 0.4936, 0.3448],
          [0.4823, 0.4416, 0.3670,  ..., 0.4968, 0.5433, 0.4694]],

         [[0.5844, 0.4399, 0.5569,  ..., 0.3566, 0.5190, 0.3554],
          [0.4950, 0.4954, 0.5154,  ..., 0.3788, 0.5488, 0.5537],
          [0.6767, 0.4799, 0.5398,  ..., 0.4986, 0.5953, 0.4682],
          [0.4696, 0.4010, 0.5379,  ..., 0.6288, 0.4095, 0.5811]]],


        [[[0.6101, 0.4833, 0.5153,  ..., 0.4756, 0.5925, 0.6293],
          [0.5163, 0.6779, 0.4061,  ..., 0.5798, 0.4782, 0.4636],
          [0.5867, 0.4769, 0.5335,  ..., 0.4390, 0.5120, 0.4034],
          [0.4151, 0.5090, 0.5156,  ..., 0.5006, 0.5122, 0.3770]],

         [[0.5183, 0.4469, 0.5323,  ..., 0.4590, 0.5394, 0.5782],
          [0.4972, 0.4827, 0.4056,  ..., 0.5513, 0.6182, 0.4266],
          [0.5734, 0.5113, 0.5202,  ..., 0.5256, 0.5636, 0.4404],
          [0.4803, 0.6921, 0.6059,  ..., 0.5020, 0.4633, 0.5887]],

         [[0.6129, 0.4587, 0.5782,  ..., 0.5912, 0.5440, 0.6599],
          [0.4319, 0.4811, 0.5104,  ..., 0.4956, 0.4711, 0.4390],
          [0.4536, 0.4580, 0.4471,  ..., 0.5084, 0.5131, 0.4204],
          [0.4427, 0.5577, 0.5105,  ..., 0.5931, 0.6465, 0.4356]],

         ...,

         [[0.6595, 0.3847, 0.5829,  ..., 0.4111, 0.4227, 0.5090],
          [0.5853, 0.4119, 0.5415,  ..., 0.5356, 0.4863, 0.4006],
          [0.3099, 0.3982, 0.3680,  ..., 0.4082, 0.3987, 0.4171],
          [0.5082, 0.5328, 0.5044,  ..., 0.5246, 0.5749, 0.4920]],

         [[0.5069, 0.5169, 0.5006,  ..., 0.3249, 0.5491, 0.6608],
          [0.4237, 0.5457, 0.6934,  ..., 0.5655, 0.4445, 0.4460],
          [0.5989, 0.4602, 0.4589,  ..., 0.5390, 0.4714, 0.3008],
          [0.5400, 0.4442, 0.5512,  ..., 0.4363, 0.4590, 0.4479]],

         [[0.5744, 0.4827, 0.4446,  ..., 0.4431, 0.4534, 0.6057],
          [0.3981, 0.5840, 0.6202,  ..., 0.5419, 0.4508, 0.4619],
          [0.4609, 0.4476, 0.6142,  ..., 0.4258, 0.5132, 0.5806],
          [0.3571, 0.4592, 0.3235,  ..., 0.4866, 0.4804, 0.3174]]]],
       device='cuda:0')
tensor([[[[0.5063, 0.5332, 0.6105,  ..., 0.5093, 0.4625, 0.4885],
          [0.6732, 0.5931, 0.4981,  ..., 0.4359, 0.3684, 0.4158],
          [0.4978, 0.3942, 0.4306,  ..., 0.5566, 0.4724, 0.3639],
          [0.5927, 0.6775, 0.5501,  ..., 0.5846, 0.5742, 0.5552]],

         [[0.5209, 0.5099, 0.6775,  ..., 0.5390, 0.4751, 0.6370],
          [0.4530, 0.6225, 0.4956,  ..., 0.7074, 0.6252, 0.5336],
          [0.5732, 0.5353, 0.5222,  ..., 0.5651, 0.4938, 0.4187],
          [0.4567, 0.6370, 0.6575,  ..., 0.5665, 0.5018, 0.4920]],

         [[0.6243, 0.4472, 0.5317,  ..., 0.5870, 0.5727, 0.4521],
          [0.5722, 0.4855, 0.4017,  ..., 0.5607, 0.4182, 0.4683],
          [0.4069, 0.4273, 0.5832,  ..., 0.4465, 0.4017, 0.4756],
          [0.5931, 0.6105, 0.5037,  ..., 0.5804, 0.6007, 0.4836]],

         ...,

         [[0.6843, 0.4868, 0.4364,  ..., 0.4789, 0.3844, 0.4586],
          [0.6007, 0.4715, 0.5409,  ..., 0.3947, 0.5353, 0.7170],
          [0.5065, 0.4854, 0.4971,  ..., 0.5446, 0.4395, 0.4383],
          [0.5699, 0.5955, 0.5319,  ..., 0.4752, 0.2759, 0.5564]],

         [[0.3803, 0.5006, 0.4579,  ..., 0.4187, 0.5955, 0.6926],
          [0.4772, 0.6478, 0.6174,  ..., 0.5651, 0.4757, 0.4083],
          [0.5718, 0.4045, 0.5865,  ..., 0.4268, 0.4886, 0.3398],
          [0.4873, 0.4446, 0.3720,  ..., 0.4978, 0.5383, 0.4644]],

         [[0.5894, 0.4429, 0.5619,  ..., 0.3576, 0.5140, 0.3504],
          [0.5000, 0.4984, 0.5204,  ..., 0.3798, 0.5438, 0.5487],
          [0.6817, 0.4829, 0.5448,  ..., 0.4996, 0.5903, 0.4632],
          [0.4746, 0.4040, 0.5429,  ..., 0.6298, 0.4045, 0.5761]]],


        [[[0.6151, 0.4863, 0.5203,  ..., 0.4766, 0.5875, 0.6243],
          [0.5213, 0.6809, 0.4111,  ..., 0.5808, 0.4732, 0.4586],
          [0.5917, 0.4799, 0.5385,  ..., 0.4400, 0.5070, 0.3984],
          [0.4201, 0.5120, 0.5206,  ..., 0.5016, 0.5072, 0.3720]],

         [[0.5233, 0.4499, 0.5373,  ..., 0.4600, 0.5344, 0.5732],
          [0.5022, 0.4857, 0.4106,  ..., 0.5523, 0.6132, 0.4216],
          [0.5784, 0.5143, 0.5252,  ..., 0.5266, 0.5586, 0.4354],
          [0.4853, 0.6951, 0.6109,  ..., 0.5030, 0.4583, 0.5837]],

         [[0.6179, 0.4617, 0.5832,  ..., 0.5922, 0.5390, 0.6549],
          [0.4369, 0.4841, 0.5154,  ..., 0.4966, 0.4661, 0.4340],
          [0.4586, 0.4610, 0.4521,  ..., 0.5094, 0.5081, 0.4154],
          [0.4477, 0.5607, 0.5155,  ..., 0.5941, 0.6415, 0.4306]],

         ...,

         [[0.6645, 0.3877, 0.5879,  ..., 0.4121, 0.4177, 0.5040],
          [0.5903, 0.4149, 0.5465,  ..., 0.5366, 0.4813, 0.3956],
          [0.3149, 0.4012, 0.3730,  ..., 0.4092, 0.3937, 0.4121],
          [0.5132, 0.5358, 0.5094,  ..., 0.5256, 0.5699, 0.4870]],

         [[0.5119, 0.5199, 0.5056,  ..., 0.3259, 0.5441, 0.6558],
          [0.4287, 0.5487, 0.6984,  ..., 0.5665, 0.4395, 0.4410],
          [0.6039, 0.4632, 0.4639,  ..., 0.5400, 0.4664, 0.2958],
          [0.5450, 0.4472, 0.5562,  ..., 0.4373, 0.4540, 0.4429]],

         [[0.5794, 0.4857, 0.4496,  ..., 0.4441, 0.4484, 0.6007],
          [0.4031, 0.5870, 0.6252,  ..., 0.5429, 0.4458, 0.4569],
          [0.4659, 0.4506, 0.6192,  ..., 0.4268, 0.5082, 0.5756],
          [0.3621, 0.4622, 0.3285,  ..., 0.4876, 0.4754, 0.3124]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0050, -0.0030, -0.0050,  0.0050, -0.0050,  0.0050, -0.0050, -0.0010,
         0.0050,  0.0050], device='cuda:0')
selected experts tensor([2182, 1745, 2188, 1366, 1718,  871, 2512, 1736, 1091,  975],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4892, 0.5471, 0.3104,  ..., 0.3324, 0.4488, 0.4136],
          [0.3613, 0.3760, 0.4384,  ..., 0.4236, 0.5256, 0.4179],
          [0.6280, 0.5485, 0.4420,  ..., 0.3714, 0.3516, 0.5829],
          [0.4943, 0.5306, 0.4241,  ..., 0.4955, 0.4918, 0.4715]],

         [[0.5314, 0.5685, 0.4341,  ..., 0.6051, 0.3732, 0.3939],
          [0.5112, 0.3534, 0.6508,  ..., 0.5324, 0.4480, 0.5263],
          [0.3470, 0.5137, 0.5162,  ..., 0.4394, 0.5114, 0.5281],
          [0.4818, 0.4584, 0.6382,  ..., 0.7023, 0.5714, 0.5176]],

         [[0.5775, 0.4803, 0.4014,  ..., 0.3385, 0.4862, 0.5013],
          [0.4615, 0.5320, 0.4341,  ..., 0.3687, 0.4322, 0.6074],
          [0.6179, 0.4444, 0.5163,  ..., 0.3446, 0.3376, 0.5819],
          [0.3901, 0.4265, 0.4497,  ..., 0.4056, 0.6273, 0.3851]],

         ...,

         [[0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030]],

         [[0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030]],

         [[0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030]]],


        [[[0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030]],

         [[0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030]],

         [[0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030]],

         ...,

         [[0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030]],

         [[0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030]],

         [[0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030],
          [0.5010, 0.5030, 0.5030,  ..., 0.5030, 0.5030, 0.5030]]]],
       device='cuda:0')
tensor([[[[0.4882, 0.5441, 0.3074,  ..., 0.3294, 0.4458, 0.4106],
          [0.3603, 0.3730, 0.4354,  ..., 0.4206, 0.5226, 0.4149],
          [0.6270, 0.5455, 0.4390,  ..., 0.3684, 0.3486, 0.5799],
          [0.4933, 0.5276, 0.4211,  ..., 0.4925, 0.4888, 0.4685]],

         [[0.5304, 0.5655, 0.4311,  ..., 0.6021, 0.3702, 0.3909],
          [0.5102, 0.3504, 0.6478,  ..., 0.5294, 0.4450, 0.5233],
          [0.3460, 0.5107, 0.5132,  ..., 0.4364, 0.5084, 0.5251],
          [0.4808, 0.4554, 0.6352,  ..., 0.6993, 0.5684, 0.5146]],

         [[0.5765, 0.4773, 0.3984,  ..., 0.3355, 0.4832, 0.4983],
          [0.4605, 0.5290, 0.4311,  ..., 0.3657, 0.4292, 0.6044],
          [0.6169, 0.4414, 0.5133,  ..., 0.3416, 0.3346, 0.5789],
          [0.3891, 0.4235, 0.4467,  ..., 0.4026, 0.6243, 0.3821]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([0.0010, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,
        0.0030], device='cuda:0')
selected experts tensor([ 761, 5485, 5589,  439, 1445,  490,  458,  544,  416,  757],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1600., 1614., 1612., 1339., 1788., 1572., 1865., 1603., 1739., 1652.],
        [ 761., 5485., 5589.,  439., 1445.,  490.,  458.,  544.,  416.,  757.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6511, 0.4031, 0.4611,  ..., 0.4165, 0.4666, 0.4208],
          [0.5046, 0.4441, 0.5918,  ..., 0.3706, 0.5366, 0.5067],
          [0.6305, 0.3320, 0.3651,  ..., 0.3571, 0.5257, 0.5330],
          [0.3020, 0.4287, 0.5667,  ..., 0.4814, 0.4926, 0.5006]],

         [[0.6511, 0.4031, 0.4611,  ..., 0.4165, 0.4666, 0.4208],
          [0.5046, 0.4441, 0.5918,  ..., 0.3706, 0.5366, 0.5067],
          [0.6305, 0.3320, 0.3651,  ..., 0.3571, 0.5257, 0.5330],
          [0.3020, 0.4287, 0.5667,  ..., 0.4814, 0.4926, 0.5006]],

         [[0.4631, 0.5212, 0.4103,  ..., 0.3360, 0.5151, 0.4080],
          [0.4196, 0.3416, 0.4897,  ..., 0.5444, 0.5552, 0.5633],
          [0.3983, 0.5117, 0.4473,  ..., 0.5272, 0.5875, 0.7016],
          [0.5784, 0.5176, 0.5257,  ..., 0.3982, 0.5179, 0.3760]],

         ...,

         [[0.5314, 0.4893, 0.5829,  ..., 0.5473, 0.4511, 0.4356],
          [0.5133, 0.6053, 0.4318,  ..., 0.5650, 0.4749, 0.4010],
          [0.3941, 0.4955, 0.4436,  ..., 0.3670, 0.4554, 0.4631],
          [0.5017, 0.4278, 0.3106,  ..., 0.6000, 0.5610, 0.4375]],

         [[0.4527, 0.5528, 0.4456,  ..., 0.5777, 0.4666, 0.4404],
          [0.4786, 0.7001, 0.5376,  ..., 0.6650, 0.3057, 0.5292],
          [0.4044, 0.4506, 0.3561,  ..., 0.5786, 0.5780, 0.5083],
          [0.4876, 0.4967, 0.5097,  ..., 0.3482, 0.5465, 0.5262]],

         [[0.6140, 0.4943, 0.6054,  ..., 0.3482, 0.4254, 0.5381],
          [0.4277, 0.4889, 0.5600,  ..., 0.4873, 0.4373, 0.4337],
          [0.4115, 0.3285, 0.4265,  ..., 0.4462, 0.5299, 0.4866],
          [0.5202, 0.4921, 0.3822,  ..., 0.4109, 0.4438, 0.4811]]],


        [[[0.5214, 0.4349, 0.5521,  ..., 0.5166, 0.4012, 0.5464],
          [0.4784, 0.5344, 0.4973,  ..., 0.5050, 0.4225, 0.3797],
          [0.6250, 0.6044, 0.4516,  ..., 0.6014, 0.4908, 0.5076],
          [0.5683, 0.4273, 0.5506,  ..., 0.3880, 0.4651, 0.4294]],

         [[0.4086, 0.4121, 0.5662,  ..., 0.6251, 0.4158, 0.6491],
          [0.6287, 0.5220, 0.4189,  ..., 0.4770, 0.5765, 0.5839],
          [0.5731, 0.6868, 0.2939,  ..., 0.4539, 0.4613, 0.4672],
          [0.4622, 0.4681, 0.4388,  ..., 0.5568, 0.4395, 0.5155]],

         [[0.5041, 0.4239, 0.3330,  ..., 0.4988, 0.4545, 0.5695],
          [0.4527, 0.3504, 0.5729,  ..., 0.5801, 0.5588, 0.4957],
          [0.5958, 0.5499, 0.4381,  ..., 0.3715, 0.5804, 0.4913],
          [0.6386, 0.4301, 0.4335,  ..., 0.4113, 0.5300, 0.5777]],

         ...,

         [[0.3511, 0.4506, 0.6105,  ..., 0.5541, 0.6935, 0.3977],
          [0.5204, 0.4397, 0.5576,  ..., 0.5173, 0.4639, 0.4361],
          [0.5102, 0.5761, 0.5004,  ..., 0.3403, 0.4722, 0.5325],
          [0.4058, 0.4359, 0.4280,  ..., 0.4939, 0.6343, 0.5539]],

         [[0.4191, 0.6628, 0.5686,  ..., 0.6899, 0.4438, 0.4687],
          [0.5257, 0.4230, 0.5364,  ..., 0.6677, 0.4154, 0.5442],
          [0.5679, 0.5014, 0.4560,  ..., 0.5319, 0.5171, 0.4232],
          [0.4469, 0.5046, 0.3688,  ..., 0.4914, 0.5794, 0.6518]],

         [[0.5042, 0.5373, 0.6462,  ..., 0.5026, 0.4301, 0.3130],
          [0.5392, 0.5213, 0.6265,  ..., 0.3544, 0.5319, 0.6437],
          [0.5090, 0.5583, 0.4815,  ..., 0.5253, 0.6593, 0.4198],
          [0.5036, 0.4971, 0.3761,  ..., 0.5473, 0.5889, 0.5962]]]],
       device='cuda:0')
tensor([[[[0.6531, 0.4031, 0.4671,  ..., 0.4125, 0.4666, 0.4168],
          [0.5066, 0.4441, 0.5978,  ..., 0.3666, 0.5366, 0.5027],
          [0.6325, 0.3320, 0.3711,  ..., 0.3531, 0.5257, 0.5290],
          [0.3040, 0.4287, 0.5727,  ..., 0.4774, 0.4926, 0.4966]],

         [[0.6531, 0.4031, 0.4671,  ..., 0.4125, 0.4666, 0.4168],
          [0.5066, 0.4441, 0.5978,  ..., 0.3666, 0.5366, 0.5027],
          [0.6325, 0.3320, 0.3711,  ..., 0.3531, 0.5257, 0.5290],
          [0.3040, 0.4287, 0.5727,  ..., 0.4774, 0.4926, 0.4966]],

         [[0.4651, 0.5212, 0.4163,  ..., 0.3320, 0.5151, 0.4040],
          [0.4216, 0.3416, 0.4957,  ..., 0.5404, 0.5552, 0.5593],
          [0.4003, 0.5117, 0.4533,  ..., 0.5232, 0.5875, 0.6976],
          [0.5804, 0.5176, 0.5317,  ..., 0.3942, 0.5179, 0.3720]],

         ...,

         [[0.5334, 0.4893, 0.5889,  ..., 0.5433, 0.4511, 0.4316],
          [0.5153, 0.6053, 0.4378,  ..., 0.5610, 0.4749, 0.3970],
          [0.3961, 0.4955, 0.4496,  ..., 0.3630, 0.4554, 0.4591],
          [0.5037, 0.4278, 0.3166,  ..., 0.5960, 0.5610, 0.4335]],

         [[0.4547, 0.5528, 0.4516,  ..., 0.5737, 0.4666, 0.4364],
          [0.4806, 0.7001, 0.5436,  ..., 0.6610, 0.3057, 0.5252],
          [0.4064, 0.4506, 0.3621,  ..., 0.5746, 0.5780, 0.5043],
          [0.4896, 0.4967, 0.5157,  ..., 0.3442, 0.5465, 0.5222]],

         [[0.6160, 0.4943, 0.6114,  ..., 0.3442, 0.4254, 0.5341],
          [0.4297, 0.4889, 0.5660,  ..., 0.4833, 0.4373, 0.4297],
          [0.4135, 0.3285, 0.4325,  ..., 0.4422, 0.5299, 0.4826],
          [0.5222, 0.4921, 0.3882,  ..., 0.4069, 0.4438, 0.4771]]],


        [[[0.5234, 0.4349, 0.5581,  ..., 0.5126, 0.4012, 0.5424],
          [0.4804, 0.5344, 0.5033,  ..., 0.5010, 0.4225, 0.3757],
          [0.6270, 0.6044, 0.4576,  ..., 0.5974, 0.4908, 0.5036],
          [0.5703, 0.4273, 0.5566,  ..., 0.3840, 0.4651, 0.4254]],

         [[0.4106, 0.4121, 0.5722,  ..., 0.6211, 0.4158, 0.6451],
          [0.6307, 0.5220, 0.4249,  ..., 0.4730, 0.5765, 0.5799],
          [0.5751, 0.6868, 0.2999,  ..., 0.4499, 0.4613, 0.4632],
          [0.4642, 0.4681, 0.4448,  ..., 0.5528, 0.4395, 0.5115]],

         [[0.5061, 0.4239, 0.3390,  ..., 0.4948, 0.4545, 0.5655],
          [0.4547, 0.3504, 0.5789,  ..., 0.5761, 0.5588, 0.4917],
          [0.5978, 0.5499, 0.4441,  ..., 0.3675, 0.5804, 0.4873],
          [0.6406, 0.4301, 0.4395,  ..., 0.4073, 0.5300, 0.5737]],

         ...,

         [[0.3531, 0.4506, 0.6165,  ..., 0.5501, 0.6935, 0.3937],
          [0.5224, 0.4397, 0.5636,  ..., 0.5133, 0.4639, 0.4321],
          [0.5122, 0.5761, 0.5064,  ..., 0.3363, 0.4722, 0.5285],
          [0.4078, 0.4359, 0.4340,  ..., 0.4899, 0.6343, 0.5499]],

         [[0.4211, 0.6628, 0.5746,  ..., 0.6859, 0.4438, 0.4647],
          [0.5277, 0.4230, 0.5424,  ..., 0.6637, 0.4154, 0.5402],
          [0.5699, 0.5014, 0.4620,  ..., 0.5279, 0.5171, 0.4192],
          [0.4489, 0.5046, 0.3748,  ..., 0.4874, 0.5794, 0.6478]],

         [[0.5062, 0.5373, 0.6522,  ..., 0.4986, 0.4301, 0.3090],
          [0.5412, 0.5213, 0.6325,  ..., 0.3504, 0.5319, 0.6397],
          [0.5110, 0.5583, 0.4875,  ..., 0.5213, 0.6593, 0.4158],
          [0.5056, 0.4971, 0.3821,  ..., 0.5433, 0.5889, 0.5922]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0020,  0.0000, -0.0060,  0.0020, -0.0040, -0.0040,  0.0060,  0.0040,
         0.0000,  0.0040], device='cuda:0')
selected experts tensor([1712, 1572, 1708, 1650, 1612, 1576, 1681, 1746, 1563, 1564],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.3719, 0.5016, 0.3724,  ..., 0.4082, 0.5246, 0.5558],
          [0.4935, 0.5917, 0.4638,  ..., 0.3283, 0.4066, 0.5517],
          [0.5377, 0.4287, 0.4447,  ..., 0.5443, 0.4986, 0.5060],
          [0.5987, 0.4664, 0.5408,  ..., 0.4229, 0.3797, 0.4525]],

         [[0.3719, 0.5016, 0.3724,  ..., 0.4082, 0.5246, 0.5558],
          [0.4935, 0.5917, 0.4638,  ..., 0.3283, 0.4066, 0.5517],
          [0.5377, 0.4287, 0.4447,  ..., 0.5443, 0.4986, 0.5060],
          [0.5987, 0.4664, 0.5408,  ..., 0.4229, 0.3797, 0.4525]],

         [[0.6755, 0.5262, 0.5505,  ..., 0.5236, 0.5556, 0.3396],
          [0.3440, 0.4235, 0.5616,  ..., 0.4315, 0.5863, 0.3899],
          [0.5035, 0.4116, 0.3509,  ..., 0.5537, 0.4404, 0.5056],
          [0.6856, 0.6165, 0.5376,  ..., 0.5907, 0.5097, 0.3997]],

         ...,

         [[0.4044, 0.5395, 0.5295,  ..., 0.4011, 0.3806, 0.4505],
          [0.4399, 0.6016, 0.5805,  ..., 0.4167, 0.5551, 0.4724],
          [0.5774, 0.6370, 0.6028,  ..., 0.4517, 0.5490, 0.3529],
          [0.5542, 0.3720, 0.4662,  ..., 0.3959, 0.7853, 0.6377]],

         [[0.4906, 0.3711, 0.4322,  ..., 0.3449, 0.6075, 0.5136],
          [0.6449, 0.4931, 0.4541,  ..., 0.4390, 0.3607, 0.6089],
          [0.4634, 0.5224, 0.4375,  ..., 0.3655, 0.5652, 0.4939],
          [0.6314, 0.4922, 0.3820,  ..., 0.4806, 0.3894, 0.5831]],

         [[0.5717, 0.5380, 0.5207,  ..., 0.5539, 0.4010, 0.5611],
          [0.4787, 0.6316, 0.5052,  ..., 0.4665, 0.5971, 0.4644],
          [0.3601, 0.5794, 0.4906,  ..., 0.5319, 0.5444, 0.5316],
          [0.5241, 0.6072, 0.5929,  ..., 0.4658, 0.4747, 0.4501]]],


        [[[0.6377, 0.5139, 0.5146,  ..., 0.4399, 0.5285, 0.5793],
          [0.6573, 0.5207, 0.5590,  ..., 0.3601, 0.3697, 0.4484],
          [0.6071, 0.6007, 0.4691,  ..., 0.4619, 0.5858, 0.5392],
          [0.5445, 0.3684, 0.4080,  ..., 0.4320, 0.3982, 0.6476]],

         [[0.4210, 0.5501, 0.5650,  ..., 0.4614, 0.6168, 0.6071],
          [0.6024, 0.4340, 0.5316,  ..., 0.5433, 0.6685, 0.3700],
          [0.4370, 0.5765, 0.4265,  ..., 0.3248, 0.2966, 0.5141],
          [0.5333, 0.4419, 0.6214,  ..., 0.5561, 0.5236, 0.3992]],

         [[0.3413, 0.3933, 0.4752,  ..., 0.5599, 0.5577, 0.4850],
          [0.4742, 0.5012, 0.5834,  ..., 0.5590, 0.4841, 0.5018],
          [0.5534, 0.4121, 0.6365,  ..., 0.5479, 0.5345, 0.4406],
          [0.5363, 0.5051, 0.5272,  ..., 0.4353, 0.6383, 0.5406]],

         ...,

         [[0.4663, 0.4106, 0.6061,  ..., 0.5464, 0.6182, 0.6296],
          [0.5100, 0.5651, 0.5130,  ..., 0.5583, 0.6329, 0.5640],
          [0.5544, 0.4932, 0.5573,  ..., 0.5769, 0.5437, 0.4058],
          [0.4387, 0.5034, 0.4142,  ..., 0.4334, 0.5621, 0.4740]],

         [[0.4537, 0.6637, 0.4835,  ..., 0.4067, 0.5067, 0.5878],
          [0.4399, 0.5170, 0.5023,  ..., 0.5421, 0.5440, 0.6359],
          [0.4614, 0.3780, 0.3688,  ..., 0.3760, 0.3751, 0.2810],
          [0.6001, 0.3513, 0.4844,  ..., 0.5411, 0.3189, 0.4929]],

         [[0.4912, 0.5356, 0.5208,  ..., 0.5769, 0.4945, 0.5457],
          [0.4191, 0.6215, 0.6135,  ..., 0.5472, 0.4517, 0.3875],
          [0.6314, 0.5174, 0.4198,  ..., 0.5822, 0.4747, 0.5411],
          [0.4105, 0.4412, 0.5325,  ..., 0.5154, 0.4442, 0.6159]]]],
       device='cuda:0')
tensor([[[[0.3739, 0.5016, 0.3684,  ..., 0.4102, 0.5206, 0.5578],
          [0.4955, 0.5917, 0.4598,  ..., 0.3303, 0.4026, 0.5537],
          [0.5397, 0.4287, 0.4407,  ..., 0.5463, 0.4946, 0.5080],
          [0.6007, 0.4664, 0.5368,  ..., 0.4249, 0.3757, 0.4545]],

         [[0.3739, 0.5016, 0.3684,  ..., 0.4102, 0.5206, 0.5578],
          [0.4955, 0.5917, 0.4598,  ..., 0.3303, 0.4026, 0.5537],
          [0.5397, 0.4287, 0.4407,  ..., 0.5463, 0.4946, 0.5080],
          [0.6007, 0.4664, 0.5368,  ..., 0.4249, 0.3757, 0.4545]],

         [[0.6775, 0.5262, 0.5465,  ..., 0.5256, 0.5516, 0.3416],
          [0.3460, 0.4235, 0.5576,  ..., 0.4335, 0.5823, 0.3919],
          [0.5055, 0.4116, 0.3469,  ..., 0.5557, 0.4364, 0.5076],
          [0.6876, 0.6165, 0.5336,  ..., 0.5927, 0.5057, 0.4017]],

         ...,

         [[0.4064, 0.5395, 0.5255,  ..., 0.4031, 0.3766, 0.4525],
          [0.4419, 0.6016, 0.5765,  ..., 0.4187, 0.5511, 0.4744],
          [0.5794, 0.6370, 0.5988,  ..., 0.4537, 0.5450, 0.3549],
          [0.5562, 0.3720, 0.4622,  ..., 0.3979, 0.7813, 0.6397]],

         [[0.4926, 0.3711, 0.4282,  ..., 0.3469, 0.6035, 0.5156],
          [0.6469, 0.4931, 0.4501,  ..., 0.4410, 0.3567, 0.6109],
          [0.4654, 0.5224, 0.4335,  ..., 0.3675, 0.5612, 0.4959],
          [0.6334, 0.4922, 0.3780,  ..., 0.4826, 0.3854, 0.5851]],

         [[0.5737, 0.5380, 0.5167,  ..., 0.5559, 0.3970, 0.5631],
          [0.4807, 0.6316, 0.5012,  ..., 0.4685, 0.5931, 0.4664],
          [0.3621, 0.5794, 0.4866,  ..., 0.5339, 0.5404, 0.5336],
          [0.5261, 0.6072, 0.5889,  ..., 0.4678, 0.4707, 0.4521]]],


        [[[0.6397, 0.5139, 0.5106,  ..., 0.4419, 0.5245, 0.5813],
          [0.6593, 0.5207, 0.5550,  ..., 0.3621, 0.3657, 0.4504],
          [0.6091, 0.6007, 0.4651,  ..., 0.4639, 0.5818, 0.5412],
          [0.5465, 0.3684, 0.4040,  ..., 0.4340, 0.3942, 0.6496]],

         [[0.4230, 0.5501, 0.5610,  ..., 0.4634, 0.6128, 0.6091],
          [0.6044, 0.4340, 0.5276,  ..., 0.5453, 0.6645, 0.3720],
          [0.4390, 0.5765, 0.4225,  ..., 0.3268, 0.2926, 0.5161],
          [0.5353, 0.4419, 0.6174,  ..., 0.5581, 0.5196, 0.4012]],

         [[0.3433, 0.3933, 0.4712,  ..., 0.5619, 0.5537, 0.4870],
          [0.4762, 0.5012, 0.5794,  ..., 0.5610, 0.4801, 0.5038],
          [0.5554, 0.4121, 0.6325,  ..., 0.5499, 0.5305, 0.4426],
          [0.5383, 0.5051, 0.5232,  ..., 0.4373, 0.6343, 0.5426]],

         ...,

         [[0.4683, 0.4106, 0.6021,  ..., 0.5484, 0.6142, 0.6316],
          [0.5120, 0.5651, 0.5090,  ..., 0.5603, 0.6289, 0.5660],
          [0.5564, 0.4932, 0.5533,  ..., 0.5789, 0.5397, 0.4078],
          [0.4407, 0.5034, 0.4102,  ..., 0.4354, 0.5581, 0.4760]],

         [[0.4557, 0.6637, 0.4795,  ..., 0.4087, 0.5027, 0.5898],
          [0.4419, 0.5170, 0.4983,  ..., 0.5441, 0.5400, 0.6379],
          [0.4634, 0.3780, 0.3648,  ..., 0.3780, 0.3711, 0.2830],
          [0.6021, 0.3513, 0.4804,  ..., 0.5431, 0.3149, 0.4949]],

         [[0.4932, 0.5356, 0.5168,  ..., 0.5789, 0.4905, 0.5477],
          [0.4211, 0.6215, 0.6095,  ..., 0.5492, 0.4477, 0.3895],
          [0.6334, 0.5174, 0.4158,  ..., 0.5842, 0.4707, 0.5431],
          [0.4125, 0.4412, 0.5285,  ..., 0.5174, 0.4402, 0.6179]]]],
       device='cuda:0', requires_grad=True)
tensor([-2.0000e-03,  2.3283e-10,  4.0000e-03,  6.0000e-03,  0.0000e+00,
         6.0000e-03, -6.0000e-03, -2.0000e-03,  4.0000e-03, -2.0000e-03],
       device='cuda:0')
selected experts tensor([1771, 1639, 1556, 1535, 1647, 1556, 1886, 1665, 1641, 1488],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4920, 0.5165, 0.4801,  ..., 0.4674, 0.4803, 0.4780],
          [0.5049, 0.6074, 0.4497,  ..., 0.5375, 0.4804, 0.4474],
          [0.5104, 0.5391, 0.5867,  ..., 0.4301, 0.4583, 0.4381],
          [0.6542, 0.4238, 0.4753,  ..., 0.5055, 0.5217, 0.4894]],

         [[0.4910, 0.5159, 0.4806,  ..., 0.4686, 0.4790, 0.4778],
          [0.5047, 0.6069, 0.4509,  ..., 0.5360, 0.4793, 0.4470],
          [0.5110, 0.5377, 0.5867,  ..., 0.4310, 0.4581, 0.4395],
          [0.6533, 0.4242, 0.4742,  ..., 0.5052, 0.5221, 0.4879]],

         [[0.6462, 0.4857, 0.4222,  ..., 0.5392, 0.3868, 0.5554],
          [0.5163, 0.6102, 0.4634,  ..., 0.5693, 0.4595, 0.5493],
          [0.5306, 0.4209, 0.4523,  ..., 0.4973, 0.6484, 0.4438],
          [0.6105, 0.5205, 0.5591,  ..., 0.4629, 0.5911, 0.5409]],

         ...,

         [[0.6550, 0.4837, 0.6238,  ..., 0.5245, 0.4049, 0.4953],
          [0.4507, 0.5024, 0.4294,  ..., 0.5769, 0.5754, 0.5322],
          [0.4979, 0.3635, 0.6012,  ..., 0.4653, 0.5138, 0.3354],
          [0.4482, 0.6597, 0.4986,  ..., 0.6582, 0.4948, 0.4612]],

         [[0.5648, 0.3795, 0.5715,  ..., 0.4806, 0.4453, 0.4817],
          [0.4136, 0.5326, 0.5937,  ..., 0.5404, 0.4419, 0.5711],
          [0.5767, 0.4700, 0.6706,  ..., 0.4678, 0.5278, 0.5513],
          [0.5403, 0.3818, 0.5262,  ..., 0.5930, 0.4690, 0.5720]],

         [[0.6337, 0.4406, 0.5064,  ..., 0.3755, 0.4527, 0.5067],
          [0.5286, 0.5326, 0.4013,  ..., 0.4784, 0.3735, 0.6349],
          [0.4383, 0.4881, 0.3642,  ..., 0.4503, 0.6015, 0.5072],
          [0.5400, 0.5225, 0.5239,  ..., 0.5587, 0.4898, 0.5356]]],


        [[[0.5188, 0.5333, 0.4565,  ..., 0.5343, 0.4692, 0.5401],
          [0.5523, 0.6258, 0.5533,  ..., 0.5575, 0.5228, 0.4371],
          [0.5378, 0.3626, 0.5643,  ..., 0.3801, 0.4673, 0.4523],
          [0.6328, 0.5260, 0.5231,  ..., 0.5120, 0.5501, 0.6321]],

         [[0.5124, 0.5175, 0.4189,  ..., 0.4663, 0.5337, 0.4854],
          [0.5848, 0.6509, 0.4664,  ..., 0.4990, 0.3618, 0.3988],
          [0.4246, 0.5565, 0.5108,  ..., 0.4853, 0.4506, 0.3546],
          [0.6265, 0.5514, 0.5521,  ..., 0.5583, 0.5107, 0.5219]],

         [[0.5310, 0.6221, 0.5900,  ..., 0.5073, 0.4917, 0.4385],
          [0.5824, 0.5377, 0.3697,  ..., 0.6232, 0.4991, 0.4597],
          [0.5567, 0.4727, 0.6007,  ..., 0.3440, 0.5282, 0.5576],
          [0.5715, 0.4081, 0.5744,  ..., 0.5525, 0.4882, 0.5239]],

         ...,

         [[0.6183, 0.4982, 0.4923,  ..., 0.4445, 0.6029, 0.5477],
          [0.6192, 0.5333, 0.5128,  ..., 0.5131, 0.5059, 0.4357],
          [0.6757, 0.4266, 0.4094,  ..., 0.4320, 0.4352, 0.4682],
          [0.5562, 0.4737, 0.6319,  ..., 0.4563, 0.3432, 0.4242]],

         [[0.5135, 0.3653, 0.5077,  ..., 0.4334, 0.3937, 0.4959],
          [0.5552, 0.5873, 0.3131,  ..., 0.5260, 0.5715, 0.4842],
          [0.4280, 0.6258, 0.6373,  ..., 0.5029, 0.4803, 0.4651],
          [0.4313, 0.5659, 0.5933,  ..., 0.5208, 0.4138, 0.4496]],

         [[0.5720, 0.4828, 0.4427,  ..., 0.4426, 0.4537, 0.6076],
          [0.3966, 0.5839, 0.6192,  ..., 0.5413, 0.4542, 0.4634],
          [0.4604, 0.4459, 0.6123,  ..., 0.4243, 0.5128, 0.5825],
          [0.3561, 0.4587, 0.3225,  ..., 0.4857, 0.4809, 0.3184]]]],
       device='cuda:0')
tensor([[[[0.4980, 0.5205, 0.4861,  ..., 0.4694, 0.4743, 0.4720],
          [0.5109, 0.6114, 0.4557,  ..., 0.5395, 0.4744, 0.4414],
          [0.5164, 0.5431, 0.5927,  ..., 0.4321, 0.4523, 0.4321],
          [0.6602, 0.4278, 0.4813,  ..., 0.5075, 0.5157, 0.4834]],

         [[0.4970, 0.5199, 0.4866,  ..., 0.4706, 0.4730, 0.4718],
          [0.5107, 0.6109, 0.4569,  ..., 0.5380, 0.4733, 0.4410],
          [0.5170, 0.5417, 0.5927,  ..., 0.4330, 0.4521, 0.4335],
          [0.6593, 0.4282, 0.4802,  ..., 0.5072, 0.5161, 0.4819]],

         [[0.6522, 0.4897, 0.4282,  ..., 0.5412, 0.3808, 0.5494],
          [0.5223, 0.6142, 0.4694,  ..., 0.5713, 0.4535, 0.5433],
          [0.5366, 0.4249, 0.4583,  ..., 0.4993, 0.6424, 0.4378],
          [0.6165, 0.5245, 0.5651,  ..., 0.4649, 0.5851, 0.5349]],

         ...,

         [[0.6610, 0.4877, 0.6298,  ..., 0.5265, 0.3989, 0.4893],
          [0.4567, 0.5064, 0.4354,  ..., 0.5789, 0.5694, 0.5262],
          [0.5039, 0.3675, 0.6072,  ..., 0.4673, 0.5078, 0.3294],
          [0.4542, 0.6637, 0.5046,  ..., 0.6602, 0.4888, 0.4552]],

         [[0.5708, 0.3835, 0.5775,  ..., 0.4826, 0.4393, 0.4757],
          [0.4196, 0.5366, 0.5997,  ..., 0.5424, 0.4359, 0.5651],
          [0.5827, 0.4740, 0.6766,  ..., 0.4698, 0.5218, 0.5453],
          [0.5463, 0.3858, 0.5322,  ..., 0.5950, 0.4630, 0.5660]],

         [[0.6397, 0.4446, 0.5124,  ..., 0.3775, 0.4467, 0.5007],
          [0.5346, 0.5366, 0.4073,  ..., 0.4804, 0.3675, 0.6289],
          [0.4443, 0.4921, 0.3702,  ..., 0.4523, 0.5955, 0.5012],
          [0.5460, 0.5265, 0.5299,  ..., 0.5607, 0.4838, 0.5296]]],


        [[[0.5248, 0.5373, 0.4625,  ..., 0.5363, 0.4632, 0.5341],
          [0.5583, 0.6298, 0.5593,  ..., 0.5595, 0.5168, 0.4311],
          [0.5438, 0.3666, 0.5703,  ..., 0.3821, 0.4613, 0.4463],
          [0.6388, 0.5300, 0.5291,  ..., 0.5140, 0.5441, 0.6261]],

         [[0.5184, 0.5215, 0.4249,  ..., 0.4683, 0.5277, 0.4794],
          [0.5908, 0.6549, 0.4724,  ..., 0.5010, 0.3558, 0.3928],
          [0.4306, 0.5605, 0.5168,  ..., 0.4873, 0.4446, 0.3486],
          [0.6325, 0.5554, 0.5581,  ..., 0.5603, 0.5047, 0.5159]],

         [[0.5370, 0.6261, 0.5960,  ..., 0.5093, 0.4857, 0.4325],
          [0.5884, 0.5417, 0.3757,  ..., 0.6252, 0.4931, 0.4537],
          [0.5627, 0.4767, 0.6067,  ..., 0.3460, 0.5222, 0.5516],
          [0.5775, 0.4121, 0.5804,  ..., 0.5545, 0.4822, 0.5179]],

         ...,

         [[0.6243, 0.5022, 0.4983,  ..., 0.4465, 0.5969, 0.5417],
          [0.6252, 0.5373, 0.5188,  ..., 0.5151, 0.4999, 0.4297],
          [0.6817, 0.4306, 0.4154,  ..., 0.4340, 0.4292, 0.4622],
          [0.5622, 0.4777, 0.6379,  ..., 0.4583, 0.3372, 0.4182]],

         [[0.5195, 0.3693, 0.5137,  ..., 0.4354, 0.3877, 0.4899],
          [0.5612, 0.5913, 0.3191,  ..., 0.5280, 0.5655, 0.4782],
          [0.4340, 0.6298, 0.6433,  ..., 0.5049, 0.4743, 0.4591],
          [0.4373, 0.5699, 0.5993,  ..., 0.5228, 0.4078, 0.4436]],

         [[0.5780, 0.4868, 0.4487,  ..., 0.4446, 0.4477, 0.6016],
          [0.4026, 0.5879, 0.6252,  ..., 0.5433, 0.4482, 0.4574],
          [0.4664, 0.4499, 0.6183,  ..., 0.4263, 0.5068, 0.5765],
          [0.3621, 0.4627, 0.3285,  ..., 0.4877, 0.4749, 0.3124]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([-0.0060, -0.0040, -0.0060,  0.0060, -0.0060,  0.0060, -0.0060, -0.0020,
         0.0060,  0.0060], device='cuda:0')
selected experts tensor([2288, 1686, 1971, 1471, 1684,  820, 2189, 1573, 1423, 1279],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 768, 4, 10])
tensor([[[[0.4478, 0.4969, 0.4478,  ..., 0.3164, 0.4561, 0.5719],
          [0.4528, 0.3560, 0.5058,  ..., 0.6711, 0.4298, 0.4689],
          [0.5596, 0.6013, 0.4415,  ..., 0.4582, 0.5049, 0.3779],
          [0.5326, 0.4782, 0.5594,  ..., 0.5156, 0.5406, 0.4772]],

         [[0.5205, 0.4470, 0.5359,  ..., 0.3733, 0.4156, 0.5986],
          [0.5199, 0.4514, 0.5356,  ..., 0.6223, 0.4409, 0.4869],
          [0.4528, 0.5296, 0.4259,  ..., 0.3866, 0.4241, 0.3607],
          [0.5031, 0.5909, 0.6318,  ..., 0.5915, 0.3788, 0.3571]],

         [[0.5410, 0.5140, 0.5814,  ..., 0.4486, 0.2966, 0.4633],
          [0.4730, 0.3740, 0.4688,  ..., 0.4213, 0.4775, 0.5253],
          [0.3695, 0.5914, 0.3962,  ..., 0.5553, 0.4884, 0.3616],
          [0.4434, 0.4681, 0.6078,  ..., 0.4837, 0.5430, 0.2934]],

         ...,

         [[0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040]],

         [[0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040]],

         [[0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040]]],


        [[[0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040]],

         [[0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040]],

         [[0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040]],

         ...,

         [[0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040]],

         [[0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040]],

         [[0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040],
          [0.5020, 0.5020, 0.5020,  ..., 0.5040, 0.5040, 0.5040]]]],
       device='cuda:0')
tensor([[[[0.4458, 0.4949, 0.4458,  ..., 0.3124, 0.4521, 0.5679],
          [0.4508, 0.3540, 0.5038,  ..., 0.6671, 0.4258, 0.4649],
          [0.5576, 0.5993, 0.4395,  ..., 0.4542, 0.5009, 0.3739],
          [0.5306, 0.4762, 0.5574,  ..., 0.5116, 0.5366, 0.4732]],

         [[0.5185, 0.4450, 0.5339,  ..., 0.3693, 0.4116, 0.5946],
          [0.5179, 0.4494, 0.5336,  ..., 0.6183, 0.4369, 0.4829],
          [0.4508, 0.5276, 0.4239,  ..., 0.3826, 0.4201, 0.3567],
          [0.5011, 0.5889, 0.6298,  ..., 0.5875, 0.3748, 0.3531]],

         [[0.5390, 0.5120, 0.5794,  ..., 0.4446, 0.2926, 0.4593],
          [0.4710, 0.3720, 0.4668,  ..., 0.4173, 0.4735, 0.5213],
          [0.3675, 0.5894, 0.3942,  ..., 0.5513, 0.4844, 0.3576],
          [0.4414, 0.4661, 0.6058,  ..., 0.4797, 0.5390, 0.2894]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0020, 0.0020, 0.0020, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,
        0.0040], device='cuda:0')
selected experts tensor([ 410,  527,  471, 4232, 4872,  267,  346,  289,  320,  554],
       device='cuda:0')
total tokens tensor(12288, device='cuda:0')
attn_o for batch tensor([[1771., 1639., 1556., 1535., 1647., 1556., 1886., 1665., 1641., 1488.],
        [ 410.,  527.,  471., 4232., 4872.,  267.,  346.,  289.,  320.,  554.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4062, 0.4093, 0.4065,  ..., 0.5555, 0.4659, 0.4534],
          [0.7431, 0.4695, 0.5595,  ..., 0.5021, 0.5574, 0.5665],
          [0.4380, 0.4640, 0.4856,  ..., 0.4662, 0.4700, 0.6599],
          [0.4961, 0.4974, 0.3877,  ..., 0.6083, 0.4268, 0.5650]],

         [[0.3819, 0.4715, 0.3515,  ..., 0.5572, 0.4596, 0.5636],
          [0.4534, 0.5131, 0.5913,  ..., 0.5130, 0.6425, 0.4911],
          [0.3982, 0.5979, 0.5334,  ..., 0.4331, 0.4555, 0.5221],
          [0.6162, 0.4159, 0.4165,  ..., 0.6046, 0.5670, 0.4390]],

         [[0.4671, 0.5913, 0.3738,  ..., 0.4845, 0.4216, 0.5338],
          [0.4195, 0.5870, 0.6053,  ..., 0.5292, 0.6225, 0.5963],
          [0.5403, 0.6175, 0.6255,  ..., 0.2932, 0.4887, 0.5403],
          [0.5621, 0.5473, 0.5136,  ..., 0.3579, 0.4135, 0.5953]],

         ...,

         [[0.3828, 0.4545, 0.4685,  ..., 0.4536, 0.4216, 0.5628],
          [0.5807, 0.3532, 0.4889,  ..., 0.7402, 0.5321, 0.3344],
          [0.6572, 0.4632, 0.6039,  ..., 0.4934, 0.5775, 0.5849],
          [0.5520, 0.4504, 0.4785,  ..., 0.5240, 0.5356, 0.6127]],

         [[0.4998, 0.4436, 0.5201,  ..., 0.5108, 0.4441, 0.3881],
          [0.4517, 0.6059, 0.4660,  ..., 0.5881, 0.4946, 0.3908],
          [0.5379, 0.5342, 0.6002,  ..., 0.5202, 0.5300, 0.3590],
          [0.5712, 0.5170, 0.5308,  ..., 0.5709, 0.5794, 0.6206]],

         [[0.6580, 0.4758, 0.4987,  ..., 0.5990, 0.5042, 0.4488],
          [0.3663, 0.5211, 0.5624,  ..., 0.4089, 0.5761, 0.4242],
          [0.4396, 0.5593, 0.4337,  ..., 0.4751, 0.4790, 0.4828],
          [0.6098, 0.4853, 0.3207,  ..., 0.5186, 0.4197, 0.4190]]],


        [[[0.4551, 0.4374, 0.5974,  ..., 0.4874, 0.6461, 0.4076],
          [0.6340, 0.4379, 0.5262,  ..., 0.4384, 0.4725, 0.5763],
          [0.4839, 0.6861, 0.3614,  ..., 0.6037, 0.4383, 0.5782],
          [0.5411, 0.5262, 0.4501,  ..., 0.4784, 0.4625, 0.5437]],

         [[0.5108, 0.4518, 0.5506,  ..., 0.5642, 0.4473, 0.2709],
          [0.4152, 0.4316, 0.5031,  ..., 0.4604, 0.4355, 0.3987],
          [0.4020, 0.6082, 0.4681,  ..., 0.4881, 0.4706, 0.3913],
          [0.4901, 0.5024, 0.4994,  ..., 0.5867, 0.5651, 0.4123]],

         [[0.5140, 0.4822, 0.4543,  ..., 0.5876, 0.4691, 0.7361],
          [0.5416, 0.5923, 0.5714,  ..., 0.6684, 0.5514, 0.4905],
          [0.4243, 0.4240, 0.3872,  ..., 0.5408, 0.5965, 0.5097],
          [0.4842, 0.3667, 0.6390,  ..., 0.4976, 0.5295, 0.5815]],

         ...,

         [[0.4668, 0.4678, 0.5305,  ..., 0.5451, 0.5713, 0.4109],
          [0.4617, 0.4952, 0.4971,  ..., 0.5661, 0.4871, 0.6599],
          [0.5360, 0.6524, 0.5252,  ..., 0.5341, 0.4940, 0.3798],
          [0.5565, 0.5946, 0.4945,  ..., 0.2997, 0.5456, 0.4617]],

         [[0.5640, 0.5261, 0.5383,  ..., 0.4483, 0.4782, 0.4981],
          [0.4363, 0.4997, 0.5215,  ..., 0.4430, 0.3649, 0.3017],
          [0.4759, 0.3694, 0.4431,  ..., 0.5263, 0.5584, 0.5744],
          [0.4708, 0.5927, 0.5619,  ..., 0.5635, 0.5523, 0.4520]],

         [[0.6580, 0.4758, 0.4987,  ..., 0.5990, 0.5042, 0.4488],
          [0.3663, 0.5211, 0.5624,  ..., 0.4089, 0.5761, 0.4242],
          [0.4396, 0.5593, 0.4337,  ..., 0.4751, 0.4790, 0.4828],
          [0.6098, 0.4853, 0.3207,  ..., 0.5186, 0.4197, 0.4190]]]],
       device='cuda:0')
tensor([[[[0.4092, 0.4083, 0.4135,  ..., 0.5525, 0.4649, 0.4484],
          [0.7461, 0.4685, 0.5665,  ..., 0.4991, 0.5564, 0.5615],
          [0.4410, 0.4630, 0.4926,  ..., 0.4632, 0.4690, 0.6549],
          [0.4991, 0.4964, 0.3947,  ..., 0.6053, 0.4258, 0.5600]],

         [[0.3849, 0.4705, 0.3585,  ..., 0.5542, 0.4586, 0.5586],
          [0.4564, 0.5121, 0.5983,  ..., 0.5100, 0.6415, 0.4861],
          [0.4012, 0.5969, 0.5404,  ..., 0.4301, 0.4545, 0.5171],
          [0.6192, 0.4149, 0.4235,  ..., 0.6016, 0.5660, 0.4340]],

         [[0.4701, 0.5903, 0.3808,  ..., 0.4815, 0.4206, 0.5288],
          [0.4225, 0.5860, 0.6123,  ..., 0.5262, 0.6215, 0.5913],
          [0.5433, 0.6165, 0.6325,  ..., 0.2902, 0.4877, 0.5353],
          [0.5651, 0.5463, 0.5206,  ..., 0.3549, 0.4125, 0.5903]],

         ...,

         [[0.3858, 0.4535, 0.4755,  ..., 0.4506, 0.4206, 0.5578],
          [0.5837, 0.3522, 0.4959,  ..., 0.7372, 0.5311, 0.3294],
          [0.6602, 0.4622, 0.6109,  ..., 0.4904, 0.5765, 0.5799],
          [0.5550, 0.4494, 0.4855,  ..., 0.5210, 0.5346, 0.6077]],

         [[0.5028, 0.4426, 0.5271,  ..., 0.5078, 0.4431, 0.3831],
          [0.4547, 0.6049, 0.4730,  ..., 0.5851, 0.4936, 0.3858],
          [0.5409, 0.5332, 0.6072,  ..., 0.5172, 0.5290, 0.3540],
          [0.5742, 0.5160, 0.5378,  ..., 0.5679, 0.5784, 0.6156]],

         [[0.6610, 0.4748, 0.5057,  ..., 0.5960, 0.5032, 0.4438],
          [0.3693, 0.5201, 0.5694,  ..., 0.4059, 0.5751, 0.4192],
          [0.4426, 0.5583, 0.4407,  ..., 0.4721, 0.4780, 0.4778],
          [0.6128, 0.4843, 0.3277,  ..., 0.5156, 0.4187, 0.4140]]],


        [[[0.4581, 0.4364, 0.6044,  ..., 0.4844, 0.6451, 0.4026],
          [0.6370, 0.4369, 0.5332,  ..., 0.4354, 0.4715, 0.5713],
          [0.4869, 0.6851, 0.3684,  ..., 0.6007, 0.4373, 0.5732],
          [0.5441, 0.5252, 0.4571,  ..., 0.4754, 0.4615, 0.5387]],

         [[0.5138, 0.4508, 0.5576,  ..., 0.5612, 0.4463, 0.2659],
          [0.4182, 0.4306, 0.5101,  ..., 0.4574, 0.4345, 0.3937],
          [0.4050, 0.6072, 0.4751,  ..., 0.4851, 0.4696, 0.3863],
          [0.4931, 0.5014, 0.5064,  ..., 0.5837, 0.5641, 0.4073]],

         [[0.5170, 0.4812, 0.4613,  ..., 0.5846, 0.4681, 0.7311],
          [0.5446, 0.5913, 0.5784,  ..., 0.6654, 0.5504, 0.4855],
          [0.4273, 0.4230, 0.3942,  ..., 0.5378, 0.5955, 0.5047],
          [0.4872, 0.3657, 0.6460,  ..., 0.4946, 0.5285, 0.5765]],

         ...,

         [[0.4698, 0.4668, 0.5375,  ..., 0.5421, 0.5703, 0.4059],
          [0.4647, 0.4942, 0.5041,  ..., 0.5631, 0.4861, 0.6549],
          [0.5390, 0.6514, 0.5322,  ..., 0.5311, 0.4930, 0.3748],
          [0.5595, 0.5936, 0.5015,  ..., 0.2967, 0.5446, 0.4567]],

         [[0.5670, 0.5251, 0.5453,  ..., 0.4453, 0.4772, 0.4931],
          [0.4393, 0.4987, 0.5285,  ..., 0.4400, 0.3639, 0.2967],
          [0.4789, 0.3684, 0.4501,  ..., 0.5233, 0.5574, 0.5694],
          [0.4738, 0.5917, 0.5689,  ..., 0.5605, 0.5513, 0.4470]],

         [[0.6610, 0.4748, 0.5057,  ..., 0.5960, 0.5032, 0.4438],
          [0.3693, 0.5201, 0.5694,  ..., 0.4059, 0.5751, 0.4192],
          [0.4426, 0.5583, 0.4407,  ..., 0.4721, 0.4780, 0.4778],
          [0.6128, 0.4843, 0.3277,  ..., 0.5156, 0.4187, 0.4140]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0030,  0.0010, -0.0070,  0.0010, -0.0030, -0.0030,  0.0050,  0.0030,
         0.0010,  0.0050], device='cuda:0')
selected experts tensor([1692, 1646, 1556, 1642, 1667, 1752, 1571, 1603, 1612, 1643],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5731, 0.4595, 0.6233,  ..., 0.5152, 0.4863, 0.6916],
          [0.5236, 0.5111, 0.5592,  ..., 0.4791, 0.5321, 0.4753],
          [0.4655, 0.5602, 0.7034,  ..., 0.5226, 0.5476, 0.5407],
          [0.4488, 0.5931, 0.5854,  ..., 0.5121, 0.5500, 0.4477]],

         [[0.5797, 0.3647, 0.6113,  ..., 0.5783, 0.5709, 0.5171],
          [0.5649, 0.5832, 0.6057,  ..., 0.4360, 0.6158, 0.5322],
          [0.5391, 0.4945, 0.5191,  ..., 0.4869, 0.4597, 0.4792],
          [0.4631, 0.4957, 0.4438,  ..., 0.5476, 0.5066, 0.4523]],

         [[0.5294, 0.5075, 0.4558,  ..., 0.5664, 0.4935, 0.6090],
          [0.4875, 0.2796, 0.6028,  ..., 0.4803, 0.4887, 0.5334],
          [0.4110, 0.5329, 0.6617,  ..., 0.3982, 0.5843, 0.4720],
          [0.4578, 0.4115, 0.5018,  ..., 0.3879, 0.5212, 0.5535]],

         ...,

         [[0.4483, 0.5832, 0.5289,  ..., 0.4971, 0.6167, 0.4253],
          [0.4471, 0.5641, 0.5944,  ..., 0.5357, 0.4684, 0.5542],
          [0.5433, 0.5556, 0.5701,  ..., 0.5454, 0.3463, 0.4258],
          [0.4219, 0.5171, 0.5626,  ..., 0.4674, 0.5580, 0.3232]],

         [[0.4363, 0.6071, 0.5496,  ..., 0.3187, 0.5601, 0.5607],
          [0.4848, 0.2679, 0.4973,  ..., 0.4847, 0.5253, 0.4320],
          [0.3290, 0.3441, 0.4904,  ..., 0.4960, 0.5895, 0.6067],
          [0.5953, 0.5774, 0.4208,  ..., 0.5712, 0.6051, 0.4016]],

         [[0.5345, 0.6215, 0.3335,  ..., 0.4233, 0.4413, 0.6150],
          [0.4522, 0.3468, 0.4919,  ..., 0.5127, 0.4505, 0.4909],
          [0.5868, 0.4096, 0.4020,  ..., 0.4495, 0.5012, 0.4277],
          [0.3582, 0.6635, 0.3955,  ..., 0.3954, 0.6107, 0.5322]]],


        [[[0.4072, 0.6251, 0.4863,  ..., 0.5697, 0.6004, 0.6192],
          [0.3987, 0.5494, 0.3969,  ..., 0.4162, 0.6517, 0.4518],
          [0.4243, 0.4363, 0.5628,  ..., 0.5702, 0.4113, 0.4040],
          [0.6313, 0.6670, 0.5996,  ..., 0.5336, 0.4084, 0.6224]],

         [[0.4532, 0.5450, 0.3789,  ..., 0.5669, 0.5254, 0.5645],
          [0.4015, 0.5322, 0.4687,  ..., 0.5272, 0.6291, 0.5621],
          [0.4842, 0.6146, 0.5715,  ..., 0.4522, 0.3561, 0.5936],
          [0.4694, 0.5583, 0.6293,  ..., 0.5336, 0.5091, 0.5267]],

         [[0.3917, 0.6076, 0.5677,  ..., 0.5640, 0.6111, 0.4870],
          [0.4634, 0.4951, 0.6438,  ..., 0.4733, 0.4000, 0.4409],
          [0.5144, 0.4935, 0.4592,  ..., 0.6676, 0.5565, 0.4082],
          [0.4276, 0.5370, 0.5505,  ..., 0.4343, 0.4560, 0.5542]],

         ...,

         [[0.5205, 0.3853, 0.4313,  ..., 0.5986, 0.4842, 0.4837],
          [0.4067, 0.5105, 0.3457,  ..., 0.5261, 0.4132, 0.5324],
          [0.5740, 0.4593, 0.4043,  ..., 0.5873, 0.4808, 0.4566],
          [0.6112, 0.5556, 0.5758,  ..., 0.3838, 0.7128, 0.5612]],

         [[0.5233, 0.3345, 0.3581,  ..., 0.3773, 0.3696, 0.3415],
          [0.5721, 0.6635, 0.3617,  ..., 0.3773, 0.4298, 0.3476],
          [0.6231, 0.6279, 0.5024,  ..., 0.5072, 0.5345, 0.5860],
          [0.5299, 0.3979, 0.6730,  ..., 0.4691, 0.6310, 0.4758]],

         [[0.5345, 0.6215, 0.3335,  ..., 0.4233, 0.4413, 0.6150],
          [0.4522, 0.3468, 0.4919,  ..., 0.5127, 0.4505, 0.4909],
          [0.5868, 0.4096, 0.4020,  ..., 0.4495, 0.5012, 0.4277],
          [0.3582, 0.6635, 0.3955,  ..., 0.3954, 0.6107, 0.5322]]]],
       device='cuda:0')
tensor([[[[0.5761, 0.4605, 0.6183,  ..., 0.5182, 0.4833, 0.6926],
          [0.5266, 0.5121, 0.5542,  ..., 0.4821, 0.5291, 0.4763],
          [0.4685, 0.5612, 0.6984,  ..., 0.5256, 0.5446, 0.5417],
          [0.4518, 0.5941, 0.5804,  ..., 0.5151, 0.5470, 0.4487]],

         [[0.5827, 0.3657, 0.6063,  ..., 0.5813, 0.5679, 0.5181],
          [0.5679, 0.5842, 0.6007,  ..., 0.4390, 0.6128, 0.5332],
          [0.5421, 0.4955, 0.5141,  ..., 0.4899, 0.4567, 0.4802],
          [0.4661, 0.4967, 0.4388,  ..., 0.5506, 0.5036, 0.4533]],

         [[0.5324, 0.5085, 0.4508,  ..., 0.5694, 0.4905, 0.6100],
          [0.4905, 0.2806, 0.5978,  ..., 0.4833, 0.4857, 0.5344],
          [0.4140, 0.5339, 0.6567,  ..., 0.4012, 0.5813, 0.4730],
          [0.4608, 0.4125, 0.4968,  ..., 0.3909, 0.5182, 0.5545]],

         ...,

         [[0.4513, 0.5842, 0.5239,  ..., 0.5001, 0.6137, 0.4263],
          [0.4501, 0.5651, 0.5894,  ..., 0.5387, 0.4654, 0.5552],
          [0.5463, 0.5566, 0.5651,  ..., 0.5484, 0.3433, 0.4268],
          [0.4249, 0.5181, 0.5576,  ..., 0.4704, 0.5550, 0.3242]],

         [[0.4393, 0.6081, 0.5446,  ..., 0.3217, 0.5571, 0.5617],
          [0.4878, 0.2689, 0.4923,  ..., 0.4877, 0.5223, 0.4330],
          [0.3320, 0.3451, 0.4854,  ..., 0.4990, 0.5865, 0.6077],
          [0.5983, 0.5784, 0.4158,  ..., 0.5742, 0.6021, 0.4026]],

         [[0.5375, 0.6225, 0.3285,  ..., 0.4263, 0.4383, 0.6160],
          [0.4552, 0.3478, 0.4869,  ..., 0.5157, 0.4475, 0.4919],
          [0.5898, 0.4106, 0.3970,  ..., 0.4525, 0.4982, 0.4287],
          [0.3612, 0.6645, 0.3905,  ..., 0.3984, 0.6077, 0.5332]]],


        [[[0.4102, 0.6261, 0.4813,  ..., 0.5727, 0.5974, 0.6202],
          [0.4017, 0.5504, 0.3919,  ..., 0.4192, 0.6487, 0.4528],
          [0.4273, 0.4373, 0.5578,  ..., 0.5732, 0.4083, 0.4050],
          [0.6343, 0.6680, 0.5946,  ..., 0.5366, 0.4054, 0.6234]],

         [[0.4562, 0.5460, 0.3739,  ..., 0.5699, 0.5224, 0.5655],
          [0.4045, 0.5332, 0.4637,  ..., 0.5302, 0.6261, 0.5631],
          [0.4872, 0.6156, 0.5665,  ..., 0.4552, 0.3531, 0.5946],
          [0.4724, 0.5593, 0.6243,  ..., 0.5366, 0.5061, 0.5277]],

         [[0.3947, 0.6086, 0.5627,  ..., 0.5670, 0.6081, 0.4880],
          [0.4664, 0.4961, 0.6388,  ..., 0.4763, 0.3970, 0.4419],
          [0.5174, 0.4945, 0.4542,  ..., 0.6706, 0.5535, 0.4092],
          [0.4306, 0.5380, 0.5455,  ..., 0.4373, 0.4530, 0.5552]],

         ...,

         [[0.5235, 0.3863, 0.4263,  ..., 0.6016, 0.4812, 0.4847],
          [0.4097, 0.5115, 0.3407,  ..., 0.5291, 0.4102, 0.5334],
          [0.5770, 0.4603, 0.3993,  ..., 0.5903, 0.4778, 0.4576],
          [0.6142, 0.5566, 0.5708,  ..., 0.3868, 0.7098, 0.5622]],

         [[0.5263, 0.3355, 0.3531,  ..., 0.3803, 0.3666, 0.3425],
          [0.5751, 0.6645, 0.3567,  ..., 0.3803, 0.4268, 0.3486],
          [0.6261, 0.6289, 0.4974,  ..., 0.5102, 0.5315, 0.5870],
          [0.5329, 0.3989, 0.6680,  ..., 0.4721, 0.6280, 0.4768]],

         [[0.5375, 0.6225, 0.3285,  ..., 0.4263, 0.4383, 0.6160],
          [0.4552, 0.3478, 0.4869,  ..., 0.5157, 0.4475, 0.4919],
          [0.5898, 0.4106, 0.3970,  ..., 0.4525, 0.4982, 0.4287],
          [0.3612, 0.6645, 0.3905,  ..., 0.3984, 0.6077, 0.5332]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([-0.0030, -0.0010,  0.0050,  0.0070, -0.0010,  0.0070, -0.0070, -0.0030,
         0.0030, -0.0010], device='cuda:0')
selected experts tensor([1702, 1660, 1621, 1541, 1697, 1529, 1909, 1573, 1682, 1470],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1702., 1660., 1621., 1541., 1697., 1529., 1909., 1573., 1682., 1470.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4772, 0.3882, 0.5918,  ..., 0.4260, 0.6507, 0.4066],
          [0.4799, 0.5045, 0.5390,  ..., 0.5168, 0.6516, 0.4592],
          [0.4019, 0.5237, 0.5562,  ..., 0.3097, 0.4572, 0.4561],
          [0.6321, 0.4496, 0.6096,  ..., 0.5796, 0.5366, 0.4594]],

         [[0.5265, 0.3603, 0.5838,  ..., 0.4689, 0.3220, 0.6232],
          [0.5005, 0.2806, 0.4981,  ..., 0.5283, 0.5800, 0.4611],
          [0.6649, 0.4390, 0.5286,  ..., 0.4648, 0.4408, 0.4341],
          [0.5379, 0.4758, 0.5241,  ..., 0.6580, 0.6551, 0.5164]],

         [[0.5052, 0.6775, 0.4847,  ..., 0.4109, 0.6281, 0.4435],
          [0.4604, 0.4268, 0.4309,  ..., 0.5211, 0.4893, 0.6093],
          [0.5702, 0.4163, 0.3734,  ..., 0.5308, 0.5238, 0.4769],
          [0.4118, 0.4073, 0.5279,  ..., 0.4979, 0.4908, 0.4118]],

         ...,

         [[0.4214, 0.6951, 0.2898,  ..., 0.5461, 0.4117, 0.4691],
          [0.5369, 0.4779, 0.4925,  ..., 0.4471, 0.3740, 0.4899],
          [0.5529, 0.4802, 0.5211,  ..., 0.4544, 0.4065, 0.6292],
          [0.4285, 0.3016, 0.5105,  ..., 0.4524, 0.7182, 0.4236]],

         [[0.6570, 0.4744, 0.4997,  ..., 0.6000, 0.5054, 0.4478],
          [0.3653, 0.5201, 0.5634,  ..., 0.4099, 0.5766, 0.4227],
          [0.4386, 0.5581, 0.4347,  ..., 0.4761, 0.4803, 0.4817],
          [0.6088, 0.4844, 0.3217,  ..., 0.5195, 0.4207, 0.4175]],

         [[0.5625, 0.4268, 0.5383,  ..., 0.6356, 0.2717, 0.6473],
          [0.4981, 0.4479, 0.3165,  ..., 0.5820, 0.4664, 0.6365],
          [0.4490, 0.5455, 0.6319,  ..., 0.4517, 0.4112, 0.5919],
          [0.4522, 0.4673, 0.5403,  ..., 0.5791, 0.5265, 0.6292]]],


        [[[0.6447, 0.4704, 0.4037,  ..., 0.5536, 0.6120, 0.4645],
          [0.3617, 0.5655, 0.5533,  ..., 0.5007, 0.6507, 0.4361],
          [0.5110, 0.5412, 0.6355,  ..., 0.5181, 0.6381, 0.5398],
          [0.5502, 0.5765, 0.3706,  ..., 0.3661, 0.2995, 0.3105]],

         [[0.4367, 0.5879, 0.6210,  ..., 0.5777, 0.3271, 0.5568],
          [0.4594, 0.5178, 0.4376,  ..., 0.4937, 0.5192, 0.5083],
          [0.4394, 0.6044, 0.4791,  ..., 0.5109, 0.4541, 0.3197],
          [0.5172, 0.5612, 0.4908,  ..., 0.5376, 0.5252, 0.5490]],

         [[0.4285, 0.6442, 0.4673,  ..., 0.6737, 0.5326, 0.4573],
          [0.4261, 0.4644, 0.3426,  ..., 0.5386, 0.4074, 0.4945],
          [0.4804, 0.4940, 0.6373,  ..., 0.4909, 0.5728, 0.4716],
          [0.5778, 0.5178, 0.5786,  ..., 0.5286, 0.3357, 0.4954]],

         ...,

         [[0.6500, 0.4026, 0.4611,  ..., 0.4161, 0.4684, 0.4208],
          [0.5027, 0.4441, 0.5914,  ..., 0.3706, 0.5383, 0.5071],
          [0.6294, 0.3320, 0.3651,  ..., 0.3571, 0.5276, 0.5331],
          [0.3000, 0.4287, 0.5667,  ..., 0.4814, 0.4948, 0.5006]],

         [[0.5635, 0.5271, 0.5705,  ..., 0.6301, 0.3962, 0.4718],
          [0.5478, 0.4492, 0.5672,  ..., 0.4798, 0.6050, 0.4361],
          [0.5133, 0.3372, 0.5571,  ..., 0.4999, 0.4930, 0.6703],
          [0.6276, 0.5842, 0.5881,  ..., 0.6135, 0.3445, 0.3598]],

         [[0.6065, 0.4880, 0.5055,  ..., 0.4275, 0.4046, 0.5435],
          [0.4047, 0.5578, 0.4621,  ..., 0.6163, 0.6327, 0.4906],
          [0.5289, 0.4721, 0.5175,  ..., 0.5657, 0.5398, 0.3598],
          [0.5514, 0.5492, 0.4485,  ..., 0.6223, 0.5168, 0.5154]]]],
       device='cuda:0')
tensor([[[[0.4812, 0.3882, 0.5978,  ..., 0.4220, 0.6487, 0.4026],
          [0.4839, 0.5045, 0.5450,  ..., 0.5128, 0.6496, 0.4552],
          [0.4059, 0.5237, 0.5622,  ..., 0.3057, 0.4552, 0.4521],
          [0.6361, 0.4496, 0.6156,  ..., 0.5756, 0.5346, 0.4554]],

         [[0.5305, 0.3603, 0.5898,  ..., 0.4649, 0.3200, 0.6192],
          [0.5045, 0.2806, 0.5041,  ..., 0.5243, 0.5780, 0.4571],
          [0.6689, 0.4390, 0.5346,  ..., 0.4608, 0.4388, 0.4301],
          [0.5419, 0.4758, 0.5301,  ..., 0.6540, 0.6531, 0.5124]],

         [[0.5092, 0.6775, 0.4907,  ..., 0.4069, 0.6261, 0.4395],
          [0.4644, 0.4268, 0.4369,  ..., 0.5171, 0.4873, 0.6053],
          [0.5742, 0.4163, 0.3794,  ..., 0.5268, 0.5218, 0.4729],
          [0.4158, 0.4073, 0.5339,  ..., 0.4939, 0.4888, 0.4078]],

         ...,

         [[0.4254, 0.6951, 0.2958,  ..., 0.5421, 0.4097, 0.4651],
          [0.5409, 0.4779, 0.4985,  ..., 0.4431, 0.3720, 0.4859],
          [0.5569, 0.4802, 0.5271,  ..., 0.4504, 0.4045, 0.6252],
          [0.4325, 0.3016, 0.5165,  ..., 0.4484, 0.7162, 0.4196]],

         [[0.6610, 0.4744, 0.5057,  ..., 0.5960, 0.5034, 0.4438],
          [0.3693, 0.5201, 0.5694,  ..., 0.4059, 0.5746, 0.4187],
          [0.4426, 0.5581, 0.4407,  ..., 0.4721, 0.4783, 0.4777],
          [0.6128, 0.4844, 0.3277,  ..., 0.5155, 0.4187, 0.4135]],

         [[0.5665, 0.4268, 0.5443,  ..., 0.6316, 0.2697, 0.6433],
          [0.5021, 0.4479, 0.3225,  ..., 0.5780, 0.4644, 0.6325],
          [0.4530, 0.5455, 0.6379,  ..., 0.4477, 0.4092, 0.5879],
          [0.4562, 0.4673, 0.5463,  ..., 0.5751, 0.5245, 0.6252]]],


        [[[0.6487, 0.4704, 0.4097,  ..., 0.5496, 0.6100, 0.4605],
          [0.3657, 0.5655, 0.5593,  ..., 0.4967, 0.6487, 0.4321],
          [0.5150, 0.5412, 0.6415,  ..., 0.5141, 0.6361, 0.5358],
          [0.5542, 0.5765, 0.3766,  ..., 0.3621, 0.2975, 0.3065]],

         [[0.4407, 0.5879, 0.6270,  ..., 0.5737, 0.3251, 0.5528],
          [0.4634, 0.5178, 0.4436,  ..., 0.4897, 0.5172, 0.5043],
          [0.4434, 0.6044, 0.4851,  ..., 0.5069, 0.4521, 0.3157],
          [0.5212, 0.5612, 0.4968,  ..., 0.5336, 0.5232, 0.5450]],

         [[0.4325, 0.6442, 0.4733,  ..., 0.6697, 0.5306, 0.4533],
          [0.4301, 0.4644, 0.3486,  ..., 0.5346, 0.4054, 0.4905],
          [0.4844, 0.4940, 0.6433,  ..., 0.4869, 0.5708, 0.4676],
          [0.5818, 0.5178, 0.5846,  ..., 0.5246, 0.3337, 0.4914]],

         ...,

         [[0.6540, 0.4026, 0.4671,  ..., 0.4121, 0.4664, 0.4168],
          [0.5067, 0.4441, 0.5974,  ..., 0.3666, 0.5363, 0.5031],
          [0.6334, 0.3320, 0.3711,  ..., 0.3531, 0.5256, 0.5291],
          [0.3040, 0.4287, 0.5727,  ..., 0.4774, 0.4928, 0.4966]],

         [[0.5675, 0.5271, 0.5765,  ..., 0.6261, 0.3942, 0.4678],
          [0.5518, 0.4492, 0.5732,  ..., 0.4758, 0.6030, 0.4321],
          [0.5173, 0.3372, 0.5631,  ..., 0.4959, 0.4910, 0.6663],
          [0.6316, 0.5842, 0.5941,  ..., 0.6095, 0.3425, 0.3558]],

         [[0.6105, 0.4880, 0.5115,  ..., 0.4235, 0.4026, 0.5395],
          [0.4087, 0.5578, 0.4681,  ..., 0.6123, 0.6307, 0.4866],
          [0.5329, 0.4721, 0.5235,  ..., 0.5617, 0.5378, 0.3558],
          [0.5554, 0.5492, 0.4545,  ..., 0.6183, 0.5148, 0.5114]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0040,  0.0000, -0.0060,  0.0000, -0.0040, -0.0040,  0.0060,  0.0040,
         0.0020,  0.0040], device='cuda:0')
selected experts tensor([1576, 1588, 1763, 1558, 1678, 1630, 1648, 1600, 1721, 1622],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5099, 0.6094, 0.4929,  ..., 0.4876, 0.4774, 0.4884],
          [0.6088, 0.6001, 0.5944,  ..., 0.3335, 0.5923, 0.4111],
          [0.5100, 0.5674, 0.5537,  ..., 0.3992, 0.4365, 0.3639],
          [0.4114, 0.5611, 0.6071,  ..., 0.4840, 0.5603, 0.4706]],

         [[0.4295, 0.4435, 0.5321,  ..., 0.4617, 0.4155, 0.5341],
          [0.4703, 0.4349, 0.5653,  ..., 0.5384, 0.5989, 0.5894],
          [0.4281, 0.6305, 0.5130,  ..., 0.6660, 0.4871, 0.5139],
          [0.4128, 0.4281, 0.4195,  ..., 0.5764, 0.6176, 0.5226]],

         [[0.4937, 0.5784, 0.5194,  ..., 0.5926, 0.4560, 0.4805],
          [0.4914, 0.4614, 0.5289,  ..., 0.3020, 0.4379, 0.4828],
          [0.4156, 0.5350, 0.3538,  ..., 0.5159, 0.5225, 0.6123],
          [0.3907, 0.5712, 0.4338,  ..., 0.6555, 0.5536, 0.4301]],

         ...,

         [[0.3768, 0.5213, 0.5730,  ..., 0.4781, 0.6846, 0.4929],
          [0.4626, 0.4830, 0.5614,  ..., 0.4886, 0.5215, 0.6192],
          [0.5374, 0.5329, 0.5484,  ..., 0.4728, 0.5579, 0.5027],
          [0.4768, 0.5616, 0.5372,  ..., 0.5236, 0.6083, 0.3868]],

         [[0.5330, 0.6205, 0.3345,  ..., 0.4243, 0.4401, 0.6160],
          [0.4512, 0.3458, 0.4929,  ..., 0.5137, 0.4495, 0.4918],
          [0.5858, 0.4086, 0.4030,  ..., 0.4505, 0.5002, 0.4287],
          [0.3572, 0.6617, 0.3965,  ..., 0.3964, 0.6097, 0.5329]],

         [[0.7018, 0.2778, 0.6053,  ..., 0.4272, 0.5514, 0.4116],
          [0.5318, 0.5285, 0.5253,  ..., 0.4850, 0.3704, 0.4407],
          [0.4137, 0.5655, 0.4067,  ..., 0.5214, 0.5197, 0.5851],
          [0.5953, 0.5599, 0.4795,  ..., 0.5168, 0.6041, 0.4782]]],


        [[[0.4915, 0.4229, 0.4204,  ..., 0.3992, 0.5526, 0.4777],
          [0.5384, 0.5329, 0.5231,  ..., 0.7190, 0.4681, 0.4268],
          [0.4161, 0.5133, 0.4338,  ..., 0.5365, 0.6263, 0.5898],
          [0.5262, 0.3396, 0.4486,  ..., 0.4571, 0.4652, 0.4891]],

         [[0.5901, 0.3413, 0.6403,  ..., 0.4573, 0.6031, 0.6298],
          [0.4960, 0.4692, 0.6618,  ..., 0.4869, 0.3750, 0.3711],
          [0.6657, 0.4486, 0.5973,  ..., 0.5616, 0.5502, 0.4789],
          [0.5505, 0.6440, 0.4204,  ..., 0.5430, 0.4723, 0.4692]],

         [[0.4845, 0.4762, 0.4921,  ..., 0.4430, 0.3851, 0.5997],
          [0.6046, 0.5421, 0.5646,  ..., 0.6122, 0.3832, 0.4381],
          [0.4137, 0.5309, 0.4039,  ..., 0.5292, 0.6231, 0.3141],
          [0.5279, 0.3945, 0.5105,  ..., 0.4358, 0.5347, 0.3849]],

         ...,

         [[0.3699, 0.4999, 0.3735,  ..., 0.4077, 0.5225, 0.5576],
          [0.4912, 0.5897, 0.4658,  ..., 0.3283, 0.4046, 0.5535],
          [0.5355, 0.4267, 0.4467,  ..., 0.5443, 0.4966, 0.5080],
          [0.5967, 0.4639, 0.5430,  ..., 0.4224, 0.3777, 0.4545]],

         [[0.6330, 0.4721, 0.5116,  ..., 0.5039, 0.4647, 0.5079],
          [0.3271, 0.5530, 0.3450,  ..., 0.4344, 0.6372, 0.6100],
          [0.3837, 0.5874, 0.5520,  ..., 0.3511, 0.6795, 0.5007],
          [0.4502, 0.4875, 0.3891,  ..., 0.6485, 0.5562, 0.3451]],

         [[0.5345, 0.3493, 0.4530,  ..., 0.4646, 0.6444, 0.4542],
          [0.6474, 0.3945, 0.4692,  ..., 0.6476, 0.5680, 0.5246],
          [0.6125, 0.6047, 0.4731,  ..., 0.3646, 0.4774, 0.5737],
          [0.3545, 0.4296, 0.4119,  ..., 0.4771, 0.4591, 0.4723]]]],
       device='cuda:0')
tensor([[[[0.5139, 0.6114, 0.4869,  ..., 0.4896, 0.4754, 0.4884],
          [0.6128, 0.6021, 0.5884,  ..., 0.3355, 0.5903, 0.4111],
          [0.5140, 0.5694, 0.5477,  ..., 0.4012, 0.4345, 0.3639],
          [0.4154, 0.5631, 0.6011,  ..., 0.4860, 0.5583, 0.4706]],

         [[0.4335, 0.4455, 0.5261,  ..., 0.4637, 0.4135, 0.5341],
          [0.4743, 0.4369, 0.5593,  ..., 0.5404, 0.5969, 0.5894],
          [0.4321, 0.6325, 0.5070,  ..., 0.6680, 0.4851, 0.5139],
          [0.4168, 0.4301, 0.4135,  ..., 0.5784, 0.6156, 0.5226]],

         [[0.4977, 0.5804, 0.5134,  ..., 0.5946, 0.4540, 0.4805],
          [0.4954, 0.4634, 0.5229,  ..., 0.3040, 0.4359, 0.4828],
          [0.4196, 0.5370, 0.3478,  ..., 0.5179, 0.5205, 0.6123],
          [0.3947, 0.5732, 0.4278,  ..., 0.6575, 0.5516, 0.4301]],

         ...,

         [[0.3808, 0.5233, 0.5670,  ..., 0.4801, 0.6826, 0.4929],
          [0.4666, 0.4850, 0.5554,  ..., 0.4906, 0.5195, 0.6192],
          [0.5414, 0.5349, 0.5424,  ..., 0.4748, 0.5559, 0.5027],
          [0.4808, 0.5636, 0.5312,  ..., 0.5256, 0.6063, 0.3868]],

         [[0.5370, 0.6225, 0.3285,  ..., 0.4263, 0.4381, 0.6160],
          [0.4552, 0.3478, 0.4869,  ..., 0.5157, 0.4475, 0.4918],
          [0.5898, 0.4106, 0.3970,  ..., 0.4525, 0.4982, 0.4287],
          [0.3612, 0.6637, 0.3905,  ..., 0.3984, 0.6077, 0.5329]],

         [[0.7058, 0.2798, 0.5993,  ..., 0.4292, 0.5494, 0.4116],
          [0.5358, 0.5305, 0.5193,  ..., 0.4870, 0.3684, 0.4407],
          [0.4177, 0.5675, 0.4007,  ..., 0.5234, 0.5177, 0.5851],
          [0.5993, 0.5619, 0.4735,  ..., 0.5188, 0.6021, 0.4782]]],


        [[[0.4955, 0.4249, 0.4144,  ..., 0.4012, 0.5506, 0.4777],
          [0.5424, 0.5349, 0.5171,  ..., 0.7210, 0.4661, 0.4268],
          [0.4201, 0.5153, 0.4278,  ..., 0.5385, 0.6243, 0.5898],
          [0.5302, 0.3416, 0.4426,  ..., 0.4591, 0.4632, 0.4891]],

         [[0.5941, 0.3433, 0.6343,  ..., 0.4593, 0.6011, 0.6298],
          [0.5000, 0.4712, 0.6558,  ..., 0.4889, 0.3730, 0.3711],
          [0.6697, 0.4506, 0.5913,  ..., 0.5636, 0.5482, 0.4789],
          [0.5545, 0.6460, 0.4144,  ..., 0.5450, 0.4703, 0.4692]],

         [[0.4885, 0.4782, 0.4861,  ..., 0.4450, 0.3831, 0.5997],
          [0.6086, 0.5441, 0.5586,  ..., 0.6142, 0.3812, 0.4381],
          [0.4177, 0.5329, 0.3979,  ..., 0.5312, 0.6211, 0.3141],
          [0.5319, 0.3965, 0.5045,  ..., 0.4378, 0.5327, 0.3849]],

         ...,

         [[0.3739, 0.5019, 0.3675,  ..., 0.4097, 0.5205, 0.5576],
          [0.4952, 0.5917, 0.4598,  ..., 0.3303, 0.4026, 0.5535],
          [0.5395, 0.4287, 0.4407,  ..., 0.5463, 0.4946, 0.5080],
          [0.6007, 0.4659, 0.5370,  ..., 0.4244, 0.3757, 0.4545]],

         [[0.6370, 0.4741, 0.5056,  ..., 0.5059, 0.4627, 0.5079],
          [0.3311, 0.5550, 0.3390,  ..., 0.4364, 0.6352, 0.6100],
          [0.3877, 0.5894, 0.5460,  ..., 0.3531, 0.6775, 0.5007],
          [0.4542, 0.4895, 0.3831,  ..., 0.6505, 0.5542, 0.3451]],

         [[0.5385, 0.3513, 0.4470,  ..., 0.4666, 0.6424, 0.4542],
          [0.6514, 0.3965, 0.4632,  ..., 0.6496, 0.5660, 0.5246],
          [0.6165, 0.6067, 0.4671,  ..., 0.3666, 0.4754, 0.5737],
          [0.3585, 0.4316, 0.4059,  ..., 0.4791, 0.4571, 0.4723]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0040, -0.0020,  0.0060,  0.0080, -0.0020,  0.0080, -0.0080, -0.0020,
         0.0020,  0.0000], device='cuda:0')
selected experts tensor([1585, 1596, 1713, 1593, 1538, 1654, 1698, 1839, 1651, 1517],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4747, 0.5367, 0.4701,  ..., 0.4409, 0.5436, 0.6161],
          [0.4472, 0.4601, 0.4241,  ..., 0.6378, 0.5864, 0.2790],
          [0.5283, 0.5074, 0.5714,  ..., 0.4549, 0.3956, 0.5292],
          [0.5414, 0.6266, 0.4993,  ..., 0.6182, 0.4077, 0.4451]],

         [[0.6164, 0.4621, 0.5506,  ..., 0.4679, 0.5231, 0.4391],
          [0.5235, 0.5773, 0.4303,  ..., 0.6816, 0.4852, 0.3399],
          [0.5795, 0.4280, 0.4796,  ..., 0.4861, 0.5465, 0.4191],
          [0.5029, 0.5050, 0.6255,  ..., 0.5813, 0.5954, 0.5088]],

         [[0.5334, 0.5299, 0.5414,  ..., 0.3955, 0.3998, 0.4276],
          [0.4658, 0.4980, 0.3952,  ..., 0.6081, 0.4782, 0.4195],
          [0.5781, 0.6106, 0.5772,  ..., 0.5161, 0.4831, 0.4675],
          [0.5431, 0.4878, 0.5757,  ..., 0.5091, 0.4910, 0.5569]],

         ...,

         [[0.5446, 0.5758, 0.6653,  ..., 0.5375, 0.5612, 0.4484],
          [0.4976, 0.5872, 0.4608,  ..., 0.5813, 0.5178, 0.5569],
          [0.5081, 0.4483, 0.5313,  ..., 0.6974, 0.6395, 0.3477],
          [0.5162, 0.5211, 0.5974,  ..., 0.5129, 0.5221, 0.5634]],

         [[0.6756, 0.4170, 0.5363,  ..., 0.5001, 0.5470, 0.4830],
          [0.5339, 0.5677, 0.3961,  ..., 0.5846, 0.4811, 0.6184],
          [0.4311, 0.4019, 0.5419,  ..., 0.4301, 0.5511, 0.3850],
          [0.6058, 0.6365, 0.4891,  ..., 0.4920, 0.4031, 0.5697]],

         [[0.5368, 0.4741, 0.6011,  ..., 0.4937, 0.4831, 0.3355],
          [0.5137, 0.4495, 0.3641,  ..., 0.4040, 0.5203, 0.4976],
          [0.4586, 0.3340, 0.5354,  ..., 0.6816, 0.4935, 0.4860],
          [0.5264, 0.4567, 0.5537,  ..., 0.6178, 0.3673, 0.5477]]],


        [[[0.5283, 0.4360, 0.5347,  ..., 0.4191, 0.4219, 0.5416],
          [0.5566, 0.5007, 0.5036,  ..., 0.5950, 0.4205, 0.4906],
          [0.7012, 0.4108, 0.6532,  ..., 0.5132, 0.5533, 0.4900],
          [0.6939, 0.5605, 0.6058,  ..., 0.4898, 0.4371, 0.5644]],

         [[0.5030, 0.6293, 0.5429,  ..., 0.5053, 0.5039, 0.6322],
          [0.4279, 0.7040, 0.5443,  ..., 0.6132, 0.4673, 0.4424],
          [0.4236, 0.4463, 0.5667,  ..., 0.4612, 0.6105, 0.4830],
          [0.4579, 0.4422, 0.4972,  ..., 0.5600, 0.4909, 0.4566]],

         [[0.5133, 0.4832, 0.6980,  ..., 0.4537, 0.4542, 0.4709],
          [0.5847, 0.5749, 0.5614,  ..., 0.5556, 0.4228, 0.4748],
          [0.6773, 0.4471, 0.5561,  ..., 0.5433, 0.6413, 0.6221],
          [0.5134, 0.4242, 0.5393,  ..., 0.5540, 0.3512, 0.6198]],

         ...,

         [[0.4311, 0.4822, 0.4731,  ..., 0.5309, 0.3637, 0.4419],
          [0.5060, 0.5371, 0.4217,  ..., 0.5813, 0.5491, 0.5921],
          [0.4764, 0.4991, 0.6470,  ..., 0.5448, 0.4844, 0.4059],
          [0.5681, 0.5867, 0.6645,  ..., 0.4229, 0.3878, 0.5045]],

         [[0.5402, 0.3739, 0.5198,  ..., 0.3665, 0.5479, 0.4120],
          [0.6947, 0.4757, 0.5187,  ..., 0.4450, 0.5313, 0.6680],
          [0.3533, 0.4189, 0.6127,  ..., 0.6251, 0.5445, 0.5337],
          [0.4361, 0.4829, 0.4678,  ..., 0.5576, 0.4607, 0.5816]],

         [[0.5249, 0.4396, 0.5090,  ..., 0.4987, 0.5281, 0.5617],
          [0.4472, 0.4940, 0.5136,  ..., 0.4914, 0.4945, 0.4680],
          [0.4079, 0.4688, 0.5492,  ..., 0.4320, 0.4644, 0.4376],
          [0.4670, 0.6302, 0.6282,  ..., 0.3770, 0.3637, 0.3503]]]],
       device='cuda:0')
tensor([[[[0.4817, 0.5417, 0.4771,  ..., 0.4419, 0.5366, 0.6091],
          [0.4542, 0.4651, 0.4311,  ..., 0.6388, 0.5794, 0.2720],
          [0.5353, 0.5124, 0.5784,  ..., 0.4559, 0.3886, 0.5222],
          [0.5484, 0.6316, 0.5063,  ..., 0.6192, 0.4007, 0.4381]],

         [[0.6234, 0.4671, 0.5576,  ..., 0.4689, 0.5161, 0.4321],
          [0.5305, 0.5823, 0.4373,  ..., 0.6826, 0.4782, 0.3329],
          [0.5865, 0.4330, 0.4866,  ..., 0.4871, 0.5395, 0.4121],
          [0.5099, 0.5100, 0.6325,  ..., 0.5823, 0.5884, 0.5018]],

         [[0.5404, 0.5349, 0.5484,  ..., 0.3965, 0.3928, 0.4206],
          [0.4728, 0.5030, 0.4022,  ..., 0.6091, 0.4712, 0.4125],
          [0.5851, 0.6156, 0.5842,  ..., 0.5171, 0.4761, 0.4605],
          [0.5501, 0.4928, 0.5827,  ..., 0.5101, 0.4840, 0.5499]],

         ...,

         [[0.5516, 0.5808, 0.6723,  ..., 0.5385, 0.5542, 0.4414],
          [0.5046, 0.5922, 0.4678,  ..., 0.5823, 0.5108, 0.5499],
          [0.5151, 0.4533, 0.5383,  ..., 0.6984, 0.6325, 0.3407],
          [0.5232, 0.5261, 0.6044,  ..., 0.5139, 0.5151, 0.5564]],

         [[0.6826, 0.4220, 0.5433,  ..., 0.5011, 0.5400, 0.4760],
          [0.5409, 0.5727, 0.4031,  ..., 0.5856, 0.4741, 0.6114],
          [0.4381, 0.4069, 0.5489,  ..., 0.4311, 0.5441, 0.3780],
          [0.6128, 0.6415, 0.4961,  ..., 0.4930, 0.3961, 0.5627]],

         [[0.5438, 0.4791, 0.6081,  ..., 0.4947, 0.4761, 0.3285],
          [0.5207, 0.4545, 0.3711,  ..., 0.4050, 0.5133, 0.4906],
          [0.4656, 0.3390, 0.5424,  ..., 0.6826, 0.4865, 0.4790],
          [0.5334, 0.4617, 0.5607,  ..., 0.6188, 0.3603, 0.5407]]],


        [[[0.5353, 0.4410, 0.5417,  ..., 0.4201, 0.4149, 0.5346],
          [0.5636, 0.5057, 0.5106,  ..., 0.5960, 0.4135, 0.4836],
          [0.7082, 0.4158, 0.6602,  ..., 0.5142, 0.5463, 0.4830],
          [0.7009, 0.5655, 0.6128,  ..., 0.4908, 0.4301, 0.5574]],

         [[0.5100, 0.6343, 0.5499,  ..., 0.5063, 0.4969, 0.6252],
          [0.4349, 0.7090, 0.5513,  ..., 0.6142, 0.4603, 0.4354],
          [0.4306, 0.4513, 0.5737,  ..., 0.4622, 0.6035, 0.4760],
          [0.4649, 0.4472, 0.5042,  ..., 0.5610, 0.4839, 0.4496]],

         [[0.5203, 0.4882, 0.7050,  ..., 0.4547, 0.4472, 0.4639],
          [0.5917, 0.5799, 0.5684,  ..., 0.5566, 0.4158, 0.4678],
          [0.6843, 0.4521, 0.5631,  ..., 0.5443, 0.6343, 0.6151],
          [0.5204, 0.4292, 0.5463,  ..., 0.5550, 0.3442, 0.6128]],

         ...,

         [[0.4381, 0.4872, 0.4801,  ..., 0.5319, 0.3567, 0.4349],
          [0.5130, 0.5421, 0.4287,  ..., 0.5823, 0.5421, 0.5851],
          [0.4834, 0.5041, 0.6540,  ..., 0.5458, 0.4774, 0.3989],
          [0.5751, 0.5917, 0.6715,  ..., 0.4239, 0.3808, 0.4975]],

         [[0.5472, 0.3789, 0.5268,  ..., 0.3675, 0.5409, 0.4050],
          [0.7017, 0.4807, 0.5257,  ..., 0.4460, 0.5243, 0.6610],
          [0.3603, 0.4239, 0.6197,  ..., 0.6261, 0.5375, 0.5267],
          [0.4431, 0.4879, 0.4748,  ..., 0.5586, 0.4537, 0.5746]],

         [[0.5319, 0.4446, 0.5160,  ..., 0.4997, 0.5211, 0.5547],
          [0.4542, 0.4990, 0.5206,  ..., 0.4924, 0.4875, 0.4610],
          [0.4149, 0.4738, 0.5562,  ..., 0.4330, 0.4574, 0.4306],
          [0.4740, 0.6352, 0.6352,  ..., 0.3780, 0.3567, 0.3433]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([-0.0070, -0.0050, -0.0070,  0.0070, -0.0070,  0.0070, -0.0070, -0.0010,
         0.0070,  0.0070], device='cuda:0')
selected experts tensor([2226, 1570, 1845, 1707, 1920,  698, 2376, 1808, 1293,  941],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6135, 0.4461, 0.4529,  ..., 0.3671, 0.4332, 0.5583],
          [0.4160, 0.4284, 0.5326,  ..., 0.5508, 0.5614, 0.4294],
          [0.6990, 0.4529, 0.6310,  ..., 0.5095, 0.5844, 0.5830],
          [0.4808, 0.6544, 0.4543,  ..., 0.4767, 0.4537, 0.4455]],

         [[0.6337, 0.3696, 0.4910,  ..., 0.4799, 0.4476, 0.5863],
          [0.5729, 0.3898, 0.6552,  ..., 0.4941, 0.5011, 0.4549],
          [0.4774, 0.4449, 0.5649,  ..., 0.4472, 0.3807, 0.4946],
          [0.4222, 0.6204, 0.4701,  ..., 0.5701, 0.6581, 0.4347]],

         [[0.6526, 0.4786, 0.4718,  ..., 0.3318, 0.5734, 0.5854],
          [0.5473, 0.4825, 0.4531,  ..., 0.4748, 0.4600, 0.4701],
          [0.4322, 0.5085, 0.5862,  ..., 0.4304, 0.5488, 0.4213],
          [0.4616, 0.5824, 0.4028,  ..., 0.4479, 0.7132, 0.3292]],

         ...,

         [[0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050]],

         [[0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050]],

         [[0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050]]],


        [[[0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050]],

         [[0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050]],

         [[0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050]],

         ...,

         [[0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050]],

         [[0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050]],

         [[0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050],
          [0.5030, 0.5030, 0.5030,  ..., 0.5050, 0.5050, 0.5050]]]],
       device='cuda:0')
tensor([[[[0.6105, 0.4431, 0.4499,  ..., 0.3621, 0.4282, 0.5533],
          [0.4130, 0.4254, 0.5296,  ..., 0.5458, 0.5564, 0.4244],
          [0.6960, 0.4499, 0.6280,  ..., 0.5045, 0.5794, 0.5780],
          [0.4778, 0.6514, 0.4513,  ..., 0.4717, 0.4487, 0.4405]],

         [[0.6307, 0.3666, 0.4880,  ..., 0.4749, 0.4426, 0.5813],
          [0.5699, 0.3868, 0.6522,  ..., 0.4891, 0.4961, 0.4499],
          [0.4744, 0.4419, 0.5619,  ..., 0.4422, 0.3757, 0.4896],
          [0.4192, 0.6174, 0.4671,  ..., 0.5651, 0.6531, 0.4297]],

         [[0.6496, 0.4756, 0.4688,  ..., 0.3268, 0.5684, 0.5804],
          [0.5443, 0.4795, 0.4501,  ..., 0.4698, 0.4550, 0.4651],
          [0.4292, 0.5055, 0.5832,  ..., 0.4254, 0.5438, 0.4163],
          [0.4586, 0.5794, 0.3998,  ..., 0.4429, 0.7082, 0.3242]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0050, 0.0050, 0.0050, 0.0050,
        0.0050], device='cuda:0')
selected experts tensor([ 720,  573,  875,  457, 1179, 5504, 5510,  523,  515,  528],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1585., 1596., 1713., 1593., 1538., 1654., 1698., 1839., 1651., 1517.],
        [ 720.,  573.,  875.,  457., 1179., 5504., 5510.,  523.,  515.,  528.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4442, 0.6488, 0.6273,  ..., 0.5981, 0.4321, 0.4342],
          [0.3627, 0.5704, 0.5064,  ..., 0.4194, 0.4678, 0.6850],
          [0.5276, 0.5424, 0.4227,  ..., 0.5901, 0.4881, 0.5513],
          [0.4262, 0.5238, 0.5385,  ..., 0.3959, 0.5385, 0.3969]],

         [[0.6466, 0.4079, 0.6318,  ..., 0.5255, 0.5019, 0.4076],
          [0.2583, 0.2684, 0.5102,  ..., 0.4142, 0.5685, 0.5064],
          [0.3060, 0.5032, 0.4882,  ..., 0.4785, 0.4567, 0.3922],
          [0.4209, 0.5424, 0.5614,  ..., 0.6348, 0.6235, 0.4872]],

         [[0.4914, 0.6673, 0.5494,  ..., 0.4660, 0.5021, 0.5346],
          [0.5062, 0.4881, 0.5824,  ..., 0.6420, 0.4226, 0.4801],
          [0.4276, 0.6253, 0.5158,  ..., 0.4847, 0.4202, 0.5399],
          [0.6519, 0.5927, 0.5441,  ..., 0.6178, 0.4676, 0.4612]],

         ...,

         [[0.4602, 0.2936, 0.5460,  ..., 0.5967, 0.5099, 0.4280],
          [0.4946, 0.3400, 0.4349,  ..., 0.5758, 0.5809, 0.4323],
          [0.4583, 0.5014, 0.6372,  ..., 0.3635, 0.5984, 0.4948],
          [0.5654, 0.4458, 0.5291,  ..., 0.6808, 0.4093, 0.4913]],

         [[0.4792, 0.5158, 0.5296,  ..., 0.3871, 0.7366, 0.3844],
          [0.5503, 0.4943, 0.5552,  ..., 0.4888, 0.3758, 0.4208],
          [0.5740, 0.5470, 0.4806,  ..., 0.6867, 0.4870, 0.4896],
          [0.4953, 0.4107, 0.4629,  ..., 0.5677, 0.4514, 0.6687]],

         [[0.3681, 0.4721, 0.5710,  ..., 0.4980, 0.5856, 0.5284],
          [0.5716, 0.5600, 0.6363,  ..., 0.3950, 0.4379, 0.5590],
          [0.3833, 0.5275, 0.3425,  ..., 0.4583, 0.5908, 0.5796],
          [0.5099, 0.4475, 0.3533,  ..., 0.3309, 0.4316, 0.4692]]],


        [[[0.5906, 0.5011, 0.5809,  ..., 0.4450, 0.5511, 0.4905],
          [0.5678, 0.5388, 0.4335,  ..., 0.4109, 0.3896, 0.6033],
          [0.6098, 0.4579, 0.4511,  ..., 0.3041, 0.6776, 0.3635],
          [0.6181, 0.5651, 0.6164,  ..., 0.6625, 0.3808, 0.3770]],

         [[0.2800, 0.5431, 0.4499,  ..., 0.5238, 0.4992, 0.4062],
          [0.6102, 0.6077, 0.5113,  ..., 0.6850, 0.5685, 0.5362],
          [0.5394, 0.6785, 0.4944,  ..., 0.5265, 0.3461, 0.5739],
          [0.3627, 0.5258, 0.4715,  ..., 0.3770, 0.7545, 0.5406]],

         [[0.6075, 0.4889, 0.5042,  ..., 0.4285, 0.4036, 0.5445],
          [0.4057, 0.5588, 0.4613,  ..., 0.6173, 0.6317, 0.4916],
          [0.5299, 0.4731, 0.5165,  ..., 0.5667, 0.5388, 0.3617],
          [0.5522, 0.5499, 0.4477,  ..., 0.6233, 0.5157, 0.5162]],

         ...,

         [[0.5297, 0.6398, 0.3524,  ..., 0.3789, 0.6207, 0.5411],
          [0.5148, 0.6068, 0.5609,  ..., 0.5391, 0.4283, 0.4631],
          [0.4817, 0.5176, 0.4966,  ..., 0.5128, 0.5043, 0.4746],
          [0.4435, 0.5799, 0.5629,  ..., 0.3680, 0.6049, 0.5725]],

         [[0.6385, 0.5329, 0.5164,  ..., 0.4771, 0.6577, 0.4788],
          [0.2768, 0.5598, 0.5525,  ..., 0.5830, 0.4719, 0.3959],
          [0.5669, 0.4969, 0.4275,  ..., 0.4137, 0.4790, 0.4204],
          [0.5702, 0.5030, 0.4787,  ..., 0.3908, 0.3947, 0.3969]],

         [[0.5712, 0.6733, 0.5974,  ..., 0.4837, 0.3550, 0.5571],
          [0.4486, 0.5026, 0.5197,  ..., 0.5844, 0.4403, 0.5587],
          [0.5807, 0.3919, 0.4107,  ..., 0.4933, 0.5528, 0.4898],
          [0.5645, 0.4589, 0.5861,  ..., 0.6782, 0.5617, 0.4100]]]],
       device='cuda:0')
tensor([[[[0.4472, 0.6478, 0.6343,  ..., 0.5931, 0.4311, 0.4292],
          [0.3657, 0.5694, 0.5134,  ..., 0.4144, 0.4668, 0.6800],
          [0.5306, 0.5414, 0.4297,  ..., 0.5851, 0.4871, 0.5463],
          [0.4292, 0.5228, 0.5455,  ..., 0.3909, 0.5375, 0.3919]],

         [[0.6496, 0.4069, 0.6388,  ..., 0.5205, 0.5009, 0.4026],
          [0.2613, 0.2674, 0.5172,  ..., 0.4092, 0.5675, 0.5014],
          [0.3090, 0.5022, 0.4952,  ..., 0.4735, 0.4557, 0.3872],
          [0.4239, 0.5414, 0.5684,  ..., 0.6298, 0.6225, 0.4822]],

         [[0.4944, 0.6663, 0.5564,  ..., 0.4610, 0.5011, 0.5296],
          [0.5092, 0.4871, 0.5894,  ..., 0.6370, 0.4216, 0.4751],
          [0.4306, 0.6243, 0.5228,  ..., 0.4797, 0.4192, 0.5349],
          [0.6549, 0.5917, 0.5511,  ..., 0.6128, 0.4666, 0.4562]],

         ...,

         [[0.4632, 0.2926, 0.5530,  ..., 0.5917, 0.5089, 0.4230],
          [0.4976, 0.3390, 0.4419,  ..., 0.5708, 0.5799, 0.4273],
          [0.4613, 0.5004, 0.6442,  ..., 0.3585, 0.5974, 0.4898],
          [0.5684, 0.4448, 0.5361,  ..., 0.6758, 0.4083, 0.4863]],

         [[0.4822, 0.5148, 0.5366,  ..., 0.3821, 0.7356, 0.3794],
          [0.5533, 0.4933, 0.5622,  ..., 0.4838, 0.3748, 0.4158],
          [0.5770, 0.5460, 0.4876,  ..., 0.6817, 0.4860, 0.4846],
          [0.4983, 0.4097, 0.4699,  ..., 0.5627, 0.4504, 0.6637]],

         [[0.3711, 0.4711, 0.5780,  ..., 0.4930, 0.5846, 0.5234],
          [0.5746, 0.5590, 0.6433,  ..., 0.3900, 0.4369, 0.5540],
          [0.3863, 0.5265, 0.3495,  ..., 0.4533, 0.5898, 0.5746],
          [0.5129, 0.4465, 0.3603,  ..., 0.3259, 0.4306, 0.4642]]],


        [[[0.5936, 0.5001, 0.5879,  ..., 0.4400, 0.5501, 0.4855],
          [0.5708, 0.5378, 0.4405,  ..., 0.4059, 0.3886, 0.5983],
          [0.6128, 0.4569, 0.4581,  ..., 0.2991, 0.6766, 0.3585],
          [0.6211, 0.5641, 0.6234,  ..., 0.6575, 0.3798, 0.3720]],

         [[0.2830, 0.5421, 0.4569,  ..., 0.5188, 0.4982, 0.4012],
          [0.6132, 0.6067, 0.5183,  ..., 0.6800, 0.5675, 0.5312],
          [0.5424, 0.6775, 0.5014,  ..., 0.5215, 0.3451, 0.5689],
          [0.3657, 0.5248, 0.4785,  ..., 0.3720, 0.7535, 0.5356]],

         [[0.6105, 0.4879, 0.5112,  ..., 0.4235, 0.4026, 0.5395],
          [0.4087, 0.5578, 0.4683,  ..., 0.6123, 0.6307, 0.4866],
          [0.5329, 0.4721, 0.5235,  ..., 0.5617, 0.5378, 0.3567],
          [0.5552, 0.5489, 0.4547,  ..., 0.6183, 0.5147, 0.5112]],

         ...,

         [[0.5327, 0.6388, 0.3594,  ..., 0.3739, 0.6197, 0.5361],
          [0.5178, 0.6058, 0.5679,  ..., 0.5341, 0.4273, 0.4581],
          [0.4847, 0.5166, 0.5036,  ..., 0.5078, 0.5033, 0.4696],
          [0.4465, 0.5789, 0.5699,  ..., 0.3630, 0.6039, 0.5675]],

         [[0.6415, 0.5319, 0.5234,  ..., 0.4721, 0.6567, 0.4738],
          [0.2798, 0.5588, 0.5595,  ..., 0.5780, 0.4709, 0.3909],
          [0.5699, 0.4959, 0.4345,  ..., 0.4087, 0.4780, 0.4154],
          [0.5732, 0.5020, 0.4857,  ..., 0.3858, 0.3937, 0.3919]],

         [[0.5742, 0.6723, 0.6044,  ..., 0.4787, 0.3540, 0.5521],
          [0.4516, 0.5016, 0.5267,  ..., 0.5794, 0.4393, 0.5537],
          [0.5837, 0.3909, 0.4177,  ..., 0.4883, 0.5518, 0.4848],
          [0.5675, 0.4579, 0.5931,  ..., 0.6732, 0.5607, 0.4050]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0030,  0.0010, -0.0070,  0.0010, -0.0050, -0.0030,  0.0050,  0.0050,
         0.0010,  0.0050], device='cuda:0')
selected experts tensor([1707, 1625, 1644, 1685, 1666, 1652, 1569, 1617, 1627, 1592],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6093, 0.4431, 0.5996,  ..., 0.4573, 0.3841, 0.4434],
          [0.4819, 0.3844, 0.5566,  ..., 0.6047, 0.2816, 0.5656],
          [0.6190, 0.4416, 0.5110,  ..., 0.3912, 0.3721, 0.6836],
          [0.6116, 0.5053, 0.4299,  ..., 0.5630, 0.2800, 0.5029]],

         [[0.5107, 0.5484, 0.4261,  ..., 0.5141, 0.3892, 0.4772],
          [0.4493, 0.3821, 0.4748,  ..., 0.5233, 0.4932, 0.4722],
          [0.4735, 0.5096, 0.3881,  ..., 0.5199, 0.4526, 0.4216],
          [0.3736, 0.5912, 0.6178,  ..., 0.4720, 0.4766, 0.5003]],

         [[0.3954, 0.3816, 0.3825,  ..., 0.3931, 0.3933, 0.4032],
          [0.5391, 0.5959, 0.4522,  ..., 0.3403, 0.5276, 0.4976],
          [0.5616, 0.4717, 0.4445,  ..., 0.4929, 0.4112, 0.4335],
          [0.3645, 0.6297, 0.3379,  ..., 0.5049, 0.3841, 0.5694]],

         ...,

         [[0.4999, 0.5069, 0.4488,  ..., 0.4195, 0.5670, 0.5680],
          [0.3861, 0.6108, 0.4171,  ..., 0.4629, 0.5742, 0.4781],
          [0.4334, 0.3656, 0.5792,  ..., 0.6023, 0.4809, 0.4909],
          [0.5630, 0.5846, 0.4072,  ..., 0.4987, 0.5066, 0.4750]],

         [[0.5350, 0.4666, 0.5167,  ..., 0.4920, 0.3776, 0.3901],
          [0.5707, 0.4371, 0.3140,  ..., 0.4091, 0.3703, 0.4593],
          [0.5396, 0.3720, 0.4034,  ..., 0.5707, 0.4647, 0.5689],
          [0.5495, 0.4615, 0.5377,  ..., 0.4454, 0.3703, 0.5598]],

         [[0.5050, 0.5469, 0.4571,  ..., 0.3492, 0.4693, 0.4444],
          [0.6195, 0.5005, 0.5006,  ..., 0.4295, 0.5206, 0.4937],
          [0.4020, 0.5051, 0.3492,  ..., 0.5144, 0.4159, 0.4922],
          [0.5735, 0.4843, 0.4076,  ..., 0.4659, 0.7397, 0.3496]]],


        [[[0.4580, 0.5397, 0.4289,  ..., 0.5067, 0.5142, 0.6142],
          [0.6158, 0.5636, 0.3992,  ..., 0.5802, 0.5487, 0.5166],
          [0.5233, 0.7120, 0.5686,  ..., 0.4575, 0.5766, 0.6138],
          [0.4365, 0.4489, 0.5428,  ..., 0.4445, 0.5383, 0.4473]],

         [[0.4720, 0.4712, 0.7220,  ..., 0.5321, 0.5150, 0.4898],
          [0.4624, 0.5535, 0.6375,  ..., 0.3019, 0.4340, 0.6087],
          [0.4944, 0.5874, 0.4438,  ..., 0.4585, 0.3892, 0.5000],
          [0.5887, 0.5564, 0.4067,  ..., 0.4105, 0.5378, 0.6920]],

         [[0.5357, 0.3503, 0.4520,  ..., 0.4634, 0.6434, 0.4552],
          [0.6484, 0.3955, 0.4682,  ..., 0.6466, 0.5670, 0.5255],
          [0.6135, 0.6062, 0.4721,  ..., 0.3627, 0.4764, 0.5747],
          [0.3555, 0.4311, 0.4109,  ..., 0.4761, 0.4581, 0.4734]],

         ...,

         [[0.4699, 0.4910, 0.3904,  ..., 0.3144, 0.4436, 0.4836],
          [0.4676, 0.5535, 0.5216,  ..., 0.5126, 0.6225, 0.4683],
          [0.4171, 0.5693, 0.4837,  ..., 0.6065, 0.4706, 0.6280],
          [0.5297, 0.5260, 0.5447,  ..., 0.5137, 0.5312, 0.4240]],

         [[0.5488, 0.3993, 0.4476,  ..., 0.4573, 0.3505, 0.5072],
          [0.5154, 0.4445, 0.4043,  ..., 0.2856, 0.5199, 0.4187],
          [0.4738, 0.5779, 0.4626,  ..., 0.4166, 0.5247, 0.4492],
          [0.5411, 0.3974, 0.3626,  ..., 0.4024, 0.4088, 0.5142]],

         [[0.3745, 0.5283, 0.5262,  ..., 0.4918, 0.4355, 0.4581],
          [0.6222, 0.5109, 0.4399,  ..., 0.7180, 0.6793, 0.5150],
          [0.6051, 0.4144, 0.5408,  ..., 0.3907, 0.6087, 0.5610],
          [0.4081, 0.5036, 0.4161,  ..., 0.5967, 0.4878, 0.4311]]]],
       device='cuda:0')
tensor([[[[0.6123, 0.4441, 0.5946,  ..., 0.4603, 0.3831, 0.4424],
          [0.4849, 0.3854, 0.5516,  ..., 0.6077, 0.2806, 0.5646],
          [0.6220, 0.4426, 0.5060,  ..., 0.3942, 0.3711, 0.6826],
          [0.6146, 0.5063, 0.4249,  ..., 0.5660, 0.2790, 0.5019]],

         [[0.5137, 0.5494, 0.4211,  ..., 0.5171, 0.3882, 0.4762],
          [0.4523, 0.3831, 0.4698,  ..., 0.5263, 0.4922, 0.4712],
          [0.4765, 0.5106, 0.3831,  ..., 0.5229, 0.4516, 0.4206],
          [0.3766, 0.5922, 0.6128,  ..., 0.4750, 0.4756, 0.4993]],

         [[0.3984, 0.3826, 0.3775,  ..., 0.3961, 0.3923, 0.4022],
          [0.5421, 0.5969, 0.4472,  ..., 0.3433, 0.5266, 0.4966],
          [0.5646, 0.4727, 0.4395,  ..., 0.4959, 0.4102, 0.4325],
          [0.3675, 0.6307, 0.3329,  ..., 0.5079, 0.3831, 0.5684]],

         ...,

         [[0.5029, 0.5079, 0.4438,  ..., 0.4225, 0.5660, 0.5670],
          [0.3891, 0.6118, 0.4121,  ..., 0.4659, 0.5732, 0.4771],
          [0.4364, 0.3666, 0.5742,  ..., 0.6053, 0.4799, 0.4899],
          [0.5660, 0.5856, 0.4022,  ..., 0.5017, 0.5056, 0.4740]],

         [[0.5380, 0.4676, 0.5117,  ..., 0.4950, 0.3766, 0.3891],
          [0.5737, 0.4381, 0.3090,  ..., 0.4121, 0.3693, 0.4583],
          [0.5426, 0.3730, 0.3984,  ..., 0.5737, 0.4637, 0.5679],
          [0.5525, 0.4625, 0.5327,  ..., 0.4484, 0.3693, 0.5588]],

         [[0.5080, 0.5479, 0.4521,  ..., 0.3522, 0.4683, 0.4434],
          [0.6225, 0.5015, 0.4956,  ..., 0.4325, 0.5196, 0.4927],
          [0.4050, 0.5061, 0.3442,  ..., 0.5174, 0.4149, 0.4912],
          [0.5765, 0.4853, 0.4026,  ..., 0.4689, 0.7387, 0.3486]]],


        [[[0.4610, 0.5407, 0.4239,  ..., 0.5097, 0.5132, 0.6132],
          [0.6188, 0.5646, 0.3942,  ..., 0.5832, 0.5477, 0.5156],
          [0.5263, 0.7130, 0.5636,  ..., 0.4605, 0.5756, 0.6128],
          [0.4395, 0.4499, 0.5378,  ..., 0.4475, 0.5373, 0.4463]],

         [[0.4750, 0.4722, 0.7170,  ..., 0.5351, 0.5140, 0.4888],
          [0.4654, 0.5545, 0.6325,  ..., 0.3049, 0.4330, 0.6077],
          [0.4974, 0.5884, 0.4388,  ..., 0.4615, 0.3882, 0.4990],
          [0.5917, 0.5574, 0.4017,  ..., 0.4135, 0.5368, 0.6910]],

         [[0.5387, 0.3513, 0.4470,  ..., 0.4664, 0.6424, 0.4542],
          [0.6514, 0.3965, 0.4632,  ..., 0.6496, 0.5660, 0.5245],
          [0.6165, 0.6072, 0.4671,  ..., 0.3657, 0.4754, 0.5737],
          [0.3585, 0.4321, 0.4059,  ..., 0.4791, 0.4571, 0.4724]],

         ...,

         [[0.4729, 0.4920, 0.3854,  ..., 0.3174, 0.4426, 0.4826],
          [0.4706, 0.5545, 0.5166,  ..., 0.5156, 0.6215, 0.4673],
          [0.4201, 0.5703, 0.4787,  ..., 0.6095, 0.4696, 0.6270],
          [0.5327, 0.5270, 0.5397,  ..., 0.5167, 0.5302, 0.4230]],

         [[0.5518, 0.4003, 0.4426,  ..., 0.4603, 0.3495, 0.5062],
          [0.5184, 0.4455, 0.3993,  ..., 0.2886, 0.5189, 0.4177],
          [0.4768, 0.5789, 0.4576,  ..., 0.4196, 0.5237, 0.4482],
          [0.5441, 0.3984, 0.3576,  ..., 0.4054, 0.4078, 0.5132]],

         [[0.3775, 0.5293, 0.5212,  ..., 0.4948, 0.4345, 0.4571],
          [0.6252, 0.5119, 0.4349,  ..., 0.7210, 0.6783, 0.5140],
          [0.6081, 0.4154, 0.5358,  ..., 0.3937, 0.6077, 0.5600],
          [0.4111, 0.5046, 0.4111,  ..., 0.5997, 0.4868, 0.4301]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0030, -0.0010,  0.0050,  0.0090, -0.0010,  0.0070, -0.0090, -0.0030,
         0.0010,  0.0010], device='cuda:0')
selected experts tensor([1577, 1707, 1618, 1661, 1680, 1634, 1700, 1575, 1588, 1644],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6855, 0.5413, 0.5344,  ..., 0.4974, 0.4768, 0.6171],
          [0.3802, 0.5548, 0.5035,  ..., 0.5421, 0.5011, 0.3728],
          [0.5426, 0.5326, 0.4017,  ..., 0.4583, 0.4324, 0.5235],
          [0.4658, 0.5534, 0.5180,  ..., 0.6052, 0.6459, 0.4579]],

         [[0.5192, 0.5744, 0.4766,  ..., 0.5251, 0.5552, 0.5936],
          [0.4918, 0.4991, 0.4766,  ..., 0.6511, 0.5632, 0.4045],
          [0.3839, 0.6339, 0.4581,  ..., 0.4775, 0.3348, 0.4921],
          [0.4088, 0.5524, 0.6200,  ..., 0.4859, 0.4186, 0.6459]],

         [[0.4957, 0.5825, 0.4298,  ..., 0.4799, 0.5630, 0.4281],
          [0.4193, 0.5181, 0.4579,  ..., 0.5822, 0.4642, 0.3638],
          [0.5147, 0.4599, 0.6469,  ..., 0.3728, 0.4538, 0.4722],
          [0.5157, 0.4683, 0.4875,  ..., 0.5954, 0.4453, 0.4761]],

         ...,

         [[0.5539, 0.6384, 0.6380,  ..., 0.4077, 0.3773, 0.6011],
          [0.4609, 0.4986, 0.5363,  ..., 0.4600, 0.4434, 0.5625],
          [0.4022, 0.3874, 0.5599,  ..., 0.7118, 0.5721, 0.4921],
          [0.6945, 0.5920, 0.3989,  ..., 0.4636, 0.3280, 0.5845]],

         [[0.6066, 0.5620, 0.5671,  ..., 0.5621, 0.3837, 0.5018],
          [0.5494, 0.5265, 0.5395,  ..., 0.5304, 0.3943, 0.5567],
          [0.7042, 0.4725, 0.5776,  ..., 0.4904, 0.5284, 0.4956],
          [0.5124, 0.5582, 0.5337,  ..., 0.3899, 0.5630, 0.4802]],

         [[0.5539, 0.3545, 0.5317,  ..., 0.4946, 0.4125, 0.5769],
          [0.5628, 0.5611, 0.6122,  ..., 0.6140, 0.5441, 0.4938],
          [0.4074, 0.4672, 0.5101,  ..., 0.5669, 0.5023, 0.3962],
          [0.3755, 0.4038, 0.5757,  ..., 0.4946, 0.3869, 0.5345]]],


        [[[0.5194, 0.5226, 0.5936,  ..., 0.4447, 0.5201, 0.5253],
          [0.5666, 0.5444, 0.3389,  ..., 0.5803, 0.4605, 0.3888],
          [0.5337, 0.5049, 0.5315,  ..., 0.4617, 0.4841, 0.5182],
          [0.4540, 0.4893, 0.5950,  ..., 0.6232, 0.3602, 0.6180]],

         [[0.4140, 0.4597, 0.4994,  ..., 0.3095, 0.4334, 0.6522],
          [0.4202, 0.5069, 0.4571,  ..., 0.6250, 0.5917, 0.3331],
          [0.7026, 0.4928, 0.3946,  ..., 0.4464, 0.5140, 0.4492],
          [0.3937, 0.4984, 0.4899,  ..., 0.5299, 0.4601, 0.4639]],

         [[0.6530, 0.4851, 0.5973,  ..., 0.5397, 0.6171, 0.4915],
          [0.4121, 0.3869, 0.4653,  ..., 0.5078, 0.4688, 0.4139],
          [0.4834, 0.4490, 0.5513,  ..., 0.4058, 0.5022, 0.3549],
          [0.6071, 0.7362, 0.6830,  ..., 0.4979, 0.5297, 0.5172]],

         ...,

         [[0.5922, 0.5531, 0.6263,  ..., 0.5440, 0.5969, 0.6611],
          [0.3613, 0.5170, 0.4827,  ..., 0.6214, 0.4420, 0.4818],
          [0.5436, 0.4619, 0.4803,  ..., 0.5336, 0.6096, 0.5216],
          [0.5728, 0.5306, 0.5399,  ..., 0.6341, 0.4768, 0.5376]],

         [[0.5959, 0.4973, 0.5489,  ..., 0.5558, 0.4106, 0.5040],
          [0.4443, 0.5279, 0.5527,  ..., 0.5169, 0.5632, 0.4073],
          [0.4945, 0.4641, 0.5460,  ..., 0.4876, 0.4818, 0.3602],
          [0.5728, 0.4930, 0.3568,  ..., 0.4361, 0.5922, 0.5256]],

         [[0.5547, 0.4717, 0.4217,  ..., 0.4243, 0.5350, 0.4719],
          [0.3923, 0.6348, 0.4996,  ..., 0.6520, 0.4845, 0.4492],
          [0.5322, 0.3804, 0.4412,  ..., 0.5302, 0.5384, 0.5644],
          [0.4610, 0.4904, 0.5973,  ..., 0.5043, 0.4281, 0.6133]]]],
       device='cuda:0')
tensor([[[[0.6935, 0.5453, 0.5424,  ..., 0.4994, 0.4688, 0.6091],
          [0.3882, 0.5588, 0.5115,  ..., 0.5441, 0.4931, 0.3648],
          [0.5506, 0.5366, 0.4097,  ..., 0.4603, 0.4244, 0.5155],
          [0.4738, 0.5574, 0.5260,  ..., 0.6072, 0.6379, 0.4499]],

         [[0.5272, 0.5784, 0.4846,  ..., 0.5271, 0.5472, 0.5856],
          [0.4998, 0.5031, 0.4846,  ..., 0.6531, 0.5552, 0.3965],
          [0.3919, 0.6379, 0.4661,  ..., 0.4795, 0.3268, 0.4841],
          [0.4168, 0.5564, 0.6280,  ..., 0.4879, 0.4106, 0.6379]],

         [[0.5037, 0.5865, 0.4378,  ..., 0.4819, 0.5550, 0.4201],
          [0.4273, 0.5221, 0.4659,  ..., 0.5842, 0.4562, 0.3558],
          [0.5227, 0.4639, 0.6549,  ..., 0.3748, 0.4458, 0.4642],
          [0.5237, 0.4723, 0.4955,  ..., 0.5974, 0.4373, 0.4681]],

         ...,

         [[0.5619, 0.6424, 0.6460,  ..., 0.4097, 0.3693, 0.5931],
          [0.4689, 0.5026, 0.5443,  ..., 0.4620, 0.4354, 0.5545],
          [0.4102, 0.3914, 0.5679,  ..., 0.7138, 0.5641, 0.4841],
          [0.7025, 0.5960, 0.4069,  ..., 0.4656, 0.3200, 0.5765]],

         [[0.6146, 0.5660, 0.5751,  ..., 0.5641, 0.3757, 0.4938],
          [0.5574, 0.5305, 0.5475,  ..., 0.5324, 0.3863, 0.5487],
          [0.7122, 0.4765, 0.5856,  ..., 0.4924, 0.5204, 0.4876],
          [0.5204, 0.5622, 0.5417,  ..., 0.3919, 0.5550, 0.4722]],

         [[0.5619, 0.3585, 0.5397,  ..., 0.4966, 0.4045, 0.5689],
          [0.5708, 0.5651, 0.6202,  ..., 0.6160, 0.5361, 0.4858],
          [0.4154, 0.4712, 0.5181,  ..., 0.5689, 0.4943, 0.3882],
          [0.3835, 0.4078, 0.5837,  ..., 0.4966, 0.3789, 0.5265]]],


        [[[0.5274, 0.5266, 0.6016,  ..., 0.4467, 0.5121, 0.5173],
          [0.5746, 0.5484, 0.3469,  ..., 0.5823, 0.4525, 0.3808],
          [0.5417, 0.5089, 0.5395,  ..., 0.4637, 0.4761, 0.5102],
          [0.4620, 0.4933, 0.6030,  ..., 0.6252, 0.3522, 0.6100]],

         [[0.4220, 0.4637, 0.5074,  ..., 0.3115, 0.4254, 0.6442],
          [0.4282, 0.5109, 0.4651,  ..., 0.6270, 0.5837, 0.3251],
          [0.7106, 0.4968, 0.4026,  ..., 0.4484, 0.5060, 0.4412],
          [0.4017, 0.5024, 0.4979,  ..., 0.5319, 0.4521, 0.4559]],

         [[0.6610, 0.4891, 0.6053,  ..., 0.5417, 0.6091, 0.4835],
          [0.4201, 0.3909, 0.4733,  ..., 0.5098, 0.4608, 0.4059],
          [0.4914, 0.4530, 0.5593,  ..., 0.4078, 0.4942, 0.3469],
          [0.6151, 0.7402, 0.6910,  ..., 0.4999, 0.5217, 0.5092]],

         ...,

         [[0.6002, 0.5571, 0.6343,  ..., 0.5460, 0.5889, 0.6531],
          [0.3693, 0.5210, 0.4907,  ..., 0.6234, 0.4340, 0.4738],
          [0.5516, 0.4659, 0.4883,  ..., 0.5356, 0.6016, 0.5136],
          [0.5808, 0.5346, 0.5479,  ..., 0.6361, 0.4688, 0.5296]],

         [[0.6039, 0.5013, 0.5569,  ..., 0.5578, 0.4026, 0.4960],
          [0.4523, 0.5319, 0.5607,  ..., 0.5189, 0.5552, 0.3993],
          [0.5025, 0.4681, 0.5540,  ..., 0.4896, 0.4738, 0.3522],
          [0.5808, 0.4970, 0.3648,  ..., 0.4381, 0.5842, 0.5176]],

         [[0.5627, 0.4757, 0.4297,  ..., 0.4263, 0.5270, 0.4639],
          [0.4003, 0.6388, 0.5076,  ..., 0.6540, 0.4765, 0.4412],
          [0.5402, 0.3844, 0.4492,  ..., 0.5322, 0.5304, 0.5564],
          [0.4690, 0.4944, 0.6053,  ..., 0.5063, 0.4201, 0.6053]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0080, -0.0040, -0.0080,  0.0060, -0.0080,  0.0080, -0.0080, -0.0020,
         0.0080,  0.0080], device='cuda:0')
selected experts tensor([1788, 2009, 1959, 1496, 2084,  966, 2346, 1831,  821, 1084],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6320, 0.4365, 0.6437,  ..., 0.4648, 0.3762, 0.6475],
          [0.5039, 0.4454, 0.5162,  ..., 0.3681, 0.5304, 0.4660],
          [0.5301, 0.4318, 0.4677,  ..., 0.5311, 0.4959, 0.3849],
          [0.4602, 0.6428, 0.5471,  ..., 0.4280, 0.6146, 0.3234]],

         [[0.5580, 0.4454, 0.4756,  ..., 0.4673, 0.4963, 0.4525],
          [0.4696, 0.5652, 0.6292,  ..., 0.4697, 0.5612, 0.4342],
          [0.5249, 0.4917, 0.4662,  ..., 0.4271, 0.4152, 0.5245],
          [0.4365, 0.5667, 0.6047,  ..., 0.5100, 0.5280, 0.3826]],

         [[0.4246, 0.4057, 0.5415,  ..., 0.4455, 0.3654, 0.6010],
          [0.4981, 0.4544, 0.5263,  ..., 0.4697, 0.5017, 0.4299],
          [0.5858, 0.5553, 0.5202,  ..., 0.5042, 0.5191, 0.5264],
          [0.3447, 0.5037, 0.5570,  ..., 0.6358, 0.4082, 0.4366]],

         ...,

         [[0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5060, 0.5060]],

         [[0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5060, 0.5060]],

         [[0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5060, 0.5060]]],


        [[[0.5719, 0.3834, 0.4356,  ..., 0.4614, 0.3389, 0.5273],
          [0.5112, 0.6624, 0.4256,  ..., 0.3681, 0.5039, 0.5375],
          [0.6214, 0.5263, 0.4718,  ..., 0.4975, 0.4875, 0.5600],
          [0.5563, 0.6491, 0.4766,  ..., 0.5835, 0.5489, 0.4204]],

         [[0.4631, 0.5570, 0.5042,  ..., 0.3423, 0.3663, 0.4956],
          [0.5729, 0.3715, 0.5539,  ..., 0.4477, 0.4489, 0.4759],
          [0.4498, 0.5330, 0.4996,  ..., 0.3251, 0.5377, 0.4812],
          [0.5067, 0.5829, 0.5829,  ..., 0.5138, 0.5377, 0.3895]],

         [[0.4941, 0.3922, 0.5208,  ..., 0.3946, 0.2287, 0.4357],
          [0.4442, 0.4232, 0.5449,  ..., 0.5053, 0.4660, 0.4209],
          [0.5289, 0.5820, 0.4734,  ..., 0.4328, 0.3835, 0.4515],
          [0.4146, 0.5753, 0.4404,  ..., 0.5585, 0.5277, 0.4266]],

         ...,

         [[0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5060, 0.5060]],

         [[0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5060, 0.5060]],

         [[0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5040, 0.5040,  ..., 0.5060, 0.5060, 0.5060]]]],
       device='cuda:0')[batch=10/40]:
	 Train time/batch: 9
	 Train time/sample: 18
	 Train time/batch_in_epoch: 9
	 Train time/sample_in_epoch: 18
	 Train time/token: 18432
	 Train time/token_in_epoch: 18432
	 Train memory/current_allocated_mem: 1.1143
	 Train memory/current_active_mem: 1.1143
	 Train memory/current_inactive_mem: 0.7374
	 Train memory/current_reserved_mem: 3.7665
	 Train memory/peak_allocated_mem: 2.7252
	 Train memory/peak_active_mem: 2.7252
	 Train memory/peak_inactive_mem: 0.8572
	 Train memory/peak_reserved_mem: 3.7665
	 Train memory/alloc_retries: 0
	 Train trainer/device_train_microbatch_size: 10
	 Train loss/train/total: 0.0053
	 Train metrics/train/LanguageCrossEntropy: 10.8349
	 Train metrics/train/LanguagePerplexity: 50760.0820
	 Train metrics/train/TokenAccuracy: 0.0000
	 Train time/train: 0.0146
	 Train time/val: 0.0000
	 Train time/total: 0.0146
	 Train lr-DecoupledAdamW/group0: 0.0000
	 Train time/remaining_estimate: 0.0281
	 Train metrics/shannon_entropy: 10.6359
	 Train metrics/batch_shannon_entropy: <wandb.sdk.data_types.table.Table object at 0x77b707caac90>
	 Train metrics/seq_shannon_entropy: CustomChart(table=<wandb.sdk.data_types.table.Table object at 0x77b99fcaa8a0>, spec=CustomChartSpec(spec_name='wandb/lineseries/v0', fields={'step': 'step', 'lineKey': 'lineKey', 'lineVal': 'lineVal'}, string_fields={'title': 'Per-Layer Shannon Entropy', 'xname': 'Block Index'}, key='', panel_type='Vega2', split_table=False))
	 Train metrics/exit_entropy: 0.6641
	 Train metrics/batch_exit_entropy: <wandb.sdk.data_types.table.Table object at 0x77b9a1b2f8f0>
	 Train metrics/seq_exit_entropy: CustomChart(table=<wandb.sdk.data_types.table.Table object at 0x77b99f76b260>, spec=CustomChartSpec(spec_name='wandb/lineseries/v0', fields={'step': 'step', 'lineKey': 'lineKey', 'lineVal': 'lineVal'}, string_fields={'title': 'Per-Layer Exit Entropy', 'xname': 'Block Index'}, key='', panel_type='Vega2', split_table=False))
	 Train expert_selection/ffn_layer: <wandb.sdk.data_types.image.Image object at 0x77b99fd66420>
	 Train expert_selection/attn_o_layer: <wandb.sdk.data_types.image.Image object at 0x77b99fd65100>
	 Train expert_selection/attn_v_layer: <wandb.sdk.data_types.image.Image object at 0x77b99f792060>
	 Train l2_norm/moment/model.transformer.router: 0.0000
	 Train l2_norm/param/model.transformer.router: 0.3611
	 Train l2_norm/update/model.transformer.router: 0.0001
	 Train l2_norm/grad/model.transformer.router: 0.0000
	 Train l2_norm/moment/model.transformer.tau: 0.0000
	 Train l2_norm/param/model.transformer.tau: 1.0000
	 Train l2_norm/update/model.transformer.tau: 0.0000
	 Train l2_norm/grad/model.transformer.tau: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.attention.v: 0.0005
	 Train l2_norm/param/model.transformer.layers.0.attention.v: 20.4797
	 Train l2_norm/update/model.transformer.layers.0.attention.v: 0.0074
	 Train l2_norm/grad/model.transformer.layers.0.attention.v: 0.0008
	 Train l2_norm/moment/model.transformer.layers.0.attention.o: 0.0005
	 Train l2_norm/param/model.transformer.layers.0.attention.o: 22.6872
	 Train l2_norm/update/model.transformer.layers.0.attention.o: 0.0076
	 Train l2_norm/grad/model.transformer.layers.0.attention.o: 0.0007
	 Train l2_norm/moment/model.transformer.layers.0.attention.sel_v: 0.0001
	 Train l2_norm/param/model.transformer.layers.0.attention.sel_v: 2.2598
	 Train l2_norm/update/model.transformer.layers.0.attention.sel_v: 0.0004
	 Train l2_norm/grad/model.transformer.layers.0.attention.sel_v: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.attention.sel_o: 0.0001
	 Train l2_norm/param/model.transformer.layers.0.attention.sel_o: 2.2418
	 Train l2_norm/update/model.transformer.layers.0.attention.sel_o: 0.0005
	 Train l2_norm/grad/model.transformer.layers.0.attention.sel_o: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.attention.q.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attention.q.weight: 6.4835
	 Train l2_norm/update/model.transformer.layers.0.attention.q.weight: 0.0024
	 Train l2_norm/grad/model.transformer.layers.0.attention.q.weight: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.attention.k.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attention.k.weight: 6.4740
	 Train l2_norm/update/model.transformer.layers.0.attention.k.weight: 0.0024
	 Train l2_norm/grad/model.transformer.layers.0.attention.k.weight: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.ffn.keys: 0.0004
	 Train l2_norm/param/model.transformer.layers.0.ffn.keys: 14.9986
	 Train l2_norm/update/model.transformer.layers.0.ffn.keys: 0.0061
	 Train l2_norm/grad/model.transformer.layers.0.ffn.keys: 0.0006
	 Train l2_norm/moment/model.transformer.layers.0.ffn.values: 0.0010
	 Train l2_norm/param/model.transformer.layers.0.ffn.values: 7.1775
	 Train l2_norm/update/model.transformer.layers.0.ffn.values: 0.0066
	 Train l2_norm/grad/model.transformer.layers.0.ffn.values: 0.0016
	 Train l2_norm/moment/model.transformer.layers.0.ffn.expert_sel: 0.0001
	 Train l2_norm/param/model.transformer.layers.0.ffn.expert_sel: 4.7493
	 Train l2_norm/update/model.transformer.layers.0.ffn.expert_sel: 0.0019
	 Train l2_norm/grad/model.transformer.layers.0.ffn.expert_sel: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.attn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attn_pre.weight: 20.2978
	 Train l2_norm/update/model.transformer.layers.0.attn_pre.weight: 0.0001
	 Train l2_norm/grad/model.transformer.layers.0.attn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.attn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attn_post.weight: 20.2978
	 Train l2_norm/update/model.transformer.layers.0.attn_post.weight: 0.0002
	 Train l2_norm/grad/model.transformer.layers.0.attn_post.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.ffn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.ffn_pre.weight: 20.2978
	 Train l2_norm/update/model.transformer.layers.0.ffn_pre.weight: 0.0002
	 Train l2_norm/grad/model.transformer.layers.0.ffn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.ffn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.ffn_post.weight: 20.2978
	 Train l2_norm/update/model.transformer.layers.0.ffn_post.weight: 0.0002
	 Train l2_norm/grad/model.transformer.layers.0.ffn_post.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.v: 0.0002
	 Train l2_norm/param/model.transformer.layers.1.attention.v: 20.4915
	 Train l2_norm/update/model.transformer.layers.1.attention.v: 0.0061
	 Train l2_norm/grad/model.transformer.layers.1.attention.v: 0.0006
	 Train l2_norm/moment/model.transformer.layers.1.attention.o: 0.0001
	 Train l2_norm/param/model.transformer.layers.1.attention.o: 22.6905
	 Train l2_norm/update/model.transformer.layers.1.attention.o: 0.0060
	 Train l2_norm/grad/model.transformer.layers.1.attention.o: 0.0005
	 Train l2_norm/moment/model.transformer.layers.1.attention.sel_v: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.sel_v: 2.2543
	 Train l2_norm/update/model.transformer.layers.1.attention.sel_v: 0.0004
	 Train l2_norm/grad/model.transformer.layers.1.attention.sel_v: 0.0001
	 Train l2_norm/moment/model.transformer.layers.1.attention.sel_o: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.sel_o: 2.2406
	 Train l2_norm/update/model.transformer.layers.1.attention.sel_o: 0.0004
	 Train l2_norm/grad/model.transformer.layers.1.attention.sel_o: 0.0001
	 Train l2_norm/moment/model.transformer.layers.1.attention.q.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.q.weight: 6.4905
	 Train l2_norm/update/model.transformer.layers.1.attention.q.weight: 0.0013
	 Train l2_norm/grad/model.transformer.layers.1.attention.q.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.k.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.k.weight: 6.4852
	 Train l2_norm/update/model.transformer.layers.1.attention.k.weight: 0.0013
	 Train l2_norm/grad/model.transformer.layers.1.attention.k.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn.keys: 0.0001
	 Train l2_norm/param/model.transformer.layers.1.ffn.keys: 15.0041
	 Train l2_norm/update/model.transformer.layers.1.ffn.keys: 0.0049
	 Train l2_norm/grad/model.transformer.layers.1.ffn.keys: 0.0005
	 Train l2_norm/moment/model.transformer.layers.1.ffn.values: 0.0004
	 Train l2_norm/param/model.transformer.layers.1.ffn.values: 7.1776
	 Train l2_norm/update/model.transformer.layers.1.ffn.values: 0.0057
	 Train l2_norm/grad/model.transformer.layers.1.ffn.values: 0.0013
	 Train l2_norm/moment/model.transformer.layers.1.ffn.expert_sel: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.ffn.expert_sel: 4.7526
	 Train l2_norm/update/model.transformer.layers.1.ffn.expert_sel: 0.0016
	 Train l2_norm/grad/model.transformer.layers.1.ffn.expert_sel: 0.0001
	 Train l2_norm/moment/model.transformer.layers.1.attn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attn_pre.weight: 20.2978
	 Train l2_norm/update/model.transformer.layers.1.attn_pre.weight: 0.0001
	 Train l2_norm/grad/model.transformer.layers.1.attn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attn_post.weight: 20.2978
	 Train l2_norm/update/model.transformer.layers.1.attn_post.weight: 0.0001
	 Train l2_norm/grad/model.transformer.layers.1.attn_post.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.ffn_pre.weight: 20.2978
	 Train l2_norm/update/model.transformer.layers.1.ffn_pre.weight: 0.0001
	 Train l2_norm/grad/model.transformer.layers.1.ffn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.ffn_post.weight: 20.2978
	 Train l2_norm/update/model.transformer.layers.1.ffn_post.weight: 0.0001
	 Train l2_norm/grad/model.transformer.layers.1.ffn_post.weight: 0.0000
	 Train l2_norm/moment/model.embedding.weight: 0.0002
	 Train l2_norm/param/model.embedding.weight: 221.7083
	 Train l2_norm/update/model.embedding.weight: 0.0046
	 Train l2_norm/grad/model.embedding.weight: 0.0003
	 Train l2_norm/moment/model.lm_head.weight: 0.0004
	 Train l2_norm/param/model.lm_head.weight: 127.9977
	 Train l2_norm/update/model.lm_head.weight: 0.0145
	 Train l2_norm/grad/model.lm_head.weight: 0.0007
	 Train l2_norm/moment/model.lm_head.bias: 0.0000
	 Train l2_norm/param/model.lm_head.bias: 6.3166
	 Train l2_norm/update/model.lm_head.bias: 0.0012
	 Train l2_norm/grad/model.lm_head.bias: 0.0000
	 Train l2_norm/moment/model.out_norm.weight: 0.0000
	 Train l2_norm/param/model.out_norm.weight: 20.2975
	 Train l2_norm/update/model.out_norm.weight: 0.0002
	 Train l2_norm/grad/model.out_norm.weight: 0.0000
	 Train l2_norm/grad/global: 0.0027
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([[[[0.6280, 0.4325, 0.6397,  ..., 0.4588, 0.3702, 0.6415],
          [0.4999, 0.4414, 0.5122,  ..., 0.3621, 0.5244, 0.4600],
          [0.5261, 0.4278, 0.4637,  ..., 0.5251, 0.4899, 0.3789],
          [0.4562, 0.6388, 0.5431,  ..., 0.4220, 0.6086, 0.3174]],

         [[0.5540, 0.4414, 0.4716,  ..., 0.4613, 0.4903, 0.4465],
          [0.4656, 0.5612, 0.6252,  ..., 0.4637, 0.5552, 0.4282],
          [0.5209, 0.4877, 0.4622,  ..., 0.4211, 0.4092, 0.5185],
          [0.4325, 0.5627, 0.6007,  ..., 0.5040, 0.5220, 0.3766]],

         [[0.4206, 0.4017, 0.5375,  ..., 0.4395, 0.3594, 0.5950],
          [0.4941, 0.4504, 0.5223,  ..., 0.4637, 0.4957, 0.4239],
          [0.5818, 0.5513, 0.5162,  ..., 0.4982, 0.5131, 0.5204],
          [0.3407, 0.4997, 0.5530,  ..., 0.6298, 0.4022, 0.4306]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5679, 0.3794, 0.4316,  ..., 0.4554, 0.3329, 0.5213],
          [0.5072, 0.6584, 0.4216,  ..., 0.3621, 0.4979, 0.5315],
          [0.6174, 0.5223, 0.4678,  ..., 0.4915, 0.4815, 0.5540],
          [0.5523, 0.6451, 0.4726,  ..., 0.5775, 0.5429, 0.4144]],

         [[0.4591, 0.5530, 0.5002,  ..., 0.3363, 0.3603, 0.4896],
          [0.5689, 0.3675, 0.5499,  ..., 0.4417, 0.4429, 0.4699],
          [0.4458, 0.5290, 0.4956,  ..., 0.3191, 0.5317, 0.4752],
          [0.5027, 0.5789, 0.5789,  ..., 0.5078, 0.5317, 0.3835]],

         [[0.4901, 0.3882, 0.5168,  ..., 0.3886, 0.2227, 0.4297],
          [0.4402, 0.4192, 0.5409,  ..., 0.4993, 0.4600, 0.4149],
          [0.5249, 0.5780, 0.4694,  ..., 0.4268, 0.3775, 0.4455],
          [0.4106, 0.5713, 0.4364,  ..., 0.5525, 0.5217, 0.4206]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0060, 0.0060,
        0.0060], device='cuda:0')
selected experts tensor([1405, 1190, 1752, 1164, 2715,  906,  920, 2477, 2465, 1390],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1577., 1707., 1618., 1661., 1680., 1634., 1700., 1575., 1588., 1644.],
        [1405., 1190., 1752., 1164., 2715.,  906.,  920., 2477., 2465., 1390.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6870, 0.3560, 0.3345,  ..., 0.3564, 0.5790, 0.6015],
          [0.4673, 0.6318, 0.5124,  ..., 0.3969, 0.6217, 0.3699],
          [0.4348, 0.4832, 0.3913,  ..., 0.3942, 0.6498, 0.5249],
          [0.5021, 0.7086, 0.6043,  ..., 0.4947, 0.6913, 0.4347]],

         [[0.3653, 0.5102, 0.4748,  ..., 0.4660, 0.5319, 0.3771],
          [0.6230, 0.2962, 0.5752,  ..., 0.5539, 0.5032, 0.5930],
          [0.5971, 0.6189, 0.4332,  ..., 0.6493, 0.5728, 0.4933],
          [0.5943, 0.5781, 0.6263,  ..., 0.5150, 0.3211, 0.5164]],

         [[0.4895, 0.5135, 0.4962,  ..., 0.4622, 0.4623, 0.4261],
          [0.4362, 0.5427, 0.4990,  ..., 0.5720, 0.4065, 0.4792],
          [0.5454, 0.3851, 0.5305,  ..., 0.3835, 0.5596, 0.5797],
          [0.4799, 0.5714, 0.4962,  ..., 0.4204, 0.5819, 0.3923]],

         ...,

         [[0.4860, 0.5202, 0.5383,  ..., 0.4741, 0.5054, 0.4660],
          [0.4033, 0.4662, 0.5131,  ..., 0.4470, 0.3953, 0.4811],
          [0.4384, 0.5610, 0.4183,  ..., 0.7507, 0.6363, 0.4376],
          [0.4551, 0.5158, 0.6057,  ..., 0.5484, 0.4856, 0.4970]],

         [[0.5882, 0.3340, 0.6407,  ..., 0.5211, 0.4103, 0.5167],
          [0.4500, 0.5422, 0.4625,  ..., 0.6312, 0.4817, 0.4947],
          [0.5476, 0.5942, 0.4202,  ..., 0.5230, 0.4676, 0.4663],
          [0.4324, 0.4927, 0.4783,  ..., 0.4314, 0.5157, 0.6448]],

         [[0.4104, 0.6507, 0.5950,  ..., 0.4868, 0.4032, 0.5486],
          [0.4718, 0.5880, 0.3415,  ..., 0.5194, 0.4341, 0.2890],
          [0.4744, 0.5034, 0.5114,  ..., 0.4901, 0.4720, 0.6894],
          [0.5167, 0.4516, 0.4482,  ..., 0.3753, 0.5432, 0.5366]]],


        [[[0.5120, 0.5666, 0.5628,  ..., 0.5031, 0.4929, 0.4854],
          [0.4592, 0.6092, 0.4453,  ..., 0.4021, 0.4843, 0.4352],
          [0.4444, 0.5055, 0.4107,  ..., 0.4445, 0.5918, 0.4960],
          [0.4365, 0.4089, 0.5307,  ..., 0.5667, 0.6290, 0.3997]],

         [[0.4981, 0.5871, 0.5590,  ..., 0.5260, 0.4193, 0.4972],
          [0.5558, 0.4427, 0.4465,  ..., 0.5191, 0.5190, 0.5455],
          [0.5678, 0.5766, 0.5467,  ..., 0.4405, 0.7392, 0.5197],
          [0.4536, 0.5395, 0.5894,  ..., 0.4039, 0.4444, 0.6169]],

         [[0.5023, 0.5388, 0.6460,  ..., 0.5048, 0.4326, 0.3134],
          [0.5372, 0.5233, 0.6245,  ..., 0.3564, 0.5337, 0.6466],
          [0.5070, 0.5603, 0.4797,  ..., 0.5276, 0.6613, 0.4218],
          [0.5014, 0.4989, 0.3741,  ..., 0.5493, 0.5914, 0.5982]],

         ...,

         [[0.5034, 0.5061, 0.3197,  ..., 0.3538, 0.6106, 0.4997],
          [0.4609, 0.3777, 0.4404,  ..., 0.4295, 0.6194, 0.5691],
          [0.5249, 0.4832, 0.4567,  ..., 0.3302, 0.4696, 0.7230],
          [0.5188, 0.5417, 0.5441,  ..., 0.4588, 0.4405, 0.4848]],

         [[0.6760, 0.4938, 0.5387,  ..., 0.5131, 0.3897, 0.6582],
          [0.4324, 0.5521, 0.4870,  ..., 0.2397, 0.5714, 0.6151],
          [0.5764, 0.5601, 0.4601,  ..., 0.5147, 0.5536, 0.6502],
          [0.6013, 0.5984, 0.4576,  ..., 0.5042, 0.3874, 0.5939]],

         [[0.5663, 0.4899, 0.5153,  ..., 0.5413, 0.5141, 0.6403],
          [0.7567, 0.5809, 0.4301,  ..., 0.4347, 0.5814, 0.5044],
          [0.3509, 0.5809, 0.6011,  ..., 0.5305, 0.3177, 0.5830],
          [0.4090, 0.4892, 0.5380,  ..., 0.4585, 0.4430, 0.6099]]]],
       device='cuda:0')
tensor([[[[0.6910, 0.3540, 0.3425,  ..., 0.3504, 0.5770, 0.5955],
          [0.4713, 0.6298, 0.5204,  ..., 0.3909, 0.6197, 0.3639],
          [0.4388, 0.4812, 0.3993,  ..., 0.3882, 0.6478, 0.5189],
          [0.5061, 0.7066, 0.6123,  ..., 0.4887, 0.6893, 0.4287]],

         [[0.3693, 0.5082, 0.4828,  ..., 0.4600, 0.5299, 0.3711],
          [0.6270, 0.2942, 0.5832,  ..., 0.5479, 0.5012, 0.5870],
          [0.6011, 0.6169, 0.4412,  ..., 0.6433, 0.5708, 0.4873],
          [0.5983, 0.5761, 0.6343,  ..., 0.5090, 0.3191, 0.5104]],

         [[0.4935, 0.5115, 0.5042,  ..., 0.4562, 0.4603, 0.4201],
          [0.4402, 0.5407, 0.5070,  ..., 0.5660, 0.4045, 0.4732],
          [0.5494, 0.3831, 0.5385,  ..., 0.3775, 0.5576, 0.5737],
          [0.4839, 0.5694, 0.5042,  ..., 0.4144, 0.5799, 0.3863]],

         ...,

         [[0.4900, 0.5182, 0.5463,  ..., 0.4681, 0.5034, 0.4600],
          [0.4073, 0.4642, 0.5211,  ..., 0.4410, 0.3933, 0.4751],
          [0.4424, 0.5590, 0.4263,  ..., 0.7447, 0.6343, 0.4316],
          [0.4591, 0.5138, 0.6137,  ..., 0.5424, 0.4836, 0.4910]],

         [[0.5922, 0.3320, 0.6487,  ..., 0.5151, 0.4083, 0.5107],
          [0.4540, 0.5402, 0.4705,  ..., 0.6252, 0.4797, 0.4887],
          [0.5516, 0.5922, 0.4282,  ..., 0.5170, 0.4656, 0.4603],
          [0.4364, 0.4907, 0.4863,  ..., 0.4254, 0.5137, 0.6388]],

         [[0.4144, 0.6487, 0.6030,  ..., 0.4808, 0.4012, 0.5426],
          [0.4758, 0.5860, 0.3495,  ..., 0.5134, 0.4321, 0.2830],
          [0.4784, 0.5014, 0.5194,  ..., 0.4841, 0.4700, 0.6834],
          [0.5207, 0.4496, 0.4562,  ..., 0.3693, 0.5412, 0.5306]]],


        [[[0.5160, 0.5646, 0.5708,  ..., 0.4971, 0.4909, 0.4794],
          [0.4632, 0.6072, 0.4533,  ..., 0.3961, 0.4823, 0.4292],
          [0.4484, 0.5035, 0.4187,  ..., 0.4385, 0.5898, 0.4900],
          [0.4405, 0.4069, 0.5387,  ..., 0.5607, 0.6270, 0.3937]],

         [[0.5021, 0.5851, 0.5670,  ..., 0.5200, 0.4173, 0.4912],
          [0.5598, 0.4407, 0.4545,  ..., 0.5131, 0.5170, 0.5395],
          [0.5718, 0.5746, 0.5547,  ..., 0.4345, 0.7372, 0.5137],
          [0.4576, 0.5375, 0.5974,  ..., 0.3979, 0.4424, 0.6109]],

         [[0.5063, 0.5368, 0.6540,  ..., 0.4988, 0.4306, 0.3074],
          [0.5412, 0.5213, 0.6325,  ..., 0.3504, 0.5317, 0.6406],
          [0.5110, 0.5583, 0.4877,  ..., 0.5216, 0.6593, 0.4158],
          [0.5054, 0.4969, 0.3821,  ..., 0.5433, 0.5894, 0.5922]],

         ...,

         [[0.5074, 0.5041, 0.3277,  ..., 0.3478, 0.6086, 0.4937],
          [0.4649, 0.3757, 0.4484,  ..., 0.4235, 0.6174, 0.5631],
          [0.5289, 0.4812, 0.4647,  ..., 0.3242, 0.4676, 0.7170],
          [0.5228, 0.5397, 0.5521,  ..., 0.4528, 0.4385, 0.4788]],

         [[0.6800, 0.4918, 0.5467,  ..., 0.5071, 0.3877, 0.6522],
          [0.4364, 0.5501, 0.4950,  ..., 0.2337, 0.5694, 0.6091],
          [0.5804, 0.5581, 0.4681,  ..., 0.5087, 0.5516, 0.6442],
          [0.6053, 0.5964, 0.4656,  ..., 0.4982, 0.3854, 0.5879]],

         [[0.5703, 0.4879, 0.5233,  ..., 0.5353, 0.5121, 0.6343],
          [0.7607, 0.5789, 0.4381,  ..., 0.4287, 0.5794, 0.4984],
          [0.3549, 0.5789, 0.6091,  ..., 0.5245, 0.3157, 0.5770],
          [0.4130, 0.4872, 0.5460,  ..., 0.4525, 0.4410, 0.6039]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0040,  0.0020, -0.0080,  0.0000, -0.0060, -0.0040,  0.0060,  0.0060,
         0.0020,  0.0060], device='cuda:0')
selected experts tensor([1716, 1677, 1594, 1575, 1622, 1711, 1662, 1626, 1605, 1596],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5253, 0.6520, 0.4643,  ..., 0.4501, 0.5097, 0.5014],
          [0.4258, 0.5346, 0.4786,  ..., 0.6547, 0.3971, 0.5044],
          [0.4651, 0.4411, 0.4849,  ..., 0.6485, 0.3551, 0.5823],
          [0.4984, 0.5855, 0.4347,  ..., 0.6395, 0.4497, 0.4876]],

         [[0.4025, 0.5140, 0.4968,  ..., 0.4455, 0.5975, 0.4899],
          [0.4176, 0.5611, 0.4400,  ..., 0.4631, 0.3939, 0.4273],
          [0.4921, 0.6019, 0.5289,  ..., 0.5041, 0.4405, 0.5395],
          [0.5546, 0.4428, 0.5610,  ..., 0.3815, 0.4389, 0.4111]],

         [[0.5764, 0.5616, 0.5317,  ..., 0.5602, 0.5420, 0.6063],
          [0.4063, 0.6061, 0.3699,  ..., 0.6191, 0.3981, 0.4230],
          [0.4923, 0.5457, 0.5665,  ..., 0.5982, 0.4803, 0.4092],
          [0.5926, 0.4229, 0.3868,  ..., 0.3987, 0.5144, 0.5229]],

         ...,

         [[0.5745, 0.5098, 0.5377,  ..., 0.6386, 0.4599, 0.4873],
          [0.5228, 0.4428, 0.5048,  ..., 0.4153, 0.5282, 0.4733],
          [0.5181, 0.6341, 0.3467,  ..., 0.5231, 0.5524, 0.5107],
          [0.5527, 0.5132, 0.6574,  ..., 0.3834, 0.6726, 0.6270]],

         [[0.4196, 0.5355, 0.4376,  ..., 0.3484, 0.2779, 0.5542],
          [0.3908, 0.5232, 0.4105,  ..., 0.4815, 0.4526, 0.6558],
          [0.5664, 0.6906, 0.4266,  ..., 0.6746, 0.3722, 0.5351],
          [0.4607, 0.6098, 0.4063,  ..., 0.3889, 0.5352, 0.6156]],

         [[0.4748, 0.4605, 0.4932,  ..., 0.3511, 0.5586, 0.4244],
          [0.6377, 0.6287, 0.5254,  ..., 0.5568, 0.4526, 0.3720],
          [0.5926, 0.5309, 0.5145,  ..., 0.4925, 0.5790, 0.3984],
          [0.6404, 0.4215, 0.4347,  ..., 0.4011, 0.4589, 0.6252]]],


        [[[0.4764, 0.4988, 0.4808,  ..., 0.5270, 0.5163, 0.5547],
          [0.5080, 0.6177, 0.4290,  ..., 0.6103, 0.4365, 0.6316],
          [0.5282, 0.4310, 0.5920,  ..., 0.5307, 0.5304, 0.2838],
          [0.4741, 0.7070, 0.5375,  ..., 0.5935, 0.4934, 0.4064]],

         [[0.5022, 0.4595, 0.3555,  ..., 0.4503, 0.2733, 0.5287],
          [0.4885, 0.6476, 0.3979,  ..., 0.5556, 0.5229, 0.4385],
          [0.5491, 0.4301, 0.4639,  ..., 0.6205, 0.6111, 0.5617],
          [0.7260, 0.4880, 0.4805,  ..., 0.5025, 0.3897, 0.6234]],

         [[0.4908, 0.5324, 0.5234,  ..., 0.5764, 0.4925, 0.5479],
          [0.4191, 0.6200, 0.6155,  ..., 0.5472, 0.4497, 0.3900],
          [0.6323, 0.5157, 0.4218,  ..., 0.5822, 0.4726, 0.5431],
          [0.4105, 0.4392, 0.5345,  ..., 0.5154, 0.4425, 0.6179]],

         ...,

         [[0.2890, 0.5488, 0.5145,  ..., 0.4234, 0.6176, 0.4976],
          [0.4044, 0.4390, 0.6367,  ..., 0.6149, 0.5294, 0.5964],
          [0.5009, 0.4648, 0.6349,  ..., 0.5326, 0.5776, 0.4647],
          [0.4935, 0.5426, 0.6448,  ..., 0.4143, 0.4650, 0.4564]],

         [[0.6043, 0.4344, 0.5854,  ..., 0.4423, 0.4365, 0.5397],
          [0.4689, 0.4124, 0.5544,  ..., 0.5430, 0.5212, 0.5327],
          [0.4634, 0.7205, 0.6062,  ..., 0.5481, 0.4454, 0.5271],
          [0.4334, 0.3475, 0.5849,  ..., 0.4248, 0.5390, 0.5195]],

         [[0.5065, 0.5812, 0.4114,  ..., 0.4525, 0.3786, 0.5356],
          [0.4551, 0.4897, 0.5474,  ..., 0.4086, 0.3990, 0.5044],
          [0.4200, 0.5215, 0.5269,  ..., 0.4627, 0.5326, 0.4397],
          [0.5023, 0.5198, 0.5199,  ..., 0.5693, 0.4417, 0.4591]]]],
       device='cuda:0')
tensor([[[[0.5273, 0.6540, 0.4583,  ..., 0.4521, 0.5077, 0.5014],
          [0.4278, 0.5366, 0.4726,  ..., 0.6567, 0.3951, 0.5044],
          [0.4671, 0.4431, 0.4789,  ..., 0.6505, 0.3531, 0.5823],
          [0.5004, 0.5875, 0.4287,  ..., 0.6415, 0.4477, 0.4876]],

         [[0.4045, 0.5160, 0.4908,  ..., 0.4475, 0.5955, 0.4899],
          [0.4196, 0.5631, 0.4340,  ..., 0.4651, 0.3919, 0.4273],
          [0.4941, 0.6039, 0.5229,  ..., 0.5061, 0.4385, 0.5395],
          [0.5566, 0.4448, 0.5550,  ..., 0.3835, 0.4369, 0.4111]],

         [[0.5784, 0.5636, 0.5257,  ..., 0.5622, 0.5400, 0.6063],
          [0.4083, 0.6081, 0.3639,  ..., 0.6211, 0.3961, 0.4230],
          [0.4943, 0.5477, 0.5605,  ..., 0.6002, 0.4783, 0.4092],
          [0.5946, 0.4249, 0.3808,  ..., 0.4007, 0.5124, 0.5229]],

         ...,

         [[0.5765, 0.5118, 0.5317,  ..., 0.6406, 0.4579, 0.4873],
          [0.5248, 0.4448, 0.4988,  ..., 0.4173, 0.5262, 0.4733],
          [0.5201, 0.6361, 0.3407,  ..., 0.5251, 0.5504, 0.5107],
          [0.5547, 0.5152, 0.6514,  ..., 0.3854, 0.6706, 0.6270]],

         [[0.4216, 0.5375, 0.4316,  ..., 0.3504, 0.2759, 0.5542],
          [0.3928, 0.5252, 0.4045,  ..., 0.4835, 0.4506, 0.6558],
          [0.5684, 0.6926, 0.4206,  ..., 0.6766, 0.3702, 0.5351],
          [0.4627, 0.6118, 0.4003,  ..., 0.3909, 0.5332, 0.6156]],

         [[0.4768, 0.4625, 0.4872,  ..., 0.3531, 0.5566, 0.4244],
          [0.6397, 0.6307, 0.5194,  ..., 0.5588, 0.4506, 0.3720],
          [0.5946, 0.5329, 0.5085,  ..., 0.4945, 0.5770, 0.3984],
          [0.6424, 0.4235, 0.4287,  ..., 0.4031, 0.4569, 0.6252]]],


        [[[0.4784, 0.5008, 0.4748,  ..., 0.5290, 0.5143, 0.5547],
          [0.5100, 0.6197, 0.4230,  ..., 0.6123, 0.4345, 0.6316],
          [0.5302, 0.4330, 0.5860,  ..., 0.5327, 0.5284, 0.2838],
          [0.4761, 0.7090, 0.5315,  ..., 0.5955, 0.4914, 0.4064]],

         [[0.5042, 0.4615, 0.3495,  ..., 0.4523, 0.2713, 0.5287],
          [0.4905, 0.6496, 0.3919,  ..., 0.5576, 0.5209, 0.4385],
          [0.5511, 0.4321, 0.4579,  ..., 0.6225, 0.6091, 0.5617],
          [0.7280, 0.4900, 0.4745,  ..., 0.5045, 0.3877, 0.6234]],

         [[0.4928, 0.5344, 0.5174,  ..., 0.5784, 0.4905, 0.5479],
          [0.4211, 0.6220, 0.6095,  ..., 0.5492, 0.4477, 0.3900],
          [0.6343, 0.5177, 0.4158,  ..., 0.5842, 0.4706, 0.5431],
          [0.4125, 0.4412, 0.5285,  ..., 0.5174, 0.4405, 0.6179]],

         ...,

         [[0.2910, 0.5508, 0.5085,  ..., 0.4254, 0.6156, 0.4976],
          [0.4064, 0.4410, 0.6307,  ..., 0.6169, 0.5274, 0.5964],
          [0.5029, 0.4668, 0.6289,  ..., 0.5346, 0.5756, 0.4647],
          [0.4955, 0.5446, 0.6388,  ..., 0.4163, 0.4630, 0.4564]],

         [[0.6063, 0.4364, 0.5794,  ..., 0.4443, 0.4345, 0.5397],
          [0.4709, 0.4144, 0.5484,  ..., 0.5450, 0.5192, 0.5327],
          [0.4654, 0.7225, 0.6002,  ..., 0.5501, 0.4434, 0.5271],
          [0.4354, 0.3495, 0.5789,  ..., 0.4268, 0.5370, 0.5195]],

         [[0.5085, 0.5832, 0.4054,  ..., 0.4545, 0.3766, 0.5356],
          [0.4571, 0.4917, 0.5414,  ..., 0.4106, 0.3970, 0.5044],
          [0.4220, 0.5235, 0.5209,  ..., 0.4647, 0.5306, 0.4397],
          [0.5043, 0.5218, 0.5139,  ..., 0.5713, 0.4397, 0.4591]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0020, -0.0020,  0.0060,  0.0080, -0.0020,  0.0080, -0.0100, -0.0020,
         0.0020,  0.0000], device='cuda:0')
selected experts tensor([1632, 1686, 1675, 1478, 1627, 1567, 1667, 1791, 1515, 1746],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5180, 0.4364, 0.5190,  ..., 0.3727, 0.4729, 0.6577],
          [0.3594, 0.4732, 0.5334,  ..., 0.7068, 0.5152, 0.4468],
          [0.5637, 0.4601, 0.4988,  ..., 0.4495, 0.4550, 0.5705],
          [0.6361, 0.5947, 0.6468,  ..., 0.5252, 0.5836, 0.4879]],

         [[0.5541, 0.5625, 0.3889,  ..., 0.3736, 0.4722, 0.5697],
          [0.5106, 0.5782, 0.4462,  ..., 0.6139, 0.4531, 0.3532],
          [0.5126, 0.4495, 0.5229,  ..., 0.4476, 0.5020, 0.3048],
          [0.6441, 0.4204, 0.4795,  ..., 0.5420, 0.5004, 0.6388]],

         [[0.5404, 0.4986, 0.6001,  ..., 0.4015, 0.4615, 0.5473],
          [0.5561, 0.5668, 0.4320,  ..., 0.5442, 0.3541, 0.3612],
          [0.5203, 0.5042, 0.5652,  ..., 0.4662, 0.5993, 0.3523],
          [0.6727, 0.4295, 0.5742,  ..., 0.4901, 0.5342, 0.5798]],

         ...,

         [[0.5229, 0.6193, 0.6379,  ..., 0.5031, 0.5346, 0.6143],
          [0.4946, 0.3410, 0.5102,  ..., 0.5043, 0.5185, 0.6890],
          [0.4295, 0.5601, 0.4467,  ..., 0.5242, 0.5132, 0.5323],
          [0.5675, 0.7128, 0.5006,  ..., 0.4780, 0.5690, 0.4907]],

         [[0.5433, 0.5980, 0.5297,  ..., 0.5210, 0.5056, 0.5324],
          [0.3983, 0.4808, 0.5467,  ..., 0.6093, 0.4734, 0.5603],
          [0.5300, 0.4343, 0.5804,  ..., 0.5114, 0.5260, 0.5207],
          [0.4854, 0.5053, 0.6102,  ..., 0.5174, 0.4666, 0.7058]],

         [[0.5389, 0.4816, 0.5537,  ..., 0.4609, 0.4425, 0.5755],
          [0.4703, 0.5891, 0.6512,  ..., 0.5141, 0.6101, 0.5709],
          [0.4609, 0.5059, 0.7048,  ..., 0.6231, 0.5285, 0.4425],
          [0.6217, 0.3939, 0.6564,  ..., 0.5357, 0.3648, 0.4569]]],


        [[[0.5476, 0.5796, 0.5690,  ..., 0.3360, 0.3523, 0.4055],
          [0.5959, 0.6464, 0.5431,  ..., 0.5568, 0.2952, 0.4842],
          [0.5472, 0.4707, 0.6070,  ..., 0.4360, 0.5822, 0.5470],
          [0.6144, 0.3799, 0.4135,  ..., 0.5630, 0.4930, 0.4652]],

         [[0.5108, 0.5881, 0.5189,  ..., 0.4057, 0.3506, 0.4786],
          [0.5027, 0.6302, 0.4926,  ..., 0.6023, 0.4079, 0.4277],
          [0.4857, 0.4451, 0.4979,  ..., 0.4719, 0.6139, 0.4220],
          [0.6019, 0.3375, 0.5155,  ..., 0.5621, 0.5356, 0.5567]],

         [[0.4854, 0.4999, 0.4370,  ..., 0.5353, 0.5205, 0.4925],
          [0.3908, 0.6045, 0.3594,  ..., 0.6633, 0.4514, 0.3436],
          [0.4288, 0.4708, 0.4914,  ..., 0.3377, 0.5596, 0.4244],
          [0.4856, 0.3808, 0.5266,  ..., 0.5616, 0.5306, 0.4712]],

         ...,

         [[0.4778, 0.4950, 0.6811,  ..., 0.5692, 0.6523, 0.5523],
          [0.4956, 0.3832, 0.5851,  ..., 0.5229, 0.3488, 0.6106],
          [0.5081, 0.4543, 0.5200,  ..., 0.5367, 0.5233, 0.4989],
          [0.5505, 0.5881, 0.4756,  ..., 0.4512, 0.4248, 0.4908]],

         [[0.5846, 0.4756, 0.4375,  ..., 0.3987, 0.5836, 0.5974],
          [0.3922, 0.5449, 0.5780,  ..., 0.6719, 0.6162, 0.5189],
          [0.4293, 0.4161, 0.3531,  ..., 0.5981, 0.5317, 0.4121],
          [0.4770, 0.5364, 0.5685,  ..., 0.5906, 0.4065, 0.7204]],

         [[0.5119, 0.3934, 0.6130,  ..., 0.3842, 0.4239, 0.4055],
          [0.5053, 0.4945, 0.4002,  ..., 0.5551, 0.5932, 0.5741],
          [0.5685, 0.4137, 0.7024,  ..., 0.6139, 0.5572, 0.4439],
          [0.6325, 0.4777, 0.6468,  ..., 0.4752, 0.4154, 0.4435]]]],
       device='cuda:0')
tensor([[[[0.5270, 0.4414, 0.5280,  ..., 0.3757, 0.4639, 0.6487],
          [0.3684, 0.4782, 0.5424,  ..., 0.7098, 0.5062, 0.4378],
          [0.5727, 0.4651, 0.5078,  ..., 0.4525, 0.4460, 0.5615],
          [0.6451, 0.5997, 0.6558,  ..., 0.5282, 0.5746, 0.4789]],

         [[0.5631, 0.5675, 0.3979,  ..., 0.3766, 0.4632, 0.5607],
          [0.5196, 0.5832, 0.4552,  ..., 0.6169, 0.4441, 0.3442],
          [0.5216, 0.4545, 0.5319,  ..., 0.4506, 0.4930, 0.2958],
          [0.6531, 0.4254, 0.4885,  ..., 0.5450, 0.4914, 0.6298]],

         [[0.5494, 0.5036, 0.6091,  ..., 0.4045, 0.4525, 0.5383],
          [0.5651, 0.5718, 0.4410,  ..., 0.5472, 0.3451, 0.3522],
          [0.5293, 0.5092, 0.5742,  ..., 0.4692, 0.5903, 0.3433],
          [0.6817, 0.4345, 0.5832,  ..., 0.4931, 0.5252, 0.5708]],

         ...,

         [[0.5319, 0.6243, 0.6469,  ..., 0.5061, 0.5256, 0.6053],
          [0.5036, 0.3460, 0.5192,  ..., 0.5073, 0.5095, 0.6800],
          [0.4385, 0.5651, 0.4557,  ..., 0.5272, 0.5042, 0.5233],
          [0.5765, 0.7178, 0.5096,  ..., 0.4810, 0.5600, 0.4817]],

         [[0.5523, 0.6030, 0.5387,  ..., 0.5240, 0.4966, 0.5234],
          [0.4073, 0.4858, 0.5557,  ..., 0.6123, 0.4644, 0.5513],
          [0.5390, 0.4393, 0.5894,  ..., 0.5144, 0.5170, 0.5117],
          [0.4944, 0.5103, 0.6192,  ..., 0.5204, 0.4576, 0.6968]],

         [[0.5479, 0.4866, 0.5627,  ..., 0.4639, 0.4335, 0.5665],
          [0.4793, 0.5941, 0.6602,  ..., 0.5171, 0.6011, 0.5619],
          [0.4699, 0.5109, 0.7138,  ..., 0.6261, 0.5195, 0.4335],
          [0.6307, 0.3989, 0.6654,  ..., 0.5387, 0.3558, 0.4479]]],


        [[[0.5566, 0.5846, 0.5780,  ..., 0.3390, 0.3433, 0.3965],
          [0.6049, 0.6514, 0.5521,  ..., 0.5598, 0.2862, 0.4752],
          [0.5562, 0.4757, 0.6160,  ..., 0.4390, 0.5732, 0.5380],
          [0.6234, 0.3849, 0.4225,  ..., 0.5660, 0.4840, 0.4562]],

         [[0.5198, 0.5931, 0.5279,  ..., 0.4087, 0.3416, 0.4696],
          [0.5117, 0.6352, 0.5016,  ..., 0.6053, 0.3989, 0.4187],
          [0.4947, 0.4501, 0.5069,  ..., 0.4749, 0.6049, 0.4130],
          [0.6109, 0.3425, 0.5245,  ..., 0.5651, 0.5266, 0.5477]],

         [[0.4944, 0.5049, 0.4460,  ..., 0.5383, 0.5115, 0.4835],
          [0.3998, 0.6095, 0.3684,  ..., 0.6663, 0.4424, 0.3346],
          [0.4378, 0.4758, 0.5004,  ..., 0.3407, 0.5506, 0.4154],
          [0.4946, 0.3858, 0.5356,  ..., 0.5646, 0.5216, 0.4622]],

         ...,

         [[0.4868, 0.5000, 0.6901,  ..., 0.5722, 0.6433, 0.5433],
          [0.5046, 0.3882, 0.5941,  ..., 0.5259, 0.3398, 0.6016],
          [0.5171, 0.4593, 0.5290,  ..., 0.5397, 0.5143, 0.4899],
          [0.5595, 0.5931, 0.4846,  ..., 0.4542, 0.4158, 0.4818]],

         [[0.5936, 0.4806, 0.4465,  ..., 0.4017, 0.5746, 0.5884],
          [0.4012, 0.5499, 0.5870,  ..., 0.6749, 0.6072, 0.5099],
          [0.4383, 0.4211, 0.3621,  ..., 0.6011, 0.5227, 0.4031],
          [0.4860, 0.5414, 0.5775,  ..., 0.5936, 0.3975, 0.7114]],

         [[0.5209, 0.3984, 0.6220,  ..., 0.3872, 0.4149, 0.3965],
          [0.5143, 0.4995, 0.4092,  ..., 0.5581, 0.5842, 0.5651],
          [0.5775, 0.4187, 0.7114,  ..., 0.6169, 0.5482, 0.4349],
          [0.6415, 0.4827, 0.6558,  ..., 0.4782, 0.4064, 0.4345]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0090, -0.0050, -0.0090,  0.0070, -0.0090,  0.0090, -0.0090, -0.0030,
         0.0090,  0.0090], device='cuda:0')
selected experts tensor([1973, 1838, 1927, 1678, 1768,  689, 2429, 1807, 1272, 1003],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4721, 0.4926, 0.5262,  ..., 0.3608, 0.4472, 0.4271],
          [0.3049, 0.4261, 0.5078,  ..., 0.4546, 0.3807, 0.5513],
          [0.3475, 0.4390, 0.4384,  ..., 0.5260, 0.5929, 0.5394],
          [0.5071, 0.5156, 0.6736,  ..., 0.4829, 0.5561, 0.4026]],

         [[0.6201, 0.5149, 0.5252,  ..., 0.3725, 0.5542, 0.4810],
          [0.4443, 0.4802, 0.5362,  ..., 0.5341, 0.5887, 0.3850],
          [0.6071, 0.4542, 0.4437,  ..., 0.3698, 0.4614, 0.5812],
          [0.5500, 0.5925, 0.4919,  ..., 0.6052, 0.5024, 0.4482]],

         [[0.5650, 0.3761, 0.4179,  ..., 0.4748, 0.3644, 0.6476],
          [0.4650, 0.4419, 0.4996,  ..., 0.3770, 0.5944, 0.4880],
          [0.5268, 0.5445, 0.3472,  ..., 0.5156, 0.5145, 0.4233],
          [0.5048, 0.5315, 0.6710,  ..., 0.5102, 0.5677, 0.4870]],

         ...,

         [[0.5050, 0.5050, 0.5030,  ..., 0.5050, 0.5050, 0.5070],
          [0.5050, 0.5050, 0.5030,  ..., 0.5050, 0.5050, 0.5070],
          [0.5050, 0.5050, 0.5030,  ..., 0.5050, 0.5050, 0.5070],
          [0.5050, 0.5050, 0.5030,  ..., 0.5050, 0.5050, 0.5070]],

         [[0.5050, 0.5050, 0.5030,  ..., 0.5050, 0.5050, 0.5070],
          [0.5050, 0.5050, 0.5030,  ..., 0.5050, 0.5050, 0.5070],
          [0.5050, 0.5050, 0.5030,  ..., 0.5050, 0.5050, 0.5070],
          [0.5050, 0.5050, 0.5030,  ..., 0.5050, 0.5050, 0.5070]],

         [[0.5050, 0.5050, 0.5030,  ..., 0.5050, 0.5050, 0.5070],
          [0.5050, 0.5050, 0.5030,  ..., 0.5050, 0.5050, 0.5070],
          [0.5050, 0.5050, 0.5030,  ..., 0.5050, 0.5050, 0.5070],
          [0.5050, 0.5050, 0.5030,  ..., 0.5050, 0.5050, 0.5070]]],


        [[[0.4561, 0.5285, 0.4284,  ..., 0.4469, 0.5628, 0.3809],
          [0.3492, 0.5202, 0.4432,  ..., 0.5094, 0.5418, 0.6226],
          [0.6420, 0.5396, 0.3561,  ..., 0.4602, 0.4232, 0.5062],
          [0.5139, 0.4515, 0.5786,  ..., 0.5787, 0.5144, 0.4054]],

         [[0.5715, 0.4337, 0.4741,  ..., 0.5044, 0.5701, 0.4932],
          [0.3835, 0.4766, 0.4146,  ..., 0.3752, 0.5739, 0.5648],
          [0.5399, 0.6275, 0.4464,  ..., 0.4318, 0.4185, 0.4300],
          [0.4199, 0.5065, 0.5519,  ..., 0.6206, 0.5500, 0.4362]],

         [[0.5300, 0.4498, 0.4863,  ..., 0.4289, 0.5643, 0.4566],
          [0.4488, 0.4735, 0.4288,  ..., 0.6173, 0.4965, 0.5689],
          [0.5681, 0.5262, 0.4757,  ..., 0.5083, 0.4751, 0.4537],
          [0.4366, 0.4246, 0.5541,  ..., 0.6564, 0.4655, 0.3970]],

         ...,

         [[0.5050, 0.5050, 0.5030,  ..., 0.5050, 0.5050, 0.5070],
          [0.5050, 0.5050, 0.5030,  ..., 0.5050, 0.5050, 0.5070],
          [0.5050, 0.5050, 0.5030,  ..., 0.5050, 0.5050, 0.5070],
          [0.5050, 0.5050, 0.5030,  ..., 0.5050, 0.5050, 0.5070]],

         [[0.5050, 0.5050, 0.5030,  ..., 0.5050, 0.5050, 0.5070],
          [0.5050, 0.5050, 0.5030,  ..., 0.5050, 0.5050, 0.5070],
          [0.5050, 0.5050, 0.5030,  ..., 0.5050, 0.5050, 0.5070],
          [0.5050, 0.5050, 0.5030,  ..., 0.5050, 0.5050, 0.5070]],

         [[0.5050, 0.5050, 0.5030,  ..., 0.5050, 0.5050, 0.5070],
          [0.5050, 0.5050, 0.5030,  ..., 0.5050, 0.5050, 0.5070],
          [0.5050, 0.5050, 0.5030,  ..., 0.5050, 0.5050, 0.5070],
          [0.5050, 0.5050, 0.5030,  ..., 0.5050, 0.5050, 0.5070]]]],
       device='cuda:0')
tensor([[[[0.4671, 0.4876, 0.5232,  ..., 0.3558, 0.4422, 0.4201],
          [0.2999, 0.4211, 0.5048,  ..., 0.4496, 0.3757, 0.5443],
          [0.3425, 0.4340, 0.4354,  ..., 0.5210, 0.5879, 0.5324],
          [0.5021, 0.5106, 0.6706,  ..., 0.4779, 0.5511, 0.3956]],

         [[0.6151, 0.5099, 0.5222,  ..., 0.3675, 0.5492, 0.4740],
          [0.4393, 0.4752, 0.5332,  ..., 0.5291, 0.5837, 0.3780],
          [0.6021, 0.4492, 0.4407,  ..., 0.3648, 0.4564, 0.5742],
          [0.5450, 0.5875, 0.4889,  ..., 0.6002, 0.4974, 0.4412]],

         [[0.5600, 0.3711, 0.4149,  ..., 0.4698, 0.3594, 0.6406],
          [0.4600, 0.4369, 0.4966,  ..., 0.3720, 0.5894, 0.4810],
          [0.5218, 0.5395, 0.3442,  ..., 0.5106, 0.5095, 0.4163],
          [0.4998, 0.5265, 0.6680,  ..., 0.5052, 0.5627, 0.4800]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.4511, 0.5235, 0.4254,  ..., 0.4419, 0.5578, 0.3739],
          [0.3442, 0.5152, 0.4402,  ..., 0.5044, 0.5368, 0.6156],
          [0.6370, 0.5346, 0.3531,  ..., 0.4552, 0.4182, 0.4992],
          [0.5089, 0.4465, 0.5756,  ..., 0.5737, 0.5094, 0.3984]],

         [[0.5665, 0.4287, 0.4711,  ..., 0.4994, 0.5651, 0.4862],
          [0.3785, 0.4716, 0.4116,  ..., 0.3702, 0.5689, 0.5578],
          [0.5349, 0.6225, 0.4434,  ..., 0.4268, 0.4135, 0.4230],
          [0.4149, 0.5015, 0.5489,  ..., 0.6156, 0.5450, 0.4292]],

         [[0.5250, 0.4448, 0.4833,  ..., 0.4239, 0.5593, 0.4496],
          [0.4438, 0.4685, 0.4258,  ..., 0.6123, 0.4915, 0.5619],
          [0.5631, 0.5212, 0.4727,  ..., 0.5033, 0.4701, 0.4467],
          [0.4316, 0.4196, 0.5511,  ..., 0.6514, 0.4605, 0.3900]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([0.0050, 0.0050, 0.0030, 0.0050, 0.0030, 0.0050, 0.0050, 0.0050, 0.0050,
        0.0070], device='cuda:0')
selected experts tensor([2933, 1326, 1517, 1042, 2491,  963,  740, 1134, 1363, 2875],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1632., 1686., 1675., 1478., 1627., 1567., 1667., 1791., 1515., 1746.],
        [2933., 1326., 1517., 1042., 2491.,  963.,  740., 1134., 1363., 2875.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5013, 0.5376, 0.6470,  ..., 0.5058, 0.4346, 0.3135],
          [0.5362, 0.5222, 0.6255,  ..., 0.3574, 0.5347, 0.6476],
          [0.5060, 0.5593, 0.4807,  ..., 0.5286, 0.6623, 0.4228],
          [0.5004, 0.4979, 0.3751,  ..., 0.5503, 0.5924, 0.5992]],

         [[0.5163, 0.4326, 0.5112,  ..., 0.4472, 0.6079, 0.6267],
          [0.4541, 0.7351, 0.5100,  ..., 0.4581, 0.5999, 0.5257],
          [0.5914, 0.4695, 0.4982,  ..., 0.4578, 0.5524, 0.5280],
          [0.5127, 0.4905, 0.4193,  ..., 0.5017, 0.5473, 0.3850]],

         [[0.4442, 0.5785, 0.4431,  ..., 0.5353, 0.6508, 0.3373],
          [0.4626, 0.6861, 0.6730,  ..., 0.5588, 0.4284, 0.3896],
          [0.5468, 0.4321, 0.4643,  ..., 0.4935, 0.6873, 0.5254],
          [0.6184, 0.5209, 0.4572,  ..., 0.5164, 0.5028, 0.6202]],

         ...,

         [[0.4933, 0.5569, 0.4093,  ..., 0.5055, 0.5254, 0.5907],
          [0.4791, 0.3901, 0.4506,  ..., 0.4726, 0.5118, 0.3495],
          [0.3571, 0.5946, 0.6113,  ..., 0.3868, 0.5961, 0.5503],
          [0.5591, 0.2872, 0.5320,  ..., 0.6221, 0.5577, 0.4300]],

         [[0.6055, 0.4884, 0.5042,  ..., 0.4309, 0.4052, 0.5465],
          [0.4037, 0.5588, 0.4613,  ..., 0.6193, 0.6337, 0.4936],
          [0.5277, 0.4731, 0.5165,  ..., 0.5687, 0.5410, 0.3637],
          [0.5502, 0.5499, 0.4475,  ..., 0.6253, 0.5177, 0.5182]],

         [[0.4570, 0.4784, 0.4817,  ..., 0.6619, 0.5752, 0.6896],
          [0.5451, 0.3339, 0.4749,  ..., 0.6637, 0.4928, 0.6467],
          [0.5187, 0.6690, 0.4088,  ..., 0.4894, 0.4331, 0.5897],
          [0.5209, 0.6603, 0.5202,  ..., 0.4087, 0.4514, 0.3965]]],


        [[[0.5531, 0.4838, 0.5103,  ..., 0.5508, 0.5432, 0.5380],
          [0.4080, 0.4439, 0.5181,  ..., 0.5525, 0.5480, 0.5549],
          [0.5758, 0.5852, 0.5686,  ..., 0.6123, 0.4023, 0.5845],
          [0.4543, 0.3804, 0.3696,  ..., 0.5598, 0.4871, 0.4012]],

         [[0.4180, 0.4706, 0.5400,  ..., 0.4410, 0.5900, 0.3975],
          [0.5359, 0.6707, 0.4982,  ..., 0.4792, 0.2964, 0.5530],
          [0.4156, 0.5545, 0.4122,  ..., 0.4400, 0.3289, 0.4338],
          [0.7120, 0.3933, 0.4103,  ..., 0.4186, 0.6060, 0.4040]],

         [[0.3348, 0.5927, 0.6514,  ..., 0.5826, 0.4331, 0.5973],
          [0.5668, 0.5264, 0.5303,  ..., 0.5166, 0.3972, 0.5545],
          [0.5289, 0.5020, 0.5337,  ..., 0.4371, 0.4394, 0.4991],
          [0.4953, 0.5378, 0.3293,  ..., 0.5474, 0.4336, 0.4286]],

         ...,

         [[0.4866, 0.5756, 0.4407,  ..., 0.5610, 0.6753, 0.5305],
          [0.2788, 0.5124, 0.5363,  ..., 0.5564, 0.6544, 0.6184],
          [0.3357, 0.4572, 0.5566,  ..., 0.4564, 0.5625, 0.5196],
          [0.4090, 0.4249, 0.4074,  ..., 0.4726, 0.5442, 0.5274]],

         [[0.3897, 0.6416, 0.5349,  ..., 0.5607, 0.3833, 0.4959],
          [0.5437, 0.5317, 0.4453,  ..., 0.3928, 0.4274, 0.5508],
          [0.4869, 0.5395, 0.6372,  ..., 0.6601, 0.4895, 0.4839],
          [0.4818, 0.6647, 0.4574,  ..., 0.3486, 0.4606, 0.3530]],

         [[0.4791, 0.4671, 0.3104,  ..., 0.4835, 0.4355, 0.5816],
          [0.4835, 0.5453, 0.3320,  ..., 0.3211, 0.4141, 0.5788],
          [0.4425, 0.5538, 0.5530,  ..., 0.4858, 0.3393, 0.4978],
          [0.3244, 0.4821, 0.4112,  ..., 0.5059, 0.4623, 0.2892]]]],
       device='cuda:0')
tensor([[[[0.5063, 0.5366, 0.6540,  ..., 0.4988, 0.4316, 0.3065],
          [0.5412, 0.5212, 0.6325,  ..., 0.3504, 0.5317, 0.6406],
          [0.5110, 0.5583, 0.4877,  ..., 0.5216, 0.6593, 0.4158],
          [0.5054, 0.4969, 0.3821,  ..., 0.5433, 0.5894, 0.5922]],

         [[0.5213, 0.4316, 0.5182,  ..., 0.4402, 0.6049, 0.6197],
          [0.4591, 0.7341, 0.5170,  ..., 0.4511, 0.5969, 0.5187],
          [0.5964, 0.4685, 0.5052,  ..., 0.4508, 0.5494, 0.5210],
          [0.5177, 0.4895, 0.4263,  ..., 0.4947, 0.5443, 0.3780]],

         [[0.4492, 0.5775, 0.4501,  ..., 0.5283, 0.6478, 0.3303],
          [0.4676, 0.6851, 0.6800,  ..., 0.5518, 0.4254, 0.3826],
          [0.5518, 0.4311, 0.4713,  ..., 0.4865, 0.6843, 0.5184],
          [0.6234, 0.5199, 0.4642,  ..., 0.5094, 0.4998, 0.6132]],

         ...,

         [[0.4983, 0.5559, 0.4163,  ..., 0.4985, 0.5224, 0.5837],
          [0.4841, 0.3891, 0.4576,  ..., 0.4656, 0.5088, 0.3425],
          [0.3621, 0.5936, 0.6183,  ..., 0.3798, 0.5931, 0.5433],
          [0.5641, 0.2862, 0.5390,  ..., 0.6151, 0.5547, 0.4230]],

         [[0.6105, 0.4874, 0.5112,  ..., 0.4239, 0.4022, 0.5395],
          [0.4087, 0.5578, 0.4683,  ..., 0.6123, 0.6307, 0.4866],
          [0.5327, 0.4721, 0.5235,  ..., 0.5617, 0.5380, 0.3567],
          [0.5552, 0.5489, 0.4545,  ..., 0.6183, 0.5147, 0.5112]],

         [[0.4620, 0.4774, 0.4887,  ..., 0.6549, 0.5722, 0.6826],
          [0.5501, 0.3329, 0.4819,  ..., 0.6567, 0.4898, 0.6397],
          [0.5237, 0.6680, 0.4158,  ..., 0.4824, 0.4301, 0.5827],
          [0.5259, 0.6593, 0.5272,  ..., 0.4017, 0.4484, 0.3895]]],


        [[[0.5581, 0.4828, 0.5173,  ..., 0.5438, 0.5402, 0.5310],
          [0.4130, 0.4429, 0.5251,  ..., 0.5455, 0.5450, 0.5479],
          [0.5808, 0.5842, 0.5756,  ..., 0.6053, 0.3993, 0.5775],
          [0.4593, 0.3794, 0.3766,  ..., 0.5528, 0.4841, 0.3942]],

         [[0.4230, 0.4696, 0.5470,  ..., 0.4340, 0.5870, 0.3905],
          [0.5409, 0.6697, 0.5052,  ..., 0.4722, 0.2934, 0.5460],
          [0.4206, 0.5535, 0.4192,  ..., 0.4330, 0.3259, 0.4268],
          [0.7170, 0.3923, 0.4173,  ..., 0.4116, 0.6030, 0.3970]],

         [[0.3398, 0.5917, 0.6584,  ..., 0.5756, 0.4301, 0.5903],
          [0.5718, 0.5254, 0.5373,  ..., 0.5096, 0.3942, 0.5475],
          [0.5339, 0.5010, 0.5407,  ..., 0.4301, 0.4364, 0.4921],
          [0.5003, 0.5368, 0.3363,  ..., 0.5404, 0.4306, 0.4216]],

         ...,

         [[0.4916, 0.5746, 0.4477,  ..., 0.5540, 0.6723, 0.5235],
          [0.2838, 0.5114, 0.5433,  ..., 0.5494, 0.6514, 0.6114],
          [0.3407, 0.4562, 0.5636,  ..., 0.4494, 0.5595, 0.5126],
          [0.4140, 0.4239, 0.4144,  ..., 0.4656, 0.5412, 0.5204]],

         [[0.3947, 0.6406, 0.5419,  ..., 0.5537, 0.3803, 0.4889],
          [0.5487, 0.5307, 0.4523,  ..., 0.3858, 0.4244, 0.5438],
          [0.4919, 0.5385, 0.6442,  ..., 0.6531, 0.4865, 0.4769],
          [0.4868, 0.6637, 0.4644,  ..., 0.3416, 0.4576, 0.3460]],

         [[0.4841, 0.4661, 0.3174,  ..., 0.4765, 0.4325, 0.5746],
          [0.4885, 0.5443, 0.3390,  ..., 0.3141, 0.4111, 0.5718],
          [0.4475, 0.5528, 0.5600,  ..., 0.4788, 0.3363, 0.4908],
          [0.3294, 0.4811, 0.4182,  ..., 0.4989, 0.4593, 0.2822]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0050,  0.0010, -0.0070,  0.0010, -0.0050, -0.0050,  0.0050,  0.0070,
         0.0030,  0.0070], device='cuda:0')
selected experts tensor([1603, 1567, 1584, 1655, 1657, 1653, 1620, 1688, 1747, 1610],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4914, 0.5309, 0.5224,  ..., 0.5754, 0.4932, 0.5472],
          [0.4201, 0.6190, 0.6145,  ..., 0.5462, 0.4507, 0.3890],
          [0.6333, 0.5147, 0.4208,  ..., 0.5812, 0.4737, 0.5423],
          [0.4115, 0.4380, 0.5334,  ..., 0.5144, 0.4432, 0.6169]],

         [[0.5032, 0.5311, 0.5452,  ..., 0.4095, 0.6981, 0.4578],
          [0.6833, 0.3926, 0.4147,  ..., 0.3144, 0.6839, 0.4622],
          [0.5511, 0.5233, 0.6043,  ..., 0.4286, 0.4847, 0.6333],
          [0.2860, 0.3782, 0.5648,  ..., 0.4510, 0.4042, 0.4002]],

         [[0.5954, 0.4238, 0.4872,  ..., 0.6938, 0.4858, 0.4945],
          [0.5588, 0.5309, 0.3825,  ..., 0.5199, 0.4679, 0.3937],
          [0.4325, 0.5348, 0.3327,  ..., 0.5102, 0.6250, 0.3937],
          [0.4929, 0.5000, 0.4701,  ..., 0.3819, 0.5347, 0.5060]],

         ...,

         [[0.4325, 0.4392, 0.3899,  ..., 0.4971, 0.5565, 0.3701],
          [0.6468, 0.5248, 0.5061,  ..., 0.4895, 0.4493, 0.4666],
          [0.6210, 0.4973, 0.4808,  ..., 0.5314, 0.5309, 0.2989],
          [0.5645, 0.6222, 0.6960,  ..., 0.4315, 0.4495, 0.6816]],

         [[0.5377, 0.3483, 0.4517,  ..., 0.4626, 0.6454, 0.4535],
          [0.6504, 0.3935, 0.4682,  ..., 0.6466, 0.5690, 0.5235],
          [0.6155, 0.6042, 0.4721,  ..., 0.3627, 0.4785, 0.5727],
          [0.3575, 0.4291, 0.4109,  ..., 0.4761, 0.4601, 0.4716]],

         [[0.5146, 0.6554, 0.5925,  ..., 0.4707, 0.5071, 0.5907],
          [0.5549, 0.5977, 0.7628,  ..., 0.4710, 0.4982, 0.5218],
          [0.3353, 0.5182, 0.3554,  ..., 0.4152, 0.5336, 0.3951],
          [0.4773, 0.5802, 0.6447,  ..., 0.4833, 0.4791, 0.3602]]],


        [[[0.3983, 0.4537, 0.4585,  ..., 0.4219, 0.3678, 0.5088],
          [0.5382, 0.4694, 0.3174,  ..., 0.6088, 0.5800, 0.7032],
          [0.6306, 0.5214, 0.4854,  ..., 0.6005, 0.5280, 0.5194],
          [0.5205, 0.3819, 0.5063,  ..., 0.4974, 0.6597, 0.5385]],

         [[0.3775, 0.6492, 0.4371,  ..., 0.5365, 0.5231, 0.3960],
          [0.4474, 0.4010, 0.5474,  ..., 0.5129, 0.4284, 0.4535],
          [0.5161, 0.5081, 0.7156,  ..., 0.5294, 0.4728, 0.5973],
          [0.4956, 0.4537, 0.3599,  ..., 0.3307, 0.4207, 0.4756]],

         [[0.6387, 0.3778, 0.6080,  ..., 0.4796, 0.5073, 0.6122],
          [0.4925, 0.5972, 0.5561,  ..., 0.5541, 0.5434, 0.6468],
          [0.5727, 0.4375, 0.5619,  ..., 0.5816, 0.5015, 0.3521],
          [0.5722, 0.4716, 0.5815,  ..., 0.4224, 0.6130, 0.4443]],

         ...,

         [[0.5959, 0.4493, 0.5839,  ..., 0.4631, 0.5829, 0.5817],
          [0.4586, 0.5442, 0.5739,  ..., 0.4425, 0.4809, 0.4277],
          [0.3441, 0.4575, 0.5281,  ..., 0.5726, 0.4799, 0.5803],
          [0.4718, 0.5026, 0.4918,  ..., 0.4960, 0.5592, 0.6053]],

         [[0.4457, 0.5507, 0.4680,  ..., 0.5152, 0.6915, 0.5058],
          [0.4530, 0.5930, 0.5849,  ..., 0.4913, 0.6032, 0.6866],
          [0.4891, 0.4686, 0.4371,  ..., 0.3672, 0.4937, 0.4007],
          [0.4931, 0.5745, 0.5725,  ..., 0.4595, 0.4831, 0.5053]],

         [[0.4167, 0.3085, 0.5571,  ..., 0.3949, 0.5862, 0.4335],
          [0.5959, 0.4800, 0.5839,  ..., 0.4890, 0.5039, 0.4598],
          [0.4848, 0.4370, 0.5491,  ..., 0.3555, 0.5729, 0.5252],
          [0.4073, 0.5454, 0.4804,  ..., 0.4849, 0.6948, 0.4272]]]],
       device='cuda:0')
tensor([[[[0.4924, 0.5339, 0.5174,  ..., 0.5784, 0.4902, 0.5482],
          [0.4211, 0.6220, 0.6095,  ..., 0.5492, 0.4477, 0.3900],
          [0.6343, 0.5177, 0.4158,  ..., 0.5842, 0.4707, 0.5433],
          [0.4125, 0.4410, 0.5284,  ..., 0.5174, 0.4402, 0.6179]],

         [[0.5042, 0.5341, 0.5402,  ..., 0.4125, 0.6951, 0.4588],
          [0.6843, 0.3956, 0.4097,  ..., 0.3174, 0.6809, 0.4632],
          [0.5521, 0.5263, 0.5993,  ..., 0.4316, 0.4817, 0.6343],
          [0.2870, 0.3812, 0.5598,  ..., 0.4540, 0.4012, 0.4012]],

         [[0.5964, 0.4268, 0.4822,  ..., 0.6968, 0.4828, 0.4955],
          [0.5598, 0.5339, 0.3775,  ..., 0.5229, 0.4649, 0.3947],
          [0.4335, 0.5378, 0.3277,  ..., 0.5132, 0.6220, 0.3947],
          [0.4939, 0.5030, 0.4651,  ..., 0.3849, 0.5317, 0.5070]],

         ...,

         [[0.4335, 0.4422, 0.3849,  ..., 0.5001, 0.5535, 0.3711],
          [0.6478, 0.5278, 0.5011,  ..., 0.4925, 0.4463, 0.4676],
          [0.6220, 0.5003, 0.4758,  ..., 0.5344, 0.5279, 0.2999],
          [0.5655, 0.6252, 0.6910,  ..., 0.4345, 0.4465, 0.6826]],

         [[0.5387, 0.3513, 0.4467,  ..., 0.4656, 0.6424, 0.4545],
          [0.6514, 0.3965, 0.4632,  ..., 0.6496, 0.5660, 0.5245],
          [0.6165, 0.6072, 0.4671,  ..., 0.3657, 0.4755, 0.5737],
          [0.3585, 0.4321, 0.4059,  ..., 0.4791, 0.4571, 0.4726]],

         [[0.5156, 0.6584, 0.5875,  ..., 0.4737, 0.5041, 0.5917],
          [0.5559, 0.6007, 0.7578,  ..., 0.4740, 0.4952, 0.5228],
          [0.3363, 0.5212, 0.3504,  ..., 0.4182, 0.5306, 0.3961],
          [0.4783, 0.5832, 0.6397,  ..., 0.4863, 0.4761, 0.3612]]],


        [[[0.3993, 0.4567, 0.4535,  ..., 0.4249, 0.3648, 0.5098],
          [0.5392, 0.4724, 0.3124,  ..., 0.6118, 0.5770, 0.7042],
          [0.6316, 0.5244, 0.4804,  ..., 0.6035, 0.5250, 0.5204],
          [0.5215, 0.3849, 0.5013,  ..., 0.5004, 0.6567, 0.5395]],

         [[0.3785, 0.6522, 0.4321,  ..., 0.5395, 0.5201, 0.3970],
          [0.4484, 0.4040, 0.5424,  ..., 0.5159, 0.4254, 0.4545],
          [0.5171, 0.5111, 0.7106,  ..., 0.5324, 0.4698, 0.5983],
          [0.4966, 0.4567, 0.3549,  ..., 0.3337, 0.4177, 0.4766]],

         [[0.6397, 0.3808, 0.6030,  ..., 0.4826, 0.5043, 0.6132],
          [0.4935, 0.6002, 0.5511,  ..., 0.5571, 0.5404, 0.6478],
          [0.5737, 0.4405, 0.5569,  ..., 0.5846, 0.4985, 0.3531],
          [0.5732, 0.4746, 0.5765,  ..., 0.4254, 0.6100, 0.4453]],

         ...,

         [[0.5969, 0.4523, 0.5789,  ..., 0.4661, 0.5799, 0.5827],
          [0.4596, 0.5472, 0.5689,  ..., 0.4455, 0.4779, 0.4287],
          [0.3451, 0.4605, 0.5231,  ..., 0.5756, 0.4769, 0.5813],
          [0.4728, 0.5056, 0.4868,  ..., 0.4990, 0.5562, 0.6063]],

         [[0.4467, 0.5537, 0.4630,  ..., 0.5182, 0.6885, 0.5068],
          [0.4540, 0.5960, 0.5799,  ..., 0.4943, 0.6002, 0.6876],
          [0.4901, 0.4716, 0.4321,  ..., 0.3702, 0.4907, 0.4017],
          [0.4941, 0.5775, 0.5675,  ..., 0.4625, 0.4801, 0.5063]],

         [[0.4177, 0.3115, 0.5521,  ..., 0.3979, 0.5832, 0.4345],
          [0.5969, 0.4830, 0.5789,  ..., 0.4920, 0.5009, 0.4608],
          [0.4858, 0.4400, 0.5441,  ..., 0.3585, 0.5699, 0.5262],
          [0.4083, 0.5484, 0.4754,  ..., 0.4879, 0.6918, 0.4282]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0010, -0.0030,  0.0050,  0.0090, -0.0010,  0.0090, -0.0110, -0.0030,
         0.0030, -0.0010], device='cuda:0')
selected experts tensor([1709, 1561, 1607, 1640, 1716, 1590, 1638, 1586, 1708, 1629],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4479, 0.5310, 0.5099,  ..., 0.5194, 0.5449, 0.4919],
          [0.4447, 0.5923, 0.4182,  ..., 0.6129, 0.4732, 0.3132],
          [0.5021, 0.4557, 0.5437,  ..., 0.3777, 0.6470, 0.3885],
          [0.5115, 0.3624, 0.4807,  ..., 0.4988, 0.5160, 0.4084]],

         [[0.6037, 0.5031, 0.5370,  ..., 0.5306, 0.5441, 0.5765],
          [0.4073, 0.5696, 0.3875,  ..., 0.5063, 0.4950, 0.3912],
          [0.4040, 0.5058, 0.5613,  ..., 0.4672, 0.3866, 0.3940],
          [0.6092, 0.5904, 0.5770,  ..., 0.6465, 0.5134, 0.4696]],

         [[0.5135, 0.3938, 0.6431,  ..., 0.6294, 0.4640, 0.5290],
          [0.5297, 0.5550, 0.4782,  ..., 0.5384, 0.4311, 0.4983],
          [0.4646, 0.4378, 0.4537,  ..., 0.5175, 0.4107, 0.4827],
          [0.6958, 0.5933, 0.6261,  ..., 0.5792, 0.4505, 0.4197]],

         ...,

         [[0.4739, 0.4660, 0.4530,  ..., 0.4123, 0.3649, 0.5942],
          [0.4989, 0.5369, 0.4813,  ..., 0.6961, 0.6064, 0.5500],
          [0.4508, 0.5744, 0.6360,  ..., 0.4882, 0.4534, 0.4230],
          [0.5718, 0.5610, 0.6449,  ..., 0.5522, 0.4785, 0.5434]],

         [[0.5188, 0.5543, 0.6597,  ..., 0.4427, 0.4401, 0.4789],
          [0.4396, 0.5686, 0.4464,  ..., 0.4104, 0.5942, 0.5827],
          [0.4235, 0.5581, 0.5651,  ..., 0.4845, 0.4107, 0.4316],
          [0.5575, 0.6655, 0.4872,  ..., 0.6203, 0.3429, 0.4154]],

         [[0.5694, 0.4764, 0.4762,  ..., 0.4751, 0.5526, 0.5502],
          [0.5307, 0.5061, 0.4454,  ..., 0.4650, 0.5611, 0.6985],
          [0.4483, 0.4444, 0.6225,  ..., 0.4357, 0.6223, 0.5257],
          [0.5646, 0.5337, 0.5575,  ..., 0.4849, 0.4268, 0.4330]]],


        [[[0.6297, 0.4468, 0.5953,  ..., 0.4810, 0.4344, 0.5594],
          [0.4951, 0.5819, 0.3837,  ..., 0.6411, 0.5633, 0.3613],
          [0.4796, 0.4733, 0.6288,  ..., 0.5507, 0.4732, 0.4320],
          [0.6028, 0.5815, 0.5756,  ..., 0.5289, 0.6551, 0.5190]],

         [[0.5090, 0.4509, 0.5831,  ..., 0.4524, 0.4865, 0.6797],
          [0.5160, 0.4032, 0.4848,  ..., 0.6258, 0.5299, 0.4529],
          [0.6069, 0.5810, 0.5794,  ..., 0.4295, 0.4425, 0.5666],
          [0.5253, 0.4915, 0.5560,  ..., 0.5106, 0.4249, 0.5036]],

         [[0.5651, 0.5993, 0.6234,  ..., 0.5219, 0.4969, 0.3968],
          [0.4641, 0.6982, 0.4610,  ..., 0.5095, 0.5204, 0.3775],
          [0.4761, 0.5272, 0.5115,  ..., 0.4725, 0.5253, 0.3622],
          [0.4649, 0.5174, 0.5622,  ..., 0.5967, 0.5519, 0.5599]],

         ...,

         [[0.5808, 0.4318, 0.4187,  ..., 0.5447, 0.4662, 0.5611],
          [0.4693, 0.5286, 0.4901,  ..., 0.3888, 0.6013, 0.5466],
          [0.6198, 0.4009, 0.4389,  ..., 0.4980, 0.3631, 0.4734],
          [0.3772, 0.6515, 0.4844,  ..., 0.6482, 0.3182, 0.4830]],

         [[0.5972, 0.4027, 0.4011,  ..., 0.3617, 0.4296, 0.4911],
          [0.5142, 0.4705, 0.5850,  ..., 0.4355, 0.5765, 0.4785],
          [0.5160, 0.4904, 0.4856,  ..., 0.4927, 0.6269, 0.5623],
          [0.3458, 0.5337, 0.5000,  ..., 0.5326, 0.3917, 0.5546]],

         [[0.6032, 0.4160, 0.6785,  ..., 0.3126, 0.4273, 0.6097],
          [0.5392, 0.5366, 0.5864,  ..., 0.6180, 0.5111, 0.4548],
          [0.5283, 0.4601, 0.5646,  ..., 0.6258, 0.5601, 0.4915],
          [0.3884, 0.5071, 0.5312,  ..., 0.5620, 0.3472, 0.4871]]]],
       device='cuda:0')
tensor([[[[0.4579, 0.5370, 0.5199,  ..., 0.5234, 0.5349, 0.4819],
          [0.4547, 0.5983, 0.4282,  ..., 0.6169, 0.4632, 0.3032],
          [0.5121, 0.4617, 0.5537,  ..., 0.3817, 0.6370, 0.3785],
          [0.5215, 0.3684, 0.4907,  ..., 0.5028, 0.5060, 0.3984]],

         [[0.6137, 0.5091, 0.5470,  ..., 0.5346, 0.5341, 0.5665],
          [0.4173, 0.5756, 0.3975,  ..., 0.5103, 0.4850, 0.3812],
          [0.4140, 0.5118, 0.5713,  ..., 0.4712, 0.3766, 0.3840],
          [0.6192, 0.5964, 0.5870,  ..., 0.6505, 0.5034, 0.4596]],

         [[0.5235, 0.3998, 0.6531,  ..., 0.6334, 0.4540, 0.5190],
          [0.5397, 0.5610, 0.4882,  ..., 0.5424, 0.4211, 0.4883],
          [0.4746, 0.4438, 0.4637,  ..., 0.5215, 0.4007, 0.4727],
          [0.7058, 0.5993, 0.6361,  ..., 0.5832, 0.4405, 0.4097]],

         ...,

         [[0.4839, 0.4720, 0.4630,  ..., 0.4163, 0.3549, 0.5842],
          [0.5089, 0.5429, 0.4913,  ..., 0.7001, 0.5964, 0.5400],
          [0.4608, 0.5804, 0.6460,  ..., 0.4922, 0.4434, 0.4130],
          [0.5818, 0.5670, 0.6549,  ..., 0.5562, 0.4685, 0.5334]],

         [[0.5288, 0.5603, 0.6697,  ..., 0.4467, 0.4301, 0.4689],
          [0.4496, 0.5746, 0.4564,  ..., 0.4144, 0.5842, 0.5727],
          [0.4335, 0.5641, 0.5751,  ..., 0.4885, 0.4007, 0.4216],
          [0.5675, 0.6715, 0.4972,  ..., 0.6243, 0.3329, 0.4054]],

         [[0.5794, 0.4824, 0.4862,  ..., 0.4791, 0.5426, 0.5402],
          [0.5407, 0.5121, 0.4554,  ..., 0.4690, 0.5511, 0.6885],
          [0.4583, 0.4504, 0.6325,  ..., 0.4397, 0.6123, 0.5157],
          [0.5746, 0.5397, 0.5675,  ..., 0.4889, 0.4168, 0.4230]]],


        [[[0.6397, 0.4528, 0.6053,  ..., 0.4850, 0.4244, 0.5494],
          [0.5051, 0.5879, 0.3937,  ..., 0.6451, 0.5533, 0.3513],
          [0.4896, 0.4793, 0.6388,  ..., 0.5547, 0.4632, 0.4220],
          [0.6128, 0.5875, 0.5856,  ..., 0.5329, 0.6451, 0.5090]],

         [[0.5190, 0.4569, 0.5931,  ..., 0.4564, 0.4765, 0.6697],
          [0.5260, 0.4092, 0.4948,  ..., 0.6298, 0.5199, 0.4429],
          [0.6169, 0.5870, 0.5894,  ..., 0.4335, 0.4325, 0.5566],
          [0.5353, 0.4975, 0.5660,  ..., 0.5146, 0.4149, 0.4936]],

         [[0.5751, 0.6053, 0.6334,  ..., 0.5259, 0.4869, 0.3868],
          [0.4741, 0.7042, 0.4710,  ..., 0.5135, 0.5104, 0.3675],
          [0.4861, 0.5332, 0.5215,  ..., 0.4765, 0.5153, 0.3522],
          [0.4749, 0.5234, 0.5722,  ..., 0.6007, 0.5419, 0.5499]],

         ...,

         [[0.5908, 0.4378, 0.4287,  ..., 0.5487, 0.4562, 0.5511],
          [0.4793, 0.5346, 0.5001,  ..., 0.3928, 0.5913, 0.5366],
          [0.6298, 0.4069, 0.4489,  ..., 0.5020, 0.3531, 0.4634],
          [0.3872, 0.6575, 0.4944,  ..., 0.6522, 0.3082, 0.4730]],

         [[0.6072, 0.4087, 0.4111,  ..., 0.3657, 0.4196, 0.4811],
          [0.5242, 0.4765, 0.5950,  ..., 0.4395, 0.5665, 0.4685],
          [0.5260, 0.4964, 0.4956,  ..., 0.4967, 0.6169, 0.5523],
          [0.3558, 0.5397, 0.5100,  ..., 0.5366, 0.3817, 0.5446]],

         [[0.6132, 0.4220, 0.6885,  ..., 0.3166, 0.4173, 0.5997],
          [0.5492, 0.5426, 0.5964,  ..., 0.6220, 0.5011, 0.4448],
          [0.5383, 0.4661, 0.5746,  ..., 0.6298, 0.5501, 0.4815],
          [0.3984, 0.5131, 0.5412,  ..., 0.5660, 0.3372, 0.4771]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([-0.0100, -0.0060, -0.0100,  0.0060, -0.0100,  0.0100, -0.0100, -0.0040,
         0.0100,  0.0100], device='cuda:0')
selected experts tensor([1862, 1834, 1866, 1554, 2149,  775, 2139, 1684, 1313, 1208],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5384, 0.4318, 0.4777,  ..., 0.4823, 0.5744, 0.4256],
          [0.5464, 0.4588, 0.3912,  ..., 0.5763, 0.5047, 0.5399],
          [0.5801, 0.4960, 0.5125,  ..., 0.4898, 0.5590, 0.5149],
          [0.4532, 0.3609, 0.5490,  ..., 0.6653, 0.5172, 0.4361]],

         [[0.5031, 0.5484, 0.4801,  ..., 0.3672, 0.2986, 0.5339],
          [0.5621, 0.5660, 0.5597,  ..., 0.6053, 0.5658, 0.5304],
          [0.5762, 0.4323, 0.4327,  ..., 0.3965, 0.3636, 0.5295],
          [0.4850, 0.5342, 0.4650,  ..., 0.5844, 0.5039, 0.5085]],

         [[0.6428, 0.5725, 0.5015,  ..., 0.4845, 0.4441, 0.5935],
          [0.5628, 0.4318, 0.4575,  ..., 0.4753, 0.6358, 0.5537],
          [0.5280, 0.4568, 0.4563,  ..., 0.5835, 0.4908, 0.4195],
          [0.5748, 0.3717, 0.5101,  ..., 0.4634, 0.5455, 0.4636]],

         ...,

         [[0.5040, 0.5060, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5060, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5060, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5060, 0.5040,  ..., 0.5060, 0.5060, 0.5060]],

         [[0.5040, 0.5060, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5060, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5060, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5060, 0.5040,  ..., 0.5060, 0.5060, 0.5060]],

         [[0.5040, 0.5060, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5060, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5060, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5060, 0.5040,  ..., 0.5060, 0.5060, 0.5060]]],


        [[[0.5261, 0.5338, 0.5277,  ..., 0.3591, 0.4769, 0.5787],
          [0.5700, 0.4347, 0.4847,  ..., 0.5581, 0.4653, 0.4677],
          [0.5318, 0.5353, 0.4246,  ..., 0.4342, 0.4753, 0.3600],
          [0.4743, 0.5234, 0.5953,  ..., 0.4619, 0.5559, 0.4921]],

         [[0.5471, 0.4950, 0.5146,  ..., 0.4903, 0.5387, 0.5435],
          [0.4080, 0.5711, 0.5401,  ..., 0.3863, 0.5612, 0.5849],
          [0.5568, 0.4571, 0.4284,  ..., 0.4371, 0.3555, 0.5636],
          [0.6246, 0.5614, 0.5853,  ..., 0.5523, 0.4936, 0.4314]],

         [[0.4626, 0.5114, 0.3751,  ..., 0.5730, 0.5217, 0.4448],
          [0.4318, 0.4096, 0.5111,  ..., 0.5131, 0.5749, 0.5624],
          [0.6668, 0.4266, 0.4194,  ..., 0.3974, 0.4763, 0.4605],
          [0.6633, 0.5294, 0.6251,  ..., 0.5008, 0.6556, 0.3942]],

         ...,

         [[0.5040, 0.5060, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5060, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5060, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5060, 0.5040,  ..., 0.5060, 0.5060, 0.5060]],

         [[0.5040, 0.5060, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5060, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5060, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5060, 0.5040,  ..., 0.5060, 0.5060, 0.5060]],

         [[0.5040, 0.5060, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5060, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5060, 0.5040,  ..., 0.5060, 0.5060, 0.5060],
          [0.5040, 0.5060, 0.5040,  ..., 0.5060, 0.5060, 0.5060]]]],
       device='cuda:0')
tensor([[[[0.5344, 0.4258, 0.4737,  ..., 0.4763, 0.5684, 0.4196],
          [0.5424, 0.4528, 0.3872,  ..., 0.5703, 0.4987, 0.5339],
          [0.5761, 0.4900, 0.5085,  ..., 0.4838, 0.5530, 0.5089],
          [0.4492, 0.3549, 0.5450,  ..., 0.6593, 0.5112, 0.4301]],

         [[0.4991, 0.5424, 0.4761,  ..., 0.3612, 0.2926, 0.5279],
          [0.5581, 0.5600, 0.5557,  ..., 0.5993, 0.5598, 0.5244],
          [0.5722, 0.4263, 0.4287,  ..., 0.3905, 0.3576, 0.5235],
          [0.4810, 0.5282, 0.4610,  ..., 0.5784, 0.4979, 0.5025]],

         [[0.6388, 0.5665, 0.4975,  ..., 0.4785, 0.4381, 0.5875],
          [0.5588, 0.4258, 0.4535,  ..., 0.4693, 0.6298, 0.5477],
          [0.5240, 0.4508, 0.4523,  ..., 0.5775, 0.4848, 0.4135],
          [0.5708, 0.3657, 0.5061,  ..., 0.4574, 0.5395, 0.4576]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5221, 0.5278, 0.5237,  ..., 0.3531, 0.4709, 0.5727],
          [0.5660, 0.4287, 0.4807,  ..., 0.5521, 0.4593, 0.4617],
          [0.5278, 0.5293, 0.4206,  ..., 0.4282, 0.4693, 0.3540],
          [0.4703, 0.5174, 0.5913,  ..., 0.4559, 0.5499, 0.4861]],

         [[0.5431, 0.4890, 0.5106,  ..., 0.4843, 0.5327, 0.5375],
          [0.4040, 0.5651, 0.5361,  ..., 0.3803, 0.5552, 0.5789],
          [0.5528, 0.4511, 0.4244,  ..., 0.4311, 0.3495, 0.5576],
          [0.6206, 0.5554, 0.5813,  ..., 0.5463, 0.4876, 0.4254]],

         [[0.4586, 0.5054, 0.3711,  ..., 0.5670, 0.5157, 0.4388],
          [0.4278, 0.4036, 0.5071,  ..., 0.5071, 0.5689, 0.5564],
          [0.6628, 0.4206, 0.4154,  ..., 0.3914, 0.4703, 0.4545],
          [0.6593, 0.5234, 0.6211,  ..., 0.4948, 0.6496, 0.3882]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0040, 0.0060, 0.0040, 0.0060, 0.0020, 0.0060, 0.0060, 0.0060, 0.0060,
        0.0060], device='cuda:0')
selected experts tensor([2012, 2894, 1412, 2246, 2391,  873,  956,  989, 1408, 1203],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1709., 1561., 1607., 1640., 1716., 1590., 1638., 1586., 1708., 1629.],
        [2012., 2894., 1412., 2246., 2391.,  873.,  956.,  989., 1408., 1203.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5357, 0.7237, 0.5354,  ..., 0.5311, 0.3506, 0.4215],
          [0.3996, 0.4865, 0.5591,  ..., 0.6312, 0.5055, 0.5903],
          [0.4257, 0.6408, 0.3840,  ..., 0.5367, 0.5335, 0.4528],
          [0.4725, 0.5084, 0.4357,  ..., 0.5687, 0.7467, 0.4712]],

         [[0.5023, 0.5386, 0.6489,  ..., 0.5045, 0.4341, 0.3145],
          [0.5372, 0.5232, 0.6265,  ..., 0.3564, 0.5337, 0.6486],
          [0.5072, 0.5603, 0.4817,  ..., 0.5276, 0.6613, 0.4238],
          [0.5014, 0.4988, 0.3761,  ..., 0.5493, 0.5914, 0.6002]],

         [[0.5953, 0.3596, 0.5373,  ..., 0.5058, 0.6595, 0.5492],
          [0.5783, 0.6390, 0.5051,  ..., 0.4247, 0.4712, 0.6350],
          [0.6065, 0.5918, 0.4061,  ..., 0.5364, 0.5232, 0.5759],
          [0.4338, 0.5918, 0.5349,  ..., 0.5189, 0.6059, 0.4644]],

         ...,

         [[0.3653, 0.5102, 0.4774,  ..., 0.4663, 0.5320, 0.3791],
          [0.6240, 0.2970, 0.5772,  ..., 0.5537, 0.5033, 0.5945],
          [0.5976, 0.6189, 0.4350,  ..., 0.6493, 0.5728, 0.4953],
          [0.5943, 0.5781, 0.6283,  ..., 0.5149, 0.3211, 0.5185]],

         [[0.6491, 0.4350, 0.3994,  ..., 0.5641, 0.6254, 0.4296],
          [0.4176, 0.4141, 0.5325,  ..., 0.5028, 0.5928, 0.3611],
          [0.4109, 0.5313, 0.5007,  ..., 0.5320, 0.4954, 0.5506],
          [0.4568, 0.4360, 0.3966,  ..., 0.5210, 0.4973, 0.5598]],

         [[0.5427, 0.4098, 0.4427,  ..., 0.3234, 0.4536, 0.5224],
          [0.5369, 0.6390, 0.4299,  ..., 0.5270, 0.4764, 0.2824],
          [0.3662, 0.5229, 0.5383,  ..., 0.2978, 0.4905, 0.6030],
          [0.5730, 0.4193, 0.3943,  ..., 0.6627, 0.5560, 0.5273]]],


        [[[0.5538, 0.6812, 0.3047,  ..., 0.6216, 0.5961, 0.4092],
          [0.5289, 0.5223, 0.5313,  ..., 0.4909, 0.4770, 0.4492],
          [0.5384, 0.5937, 0.5373,  ..., 0.4002, 0.4780, 0.3883],
          [0.3777, 0.5317, 0.6142,  ..., 0.5394, 0.4060, 0.4642]],

         [[0.4885, 0.4408, 0.4538,  ..., 0.4058, 0.5202, 0.5300],
          [0.5099, 0.3668, 0.4429,  ..., 0.4825, 0.5451, 0.5148],
          [0.5934, 0.5876, 0.5386,  ..., 0.6206, 0.5244, 0.5588],
          [0.4812, 0.6022, 0.4545,  ..., 0.5132, 0.4977, 0.3246]],

         [[0.6570, 0.4732, 0.5001,  ..., 0.6024, 0.5052, 0.4523],
          [0.3653, 0.5219, 0.5629,  ..., 0.4114, 0.5766, 0.4272],
          [0.4386, 0.5601, 0.4347,  ..., 0.4782, 0.4800, 0.4854],
          [0.6088, 0.4864, 0.3217,  ..., 0.5216, 0.4202, 0.4215]],

         ...,

         [[0.5264, 0.4278, 0.7062,  ..., 0.6034, 0.5029, 0.5697],
          [0.6032, 0.3948, 0.5667,  ..., 0.4939, 0.4155, 0.5253],
          [0.5156, 0.5620, 0.4840,  ..., 0.4237, 0.5521, 0.4248],
          [0.5113, 0.3551, 0.4861,  ..., 0.5203, 0.4916, 0.6021]],

         [[0.4621, 0.5480, 0.4463,  ..., 0.5138, 0.6534, 0.5236],
          [0.4871, 0.5582, 0.5956,  ..., 0.3914, 0.5140, 0.4348],
          [0.5891, 0.2858, 0.4480,  ..., 0.5015, 0.4466, 0.4962],
          [0.4476, 0.4973, 0.3507,  ..., 0.6869, 0.4037, 0.3924]],

         [[0.4461, 0.5521, 0.5112,  ..., 0.3717, 0.5230, 0.3929],
          [0.4739, 0.5598, 0.4897,  ..., 0.3485, 0.4839, 0.4603],
          [0.4495, 0.6125, 0.5705,  ..., 0.4860, 0.3925, 0.5530],
          [0.6013, 0.5458, 0.5027,  ..., 0.3735, 0.6829, 0.6021]]]],
       device='cuda:0')
tensor([[[[0.5397, 0.7217, 0.5414,  ..., 0.5251, 0.3486, 0.4135],
          [0.4036, 0.4845, 0.5651,  ..., 0.6252, 0.5035, 0.5823],
          [0.4297, 0.6388, 0.3900,  ..., 0.5307, 0.5315, 0.4448],
          [0.4765, 0.5064, 0.4417,  ..., 0.5627, 0.7447, 0.4632]],

         [[0.5063, 0.5366, 0.6549,  ..., 0.4985, 0.4321, 0.3065],
          [0.5412, 0.5212, 0.6325,  ..., 0.3504, 0.5317, 0.6406],
          [0.5112, 0.5583, 0.4877,  ..., 0.5216, 0.6593, 0.4158],
          [0.5054, 0.4968, 0.3821,  ..., 0.5433, 0.5894, 0.5922]],

         [[0.5993, 0.3576, 0.5433,  ..., 0.4998, 0.6575, 0.5412],
          [0.5823, 0.6370, 0.5111,  ..., 0.4187, 0.4692, 0.6270],
          [0.6105, 0.5898, 0.4121,  ..., 0.5304, 0.5212, 0.5679],
          [0.4378, 0.5898, 0.5409,  ..., 0.5129, 0.6039, 0.4564]],

         ...,

         [[0.3693, 0.5082, 0.4834,  ..., 0.4603, 0.5300, 0.3711],
          [0.6280, 0.2950, 0.5832,  ..., 0.5477, 0.5013, 0.5865],
          [0.6016, 0.6169, 0.4410,  ..., 0.6433, 0.5708, 0.4873],
          [0.5983, 0.5761, 0.6343,  ..., 0.5089, 0.3191, 0.5105]],

         [[0.6531, 0.4330, 0.4054,  ..., 0.5581, 0.6234, 0.4216],
          [0.4216, 0.4121, 0.5385,  ..., 0.4968, 0.5908, 0.3531],
          [0.4149, 0.5293, 0.5067,  ..., 0.5260, 0.4934, 0.5426],
          [0.4608, 0.4340, 0.4026,  ..., 0.5150, 0.4953, 0.5518]],

         [[0.5467, 0.4078, 0.4487,  ..., 0.3174, 0.4516, 0.5144],
          [0.5409, 0.6370, 0.4359,  ..., 0.5210, 0.4744, 0.2744],
          [0.3702, 0.5209, 0.5443,  ..., 0.2918, 0.4885, 0.5950],
          [0.5770, 0.4173, 0.4003,  ..., 0.6567, 0.5540, 0.5193]]],


        [[[0.5578, 0.6792, 0.3107,  ..., 0.6156, 0.5941, 0.4012],
          [0.5329, 0.5203, 0.5373,  ..., 0.4849, 0.4750, 0.4412],
          [0.5424, 0.5917, 0.5433,  ..., 0.3942, 0.4760, 0.3803],
          [0.3817, 0.5297, 0.6202,  ..., 0.5334, 0.4040, 0.4562]],

         [[0.4925, 0.4388, 0.4598,  ..., 0.3998, 0.5182, 0.5220],
          [0.5139, 0.3648, 0.4489,  ..., 0.4765, 0.5431, 0.5068],
          [0.5974, 0.5856, 0.5446,  ..., 0.6146, 0.5224, 0.5508],
          [0.4852, 0.6002, 0.4605,  ..., 0.5072, 0.4957, 0.3166]],

         [[0.6610, 0.4712, 0.5061,  ..., 0.5964, 0.5032, 0.4443],
          [0.3693, 0.5199, 0.5689,  ..., 0.4054, 0.5746, 0.4192],
          [0.4426, 0.5581, 0.4407,  ..., 0.4722, 0.4780, 0.4774],
          [0.6128, 0.4844, 0.3277,  ..., 0.5156, 0.4182, 0.4135]],

         ...,

         [[0.5304, 0.4258, 0.7122,  ..., 0.5974, 0.5009, 0.5617],
          [0.6072, 0.3928, 0.5727,  ..., 0.4879, 0.4135, 0.5173],
          [0.5196, 0.5600, 0.4900,  ..., 0.4177, 0.5501, 0.4168],
          [0.5153, 0.3531, 0.4921,  ..., 0.5143, 0.4896, 0.5941]],

         [[0.4661, 0.5460, 0.4523,  ..., 0.5078, 0.6514, 0.5156],
          [0.4911, 0.5562, 0.6016,  ..., 0.3854, 0.5120, 0.4268],
          [0.5931, 0.2838, 0.4540,  ..., 0.4955, 0.4446, 0.4882],
          [0.4516, 0.4953, 0.3567,  ..., 0.6809, 0.4017, 0.3844]],

         [[0.4501, 0.5501, 0.5172,  ..., 0.3657, 0.5210, 0.3849],
          [0.4779, 0.5578, 0.4957,  ..., 0.3425, 0.4819, 0.4523],
          [0.4535, 0.6105, 0.5765,  ..., 0.4800, 0.3905, 0.5450],
          [0.6053, 0.5438, 0.5087,  ..., 0.3675, 0.6809, 0.5941]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0040,  0.0020, -0.0060,  0.0000, -0.0060, -0.0060,  0.0060,  0.0060,
         0.0020,  0.0080], device='cuda:0')
selected experts tensor([1729, 1653, 1545, 1523, 1649, 1678, 1616, 1627, 1689, 1675],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4593, 0.3760, 0.4271,  ..., 0.5731, 0.4966, 0.5082],
          [0.5072, 0.4905, 0.5018,  ..., 0.6964, 0.4060, 0.6025],
          [0.5585, 0.5712, 0.3467,  ..., 0.4305, 0.5084, 0.5385],
          [0.5459, 0.6278, 0.5411,  ..., 0.4835, 0.5838, 0.5542]],

         [[0.4902, 0.5314, 0.5239,  ..., 0.5764, 0.4921, 0.5479],
          [0.4191, 0.6200, 0.6155,  ..., 0.5472, 0.4497, 0.3900],
          [0.6323, 0.5156, 0.4218,  ..., 0.5822, 0.4727, 0.5431],
          [0.4105, 0.4390, 0.5344,  ..., 0.5154, 0.4425, 0.6179]],

         [[0.5779, 0.3797, 0.3928,  ..., 0.3908, 0.3587, 0.6280],
          [0.6755, 0.5423, 0.3858,  ..., 0.3520, 0.5947, 0.4634],
          [0.6573, 0.4196, 0.4950,  ..., 0.6695, 0.6064, 0.5746],
          [0.5513, 0.4006, 0.4783,  ..., 0.6269, 0.4126, 0.4484]],

         ...,

         [[0.4030, 0.5139, 0.4962,  ..., 0.4452, 0.5975, 0.4893],
          [0.4176, 0.5616, 0.4400,  ..., 0.4631, 0.3939, 0.4268],
          [0.4921, 0.6019, 0.5287,  ..., 0.5041, 0.4405, 0.5392],
          [0.5546, 0.4428, 0.5610,  ..., 0.3811, 0.4389, 0.4116]],

         [[0.4607, 0.4469, 0.4299,  ..., 0.4082, 0.4761, 0.3585],
          [0.5372, 0.3171, 0.5157,  ..., 0.4363, 0.3990, 0.5660],
          [0.4759, 0.5549, 0.5537,  ..., 0.6695, 0.3800, 0.5689],
          [0.5779, 0.3592, 0.3226,  ..., 0.3352, 0.4572, 0.3372]],

         [[0.6103, 0.6721, 0.3886,  ..., 0.5093, 0.5890, 0.4187],
          [0.4686, 0.4952, 0.5342,  ..., 0.5688, 0.5126, 0.3739],
          [0.3852, 0.4096, 0.5556,  ..., 0.6001, 0.4916, 0.4489],
          [0.6191, 0.5278, 0.3423,  ..., 0.5186, 0.4650, 0.5071]]],


        [[[0.4025, 0.3619, 0.5588,  ..., 0.5430, 0.6863, 0.4785],
          [0.6431, 0.5515, 0.3294,  ..., 0.4320, 0.3957, 0.5048],
          [0.6122, 0.3326, 0.3799,  ..., 0.5329, 0.5847, 0.5903],
          [0.4229, 0.5650, 0.5868,  ..., 0.5897, 0.5970, 0.3311]],

         [[0.5276, 0.4838, 0.4788,  ..., 0.5505, 0.4866, 0.5064],
          [0.2842, 0.4830, 0.2220,  ..., 0.4651, 0.5131, 0.4713],
          [0.4590, 0.3673, 0.3868,  ..., 0.4229, 0.6203, 0.4424],
          [0.6287, 0.3646, 0.6714,  ..., 0.4368, 0.5531, 0.2200]],

         [[0.5329, 0.6214, 0.3345,  ..., 0.4248, 0.4369, 0.6183],
          [0.4532, 0.3458, 0.4930,  ..., 0.5139, 0.4495, 0.4919],
          [0.5878, 0.4086, 0.4030,  ..., 0.4505, 0.5001, 0.4287],
          [0.3592, 0.6625, 0.3965,  ..., 0.3964, 0.6097, 0.5334]],

         ...,

         [[0.4016, 0.5189, 0.4523,  ..., 0.4191, 0.4803, 0.3531],
          [0.5674, 0.5859, 0.5152,  ..., 0.3547, 0.5240, 0.6442],
          [0.4409, 0.4404, 0.4209,  ..., 0.5944, 0.2914, 0.5132],
          [0.4593, 0.5812, 0.5326,  ..., 0.5693, 0.5218, 0.5211]],

         [[0.4258, 0.6205, 0.5479,  ..., 0.5147, 0.6199, 0.6433],
          [0.6386, 0.4929, 0.6457,  ..., 0.3903, 0.5695, 0.7786],
          [0.3317, 0.5185, 0.5648,  ..., 0.4735, 0.5266, 0.4765],
          [0.4950, 0.4339, 0.3817,  ..., 0.4924, 0.6560, 0.4201]],

         [[0.6080, 0.4091, 0.4181,  ..., 0.3788, 0.3906, 0.4644],
          [0.5626, 0.5197, 0.4948,  ..., 0.4375, 0.4804, 0.4268],
          [0.5316, 0.5455, 0.4361,  ..., 0.4955, 0.5232, 0.5160],
          [0.4922, 0.4979, 0.4119,  ..., 0.4595, 0.4721, 0.4849]]]],
       device='cuda:0')
tensor([[[[0.4613, 0.3780, 0.4211,  ..., 0.5751, 0.4946, 0.5082],
          [0.5092, 0.4925, 0.4958,  ..., 0.6984, 0.4040, 0.6025],
          [0.5605, 0.5732, 0.3407,  ..., 0.4325, 0.5064, 0.5385],
          [0.5479, 0.6298, 0.5351,  ..., 0.4855, 0.5818, 0.5542]],

         [[0.4922, 0.5334, 0.5179,  ..., 0.5784, 0.4901, 0.5479],
          [0.4211, 0.6220, 0.6095,  ..., 0.5492, 0.4477, 0.3900],
          [0.6343, 0.5176, 0.4158,  ..., 0.5842, 0.4707, 0.5431],
          [0.4125, 0.4410, 0.5284,  ..., 0.5174, 0.4405, 0.6179]],

         [[0.5799, 0.3817, 0.3868,  ..., 0.3928, 0.3567, 0.6280],
          [0.6775, 0.5443, 0.3798,  ..., 0.3540, 0.5927, 0.4634],
          [0.6593, 0.4216, 0.4890,  ..., 0.6715, 0.6044, 0.5746],
          [0.5533, 0.4026, 0.4723,  ..., 0.6289, 0.4106, 0.4484]],

         ...,

         [[0.4050, 0.5159, 0.4902,  ..., 0.4472, 0.5955, 0.4893],
          [0.4196, 0.5636, 0.4340,  ..., 0.4651, 0.3919, 0.4268],
          [0.4941, 0.6039, 0.5227,  ..., 0.5061, 0.4385, 0.5392],
          [0.5566, 0.4448, 0.5550,  ..., 0.3831, 0.4369, 0.4116]],

         [[0.4627, 0.4489, 0.4239,  ..., 0.4102, 0.4741, 0.3585],
          [0.5392, 0.3191, 0.5097,  ..., 0.4383, 0.3970, 0.5660],
          [0.4779, 0.5569, 0.5477,  ..., 0.6715, 0.3780, 0.5689],
          [0.5799, 0.3612, 0.3166,  ..., 0.3372, 0.4552, 0.3372]],

         [[0.6123, 0.6741, 0.3826,  ..., 0.5113, 0.5870, 0.4187],
          [0.4706, 0.4972, 0.5282,  ..., 0.5708, 0.5106, 0.3739],
          [0.3872, 0.4116, 0.5496,  ..., 0.6021, 0.4896, 0.4489],
          [0.6211, 0.5297, 0.3363,  ..., 0.5206, 0.4630, 0.5071]]],


        [[[0.4045, 0.3639, 0.5528,  ..., 0.5450, 0.6843, 0.4785],
          [0.6451, 0.5535, 0.3234,  ..., 0.4340, 0.3937, 0.5048],
          [0.6142, 0.3346, 0.3739,  ..., 0.5349, 0.5827, 0.5903],
          [0.4249, 0.5670, 0.5808,  ..., 0.5917, 0.5950, 0.3311]],

         [[0.5296, 0.4858, 0.4728,  ..., 0.5525, 0.4846, 0.5064],
          [0.2862, 0.4850, 0.2160,  ..., 0.4671, 0.5111, 0.4713],
          [0.4610, 0.3693, 0.3808,  ..., 0.4249, 0.6183, 0.4424],
          [0.6307, 0.3666, 0.6654,  ..., 0.4388, 0.5511, 0.2200]],

         [[0.5349, 0.6234, 0.3285,  ..., 0.4268, 0.4349, 0.6183],
          [0.4552, 0.3478, 0.4870,  ..., 0.5159, 0.4475, 0.4919],
          [0.5898, 0.4106, 0.3970,  ..., 0.4525, 0.4981, 0.4287],
          [0.3612, 0.6645, 0.3905,  ..., 0.3984, 0.6077, 0.5334]],

         ...,

         [[0.4036, 0.5209, 0.4463,  ..., 0.4211, 0.4783, 0.3531],
          [0.5694, 0.5879, 0.5092,  ..., 0.3567, 0.5220, 0.6442],
          [0.4429, 0.4424, 0.4149,  ..., 0.5964, 0.2894, 0.5132],
          [0.4613, 0.5832, 0.5266,  ..., 0.5713, 0.5198, 0.5211]],

         [[0.4278, 0.6225, 0.5419,  ..., 0.5167, 0.6179, 0.6433],
          [0.6406, 0.4949, 0.6397,  ..., 0.3923, 0.5675, 0.7786],
          [0.3337, 0.5205, 0.5588,  ..., 0.4755, 0.5246, 0.4765],
          [0.4970, 0.4359, 0.3757,  ..., 0.4944, 0.6540, 0.4201]],

         [[0.6100, 0.4111, 0.4121,  ..., 0.3808, 0.3886, 0.4644],
          [0.5646, 0.5217, 0.4888,  ..., 0.4395, 0.4784, 0.4268],
          [0.5336, 0.5475, 0.4301,  ..., 0.4975, 0.5212, 0.5160],
          [0.4942, 0.4999, 0.4059,  ..., 0.4615, 0.4701, 0.4849]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0020, -0.0020,  0.0060,  0.0080, -0.0020,  0.0100, -0.0100, -0.0020,
         0.0020,  0.0000], device='cuda:0')
selected experts tensor([1665, 1738, 1577, 1580, 1554, 1730, 1590, 1722, 1601, 1627],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5490, 0.5451, 0.5967,  ..., 0.4616, 0.5117, 0.4541],
          [0.4627, 0.5908, 0.4579,  ..., 0.6248, 0.4725, 0.4594],
          [0.4827, 0.4867, 0.5437,  ..., 0.4905, 0.4450, 0.4383],
          [0.5471, 0.5734, 0.5270,  ..., 0.5294, 0.4416, 0.5847]],

         [[0.4544, 0.4774, 0.4300,  ..., 0.5734, 0.5354, 0.5100],
          [0.4148, 0.5714, 0.4134,  ..., 0.7000, 0.4500, 0.3614],
          [0.4731, 0.4606, 0.5019,  ..., 0.3962, 0.5444, 0.4099],
          [0.5420, 0.3835, 0.6069,  ..., 0.4893, 0.5120, 0.5135]],

         [[0.4254, 0.4787, 0.5593,  ..., 0.4275, 0.4311, 0.3632],
          [0.5741, 0.6219, 0.4904,  ..., 0.5425, 0.4621, 0.3982],
          [0.6055, 0.4838, 0.4177,  ..., 0.4751, 0.5558, 0.4701],
          [0.6069, 0.5182, 0.6350,  ..., 0.5620, 0.5407, 0.6266]],

         ...,

         [[0.6622, 0.6354, 0.3280,  ..., 0.2780, 0.6210, 0.5789],
          [0.5408, 0.6062, 0.4858,  ..., 0.6124, 0.6471, 0.5119],
          [0.3684, 0.5443, 0.4594,  ..., 0.4189, 0.5444, 0.3749],
          [0.5713, 0.4484, 0.4505,  ..., 0.4920, 0.4749, 0.4691]],

         [[0.5612, 0.3747, 0.5670,  ..., 0.5514, 0.4529, 0.5166],
          [0.6069, 0.4626, 0.6004,  ..., 0.5924, 0.5222, 0.4618],
          [0.4211, 0.3956, 0.5224,  ..., 0.4466, 0.4580, 0.5548],
          [0.3968, 0.5139, 0.4871,  ..., 0.4606, 0.4890, 0.5303]],

         [[0.6059, 0.5196, 0.5769,  ..., 0.4811, 0.4900, 0.5985],
          [0.4048, 0.5385, 0.5612,  ..., 0.4832, 0.4860, 0.3987],
          [0.4980, 0.2921, 0.4624,  ..., 0.4266, 0.4725, 0.5384],
          [0.4498, 0.3715, 0.5095,  ..., 0.5371, 0.3361, 0.4911]]],


        [[[0.6069, 0.5117, 0.6733,  ..., 0.4256, 0.4752, 0.6790],
          [0.4382, 0.5662, 0.4507,  ..., 0.6639, 0.4440, 0.4373],
          [0.4756, 0.5173, 0.4177,  ..., 0.4108, 0.4778, 0.3922],
          [0.5031, 0.5624, 0.5226,  ..., 0.6220, 0.4183, 0.4563]],

         [[0.6046, 0.5482, 0.5224,  ..., 0.4670, 0.3259, 0.6659],
          [0.4148, 0.6593, 0.3323,  ..., 0.6284, 0.4669, 0.4856],
          [0.4039, 0.6300, 0.6296,  ..., 0.4405, 0.3004, 0.4961],
          [0.6833, 0.4008, 0.5976,  ..., 0.5905, 0.4556, 0.5165]],

         [[0.5309, 0.6479, 0.4862,  ..., 0.3953, 0.5989, 0.5456],
          [0.4940, 0.6627, 0.4391,  ..., 0.6604, 0.4795, 0.4510],
          [0.5840, 0.5885, 0.6305,  ..., 0.4816, 0.4038, 0.4669],
          [0.5147, 0.5245, 0.5411,  ..., 0.5210, 0.4934, 0.5478]],

         ...,

         [[0.5703, 0.5337, 0.4803,  ..., 0.5364, 0.5136, 0.5380],
          [0.4706, 0.5595, 0.5755,  ..., 0.4355, 0.5334, 0.5890],
          [0.4582, 0.4308, 0.4764,  ..., 0.5215, 0.4179, 0.4340],
          [0.5015, 0.4311, 0.4797,  ..., 0.5653, 0.6279, 0.4264]],

         [[0.5147, 0.4528, 0.4211,  ..., 0.4684, 0.2996, 0.5417],
          [0.3721, 0.4669, 0.4958,  ..., 0.5138, 0.5524, 0.6242],
          [0.5253, 0.4088, 0.4927,  ..., 0.4343, 0.4980, 0.5691],
          [0.3707, 0.5136, 0.4762,  ..., 0.5649, 0.5737, 0.2948]],

         [[0.5831, 0.4465, 0.4864,  ..., 0.4906, 0.5449, 0.5626],
          [0.4666, 0.6514, 0.6027,  ..., 0.5754, 0.4650, 0.3812],
          [0.6544, 0.3578, 0.6287,  ..., 0.6087, 0.4914, 0.5200],
          [0.5665, 0.5662, 0.5641,  ..., 0.4333, 0.3217, 0.4565]]]],
       device='cuda:0')
tensor([[[[0.5600, 0.5521, 0.6077,  ..., 0.4666, 0.5007, 0.4431],
          [0.4737, 0.5978, 0.4689,  ..., 0.6298, 0.4615, 0.4484],
          [0.4937, 0.4937, 0.5547,  ..., 0.4955, 0.4340, 0.4273],
          [0.5581, 0.5804, 0.5380,  ..., 0.5344, 0.4306, 0.5737]],

         [[0.4654, 0.4844, 0.4410,  ..., 0.5784, 0.5244, 0.4990],
          [0.4258, 0.5784, 0.4244,  ..., 0.7050, 0.4390, 0.3504],
          [0.4841, 0.4676, 0.5129,  ..., 0.4012, 0.5334, 0.3989],
          [0.5530, 0.3905, 0.6179,  ..., 0.4943, 0.5010, 0.5025]],

         [[0.4364, 0.4857, 0.5703,  ..., 0.4325, 0.4201, 0.3522],
          [0.5851, 0.6289, 0.5014,  ..., 0.5475, 0.4511, 0.3872],
          [0.6165, 0.4908, 0.4287,  ..., 0.4801, 0.5448, 0.4591],
          [0.6179, 0.5252, 0.6460,  ..., 0.5670, 0.5297, 0.6156]],

         ...,

         [[0.6732, 0.6424, 0.3390,  ..., 0.2830, 0.6100, 0.5679],
          [0.5518, 0.6132, 0.4968,  ..., 0.6174, 0.6361, 0.5009],
          [0.3794, 0.5513, 0.4704,  ..., 0.4239, 0.5334, 0.3639],
          [0.5823, 0.4554, 0.4615,  ..., 0.4970, 0.4639, 0.4581]],

         [[0.5722, 0.3817, 0.5780,  ..., 0.5564, 0.4419, 0.5056],
          [0.6179, 0.4696, 0.6114,  ..., 0.5974, 0.5112, 0.4508],
          [0.4321, 0.4026, 0.5334,  ..., 0.4516, 0.4470, 0.5438],
          [0.4078, 0.5209, 0.4981,  ..., 0.4656, 0.4780, 0.5193]],

         [[0.6169, 0.5266, 0.5879,  ..., 0.4861, 0.4790, 0.5875],
          [0.4158, 0.5455, 0.5722,  ..., 0.4882, 0.4750, 0.3877],
          [0.5090, 0.2991, 0.4734,  ..., 0.4316, 0.4615, 0.5274],
          [0.4608, 0.3785, 0.5205,  ..., 0.5421, 0.3251, 0.4801]]],


        [[[0.6179, 0.5187, 0.6843,  ..., 0.4306, 0.4642, 0.6680],
          [0.4492, 0.5732, 0.4617,  ..., 0.6689, 0.4330, 0.4263],
          [0.4866, 0.5243, 0.4287,  ..., 0.4158, 0.4668, 0.3812],
          [0.5141, 0.5694, 0.5336,  ..., 0.6270, 0.4073, 0.4453]],

         [[0.6156, 0.5552, 0.5334,  ..., 0.4720, 0.3149, 0.6549],
          [0.4258, 0.6663, 0.3433,  ..., 0.6334, 0.4559, 0.4746],
          [0.4149, 0.6370, 0.6406,  ..., 0.4455, 0.2894, 0.4851],
          [0.6943, 0.4078, 0.6086,  ..., 0.5955, 0.4446, 0.5055]],

         [[0.5419, 0.6549, 0.4972,  ..., 0.4003, 0.5879, 0.5346],
          [0.5050, 0.6697, 0.4501,  ..., 0.6654, 0.4685, 0.4400],
          [0.5950, 0.5955, 0.6415,  ..., 0.4866, 0.3928, 0.4559],
          [0.5257, 0.5315, 0.5521,  ..., 0.5260, 0.4824, 0.5368]],

         ...,

         [[0.5813, 0.5407, 0.4913,  ..., 0.5414, 0.5026, 0.5270],
          [0.4816, 0.5665, 0.5865,  ..., 0.4405, 0.5224, 0.5780],
          [0.4692, 0.4378, 0.4874,  ..., 0.5265, 0.4069, 0.4230],
          [0.5125, 0.4381, 0.4907,  ..., 0.5703, 0.6169, 0.4154]],

         [[0.5257, 0.4598, 0.4321,  ..., 0.4734, 0.2886, 0.5307],
          [0.3831, 0.4739, 0.5068,  ..., 0.5188, 0.5414, 0.6132],
          [0.5363, 0.4158, 0.5037,  ..., 0.4393, 0.4870, 0.5581],
          [0.3817, 0.5206, 0.4872,  ..., 0.5699, 0.5627, 0.2838]],

         [[0.5941, 0.4535, 0.4974,  ..., 0.4956, 0.5339, 0.5516],
          [0.4776, 0.6584, 0.6137,  ..., 0.5804, 0.4540, 0.3702],
          [0.6654, 0.3648, 0.6397,  ..., 0.6137, 0.4804, 0.5090],
          [0.5775, 0.5732, 0.5751,  ..., 0.4383, 0.3107, 0.4455]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0110, -0.0070, -0.0110,  0.0070, -0.0110,  0.0110, -0.0110, -0.0050,
         0.0110,  0.0110], device='cuda:0')
selected experts tensor([1786, 1831, 1648, 1841, 1853,  746, 2328, 1736, 1322, 1293],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5010, 0.3890, 0.4232,  ..., 0.4482, 0.4598, 0.4583],
          [0.5347, 0.4788, 0.4515,  ..., 0.5072, 0.4717, 0.5255],
          [0.6736, 0.3725, 0.3492,  ..., 0.5533, 0.3772, 0.5725],
          [0.5961, 0.5015, 0.4793,  ..., 0.4977, 0.6249, 0.3610]],

         [[0.5300, 0.5274, 0.4687,  ..., 0.3655, 0.6128, 0.5063],
          [0.5016, 0.4938, 0.3904,  ..., 0.5987, 0.5338, 0.5701],
          [0.5551, 0.4871, 0.4313,  ..., 0.5484, 0.4172, 0.4656],
          [0.4534, 0.4884, 0.5361,  ..., 0.6147, 0.4680, 0.3295]],

         [[0.5843, 0.4977, 0.4790,  ..., 0.3975, 0.5244, 0.5547],
          [0.4217, 0.4156, 0.4185,  ..., 0.4764, 0.5916, 0.6331],
          [0.5451, 0.6581, 0.5681,  ..., 0.5032, 0.3700, 0.4632],
          [0.5551, 0.5324, 0.5182,  ..., 0.5296, 0.6750, 0.4376]],

         ...,

         [[0.5030, 0.5050, 0.5050,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5050, 0.5050,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5050, 0.5050,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5050, 0.5050,  ..., 0.5070, 0.5070, 0.5070]],

         [[0.5030, 0.5050, 0.5050,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5050, 0.5050,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5050, 0.5050,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5050, 0.5050,  ..., 0.5070, 0.5070, 0.5070]],

         [[0.5030, 0.5050, 0.5050,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5050, 0.5050,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5050, 0.5050,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5050, 0.5050,  ..., 0.5070, 0.5070, 0.5070]]],


        [[[0.6454, 0.4011, 0.4428,  ..., 0.3583, 0.3868, 0.5812],
          [0.5093, 0.4933, 0.5304,  ..., 0.5689, 0.6188, 0.3800],
          [0.5107, 0.5399, 0.5691,  ..., 0.3754, 0.5254, 0.5797],
          [0.5381, 0.6182, 0.5283,  ..., 0.5706, 0.4504, 0.4343]],

         [[0.6481, 0.4328, 0.4460,  ..., 0.4195, 0.4729, 0.5745],
          [0.6218, 0.5551, 0.5246,  ..., 0.4376, 0.5588, 0.4828],
          [0.5560, 0.5113, 0.4537,  ..., 0.4511, 0.5058, 0.5355],
          [0.4274, 0.6047, 0.5631,  ..., 0.5888, 0.5162, 0.4738]],

         [[0.5136, 0.4774, 0.3835,  ..., 0.5004, 0.4214, 0.4854],
          [0.4601, 0.4908, 0.5729,  ..., 0.4736, 0.4595, 0.5935],
          [0.5657, 0.5763, 0.4964,  ..., 0.5131, 0.5357, 0.6512],
          [0.5302, 0.5217, 0.5423,  ..., 0.5835, 0.5850, 0.4238]],

         ...,

         [[0.5030, 0.5050, 0.5050,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5050, 0.5050,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5050, 0.5050,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5050, 0.5050,  ..., 0.5070, 0.5070, 0.5070]],

         [[0.5030, 0.5050, 0.5050,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5050, 0.5050,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5050, 0.5050,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5050, 0.5050,  ..., 0.5070, 0.5070, 0.5070]],

         [[0.5030, 0.5050, 0.5050,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5050, 0.5050,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5050, 0.5050,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5050, 0.5050,  ..., 0.5070, 0.5070, 0.5070]]]],
       device='cuda:0')/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([[[[0.4980, 0.3840, 0.4182,  ..., 0.4412, 0.4528, 0.4513],
          [0.5317, 0.4738, 0.4465,  ..., 0.5002, 0.4647, 0.5185],
          [0.6706, 0.3675, 0.3442,  ..., 0.5463, 0.3702, 0.5655],
          [0.5931, 0.4965, 0.4743,  ..., 0.4907, 0.6179, 0.3540]],

         [[0.5270, 0.5224, 0.4637,  ..., 0.3585, 0.6058, 0.4993],
          [0.4986, 0.4888, 0.3854,  ..., 0.5917, 0.5268, 0.5631],
          [0.5521, 0.4821, 0.4263,  ..., 0.5414, 0.4102, 0.4586],
          [0.4504, 0.4834, 0.5311,  ..., 0.6077, 0.4610, 0.3225]],

         [[0.5813, 0.4927, 0.4740,  ..., 0.3905, 0.5174, 0.5477],
          [0.4187, 0.4106, 0.4135,  ..., 0.4694, 0.5846, 0.6261],
          [0.5421, 0.6531, 0.5631,  ..., 0.4962, 0.3630, 0.4562],
          [0.5521, 0.5274, 0.5132,  ..., 0.5226, 0.6680, 0.4306]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.6424, 0.3961, 0.4378,  ..., 0.3513, 0.3798, 0.5742],
          [0.5063, 0.4883, 0.5254,  ..., 0.5619, 0.6118, 0.3730],
          [0.5077, 0.5349, 0.5641,  ..., 0.3684, 0.5184, 0.5727],
          [0.5351, 0.6132, 0.5233,  ..., 0.5636, 0.4434, 0.4273]],

         [[0.6451, 0.4278, 0.4410,  ..., 0.4125, 0.4659, 0.5675],
          [0.6188, 0.5501, 0.5196,  ..., 0.4306, 0.5518, 0.4758],
          [0.5530, 0.5063, 0.4487,  ..., 0.4441, 0.4988, 0.5285],
          [0.4244, 0.5997, 0.5581,  ..., 0.5818, 0.5092, 0.4668]],

         [[0.5106, 0.4724, 0.3785,  ..., 0.4934, 0.4144, 0.4784],
          [0.4571, 0.4858, 0.5679,  ..., 0.4666, 0.4525, 0.5865],
          [0.5627, 0.5713, 0.4914,  ..., 0.5061, 0.5287, 0.6442],
          [0.5272, 0.5167, 0.5373,  ..., 0.5765, 0.5780, 0.4168]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([0.0030, 0.0050, 0.0050, 0.0050, 0.0010, 0.0070, 0.0070, 0.0070, 0.0070,
        0.0070], device='cuda:0')
selected experts tensor([1625, 1424, 1359, 1018, 2303, 2257, 2155, 1151, 1339, 1753],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1665., 1738., 1577., 1580., 1554., 1730., 1590., 1722., 1601., 1627.],
        [1625., 1424., 1359., 1018., 2303., 2257., 2155., 1151., 1339., 1753.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4823, 0.6017, 0.4959,  ..., 0.6340, 0.5569, 0.4952],
          [0.3739, 0.4187, 0.5004,  ..., 0.4545, 0.3822, 0.4810],
          [0.3986, 0.4439, 0.4944,  ..., 0.4191, 0.4603, 0.4891],
          [0.4919, 0.5509, 0.5439,  ..., 0.5005, 0.4388, 0.6619]],

         [[0.4695, 0.5627, 0.5810,  ..., 0.4415, 0.4369, 0.4157],
          [0.5512, 0.3859, 0.6481,  ..., 0.4729, 0.5709, 0.3583],
          [0.5286, 0.5470, 0.4567,  ..., 0.4451, 0.5109, 0.5189],
          [0.4587, 0.5269, 0.5383,  ..., 0.5973, 0.2504, 0.5264]],

         [[0.3813, 0.4543, 0.4706,  ..., 0.4583, 0.4226, 0.5648],
          [0.5787, 0.3532, 0.4908,  ..., 0.7442, 0.5321, 0.3355],
          [0.6552, 0.4635, 0.6064,  ..., 0.4974, 0.5771, 0.5864],
          [0.5500, 0.4509, 0.4805,  ..., 0.5280, 0.5359, 0.6142]],

         ...,

         [[0.4500, 0.5535, 0.4463,  ..., 0.5812, 0.4683, 0.4434],
          [0.4754, 0.7011, 0.5386,  ..., 0.6689, 0.3067, 0.5321],
          [0.4014, 0.4514, 0.3571,  ..., 0.5821, 0.5790, 0.5111],
          [0.4846, 0.4980, 0.5111,  ..., 0.3512, 0.5477, 0.5291]],

         [[0.6106, 0.4953, 0.6064,  ..., 0.3512, 0.4268, 0.5414],
          [0.4242, 0.4896, 0.5610,  ..., 0.4904, 0.4383, 0.4367],
          [0.4085, 0.3295, 0.4280,  ..., 0.4489, 0.5311, 0.4896],
          [0.5174, 0.4931, 0.3832,  ..., 0.4134, 0.4451, 0.4839]],

         [[0.5610, 0.6443, 0.3855,  ..., 0.5518, 0.5704, 0.4866],
          [0.4998, 0.5295, 0.5383,  ..., 0.5171, 0.4890, 0.4827],
          [0.4232, 0.5329, 0.4688,  ..., 0.7605, 0.3873, 0.5414],
          [0.4042, 0.6202, 0.3526,  ..., 0.5840, 0.4516, 0.6285]]],


        [[[0.4037, 0.4950, 0.3832,  ..., 0.5983, 0.6299, 0.7272],
          [0.3976, 0.4731, 0.4824,  ..., 0.4448, 0.4868, 0.3984],
          [0.6239, 0.5470, 0.6742,  ..., 0.4888, 0.4055, 0.5360],
          [0.5739, 0.4618, 0.5497,  ..., 0.4424, 0.4405, 0.4419]],

         [[0.5337, 0.4800, 0.3758,  ..., 0.5721, 0.5661, 0.4766],
          [0.4142, 0.4150, 0.3813,  ..., 0.3933, 0.5473, 0.6147],
          [0.5487, 0.5230, 0.6119,  ..., 0.4627, 0.5487, 0.4695],
          [0.3305, 0.6550, 0.3375,  ..., 0.5450, 0.5033, 0.6628]],

         [[0.5106, 0.5661, 0.5653,  ..., 0.5043, 0.4920, 0.4864],
          [0.4582, 0.6077, 0.4483,  ..., 0.4031, 0.4833, 0.4362],
          [0.4434, 0.5046, 0.4137,  ..., 0.4453, 0.5908, 0.4968],
          [0.4355, 0.4083, 0.5337,  ..., 0.5677, 0.6280, 0.4007]],

         ...,

         [[0.3739, 0.4506, 0.5240,  ..., 0.5154, 0.4359, 0.4352],
          [0.5872, 0.5186, 0.4849,  ..., 0.4799, 0.4962, 0.4305],
          [0.5720, 0.4736, 0.3244,  ..., 0.3772, 0.4971, 0.4866],
          [0.3716, 0.6091, 0.5135,  ..., 0.4605, 0.5523, 0.5600]],

         [[0.3680, 0.4869, 0.4704,  ..., 0.3619, 0.5434, 0.4880],
          [0.6027, 0.6629, 0.5279,  ..., 0.6377, 0.3067, 0.4357],
          [0.4271, 0.5545, 0.4746,  ..., 0.4790, 0.4254, 0.5019],
          [0.5519, 0.4732, 0.6552,  ..., 0.5159, 0.7204, 0.4617]],

         [[0.4502, 0.5368, 0.4973,  ..., 0.5039, 0.4683, 0.3521],
          [0.3901, 0.5637, 0.6036,  ..., 0.5673, 0.3933, 0.6285],
          [0.3670, 0.5342, 0.5900,  ..., 0.6539, 0.5419, 0.5034],
          [0.5110, 0.4178, 0.5730,  ..., 0.6637, 0.4540, 0.6226]]]],
       device='cuda:0')
tensor([[[[0.4873, 0.6007, 0.5009,  ..., 0.6270, 0.5559, 0.4882],
          [0.3789, 0.4177, 0.5054,  ..., 0.4475, 0.3812, 0.4740],
          [0.4036, 0.4429, 0.4994,  ..., 0.4121, 0.4593, 0.4821],
          [0.4969, 0.5499, 0.5489,  ..., 0.4935, 0.4378, 0.6549]],

         [[0.4745, 0.5617, 0.5860,  ..., 0.4345, 0.4359, 0.4087],
          [0.5562, 0.3849, 0.6531,  ..., 0.4659, 0.5699, 0.3513],
          [0.5336, 0.5460, 0.4617,  ..., 0.4381, 0.5099, 0.5119],
          [0.4637, 0.5259, 0.5433,  ..., 0.5903, 0.2494, 0.5194]],

         [[0.3863, 0.4533, 0.4756,  ..., 0.4513, 0.4216, 0.5578],
          [0.5837, 0.3522, 0.4958,  ..., 0.7372, 0.5311, 0.3285],
          [0.6602, 0.4625, 0.6114,  ..., 0.4904, 0.5761, 0.5794],
          [0.5550, 0.4499, 0.4855,  ..., 0.5210, 0.5349, 0.6072]],

         ...,

         [[0.4550, 0.5525, 0.4513,  ..., 0.5742, 0.4673, 0.4364],
          [0.4804, 0.7001, 0.5436,  ..., 0.6619, 0.3057, 0.5251],
          [0.4064, 0.4504, 0.3621,  ..., 0.5751, 0.5780, 0.5041],
          [0.4896, 0.4970, 0.5161,  ..., 0.3442, 0.5467, 0.5221]],

         [[0.6156, 0.4943, 0.6114,  ..., 0.3442, 0.4258, 0.5344],
          [0.4292, 0.4886, 0.5660,  ..., 0.4834, 0.4373, 0.4297],
          [0.4135, 0.3285, 0.4330,  ..., 0.4419, 0.5301, 0.4826],
          [0.5224, 0.4921, 0.3882,  ..., 0.4064, 0.4441, 0.4769]],

         [[0.5660, 0.6433, 0.3905,  ..., 0.5448, 0.5694, 0.4796],
          [0.5048, 0.5285, 0.5433,  ..., 0.5101, 0.4880, 0.4757],
          [0.4282, 0.5319, 0.4738,  ..., 0.7535, 0.3863, 0.5344],
          [0.4092, 0.6192, 0.3576,  ..., 0.5770, 0.4506, 0.6215]]],


        [[[0.4087, 0.4940, 0.3882,  ..., 0.5913, 0.6289, 0.7202],
          [0.4026, 0.4721, 0.4874,  ..., 0.4378, 0.4858, 0.3914],
          [0.6289, 0.5460, 0.6792,  ..., 0.4818, 0.4045, 0.5290],
          [0.5789, 0.4608, 0.5547,  ..., 0.4354, 0.4395, 0.4349]],

         [[0.5387, 0.4790, 0.3808,  ..., 0.5651, 0.5651, 0.4696],
          [0.4192, 0.4140, 0.3863,  ..., 0.3863, 0.5463, 0.6077],
          [0.5537, 0.5220, 0.6169,  ..., 0.4557, 0.5477, 0.4625],
          [0.3355, 0.6540, 0.3425,  ..., 0.5380, 0.5023, 0.6558]],

         [[0.5156, 0.5651, 0.5703,  ..., 0.4973, 0.4910, 0.4794],
          [0.4632, 0.6067, 0.4533,  ..., 0.3961, 0.4823, 0.4292],
          [0.4484, 0.5036, 0.4187,  ..., 0.4383, 0.5898, 0.4898],
          [0.4405, 0.4073, 0.5387,  ..., 0.5607, 0.6270, 0.3937]],

         ...,

         [[0.3789, 0.4496, 0.5290,  ..., 0.5084, 0.4349, 0.4282],
          [0.5922, 0.5176, 0.4899,  ..., 0.4729, 0.4952, 0.4235],
          [0.5770, 0.4726, 0.3294,  ..., 0.3702, 0.4961, 0.4796],
          [0.3766, 0.6081, 0.5185,  ..., 0.4535, 0.5513, 0.5530]],

         [[0.3730, 0.4859, 0.4754,  ..., 0.3549, 0.5424, 0.4810],
          [0.6077, 0.6619, 0.5329,  ..., 0.6307, 0.3057, 0.4287],
          [0.4321, 0.5535, 0.4796,  ..., 0.4720, 0.4244, 0.4949],
          [0.5569, 0.4722, 0.6602,  ..., 0.5089, 0.7194, 0.4547]],

         [[0.4552, 0.5358, 0.5023,  ..., 0.4969, 0.4673, 0.3451],
          [0.3951, 0.5627, 0.6086,  ..., 0.5603, 0.3923, 0.6215],
          [0.3720, 0.5332, 0.5950,  ..., 0.6469, 0.5409, 0.4964],
          [0.5160, 0.4168, 0.5780,  ..., 0.6567, 0.4530, 0.6156]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0050,  0.0010, -0.0050,  0.0010, -0.0070, -0.0070,  0.0070,  0.0070,
         0.0010,  0.0070], device='cuda:0')
selected experts tensor([1610, 1589, 1657, 1699, 1577, 1561, 1704, 1643, 1680, 1664],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4836, 0.4339, 0.3809,  ..., 0.4891, 0.4495, 0.4593],
          [0.4524, 0.4343, 0.6601,  ..., 0.4914, 0.5862, 0.5344],
          [0.4859, 0.4809, 0.5251,  ..., 0.5592, 0.5002, 0.6138],
          [0.4863, 0.3069, 0.3565,  ..., 0.4774, 0.6079, 0.6012]],

         [[0.3555, 0.5967, 0.5864,  ..., 0.2784, 0.4582, 0.4725],
          [0.4653, 0.5159, 0.5930,  ..., 0.5254, 0.3851, 0.4545],
          [0.4643, 0.4612, 0.4835,  ..., 0.5859, 0.5154, 0.3887],
          [0.5087, 0.4271, 0.4564,  ..., 0.5845, 0.4222, 0.3667]],

         [[0.4483, 0.5807, 0.5310,  ..., 0.4966, 0.6172, 0.4273],
          [0.4474, 0.5621, 0.5964,  ..., 0.5357, 0.4684, 0.5560],
          [0.5433, 0.5536, 0.5716,  ..., 0.5454, 0.3463, 0.4278],
          [0.4214, 0.5152, 0.5648,  ..., 0.4673, 0.5582, 0.3252]],

         ...,

         [[0.4884, 0.3681, 0.4352,  ..., 0.3430, 0.6079, 0.5155],
          [0.6439, 0.4904, 0.4574,  ..., 0.4380, 0.3597, 0.6119],
          [0.4626, 0.5196, 0.4405,  ..., 0.3645, 0.5645, 0.4969],
          [0.6304, 0.4891, 0.3850,  ..., 0.4796, 0.3888, 0.5861]],

         [[0.5697, 0.5353, 0.5242,  ..., 0.5532, 0.3995, 0.5641],
          [0.4776, 0.6295, 0.5081,  ..., 0.4655, 0.5961, 0.4674],
          [0.3591, 0.5764, 0.4936,  ..., 0.5309, 0.5434, 0.5346],
          [0.5229, 0.6042, 0.5959,  ..., 0.4646, 0.4736, 0.4531]],

         [[0.4934, 0.3709, 0.4766,  ..., 0.5906, 0.4009, 0.4470],
          [0.6358, 0.4399, 0.6377,  ..., 0.5745, 0.4664, 0.6110],
          [0.6158, 0.6112, 0.3012,  ..., 0.4271, 0.6570, 0.4884],
          [0.4868, 0.5171, 0.5807,  ..., 0.5136, 0.4308, 0.4050]]],


        [[[0.4295, 0.4171, 0.5888,  ..., 0.5085, 0.6427, 0.4816],
          [0.3221, 0.5266, 0.4472,  ..., 0.4789, 0.3561, 0.4369],
          [0.4020, 0.5524, 0.6230,  ..., 0.5057, 0.4274, 0.5321],
          [0.4389, 0.3889, 0.4049,  ..., 0.5193, 0.3606, 0.4627]],

         [[0.7100, 0.5360, 0.5096,  ..., 0.3796, 0.5524, 0.5670],
          [0.5275, 0.5934, 0.4040,  ..., 0.4777, 0.5051, 0.3685],
          [0.3814, 0.5712, 0.5864,  ..., 0.5281, 0.4066, 0.4164],
          [0.4057, 0.6126, 0.5062,  ..., 0.4991, 0.5086, 0.5737]],

         [[0.4754, 0.4978, 0.4826,  ..., 0.5265, 0.5167, 0.5555],
          [0.5069, 0.6162, 0.4300,  ..., 0.6093, 0.4375, 0.6326],
          [0.5274, 0.4300, 0.5930,  ..., 0.5299, 0.5314, 0.2840],
          [0.4730, 0.7060, 0.5382,  ..., 0.5925, 0.4943, 0.4069]],

         ...,

         [[0.6322, 0.4863, 0.5997,  ..., 0.4621, 0.6508, 0.4150],
          [0.5215, 0.5401, 0.4148,  ..., 0.5963, 0.4440, 0.4766],
          [0.5274, 0.3709, 0.4947,  ..., 0.5953, 0.4217, 0.5747],
          [0.5707, 0.4736, 0.5406,  ..., 0.4727, 0.4841, 0.4574]],

         [[0.3768, 0.5163, 0.4738,  ..., 0.6014, 0.5500, 0.4779],
          [0.6589, 0.4428, 0.6006,  ..., 0.5821, 0.5371, 0.4698],
          [0.3778, 0.4612, 0.5275,  ..., 0.4416, 0.5051, 0.4531],
          [0.4873, 0.5587, 0.5530,  ..., 0.4039, 0.5013, 0.4895]],

         [[0.5079, 0.4029, 0.4796,  ..., 0.5140, 0.5714, 0.5593],
          [0.5250, 0.3474, 0.4714,  ..., 0.4884, 0.5315, 0.3957],
          [0.6259, 0.6144, 0.5603,  ..., 0.4949, 0.5186, 0.4803],
          [0.4997, 0.4609, 0.4719,  ..., 0.5445, 0.5321, 0.5349]]]],
       device='cuda:0')
tensor([[[[0.4866, 0.4369, 0.3739,  ..., 0.4921, 0.4465, 0.4583],
          [0.4554, 0.4373, 0.6531,  ..., 0.4944, 0.5832, 0.5334],
          [0.4889, 0.4839, 0.5181,  ..., 0.5622, 0.4972, 0.6128],
          [0.4893, 0.3099, 0.3495,  ..., 0.4804, 0.6049, 0.6002]],

         [[0.3585, 0.5997, 0.5794,  ..., 0.2814, 0.4552, 0.4715],
          [0.4683, 0.5189, 0.5860,  ..., 0.5284, 0.3821, 0.4535],
          [0.4673, 0.4642, 0.4765,  ..., 0.5889, 0.5124, 0.3877],
          [0.5117, 0.4301, 0.4494,  ..., 0.5875, 0.4192, 0.3657]],

         [[0.4513, 0.5837, 0.5240,  ..., 0.4996, 0.6142, 0.4263],
          [0.4504, 0.5651, 0.5894,  ..., 0.5387, 0.4654, 0.5550],
          [0.5463, 0.5566, 0.5646,  ..., 0.5484, 0.3433, 0.4268],
          [0.4244, 0.5182, 0.5578,  ..., 0.4703, 0.5552, 0.3242]],

         ...,

         [[0.4914, 0.3711, 0.4282,  ..., 0.3460, 0.6049, 0.5145],
          [0.6469, 0.4934, 0.4504,  ..., 0.4410, 0.3567, 0.6109],
          [0.4656, 0.5226, 0.4335,  ..., 0.3675, 0.5615, 0.4959],
          [0.6334, 0.4921, 0.3780,  ..., 0.4826, 0.3858, 0.5851]],

         [[0.5727, 0.5383, 0.5172,  ..., 0.5562, 0.3965, 0.5631],
          [0.4806, 0.6325, 0.5011,  ..., 0.4685, 0.5931, 0.4664],
          [0.3621, 0.5794, 0.4866,  ..., 0.5339, 0.5404, 0.5336],
          [0.5259, 0.6072, 0.5889,  ..., 0.4676, 0.4706, 0.4521]],

         [[0.4964, 0.3739, 0.4696,  ..., 0.5936, 0.3979, 0.4460],
          [0.6388, 0.4429, 0.6307,  ..., 0.5775, 0.4634, 0.6100],
          [0.6188, 0.6142, 0.2942,  ..., 0.4301, 0.6540, 0.4874],
          [0.4898, 0.5201, 0.5737,  ..., 0.5166, 0.4278, 0.4040]]],


        [[[0.4325, 0.4201, 0.5818,  ..., 0.5115, 0.6397, 0.4806],
          [0.3251, 0.5296, 0.4402,  ..., 0.4819, 0.3531, 0.4359],
          [0.4050, 0.5554, 0.6160,  ..., 0.5087, 0.4244, 0.5311],
          [0.4419, 0.3919, 0.3979,  ..., 0.5223, 0.3576, 0.4617]],

         [[0.7130, 0.5390, 0.5026,  ..., 0.3826, 0.5494, 0.5660],
          [0.5305, 0.5964, 0.3970,  ..., 0.4807, 0.5021, 0.3675],
          [0.3844, 0.5742, 0.5794,  ..., 0.5311, 0.4036, 0.4154],
          [0.4087, 0.6156, 0.4992,  ..., 0.5021, 0.5056, 0.5727]],

         [[0.4784, 0.5008, 0.4756,  ..., 0.5295, 0.5137, 0.5545],
          [0.5099, 0.6192, 0.4230,  ..., 0.6123, 0.4345, 0.6316],
          [0.5304, 0.4330, 0.5860,  ..., 0.5329, 0.5284, 0.2830],
          [0.4760, 0.7090, 0.5312,  ..., 0.5955, 0.4913, 0.4059]],

         ...,

         [[0.6352, 0.4893, 0.5927,  ..., 0.4651, 0.6478, 0.4140],
          [0.5245, 0.5431, 0.4078,  ..., 0.5993, 0.4410, 0.4756],
          [0.5304, 0.3739, 0.4877,  ..., 0.5983, 0.4187, 0.5737],
          [0.5737, 0.4766, 0.5336,  ..., 0.4757, 0.4811, 0.4564]],

         [[0.3798, 0.5193, 0.4668,  ..., 0.6044, 0.5470, 0.4769],
          [0.6619, 0.4458, 0.5936,  ..., 0.5851, 0.5341, 0.4688],
          [0.3808, 0.4642, 0.5205,  ..., 0.4446, 0.5021, 0.4521],
          [0.4903, 0.5617, 0.5460,  ..., 0.4069, 0.4983, 0.4885]],

         [[0.5109, 0.4059, 0.4726,  ..., 0.5170, 0.5684, 0.5583],
          [0.5280, 0.3504, 0.4644,  ..., 0.4914, 0.5285, 0.3947],
          [0.6289, 0.6174, 0.5533,  ..., 0.4979, 0.5156, 0.4793],
          [0.5027, 0.4639, 0.4649,  ..., 0.5475, 0.5291, 0.5339]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0030, -0.0030,  0.0070,  0.0090, -0.0010,  0.0090, -0.0090, -0.0030,
         0.0030,  0.0010], device='cuda:0')
selected experts tensor([1615, 1643, 1655, 1605, 1707, 1673, 1653, 1673, 1659, 1501],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5887, 0.4055, 0.5750,  ..., 0.4405, 0.5115, 0.4829],
          [0.5788, 0.5027, 0.5379,  ..., 0.6507, 0.4943, 0.4407],
          [0.6008, 0.4116, 0.5487,  ..., 0.4246, 0.4450, 0.5379],
          [0.5292, 0.4484, 0.6086,  ..., 0.4371, 0.4179, 0.5180]],

         [[0.4459, 0.5506, 0.4253,  ..., 0.5485, 0.5551, 0.6299],
          [0.4210, 0.6154, 0.5726,  ..., 0.5340, 0.6774, 0.5083],
          [0.5192, 0.6029, 0.5299,  ..., 0.4400, 0.3449, 0.5071],
          [0.4765, 0.4460, 0.6003,  ..., 0.5007, 0.4950, 0.5566]],

         [[0.3827, 0.5757, 0.5434,  ..., 0.6319, 0.5885, 0.5583],
          [0.4524, 0.5898, 0.5081,  ..., 0.5857, 0.5232, 0.4554],
          [0.5849, 0.4508, 0.3817,  ..., 0.4935, 0.5527, 0.3030],
          [0.4692, 0.5539, 0.5126,  ..., 0.4480, 0.5708, 0.5051]],

         ...,

         [[0.6638, 0.2903, 0.6295,  ..., 0.5494, 0.5133, 0.5616],
          [0.4531, 0.5130, 0.5362,  ..., 0.5130, 0.5590, 0.5761],
          [0.5195, 0.4997, 0.6008,  ..., 0.4770, 0.4479, 0.5021],
          [0.4253, 0.3559, 0.5393,  ..., 0.5881, 0.3492, 0.5439]],

         [[0.5703, 0.4664, 0.5311,  ..., 0.4787, 0.4983, 0.4747],
          [0.2896, 0.7050, 0.4856,  ..., 0.5567, 0.4861, 0.5999],
          [0.5540, 0.5141, 0.4818,  ..., 0.4422, 0.5616, 0.4670],
          [0.6473, 0.4708, 0.6331,  ..., 0.5682, 0.5168, 0.5134]],

         [[0.5152, 0.3460, 0.5425,  ..., 0.4774, 0.4431, 0.5349],
          [0.4201, 0.6434, 0.4153,  ..., 0.5648, 0.5103, 0.4648],
          [0.4519, 0.3631, 0.5343,  ..., 0.4606, 0.4801, 0.4407],
          [0.4100, 0.5036, 0.5432,  ..., 0.5184, 0.3978, 0.4907]]],


        [[[0.5641, 0.6317, 0.5069,  ..., 0.3901, 0.5828, 0.5645],
          [0.4191, 0.5051, 0.4483,  ..., 0.5361, 0.4436, 0.4355],
          [0.5398, 0.5917, 0.5636,  ..., 0.5255, 0.3840, 0.3988],
          [0.6672, 0.4293, 0.6864,  ..., 0.4947, 0.5026, 0.6028]],

         [[0.5364, 0.3469, 0.5124,  ..., 0.4634, 0.5079, 0.5452],
          [0.6123, 0.5527, 0.4210,  ..., 0.4859, 0.5952, 0.3795],
          [0.5100, 0.4269, 0.5129,  ..., 0.4838, 0.5418, 0.4170],
          [0.4596, 0.5085, 0.5952,  ..., 0.6021, 0.5032, 0.6159]],

         [[0.5521, 0.6029, 0.6241,  ..., 0.3597, 0.3750, 0.4558],
          [0.6187, 0.5818, 0.5420,  ..., 0.5744, 0.3615, 0.4660],
          [0.5177, 0.4924, 0.5905,  ..., 0.4227, 0.6280, 0.5258],
          [0.6482, 0.4102, 0.4432,  ..., 0.4850, 0.4580, 0.5363]],

         ...,

         [[0.6731, 0.5908, 0.3322,  ..., 0.5211, 0.4996, 0.7306],
          [0.4593, 0.4881, 0.6017,  ..., 0.6400, 0.5372, 0.4892],
          [0.4916, 0.4416, 0.4630,  ..., 0.4032, 0.5089, 0.4312],
          [0.5427, 0.3927, 0.3967,  ..., 0.6058, 0.5674, 0.5876]],

         [[0.5376, 0.6218, 0.5764,  ..., 0.5369, 0.5166, 0.5266],
          [0.5516, 0.4188, 0.3655,  ..., 0.4705, 0.5648, 0.5539],
          [0.5364, 0.3937, 0.4210,  ..., 0.6220, 0.7242, 0.5952],
          [0.4615, 0.6290, 0.5863,  ..., 0.4398, 0.3606, 0.3813]],

         [[0.5117, 0.4581, 0.5980,  ..., 0.4080, 0.4566, 0.6436],
          [0.4410, 0.6335, 0.5816,  ..., 0.5928, 0.5459, 0.4889],
          [0.4623, 0.3970, 0.4941,  ..., 0.4623, 0.5628, 0.6276],
          [0.4369, 0.5237, 0.5835,  ..., 0.5448, 0.5775, 0.6131]]]],
       device='cuda:0')
tensor([[[[0.6007, 0.4135, 0.5870,  ..., 0.4465, 0.4995, 0.4709],
          [0.5908, 0.5107, 0.5499,  ..., 0.6567, 0.4823, 0.4287],
          [0.6128, 0.4196, 0.5607,  ..., 0.4306, 0.4330, 0.5259],
          [0.5412, 0.4564, 0.6206,  ..., 0.4431, 0.4059, 0.5060]],

         [[0.4579, 0.5586, 0.4373,  ..., 0.5545, 0.5431, 0.6179],
          [0.4330, 0.6234, 0.5846,  ..., 0.5400, 0.6654, 0.4963],
          [0.5312, 0.6109, 0.5419,  ..., 0.4460, 0.3329, 0.4951],
          [0.4885, 0.4540, 0.6123,  ..., 0.5067, 0.4830, 0.5446]],

         [[0.3947, 0.5837, 0.5554,  ..., 0.6379, 0.5765, 0.5463],
          [0.4644, 0.5978, 0.5201,  ..., 0.5917, 0.5112, 0.4434],
          [0.5969, 0.4588, 0.3937,  ..., 0.4995, 0.5407, 0.2910],
          [0.4812, 0.5619, 0.5246,  ..., 0.4540, 0.5588, 0.4931]],

         ...,

         [[0.6758, 0.2983, 0.6415,  ..., 0.5554, 0.5013, 0.5496],
          [0.4651, 0.5210, 0.5482,  ..., 0.5190, 0.5470, 0.5641],
          [0.5315, 0.5077, 0.6128,  ..., 0.4830, 0.4359, 0.4901],
          [0.4373, 0.3639, 0.5513,  ..., 0.5941, 0.3372, 0.5319]],

         [[0.5823, 0.4744, 0.5431,  ..., 0.4847, 0.4863, 0.4627],
          [0.3016, 0.7130, 0.4976,  ..., 0.5627, 0.4741, 0.5879],
          [0.5660, 0.5221, 0.4938,  ..., 0.4482, 0.5496, 0.4550],
          [0.6593, 0.4788, 0.6451,  ..., 0.5742, 0.5048, 0.5014]],

         [[0.5272, 0.3540, 0.5545,  ..., 0.4834, 0.4311, 0.5229],
          [0.4321, 0.6514, 0.4273,  ..., 0.5708, 0.4983, 0.4528],
          [0.4639, 0.3711, 0.5463,  ..., 0.4666, 0.4681, 0.4287],
          [0.4220, 0.5116, 0.5552,  ..., 0.5244, 0.3858, 0.4787]]],


        [[[0.5761, 0.6397, 0.5189,  ..., 0.3961, 0.5708, 0.5525],
          [0.4311, 0.5131, 0.4603,  ..., 0.5421, 0.4316, 0.4235],
          [0.5518, 0.5997, 0.5756,  ..., 0.5315, 0.3720, 0.3868],
          [0.6792, 0.4373, 0.6984,  ..., 0.5007, 0.4906, 0.5908]],

         [[0.5484, 0.3549, 0.5244,  ..., 0.4694, 0.4959, 0.5332],
          [0.6243, 0.5607, 0.4330,  ..., 0.4919, 0.5832, 0.3675],
          [0.5220, 0.4349, 0.5249,  ..., 0.4898, 0.5297, 0.4050],
          [0.4716, 0.5165, 0.6072,  ..., 0.6081, 0.4912, 0.6039]],

         [[0.5641, 0.6109, 0.6361,  ..., 0.3657, 0.3630, 0.4438],
          [0.6307, 0.5898, 0.5540,  ..., 0.5804, 0.3495, 0.4540],
          [0.5297, 0.5004, 0.6025,  ..., 0.4287, 0.6160, 0.5138],
          [0.6602, 0.4182, 0.4552,  ..., 0.4910, 0.4460, 0.5243]],

         ...,

         [[0.6851, 0.5988, 0.3442,  ..., 0.5271, 0.4876, 0.7186],
          [0.4713, 0.4961, 0.6137,  ..., 0.6460, 0.5252, 0.4772],
          [0.5036, 0.4496, 0.4750,  ..., 0.4092, 0.4969, 0.4192],
          [0.5547, 0.4007, 0.4087,  ..., 0.6118, 0.5554, 0.5756]],

         [[0.5496, 0.6298, 0.5884,  ..., 0.5429, 0.5046, 0.5146],
          [0.5636, 0.4268, 0.3775,  ..., 0.4765, 0.5528, 0.5419],
          [0.5484, 0.4017, 0.4330,  ..., 0.6280, 0.7122, 0.5832],
          [0.4735, 0.6370, 0.5983,  ..., 0.4458, 0.3486, 0.3693]],

         [[0.5237, 0.4661, 0.6100,  ..., 0.4140, 0.4446, 0.6316],
          [0.4530, 0.6415, 0.5936,  ..., 0.5988, 0.5339, 0.4769],
          [0.4743, 0.4050, 0.5061,  ..., 0.4683, 0.5508, 0.6156],
          [0.4489, 0.5317, 0.5955,  ..., 0.5508, 0.5655, 0.6011]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0120, -0.0080, -0.0120,  0.0060, -0.0120,  0.0120, -0.0120, -0.0060,
         0.0120,  0.0120], device='cuda:0')
selected experts tensor([2263, 2003, 1495, 1589, 1704, 1096, 2362, 1587, 1017, 1268],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5772, 0.5213, 0.4395,  ..., 0.3104, 0.5205, 0.6484],
          [0.5398, 0.4629, 0.5663,  ..., 0.4729, 0.4353, 0.5479],
          [0.5623, 0.5638, 0.5184,  ..., 0.4186, 0.5735, 0.4925],
          [0.5143, 0.4129, 0.4223,  ..., 0.4518, 0.5945, 0.3799]],

         [[0.4851, 0.5457, 0.3997,  ..., 0.3971, 0.4310, 0.4670],
          [0.3535, 0.4680, 0.6109,  ..., 0.6369, 0.3187, 0.5626],
          [0.5805, 0.6688, 0.4915,  ..., 0.4050, 0.6477, 0.4063],
          [0.4232, 0.3799, 0.6312,  ..., 0.5658, 0.5397, 0.4419]],

         [[0.5413, 0.5902, 0.4496,  ..., 0.4358, 0.4083, 0.5411],
          [0.4322, 0.4450, 0.5816,  ..., 0.5617, 0.4401, 0.4665],
          [0.6410, 0.5110, 0.5291,  ..., 0.3710, 0.5247, 0.3627],
          [0.4232, 0.4829, 0.5443,  ..., 0.5822, 0.4729, 0.5147]],

         ...,

         [[0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5080, 0.5060],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5080, 0.5060],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5080, 0.5060],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5080, 0.5060]],

         [[0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5080, 0.5060],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5080, 0.5060],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5080, 0.5060],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5080, 0.5060]],

         [[0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5080, 0.5060],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5080, 0.5060],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5080, 0.5060],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5080, 0.5060]]],


        [[[0.5503, 0.5404, 0.4564,  ..., 0.3915, 0.4224, 0.5561],
          [0.5529, 0.4694, 0.5069,  ..., 0.5448, 0.5089, 0.5151],
          [0.5425, 0.5537, 0.4646,  ..., 0.4865, 0.5661, 0.4414],
          [0.3265, 0.5418, 0.5547,  ..., 0.5286, 0.4286, 0.4352]],

         [[0.6037, 0.5564, 0.6412,  ..., 0.4353, 0.3584, 0.6034],
          [0.3871, 0.5455, 0.5435,  ..., 0.4248, 0.5798, 0.5019],
          [0.5551, 0.4933, 0.4482,  ..., 0.4499, 0.3828, 0.5150],
          [0.4996, 0.5802, 0.4556,  ..., 0.4729, 0.5484, 0.4323]],

         [[0.4963, 0.4755, 0.4067,  ..., 0.4338, 0.5072, 0.4016],
          [0.3369, 0.5161, 0.5206,  ..., 0.4974, 0.5489, 0.6321],
          [0.5515, 0.5359, 0.3109,  ..., 0.5150, 0.4559, 0.4814],
          [0.4931, 0.4561, 0.5906,  ..., 0.5783, 0.6124, 0.3988]],

         ...,

         [[0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5080, 0.5060],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5080, 0.5060],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5080, 0.5060],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5080, 0.5060]],

         [[0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5080, 0.5060],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5080, 0.5060],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5080, 0.5060],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5080, 0.5060]],

         [[0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5080, 0.5060],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5080, 0.5060],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5080, 0.5060],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5080, 0.5060]]]],
       device='cuda:0')
tensor([[[[0.5732, 0.5153, 0.4335,  ..., 0.3024, 0.5125, 0.6424],
          [0.5358, 0.4569, 0.5603,  ..., 0.4649, 0.4273, 0.5419],
          [0.5583, 0.5578, 0.5124,  ..., 0.4106, 0.5655, 0.4865],
          [0.5103, 0.4069, 0.4163,  ..., 0.4438, 0.5865, 0.3739]],

         [[0.4811, 0.5397, 0.3937,  ..., 0.3891, 0.4230, 0.4610],
          [0.3495, 0.4620, 0.6049,  ..., 0.6289, 0.3107, 0.5566],
          [0.5765, 0.6628, 0.4855,  ..., 0.3970, 0.6397, 0.4003],
          [0.4192, 0.3739, 0.6252,  ..., 0.5578, 0.5317, 0.4359]],

         [[0.5373, 0.5842, 0.4436,  ..., 0.4278, 0.4003, 0.5351],
          [0.4282, 0.4390, 0.5756,  ..., 0.5537, 0.4321, 0.4605],
          [0.6370, 0.5050, 0.5231,  ..., 0.3630, 0.5167, 0.3567],
          [0.4192, 0.4769, 0.5383,  ..., 0.5742, 0.4649, 0.5087]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5463, 0.5344, 0.4504,  ..., 0.3835, 0.4144, 0.5501],
          [0.5489, 0.4634, 0.5009,  ..., 0.5368, 0.5009, 0.5091],
          [0.5385, 0.5477, 0.4586,  ..., 0.4785, 0.5581, 0.4354],
          [0.3225, 0.5358, 0.5487,  ..., 0.5206, 0.4206, 0.4292]],

         [[0.5997, 0.5504, 0.6352,  ..., 0.4273, 0.3504, 0.5974],
          [0.3831, 0.5395, 0.5375,  ..., 0.4168, 0.5718, 0.4959],
          [0.5511, 0.4873, 0.4422,  ..., 0.4419, 0.3748, 0.5090],
          [0.4956, 0.5742, 0.4496,  ..., 0.4649, 0.5404, 0.4263]],

         [[0.4923, 0.4695, 0.4007,  ..., 0.4258, 0.4992, 0.3956],
          [0.3329, 0.5101, 0.5146,  ..., 0.4894, 0.5409, 0.6261],
          [0.5475, 0.5299, 0.3049,  ..., 0.5070, 0.4479, 0.4754],
          [0.4891, 0.4501, 0.5846,  ..., 0.5703, 0.6044, 0.3928]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([ 4.0000e-03,  6.0000e-03,  6.0000e-03,  6.0000e-03, -2.3283e-10,
         6.0000e-03,  6.0000e-03,  8.0000e-03,  8.0000e-03,  6.0000e-03],
       device='cuda:0')
selected experts tensor([1694, 1202, 1399,  844, 2306,  933,  739, 2785, 3266, 1216],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1615., 1643., 1655., 1605., 1707., 1673., 1653., 1673., 1659., 1501.],
        [1694., 1202., 1399.,  844., 2306.,  933.,  739., 2785., 3266., 1216.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4653, 0.4060, 0.4949,  ..., 0.4906, 0.5417, 0.4035],
          [0.5103, 0.4212, 0.5301,  ..., 0.5583, 0.6926, 0.5825],
          [0.5133, 0.3981, 0.3831,  ..., 0.5101, 0.4808, 0.5327],
          [0.5425, 0.5037, 0.4309,  ..., 0.5883, 0.4125, 0.3900]],

         [[0.4830, 0.5293, 0.6007,  ..., 0.4707, 0.5351, 0.4185],
          [0.3306, 0.5378, 0.6418,  ..., 0.5479, 0.5248, 0.4736],
          [0.6366, 0.4676, 0.5164,  ..., 0.5320, 0.5533, 0.5375],
          [0.3846, 0.6489, 0.6105,  ..., 0.4204, 0.4797, 0.5844]],

         [[0.5006, 0.6092, 0.4803,  ..., 0.4515, 0.3082, 0.5344],
          [0.4166, 0.4809, 0.5909,  ..., 0.4955, 0.4438, 0.5445],
          [0.5171, 0.3194, 0.4871,  ..., 0.4759, 0.5960, 0.3969],
          [0.2798, 0.3874, 0.5904,  ..., 0.4443, 0.3090, 0.6197]],

         ...,

         [[0.5575, 0.5335, 0.5581,  ..., 0.5706, 0.6128, 0.3849],
          [0.4616, 0.5625, 0.3859,  ..., 0.3591, 0.5884, 0.5802],
          [0.4628, 0.3668, 0.6086,  ..., 0.5706, 0.5675, 0.4668],
          [0.6157, 0.5126, 0.4896,  ..., 0.5404, 0.4273, 0.3209]],

         [[0.4680, 0.5824, 0.4748,  ..., 0.4242, 0.3872, 0.5638],
          [0.5920, 0.4993, 0.4971,  ..., 0.5542, 0.4424, 0.4559],
          [0.4029, 0.4705, 0.5942,  ..., 0.4868, 0.4745, 0.5715],
          [0.6760, 0.5625, 0.5327,  ..., 0.5481, 0.5351, 0.6591]],

         [[0.4413, 0.4903, 0.4458,  ..., 0.6430, 0.4899, 0.4931],
          [0.6429, 0.4524, 0.3615,  ..., 0.6986, 0.6379, 0.4738],
          [0.6013, 0.5352, 0.6382,  ..., 0.4583, 0.4996, 0.5024],
          [0.3925, 0.4183, 0.4977,  ..., 0.5602, 0.3803, 0.4985]]],


        [[[0.5816, 0.3971, 0.5895,  ..., 0.4016, 0.3442, 0.4096],
          [0.4920, 0.6162, 0.5281,  ..., 0.3002, 0.6188, 0.4086],
          [0.5546, 0.4760, 0.5178,  ..., 0.5691, 0.4710, 0.5326],
          [0.5620, 0.4766, 0.4685,  ..., 0.3600, 0.4206, 0.3681]],

         [[0.5854, 0.6240, 0.3030,  ..., 0.5435, 0.4591, 0.5720],
          [0.3930, 0.4398, 0.3365,  ..., 0.5977, 0.6179, 0.5730],
          [0.6023, 0.6604, 0.4299,  ..., 0.5377, 0.3124, 0.4893],
          [0.5934, 0.5862, 0.5234,  ..., 0.4790, 0.4991, 0.6081]],

         [[0.3818, 0.5606, 0.5586,  ..., 0.5754, 0.4022, 0.5607],
          [0.6074, 0.5018, 0.5114,  ..., 0.4309, 0.5501, 0.6843],
          [0.5202, 0.5521, 0.3929,  ..., 0.4424, 0.4627, 0.5696],
          [0.4609, 0.4193, 0.5876,  ..., 0.4680, 0.3259, 0.4100]],

         ...,

         [[0.4616, 0.3823, 0.5796,  ..., 0.4767, 0.4410, 0.3923],
          [0.3911, 0.4742, 0.6155,  ..., 0.6010, 0.4026, 0.5029],
          [0.5439, 0.4360, 0.4160,  ..., 0.5479, 0.6645, 0.4486],
          [0.5002, 0.3677, 0.4122,  ..., 0.5396, 0.3666, 0.4945]],

         [[0.5668, 0.6069, 0.4849,  ..., 0.6835, 0.5646, 0.5939],
          [0.4626, 0.6534, 0.5576,  ..., 0.4673, 0.6132, 0.3997],
          [0.4687, 0.5067, 0.6247,  ..., 0.5744, 0.4727, 0.4619],
          [0.4495, 0.5966, 0.4151,  ..., 0.5987, 0.6234, 0.6211]],

         [[0.5783, 0.4630, 0.5175,  ..., 0.5696, 0.5076, 0.5629],
          [0.5563, 0.4712, 0.5386,  ..., 0.6538, 0.6680, 0.3946],
          [0.4862, 0.6272, 0.5046,  ..., 0.3681, 0.3407, 0.5749],
          [0.3509, 0.6498, 0.7387,  ..., 0.6330, 0.4970, 0.4276]]]],
       device='cuda:0')
tensor([[[[0.4693, 0.4040, 0.5009,  ..., 0.4846, 0.5417, 0.3975],
          [0.5143, 0.4192, 0.5361,  ..., 0.5523, 0.6926, 0.5765],
          [0.5173, 0.3961, 0.3891,  ..., 0.5041, 0.4808, 0.5267],
          [0.5465, 0.5017, 0.4369,  ..., 0.5823, 0.4125, 0.3840]],

         [[0.4870, 0.5273, 0.6067,  ..., 0.4647, 0.5351, 0.4125],
          [0.3346, 0.5358, 0.6478,  ..., 0.5419, 0.5248, 0.4676],
          [0.6406, 0.4656, 0.5224,  ..., 0.5260, 0.5533, 0.5315],
          [0.3886, 0.6469, 0.6165,  ..., 0.4144, 0.4797, 0.5784]],

         [[0.5046, 0.6072, 0.4863,  ..., 0.4455, 0.3082, 0.5284],
          [0.4206, 0.4789, 0.5969,  ..., 0.4895, 0.4438, 0.5385],
          [0.5211, 0.3174, 0.4931,  ..., 0.4699, 0.5960, 0.3909],
          [0.2838, 0.3854, 0.5964,  ..., 0.4383, 0.3090, 0.6137]],

         ...,

         [[0.5615, 0.5315, 0.5641,  ..., 0.5646, 0.6128, 0.3789],
          [0.4656, 0.5605, 0.3919,  ..., 0.3531, 0.5884, 0.5742],
          [0.4668, 0.3648, 0.6146,  ..., 0.5646, 0.5675, 0.4608],
          [0.6197, 0.5106, 0.4956,  ..., 0.5344, 0.4273, 0.3149]],

         [[0.4720, 0.5804, 0.4808,  ..., 0.4182, 0.3872, 0.5578],
          [0.5960, 0.4973, 0.5031,  ..., 0.5482, 0.4424, 0.4499],
          [0.4069, 0.4685, 0.6002,  ..., 0.4808, 0.4745, 0.5655],
          [0.6800, 0.5605, 0.5387,  ..., 0.5421, 0.5351, 0.6531]],

         [[0.4453, 0.4883, 0.4518,  ..., 0.6370, 0.4899, 0.4871],
          [0.6469, 0.4504, 0.3675,  ..., 0.6926, 0.6379, 0.4678],
          [0.6053, 0.5332, 0.6442,  ..., 0.4523, 0.4996, 0.4964],
          [0.3965, 0.4163, 0.5037,  ..., 0.5542, 0.3803, 0.4925]]],


        [[[0.5856, 0.3951, 0.5955,  ..., 0.3956, 0.3442, 0.4036],
          [0.4960, 0.6142, 0.5341,  ..., 0.2942, 0.6188, 0.4026],
          [0.5586, 0.4740, 0.5238,  ..., 0.5631, 0.4710, 0.5266],
          [0.5660, 0.4746, 0.4745,  ..., 0.3540, 0.4206, 0.3621]],

         [[0.5894, 0.6220, 0.3090,  ..., 0.5375, 0.4591, 0.5660],
          [0.3970, 0.4378, 0.3425,  ..., 0.5917, 0.6179, 0.5670],
          [0.6063, 0.6584, 0.4359,  ..., 0.5317, 0.3124, 0.4833],
          [0.5974, 0.5842, 0.5294,  ..., 0.4730, 0.4991, 0.6021]],

         [[0.3858, 0.5586, 0.5646,  ..., 0.5694, 0.4022, 0.5547],
          [0.6114, 0.4998, 0.5174,  ..., 0.4249, 0.5501, 0.6783],
          [0.5242, 0.5501, 0.3989,  ..., 0.4364, 0.4627, 0.5636],
          [0.4649, 0.4173, 0.5936,  ..., 0.4620, 0.3259, 0.4040]],

         ...,

         [[0.4656, 0.3803, 0.5856,  ..., 0.4707, 0.4410, 0.3863],
          [0.3951, 0.4722, 0.6215,  ..., 0.5950, 0.4026, 0.4969],
          [0.5479, 0.4340, 0.4220,  ..., 0.5419, 0.6645, 0.4426],
          [0.5042, 0.3657, 0.4182,  ..., 0.5336, 0.3666, 0.4885]],

         [[0.5708, 0.6049, 0.4909,  ..., 0.6775, 0.5646, 0.5879],
          [0.4666, 0.6514, 0.5636,  ..., 0.4613, 0.6132, 0.3937],
          [0.4727, 0.5047, 0.6307,  ..., 0.5684, 0.4727, 0.4559],
          [0.4535, 0.5946, 0.4211,  ..., 0.5927, 0.6234, 0.6151]],

         [[0.5823, 0.4610, 0.5235,  ..., 0.5636, 0.5076, 0.5569],
          [0.5603, 0.4692, 0.5446,  ..., 0.6478, 0.6680, 0.3886],
          [0.4902, 0.6252, 0.5106,  ..., 0.3621, 0.3407, 0.5689],
          [0.3549, 0.6478, 0.7447,  ..., 0.6270, 0.4970, 0.4216]]]],
       device='cuda:0', requires_grad=True)
tensor([-4.0000e-03,  2.0000e-03, -6.0000e-03,  0.0000e+00, -6.0000e-03,
        -6.0000e-03,  6.0000e-03,  6.0000e-03, -2.3283e-10,  6.0000e-03],
       device='cuda:0')
selected experts tensor([1604, 1582, 1800, 1658, 1540, 1553, 1657, 1729, 1652, 1609],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.3964, 0.4071, 0.7110,  ..., 0.6240, 0.4640, 0.5699],
          [0.4883, 0.5012, 0.4314,  ..., 0.4295, 0.3995, 0.5184],
          [0.4821, 0.4081, 0.5484,  ..., 0.4367, 0.4688, 0.4051],
          [0.4690, 0.5725, 0.4792,  ..., 0.4641, 0.3967, 0.5386]],

         [[0.5287, 0.6438, 0.4438,  ..., 0.4822, 0.6003, 0.4780],
          [0.5058, 0.5009, 0.4460,  ..., 0.3740, 0.5895, 0.3169],
          [0.5338, 0.6482, 0.4361,  ..., 0.5844, 0.5623, 0.5322],
          [0.5275, 0.4643, 0.4438,  ..., 0.4607, 0.5685, 0.4803]],

         [[0.5189, 0.5212, 0.5935,  ..., 0.4180, 0.6453, 0.4145],
          [0.5370, 0.5635, 0.5481,  ..., 0.4837, 0.4504, 0.4079],
          [0.4476, 0.5144, 0.5954,  ..., 0.4066, 0.6254, 0.4826],
          [0.4595, 0.5086, 0.6252,  ..., 0.4993, 0.4770, 0.4611]],

         ...,

         [[0.3664, 0.4367, 0.6109,  ..., 0.5439, 0.4995, 0.4013],
          [0.6163, 0.5262, 0.5610,  ..., 0.5338, 0.5899, 0.4051],
          [0.3838, 0.4787, 0.6349,  ..., 0.4751, 0.5176, 0.6534],
          [0.4301, 0.5473, 0.7053,  ..., 0.7609, 0.6281, 0.5838]],

         [[0.4281, 0.5206, 0.5525,  ..., 0.5692, 0.6587, 0.4065],
          [0.5428, 0.4624, 0.4810,  ..., 0.4510, 0.4671, 0.4874],
          [0.4267, 0.4690, 0.5844,  ..., 0.3581, 0.5942, 0.3623],
          [0.4296, 0.2854, 0.4866,  ..., 0.5427, 0.5048, 0.6008]],

         [[0.6341, 0.7026, 0.5930,  ..., 0.6134, 0.5795, 0.4718],
          [0.5196, 0.5447, 0.4627,  ..., 0.5787, 0.4107, 0.4640],
          [0.4138, 0.5024, 0.5735,  ..., 0.5456, 0.4637, 0.4688],
          [0.4430, 0.4398, 0.6024,  ..., 0.5654, 0.3948, 0.3569]]],


        [[[0.5674, 0.4209, 0.6174,  ..., 0.3916, 0.3971, 0.5044],
          [0.5798, 0.4553, 0.4822,  ..., 0.3402, 0.6003, 0.4379],
          [0.5864, 0.4534, 0.4100,  ..., 0.4372, 0.6471, 0.6913],
          [0.3843, 0.4580, 0.5210,  ..., 0.4403, 0.5111, 0.3976]],

         [[0.4248, 0.3617, 0.5133,  ..., 0.4204, 0.5371, 0.4949],
          [0.6368, 0.4789, 0.5115,  ..., 0.5882, 0.4709, 0.5951],
          [0.4871, 0.5287, 0.6609,  ..., 0.4473, 0.4766, 0.4882],
          [0.3843, 0.2543, 0.5094,  ..., 0.3280, 0.4384, 0.5661]],

         [[0.6368, 0.5403, 0.7709,  ..., 0.4853, 0.4516, 0.5809],
          [0.4845, 0.4290, 0.6439,  ..., 0.4918, 0.6272, 0.4393],
          [0.5133, 0.5802, 0.6294,  ..., 0.3726, 0.5116, 0.6426],
          [0.5185, 0.4534, 0.5806,  ..., 0.6348, 0.5632, 0.5282]],

         ...,

         [[0.5750, 0.6794, 0.4261,  ..., 0.3599, 0.4654, 0.5994],
          [0.3592, 0.4890, 0.4759,  ..., 0.5435, 0.6471, 0.5378],
          [0.4110, 0.3690, 0.4831,  ..., 0.5687, 0.4499, 0.4770],
          [0.5314, 0.4329, 0.5887,  ..., 0.5522, 0.4393, 0.5538]],

         [[0.5878, 0.3617, 0.5944,  ..., 0.5142, 0.5352, 0.4480],
          [0.5878, 0.5505, 0.2788,  ..., 0.4553, 0.4686, 0.5281],
          [0.4644, 0.7760, 0.5210,  ..., 0.2951, 0.5606, 0.4716],
          [0.2345, 0.4128, 0.3681,  ..., 0.5239, 0.4475, 0.4807]],

         [[0.6538, 0.5778, 0.4455,  ..., 0.5630, 0.4141, 0.4750],
          [0.3950, 0.4753, 0.6285,  ..., 0.4524, 0.5483, 0.5207],
          [0.4484, 0.4875, 0.6257,  ..., 0.5493, 0.4269, 0.4307],
          [0.4839, 0.5560, 0.4889,  ..., 0.4944, 0.4350, 0.6055]]]],
       device='cuda:0')
tensor([[[[0.3984, 0.4111, 0.7050,  ..., 0.6280, 0.4620, 0.5679],
          [0.4903, 0.5052, 0.4254,  ..., 0.4335, 0.3975, 0.5164],
          [0.4841, 0.4121, 0.5424,  ..., 0.4407, 0.4668, 0.4031],
          [0.4710, 0.5765, 0.4732,  ..., 0.4681, 0.3947, 0.5366]],

         [[0.5307, 0.6478, 0.4378,  ..., 0.4862, 0.5983, 0.4760],
          [0.5078, 0.5049, 0.4400,  ..., 0.3780, 0.5875, 0.3149],
          [0.5358, 0.6522, 0.4301,  ..., 0.5884, 0.5603, 0.5302],
          [0.5295, 0.4683, 0.4378,  ..., 0.4647, 0.5665, 0.4783]],

         [[0.5209, 0.5252, 0.5875,  ..., 0.4220, 0.6433, 0.4125],
          [0.5390, 0.5675, 0.5421,  ..., 0.4877, 0.4484, 0.4059],
          [0.4496, 0.5184, 0.5894,  ..., 0.4106, 0.6234, 0.4806],
          [0.4615, 0.5126, 0.6192,  ..., 0.5033, 0.4750, 0.4591]],

         ...,

         [[0.3684, 0.4407, 0.6049,  ..., 0.5479, 0.4975, 0.3993],
          [0.6183, 0.5302, 0.5550,  ..., 0.5378, 0.5879, 0.4031],
          [0.3858, 0.4827, 0.6289,  ..., 0.4791, 0.5156, 0.6514],
          [0.4321, 0.5513, 0.6993,  ..., 0.7649, 0.6261, 0.5818]],

         [[0.4301, 0.5246, 0.5465,  ..., 0.5732, 0.6567, 0.4045],
          [0.5448, 0.4664, 0.4750,  ..., 0.4550, 0.4651, 0.4854],
          [0.4287, 0.4730, 0.5784,  ..., 0.3621, 0.5922, 0.3603],
          [0.4316, 0.2894, 0.4806,  ..., 0.5467, 0.5028, 0.5988]],

         [[0.6361, 0.7066, 0.5870,  ..., 0.6174, 0.5775, 0.4698],
          [0.5216, 0.5487, 0.4567,  ..., 0.5827, 0.4087, 0.4620],
          [0.4158, 0.5064, 0.5675,  ..., 0.5496, 0.4617, 0.4668],
          [0.4450, 0.4438, 0.5964,  ..., 0.5694, 0.3928, 0.3549]]],


        [[[0.5694, 0.4249, 0.6114,  ..., 0.3956, 0.3951, 0.5024],
          [0.5818, 0.4593, 0.4762,  ..., 0.3442, 0.5983, 0.4359],
          [0.5884, 0.4574, 0.4040,  ..., 0.4412, 0.6451, 0.6893],
          [0.3863, 0.4620, 0.5150,  ..., 0.4443, 0.5091, 0.3956]],

         [[0.4268, 0.3657, 0.5073,  ..., 0.4244, 0.5351, 0.4929],
          [0.6388, 0.4829, 0.5055,  ..., 0.5922, 0.4689, 0.5931],
          [0.4891, 0.5327, 0.6549,  ..., 0.4513, 0.4746, 0.4862],
          [0.3863, 0.2583, 0.5034,  ..., 0.3320, 0.4364, 0.5641]],

         [[0.6388, 0.5443, 0.7649,  ..., 0.4893, 0.4496, 0.5789],
          [0.4865, 0.4330, 0.6379,  ..., 0.4958, 0.6252, 0.4373],
          [0.5153, 0.5842, 0.6234,  ..., 0.3766, 0.5096, 0.6406],
          [0.5205, 0.4574, 0.5746,  ..., 0.6388, 0.5612, 0.5262]],

         ...,

         [[0.5770, 0.6834, 0.4201,  ..., 0.3639, 0.4634, 0.5974],
          [0.3612, 0.4930, 0.4699,  ..., 0.5475, 0.6451, 0.5358],
          [0.4130, 0.3730, 0.4771,  ..., 0.5727, 0.4479, 0.4750],
          [0.5334, 0.4369, 0.5827,  ..., 0.5562, 0.4373, 0.5518]],

         [[0.5898, 0.3657, 0.5884,  ..., 0.5182, 0.5332, 0.4460],
          [0.5898, 0.5545, 0.2728,  ..., 0.4593, 0.4666, 0.5261],
          [0.4664, 0.7800, 0.5150,  ..., 0.2991, 0.5586, 0.4696],
          [0.2365, 0.4168, 0.3621,  ..., 0.5279, 0.4455, 0.4787]],

         [[0.6558, 0.5818, 0.4395,  ..., 0.5670, 0.4121, 0.4730],
          [0.3970, 0.4793, 0.6225,  ..., 0.4564, 0.5463, 0.5187],
          [0.4504, 0.4915, 0.6197,  ..., 0.5533, 0.4249, 0.4287],
          [0.4859, 0.5600, 0.4829,  ..., 0.4984, 0.4330, 0.6035]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0020, -0.0040,  0.0060,  0.0100, -0.0020,  0.0080, -0.0100, -0.0040,
         0.0020,  0.0020], device='cuda:0')
selected experts tensor([1658, 1650, 1717, 1580, 1644, 1608, 1639, 1558, 1615, 1715],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5118, 0.5982, 0.5817,  ..., 0.3850, 0.5530, 0.6749],
          [0.4586, 0.5647, 0.4646,  ..., 0.6124, 0.4836, 0.2944],
          [0.4747, 0.4007, 0.5471,  ..., 0.4360, 0.6599, 0.5454],
          [0.4531, 0.3932, 0.5047,  ..., 0.5734, 0.4626, 0.4105]],

         [[0.4802, 0.4734, 0.4989,  ..., 0.4546, 0.4475, 0.6998],
          [0.4620, 0.5870, 0.3758,  ..., 0.5829, 0.6235, 0.4293],
          [0.5792, 0.4852, 0.5386,  ..., 0.5502, 0.4039, 0.5556],
          [0.5121, 0.5709, 0.5985,  ..., 0.5282, 0.5199, 0.3979]],

         [[0.4922, 0.4255, 0.5774,  ..., 0.5668, 0.4398, 0.6258],
          [0.4410, 0.6144, 0.5423,  ..., 0.5403, 0.4326, 0.5243],
          [0.4919, 0.3997, 0.5765,  ..., 0.5663, 0.4863, 0.4549],
          [0.5783, 0.3764, 0.5521,  ..., 0.5229, 0.3724, 0.6697]],

         ...,

         [[0.4378, 0.4360, 0.4823,  ..., 0.4019, 0.5140, 0.5149],
          [0.4333, 0.5508, 0.5505,  ..., 0.5413, 0.5449, 0.3951],
          [0.3849, 0.3950, 0.4840,  ..., 0.5658, 0.5212, 0.4867],
          [0.4885, 0.7024, 0.5195,  ..., 0.5516, 0.5527, 0.5324]],

         [[0.6240, 0.4472, 0.4486,  ..., 0.4480, 0.4728, 0.4573],
          [0.3645, 0.4671, 0.5161,  ..., 0.5649, 0.4825, 0.6964],
          [0.4529, 0.5128, 0.6287,  ..., 0.5018, 0.5805, 0.4270],
          [0.3849, 0.4751, 0.5036,  ..., 0.6446, 0.4721, 0.5910]],

         [[0.4990, 0.5127, 0.4091,  ..., 0.3864, 0.5267, 0.5314],
          [0.3645, 0.5484, 0.4919,  ..., 0.5374, 0.5353, 0.5561],
          [0.4248, 0.3708, 0.6332,  ..., 0.5320, 0.6014, 0.3634],
          [0.5021, 0.5031, 0.4968,  ..., 0.4335, 0.5110, 0.6509]]],


        [[[0.4827, 0.4130, 0.5612,  ..., 0.5065, 0.5012, 0.6382],
          [0.4306, 0.6325, 0.4623,  ..., 0.5198, 0.5790, 0.3984],
          [0.5549, 0.4644, 0.4280,  ..., 0.4700, 0.5643, 0.4687],
          [0.3374, 0.6271, 0.5703,  ..., 0.5787, 0.5658, 0.4614]],

         [[0.5626, 0.4211, 0.5343,  ..., 0.4609, 0.4152, 0.6183],
          [0.5631, 0.5675, 0.4585,  ..., 0.5024, 0.4956, 0.5259],
          [0.4956, 0.4207, 0.6142,  ..., 0.5749, 0.4142, 0.4422],
          [0.6679, 0.4973, 0.6682,  ..., 0.6068, 0.4035, 0.5271]],

         [[0.4761, 0.4899, 0.5411,  ..., 0.3472, 0.4288, 0.6364],
          [0.4186, 0.4982, 0.5703,  ..., 0.5296, 0.5852, 0.5099],
          [0.5378, 0.5145, 0.4273,  ..., 0.4429, 0.6235, 0.4007],
          [0.3962, 0.5756, 0.4766,  ..., 0.6129, 0.5318, 0.5517]],

         ...,

         [[0.5427, 0.5780, 0.5708,  ..., 0.4247, 0.5274, 0.3974],
          [0.5569, 0.5312, 0.4817,  ..., 0.5023, 0.6318, 0.4779],
          [0.6480, 0.5467, 0.4343,  ..., 0.4772, 0.4925, 0.5209],
          [0.4551, 0.5351, 0.4338,  ..., 0.5562, 0.4494, 0.5033]],

         [[0.5853, 0.4626, 0.4163,  ..., 0.4626, 0.5976, 0.4265],
          [0.5942, 0.6089, 0.4926,  ..., 0.5413, 0.5469, 0.4035],
          [0.5354, 0.4870, 0.4259,  ..., 0.5071, 0.4820, 0.5094],
          [0.3563, 0.4226, 0.5550,  ..., 0.5294, 0.4687, 0.4384]],

         [[0.6489, 0.5580, 0.6269,  ..., 0.4004, 0.5410, 0.4091],
          [0.3618, 0.5110, 0.4997,  ..., 0.5839, 0.5542, 0.3599],
          [0.6463, 0.4154, 0.6492,  ..., 0.4823, 0.5084, 0.3724],
          [0.5853, 0.5575, 0.4851,  ..., 0.6725, 0.4943, 0.6428]]]],
       device='cuda:0')
tensor([[[[0.5248, 0.6072, 0.5927,  ..., 0.3900, 0.5400, 0.6619],
          [0.4716, 0.5737, 0.4756,  ..., 0.6174, 0.4706, 0.2814],
          [0.4877, 0.4097, 0.5581,  ..., 0.4410, 0.6469, 0.5324],
          [0.4661, 0.4022, 0.5157,  ..., 0.5784, 0.4496, 0.3975]],

         [[0.4932, 0.4824, 0.5099,  ..., 0.4596, 0.4345, 0.6868],
          [0.4750, 0.5960, 0.3868,  ..., 0.5879, 0.6105, 0.4163],
          [0.5922, 0.4942, 0.5496,  ..., 0.5552, 0.3909, 0.5426],
          [0.5251, 0.5799, 0.6095,  ..., 0.5332, 0.5069, 0.3849]],

         [[0.5052, 0.4345, 0.5884,  ..., 0.5718, 0.4268, 0.6128],
          [0.4540, 0.6234, 0.5533,  ..., 0.5453, 0.4196, 0.5113],
          [0.5049, 0.4087, 0.5875,  ..., 0.5713, 0.4733, 0.4419],
          [0.5913, 0.3854, 0.5631,  ..., 0.5279, 0.3594, 0.6567]],

         ...,

         [[0.4508, 0.4450, 0.4933,  ..., 0.4069, 0.5010, 0.5019],
          [0.4463, 0.5598, 0.5615,  ..., 0.5463, 0.5319, 0.3821],
          [0.3979, 0.4040, 0.4950,  ..., 0.5708, 0.5082, 0.4737],
          [0.5015, 0.7114, 0.5305,  ..., 0.5566, 0.5397, 0.5194]],

         [[0.6370, 0.4562, 0.4596,  ..., 0.4530, 0.4598, 0.4443],
          [0.3775, 0.4761, 0.5271,  ..., 0.5699, 0.4695, 0.6834],
          [0.4659, 0.5218, 0.6397,  ..., 0.5068, 0.5675, 0.4140],
          [0.3979, 0.4841, 0.5146,  ..., 0.6496, 0.4591, 0.5780]],

         [[0.5120, 0.5217, 0.4201,  ..., 0.3914, 0.5137, 0.5184],
          [0.3775, 0.5574, 0.5029,  ..., 0.5424, 0.5223, 0.5431],
          [0.4378, 0.3798, 0.6442,  ..., 0.5370, 0.5884, 0.3504],
          [0.5151, 0.5121, 0.5078,  ..., 0.4385, 0.4980, 0.6379]]],


        [[[0.4957, 0.4220, 0.5722,  ..., 0.5115, 0.4882, 0.6252],
          [0.4436, 0.6415, 0.4733,  ..., 0.5248, 0.5660, 0.3854],
          [0.5679, 0.4734, 0.4390,  ..., 0.4750, 0.5513, 0.4557],
          [0.3504, 0.6361, 0.5813,  ..., 0.5837, 0.5528, 0.4484]],

         [[0.5756, 0.4301, 0.5453,  ..., 0.4659, 0.4022, 0.6053],
          [0.5761, 0.5765, 0.4695,  ..., 0.5074, 0.4826, 0.5129],
          [0.5086, 0.4297, 0.6252,  ..., 0.5799, 0.4012, 0.4292],
          [0.6809, 0.5063, 0.6792,  ..., 0.6118, 0.3905, 0.5141]],

         [[0.4891, 0.4989, 0.5521,  ..., 0.3522, 0.4158, 0.6234],
          [0.4316, 0.5072, 0.5813,  ..., 0.5346, 0.5722, 0.4969],
          [0.5508, 0.5235, 0.4383,  ..., 0.4479, 0.6105, 0.3877],
          [0.4092, 0.5846, 0.4876,  ..., 0.6179, 0.5188, 0.5387]],

         ...,

         [[0.5557, 0.5870, 0.5818,  ..., 0.4297, 0.5144, 0.3844],
          [0.5699, 0.5402, 0.4927,  ..., 0.5073, 0.6188, 0.4649],
          [0.6610, 0.5557, 0.4453,  ..., 0.4822, 0.4795, 0.5079],
          [0.4681, 0.5441, 0.4448,  ..., 0.5612, 0.4364, 0.4903]],

         [[0.5983, 0.4716, 0.4273,  ..., 0.4676, 0.5846, 0.4135],
          [0.6072, 0.6179, 0.5036,  ..., 0.5463, 0.5339, 0.3905],
          [0.5484, 0.4960, 0.4369,  ..., 0.5121, 0.4690, 0.4964],
          [0.3693, 0.4316, 0.5660,  ..., 0.5344, 0.4557, 0.4254]],

         [[0.6619, 0.5670, 0.6379,  ..., 0.4054, 0.5280, 0.3961],
          [0.3748, 0.5200, 0.5107,  ..., 0.5889, 0.5412, 0.3469],
          [0.6593, 0.4244, 0.6602,  ..., 0.4873, 0.4954, 0.3594],
          [0.5983, 0.5665, 0.4961,  ..., 0.6775, 0.4813, 0.6298]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([-0.0130, -0.0090, -0.0110,  0.0070, -0.0130,  0.0130, -0.0130, -0.0050,
         0.0130,  0.0130], device='cuda:0')
selected experts tensor([1599, 2022, 1319, 1770, 1734, 1222, 2478, 1322, 1654, 1264],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4825, 0.4595, 0.5959,  ..., 0.5310, 0.4045, 0.4769],
          [0.5379, 0.5516, 0.5533,  ..., 0.4516, 0.6281, 0.4143],
          [0.4718, 0.5095, 0.5445,  ..., 0.4751, 0.4343, 0.4980],
          [0.4729, 0.6663, 0.5749,  ..., 0.5402, 0.5191, 0.3965]],

         [[0.6454, 0.4200, 0.5157,  ..., 0.3901, 0.4504, 0.5576],
          [0.5834, 0.3800, 0.6295,  ..., 0.4822, 0.4518, 0.4371],
          [0.4626, 0.5792, 0.4816,  ..., 0.4395, 0.6267, 0.5644],
          [0.5478, 0.4924, 0.6067,  ..., 0.4774, 0.5409, 0.4605]],

         [[0.5709, 0.4938, 0.5179,  ..., 0.3956, 0.3169, 0.6698],
          [0.4701, 0.4593, 0.6048,  ..., 0.4668, 0.5706, 0.3878],
          [0.5027, 0.3583, 0.6001,  ..., 0.3381, 0.5574, 0.6063],
          [0.5100, 0.5530, 0.5378,  ..., 0.4424, 0.4928, 0.3947]],

         ...,

         [[0.5030, 0.5070, 0.5070,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5070, 0.5070,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5070, 0.5070,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5070, 0.5070,  ..., 0.5070, 0.5070, 0.5070]],

         [[0.5030, 0.5070, 0.5070,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5070, 0.5070,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5070, 0.5070,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5070, 0.5070,  ..., 0.5070, 0.5070, 0.5070]],

         [[0.5030, 0.5070, 0.5070,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5070, 0.5070,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5070, 0.5070,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5070, 0.5070,  ..., 0.5070, 0.5070, 0.5070]]],


        [[[0.4534, 0.4953, 0.4864,  ..., 0.4410, 0.3855, 0.5460],
          [0.4972, 0.3451, 0.5603,  ..., 0.5949, 0.4746, 0.5331],
          [0.4341, 0.4620, 0.4786,  ..., 0.5145, 0.4045, 0.4439],
          [0.5347, 0.4905, 0.5581,  ..., 0.4685, 0.5545, 0.3512]],

         [[0.5700, 0.5364, 0.5588,  ..., 0.4157, 0.4482, 0.5701],
          [0.6264, 0.5730, 0.5797,  ..., 0.3772, 0.5219, 0.4936],
          [0.5924, 0.5685, 0.5010,  ..., 0.4439, 0.4376, 0.4748],
          [0.4080, 0.5455, 0.5268,  ..., 0.4410, 0.5054, 0.4673]],

         [[0.4643, 0.4820, 0.4357,  ..., 0.4386, 0.4883, 0.3914],
          [0.4449, 0.3460, 0.6290,  ..., 0.5224, 0.4576, 0.5778],
          [0.4379, 0.4134, 0.3882,  ..., 0.4707, 0.4909, 0.5247],
          [0.5895, 0.5126, 0.6575,  ..., 0.4343, 0.5146, 0.3045]],

         ...,

         [[0.5030, 0.5070, 0.5070,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5070, 0.5070,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5070, 0.5070,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5070, 0.5070,  ..., 0.5070, 0.5070, 0.5070]],

         [[0.5030, 0.5070, 0.5070,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5070, 0.5070,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5070, 0.5070,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5070, 0.5070,  ..., 0.5070, 0.5070, 0.5070]],

         [[0.5030, 0.5070, 0.5070,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5070, 0.5070,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5070, 0.5070,  ..., 0.5070, 0.5070, 0.5070],
          [0.5030, 0.5070, 0.5070,  ..., 0.5070, 0.5070, 0.5070]]]],
       device='cuda:0')
tensor([[[[0.4795, 0.4525, 0.5889,  ..., 0.5240, 0.3975, 0.4699],
          [0.5349, 0.5446, 0.5463,  ..., 0.4446, 0.6211, 0.4073],
          [0.4688, 0.5025, 0.5375,  ..., 0.4681, 0.4273, 0.4910],
          [0.4699, 0.6593, 0.5679,  ..., 0.5332, 0.5121, 0.3895]],

         [[0.6424, 0.4130, 0.5087,  ..., 0.3831, 0.4434, 0.5506],
          [0.5804, 0.3730, 0.6225,  ..., 0.4752, 0.4448, 0.4301],
          [0.4596, 0.5722, 0.4746,  ..., 0.4325, 0.6197, 0.5574],
          [0.5448, 0.4854, 0.5997,  ..., 0.4704, 0.5339, 0.4535]],

         [[0.5679, 0.4868, 0.5109,  ..., 0.3886, 0.3099, 0.6628],
          [0.4671, 0.4523, 0.5978,  ..., 0.4598, 0.5636, 0.3808],
          [0.4997, 0.3513, 0.5931,  ..., 0.3311, 0.5504, 0.5993],
          [0.5070, 0.5460, 0.5308,  ..., 0.4354, 0.4858, 0.3877]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.4504, 0.4883, 0.4794,  ..., 0.4340, 0.3785, 0.5390],
          [0.4942, 0.3381, 0.5533,  ..., 0.5879, 0.4676, 0.5261],
          [0.4311, 0.4550, 0.4716,  ..., 0.5075, 0.3975, 0.4369],
          [0.5317, 0.4835, 0.5511,  ..., 0.4615, 0.5475, 0.3442]],

         [[0.5670, 0.5294, 0.5518,  ..., 0.4087, 0.4412, 0.5631],
          [0.6234, 0.5660, 0.5727,  ..., 0.3702, 0.5149, 0.4866],
          [0.5894, 0.5615, 0.4940,  ..., 0.4369, 0.4306, 0.4678],
          [0.4050, 0.5385, 0.5198,  ..., 0.4340, 0.4984, 0.4603]],

         [[0.4613, 0.4750, 0.4287,  ..., 0.4316, 0.4813, 0.3844],
          [0.4419, 0.3390, 0.6220,  ..., 0.5154, 0.4506, 0.5708],
          [0.4349, 0.4064, 0.3812,  ..., 0.4637, 0.4839, 0.5177],
          [0.5865, 0.5056, 0.6505,  ..., 0.4273, 0.5076, 0.2975]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0030,  0.0070,  0.0070,  0.0070, -0.0010,  0.0070,  0.0070,  0.0070,
         0.0070,  0.0070], device='cuda:0')
selected experts tensor([1335, 2853, 3337,  934, 2227, 1068,  988,  730, 1768, 1144],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1658., 1650., 1717., 1580., 1644., 1608., 1639., 1558., 1615., 1715.],
        [1335., 2853., 3337.,  934., 2227., 1068.,  988.,  730., 1768., 1144.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.3510, 0.5971, 0.4429,  ..., 0.7796, 0.5309, 0.5330],
          [0.5208, 0.5131, 0.4174,  ..., 0.5571, 0.6233, 0.2940],
          [0.5948, 0.4633, 0.5843,  ..., 0.5710, 0.6085, 0.4516],
          [0.5108, 0.5393, 0.5383,  ..., 0.2801, 0.4792, 0.5244]],

         [[0.5522, 0.4425, 0.6898,  ..., 0.4208, 0.2932, 0.3321],
          [0.4343, 0.5700, 0.4217,  ..., 0.6080, 0.5174, 0.4537],
          [0.4570, 0.7096, 0.4869,  ..., 0.5113, 0.5945, 0.4247],
          [0.4655, 0.4565, 0.6523,  ..., 0.4423, 0.4656, 0.4295]],

         [[0.4310, 0.6481, 0.4654,  ..., 0.6782, 0.5292, 0.4610],
          [0.4267, 0.4674, 0.3425,  ..., 0.5396, 0.4044, 0.4976],
          [0.4816, 0.4972, 0.6363,  ..., 0.4916, 0.5698, 0.4743],
          [0.5788, 0.5209, 0.5776,  ..., 0.5295, 0.3327, 0.4984]],

         ...,

         [[0.6528, 0.4056, 0.4581,  ..., 0.4171, 0.4656, 0.4243],
          [0.5036, 0.4466, 0.5904,  ..., 0.3716, 0.5356, 0.5098],
          [0.6295, 0.3350, 0.3641,  ..., 0.3581, 0.5247, 0.5358],
          [0.3010, 0.4317, 0.5652,  ..., 0.4823, 0.4915, 0.5034]],

         [[0.6528, 0.4056, 0.4581,  ..., 0.4171, 0.4656, 0.4243],
          [0.5036, 0.4466, 0.5904,  ..., 0.3716, 0.5356, 0.5098],
          [0.6295, 0.3350, 0.3641,  ..., 0.3581, 0.5247, 0.5358],
          [0.3010, 0.4317, 0.5652,  ..., 0.4823, 0.4915, 0.5034]],

         [[0.4010, 0.4207, 0.6523,  ..., 0.4171, 0.4291, 0.5325],
          [0.6448, 0.4420, 0.4497,  ..., 0.5600, 0.4997, 0.4910],
          [0.6728, 0.4136, 0.5033,  ..., 0.5873, 0.4988, 0.3901],
          [0.6745, 0.4674, 0.5790,  ..., 0.5772, 0.4713, 0.5276]]],


        [[[0.4575, 0.5198, 0.5170,  ..., 0.3908, 0.4239, 0.5912],
          [0.5503, 0.5781, 0.5276,  ..., 0.5123, 0.5102, 0.4661],
          [0.4660, 0.5251, 0.4208,  ..., 0.4242, 0.2529, 0.4576],
          [0.6023, 0.4089, 0.5368,  ..., 0.6474, 0.4862, 0.3989]],

         [[0.6528, 0.4056, 0.4581,  ..., 0.4171, 0.4656, 0.4243],
          [0.5036, 0.4466, 0.5904,  ..., 0.3716, 0.5356, 0.5098],
          [0.6295, 0.3350, 0.3641,  ..., 0.3581, 0.5247, 0.5358],
          [0.3010, 0.4317, 0.5652,  ..., 0.4823, 0.4915, 0.5034]],

         [[0.5783, 0.4667, 0.4963,  ..., 0.5340, 0.3965, 0.5004],
          [0.5131, 0.5853, 0.3742,  ..., 0.5196, 0.4769, 0.3619],
          [0.3796, 0.3633, 0.5535,  ..., 0.4600, 0.5077, 0.5440],
          [0.4209, 0.4066, 0.5890,  ..., 0.5447, 0.5034, 0.4966]],

         ...,

         [[0.4133, 0.6004, 0.4660,  ..., 0.4067, 0.5343, 0.5253],
          [0.5849, 0.5752, 0.4885,  ..., 0.5583, 0.5755, 0.5513],
          [0.4806, 0.5167, 0.3614,  ..., 0.4175, 0.5268, 0.4181],
          [0.5854, 0.5338, 0.5087,  ..., 0.5189, 0.4339, 0.5921]],

         [[0.6736, 0.6400, 0.4141,  ..., 0.5015, 0.3876, 0.4508],
          [0.5587, 0.6051, 0.4633,  ..., 0.4479, 0.5019, 0.5706],
          [0.4214, 0.3669, 0.5118,  ..., 0.4527, 0.4363, 0.5617],
          [0.5274, 0.4575, 0.5021,  ..., 0.5464, 0.3512, 0.6887]],

         [[0.5493, 0.4843, 0.5082,  ..., 0.5481, 0.5860, 0.7128],
          [0.5759, 0.5415, 0.4337,  ..., 0.5058, 0.3415, 0.5788],
          [0.4902, 0.5606, 0.4846,  ..., 0.5566, 0.4210, 0.5644],
          [0.2509, 0.6107, 0.5217,  ..., 0.3890, 0.4258, 0.3878]]]],
       device='cuda:0')
tensor([[[[0.3540, 0.5941, 0.4499,  ..., 0.7746, 0.5319, 0.5260],
          [0.5238, 0.5101, 0.4244,  ..., 0.5521, 0.6243, 0.2870],
          [0.5978, 0.4603, 0.5913,  ..., 0.5660, 0.6095, 0.4446],
          [0.5138, 0.5363, 0.5453,  ..., 0.2751, 0.4802, 0.5174]],

         [[0.5552, 0.4395, 0.6968,  ..., 0.4158, 0.2942, 0.3251],
          [0.4373, 0.5670, 0.4287,  ..., 0.6030, 0.5184, 0.4467],
          [0.4600, 0.7066, 0.4939,  ..., 0.5063, 0.5955, 0.4177],
          [0.4685, 0.4535, 0.6593,  ..., 0.4373, 0.4666, 0.4225]],

         [[0.4340, 0.6451, 0.4724,  ..., 0.6732, 0.5302, 0.4540],
          [0.4297, 0.4644, 0.3495,  ..., 0.5346, 0.4054, 0.4906],
          [0.4846, 0.4942, 0.6433,  ..., 0.4866, 0.5708, 0.4673],
          [0.5818, 0.5179, 0.5846,  ..., 0.5245, 0.3337, 0.4914]],

         ...,

         [[0.6558, 0.4026, 0.4651,  ..., 0.4121, 0.4666, 0.4173],
          [0.5066, 0.4436, 0.5974,  ..., 0.3666, 0.5366, 0.5028],
          [0.6325, 0.3320, 0.3711,  ..., 0.3531, 0.5257, 0.5288],
          [0.3040, 0.4287, 0.5722,  ..., 0.4773, 0.4925, 0.4964]],

         [[0.6558, 0.4026, 0.4651,  ..., 0.4121, 0.4666, 0.4173],
          [0.5066, 0.4436, 0.5974,  ..., 0.3666, 0.5366, 0.5028],
          [0.6325, 0.3320, 0.3711,  ..., 0.3531, 0.5257, 0.5288],
          [0.3040, 0.4287, 0.5722,  ..., 0.4773, 0.4925, 0.4964]],

         [[0.4040, 0.4177, 0.6593,  ..., 0.4121, 0.4301, 0.5255],
          [0.6478, 0.4390, 0.4567,  ..., 0.5550, 0.5007, 0.4840],
          [0.6758, 0.4106, 0.5103,  ..., 0.5823, 0.4998, 0.3831],
          [0.6775, 0.4644, 0.5860,  ..., 0.5722, 0.4723, 0.5206]]],


        [[[0.4605, 0.5168, 0.5240,  ..., 0.3858, 0.4249, 0.5842],
          [0.5533, 0.5751, 0.5346,  ..., 0.5073, 0.5112, 0.4591],
          [0.4690, 0.5221, 0.4278,  ..., 0.4192, 0.2539, 0.4506],
          [0.6053, 0.4059, 0.5438,  ..., 0.6424, 0.4872, 0.3919]],

         [[0.6558, 0.4026, 0.4651,  ..., 0.4121, 0.4666, 0.4173],
          [0.5066, 0.4436, 0.5974,  ..., 0.3666, 0.5366, 0.5028],
          [0.6325, 0.3320, 0.3711,  ..., 0.3531, 0.5257, 0.5288],
          [0.3040, 0.4287, 0.5722,  ..., 0.4773, 0.4925, 0.4964]],

         [[0.5813, 0.4637, 0.5033,  ..., 0.5290, 0.3975, 0.4934],
          [0.5161, 0.5823, 0.3812,  ..., 0.5146, 0.4779, 0.3549],
          [0.3826, 0.3603, 0.5605,  ..., 0.4550, 0.5087, 0.5370],
          [0.4239, 0.4036, 0.5960,  ..., 0.5397, 0.5044, 0.4896]],

         ...,

         [[0.4163, 0.5974, 0.4730,  ..., 0.4017, 0.5353, 0.5183],
          [0.5879, 0.5722, 0.4955,  ..., 0.5533, 0.5765, 0.5443],
          [0.4836, 0.5137, 0.3684,  ..., 0.4125, 0.5278, 0.4111],
          [0.5884, 0.5308, 0.5157,  ..., 0.5139, 0.4349, 0.5851]],

         [[0.6766, 0.6370, 0.4211,  ..., 0.4965, 0.3886, 0.4438],
          [0.5617, 0.6021, 0.4703,  ..., 0.4429, 0.5029, 0.5636],
          [0.4244, 0.3639, 0.5188,  ..., 0.4477, 0.4373, 0.5547],
          [0.5304, 0.4545, 0.5091,  ..., 0.5414, 0.3522, 0.6817]],

         [[0.5523, 0.4813, 0.5152,  ..., 0.5431, 0.5870, 0.7058],
          [0.5789, 0.5385, 0.4407,  ..., 0.5008, 0.3425, 0.5718],
          [0.4932, 0.5576, 0.4916,  ..., 0.5516, 0.4220, 0.5574],
          [0.2539, 0.6077, 0.5287,  ..., 0.3840, 0.4268, 0.3808]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0030,  0.0030, -0.0070, -0.0010, -0.0050, -0.0050,  0.0050,  0.0050,
        -0.0010,  0.0070], device='cuda:0')
selected experts tensor([1716, 1725, 1585, 1649, 1626, 1572, 1750, 1654, 1490, 1617],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4743, 0.5567, 0.5534,  ..., 0.6000, 0.4052, 0.5260],
          [0.4707, 0.5123, 0.5274,  ..., 0.6633, 0.6499, 0.4235],
          [0.4648, 0.3410, 0.3008,  ..., 0.6093, 0.4375, 0.4027],
          [0.5621, 0.3392, 0.3224,  ..., 0.6821, 0.5229, 0.5197]],

         [[0.4696, 0.4396, 0.3867,  ..., 0.3127, 0.5388, 0.4055],
          [0.3448, 0.4541, 0.5744,  ..., 0.5095, 0.4819, 0.4888],
          [0.5370, 0.4575, 0.5372,  ..., 0.5394, 0.4288, 0.3971],
          [0.5171, 0.4814, 0.4505,  ..., 0.3430, 0.4198, 0.4740]],

         [[0.4853, 0.4728, 0.4912,  ..., 0.4420, 0.3870, 0.6007],
          [0.6056, 0.5396, 0.5638,  ..., 0.6112, 0.3842, 0.4391],
          [0.4147, 0.5279, 0.4029,  ..., 0.5282, 0.6241, 0.3151],
          [0.5285, 0.3915, 0.5095,  ..., 0.4348, 0.5357, 0.3859]],

         ...,

         [[0.3700, 0.4984, 0.3689,  ..., 0.4057, 0.5230, 0.5576],
          [0.4920, 0.5867, 0.4648,  ..., 0.3273, 0.4056, 0.5545],
          [0.5365, 0.4237, 0.4455,  ..., 0.5433, 0.4974, 0.5087],
          [0.5977, 0.4611, 0.5420,  ..., 0.4219, 0.3787, 0.4555]],

         [[0.3700, 0.4984, 0.3689,  ..., 0.4057, 0.5230, 0.5576],
          [0.4920, 0.5867, 0.4648,  ..., 0.3273, 0.4056, 0.5545],
          [0.5365, 0.4237, 0.4455,  ..., 0.5433, 0.4974, 0.5087],
          [0.5977, 0.4611, 0.5420,  ..., 0.4219, 0.3787, 0.4555]],

         [[0.4981, 0.4915, 0.4663,  ..., 0.6149, 0.4490, 0.4615],
          [0.4276, 0.5692, 0.4152,  ..., 0.3764, 0.5251, 0.4900],
          [0.4190, 0.5386, 0.5696,  ..., 0.5309, 0.4360, 0.6353],
          [0.4585, 0.4923, 0.5669,  ..., 0.4351, 0.3930, 0.4448]]],


        [[[0.4777, 0.4213, 0.4612,  ..., 0.4848, 0.4312, 0.5785],
          [0.3255, 0.5810, 0.4261,  ..., 0.5466, 0.4784, 0.5637],
          [0.6457, 0.5190, 0.4821,  ..., 0.5287, 0.4563, 0.3703],
          [0.5389, 0.4760, 0.6357,  ..., 0.3996, 0.3238, 0.4460]],

         [[0.3700, 0.4984, 0.3689,  ..., 0.4057, 0.5230, 0.5576],
          [0.4920, 0.5867, 0.4648,  ..., 0.3273, 0.4056, 0.5545],
          [0.5365, 0.4237, 0.4455,  ..., 0.5433, 0.4974, 0.5087],
          [0.5977, 0.4611, 0.5420,  ..., 0.4219, 0.3787, 0.4555]],

         [[0.5106, 0.5031, 0.3716,  ..., 0.4020, 0.5022, 0.5852],
          [0.4238, 0.3481, 0.4462,  ..., 0.6190, 0.4831, 0.4965],
          [0.4629, 0.5475, 0.4866,  ..., 0.6014, 0.4616, 0.4939],
          [0.6112, 0.6266, 0.4811,  ..., 0.6736, 0.6658, 0.6152]],

         ...,

         [[0.4243, 0.4618, 0.4280,  ..., 0.3618, 0.5724, 0.4846],
          [0.5934, 0.4208, 0.5963,  ..., 0.4462, 0.6328, 0.5366],
          [0.4736, 0.4642, 0.5207,  ..., 0.5104, 0.5040, 0.4880],
          [0.3870, 0.4582, 0.4844,  ..., 0.5553, 0.5383, 0.5214]],

         [[0.3778, 0.4740, 0.3115,  ..., 0.5042, 0.4033, 0.2928],
          [0.5174, 0.5034, 0.6411,  ..., 0.4495, 0.4751, 0.4994],
          [0.5015, 0.6050, 0.4701,  ..., 0.4870, 0.3255, 0.3905],
          [0.5357, 0.6036, 0.5614,  ..., 0.5995, 0.5990, 0.5332]],

         [[0.3745, 0.5269, 0.5126,  ..., 0.6563, 0.4113, 0.5145],
          [0.4999, 0.3906, 0.3853,  ..., 0.5099, 0.5867, 0.5771],
          [0.4898, 0.3976, 0.4767,  ..., 0.5560, 0.4626, 0.5970],
          [0.5231, 0.5815, 0.5296,  ..., 0.4679, 0.4604, 0.5837]]]],
       device='cuda:0')
tensor([[[[0.4773, 0.5617, 0.5484,  ..., 0.6030, 0.4022, 0.5250],
          [0.4737, 0.5173, 0.5224,  ..., 0.6663, 0.6469, 0.4225],
          [0.4678, 0.3460, 0.2958,  ..., 0.6123, 0.4345, 0.4017],
          [0.5651, 0.3442, 0.3174,  ..., 0.6851, 0.5199, 0.5187]],

         [[0.4726, 0.4446, 0.3817,  ..., 0.3157, 0.5358, 0.4045],
          [0.3478, 0.4591, 0.5694,  ..., 0.5125, 0.4789, 0.4878],
          [0.5400, 0.4625, 0.5322,  ..., 0.5424, 0.4258, 0.3961],
          [0.5201, 0.4864, 0.4455,  ..., 0.3460, 0.4168, 0.4730]],

         [[0.4883, 0.4778, 0.4862,  ..., 0.4450, 0.3840, 0.5997],
          [0.6086, 0.5446, 0.5588,  ..., 0.6142, 0.3812, 0.4381],
          [0.4177, 0.5329, 0.3979,  ..., 0.5312, 0.6211, 0.3141],
          [0.5315, 0.3965, 0.5045,  ..., 0.4378, 0.5327, 0.3849]],

         ...,

         [[0.3730, 0.5034, 0.3639,  ..., 0.4087, 0.5200, 0.5566],
          [0.4950, 0.5917, 0.4598,  ..., 0.3303, 0.4026, 0.5535],
          [0.5395, 0.4287, 0.4405,  ..., 0.5463, 0.4944, 0.5077],
          [0.6007, 0.4661, 0.5370,  ..., 0.4249, 0.3757, 0.4545]],

         [[0.3730, 0.5034, 0.3639,  ..., 0.4087, 0.5200, 0.5566],
          [0.4950, 0.5917, 0.4598,  ..., 0.3303, 0.4026, 0.5535],
          [0.5395, 0.4287, 0.4405,  ..., 0.5463, 0.4944, 0.5077],
          [0.6007, 0.4661, 0.5370,  ..., 0.4249, 0.3757, 0.4545]],

         [[0.5011, 0.4965, 0.4613,  ..., 0.6179, 0.4460, 0.4605],
          [0.4306, 0.5742, 0.4102,  ..., 0.3794, 0.5221, 0.4890],
          [0.4220, 0.5436, 0.5646,  ..., 0.5339, 0.4330, 0.6343],
          [0.4615, 0.4973, 0.5619,  ..., 0.4381, 0.3900, 0.4438]]],


        [[[0.4807, 0.4263, 0.4562,  ..., 0.4878, 0.4282, 0.5775],
          [0.3285, 0.5860, 0.4211,  ..., 0.5496, 0.4754, 0.5627],
          [0.6487, 0.5240, 0.4771,  ..., 0.5317, 0.4533, 0.3693],
          [0.5419, 0.4810, 0.6307,  ..., 0.4026, 0.3208, 0.4450]],

         [[0.3730, 0.5034, 0.3639,  ..., 0.4087, 0.5200, 0.5566],
          [0.4950, 0.5917, 0.4598,  ..., 0.3303, 0.4026, 0.5535],
          [0.5395, 0.4287, 0.4405,  ..., 0.5463, 0.4944, 0.5077],
          [0.6007, 0.4661, 0.5370,  ..., 0.4249, 0.3757, 0.4545]],

         [[0.5136, 0.5081, 0.3666,  ..., 0.4050, 0.4992, 0.5842],
          [0.4268, 0.3531, 0.4412,  ..., 0.6220, 0.4801, 0.4955],
          [0.4659, 0.5525, 0.4816,  ..., 0.6044, 0.4586, 0.4929],
          [0.6142, 0.6316, 0.4761,  ..., 0.6766, 0.6628, 0.6142]],

         ...,

         [[0.4273, 0.4668, 0.4230,  ..., 0.3648, 0.5694, 0.4836],
          [0.5964, 0.4258, 0.5913,  ..., 0.4492, 0.6298, 0.5356],
          [0.4766, 0.4692, 0.5157,  ..., 0.5134, 0.5010, 0.4870],
          [0.3900, 0.4632, 0.4794,  ..., 0.5583, 0.5353, 0.5204]],

         [[0.3808, 0.4790, 0.3065,  ..., 0.5072, 0.4003, 0.2918],
          [0.5204, 0.5084, 0.6361,  ..., 0.4525, 0.4721, 0.4984],
          [0.5045, 0.6100, 0.4651,  ..., 0.4900, 0.3225, 0.3895],
          [0.5387, 0.6086, 0.5564,  ..., 0.6025, 0.5960, 0.5322]],

         [[0.3775, 0.5319, 0.5076,  ..., 0.6593, 0.4083, 0.5135],
          [0.5029, 0.3956, 0.3803,  ..., 0.5129, 0.5837, 0.5761],
          [0.4928, 0.4026, 0.4717,  ..., 0.5590, 0.4596, 0.5960],
          [0.5261, 0.5865, 0.5246,  ..., 0.4709, 0.4574, 0.5827]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0030, -0.0050,  0.0050,  0.0110, -0.0030,  0.0090, -0.0110, -0.0030,
         0.0030,  0.0010], device='cuda:0')
selected experts tensor([1624, 1627, 1653, 1806, 1519, 1584, 1636, 1707, 1580, 1648],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5181, 0.4399, 0.4588,  ..., 0.5080, 0.4039, 0.6184],
          [0.5318, 0.6901, 0.3593,  ..., 0.5854, 0.4151, 0.5176],
          [0.4971, 0.4486, 0.6297,  ..., 0.5606, 0.4964, 0.3635],
          [0.6848, 0.5070, 0.5490,  ..., 0.5182, 0.5017, 0.5967]],

         [[0.5131, 0.5004, 0.4372,  ..., 0.5725, 0.5592, 0.5702],
          [0.3537, 0.5522, 0.4578,  ..., 0.5924, 0.5478, 0.5024],
          [0.5788, 0.5239, 0.5099,  ..., 0.4444, 0.5495, 0.3340],
          [0.4072, 0.5183, 0.5846,  ..., 0.4541, 0.4959, 0.6254]],

         [[0.4362, 0.5603, 0.6014,  ..., 0.5242, 0.4398, 0.5227],
          [0.5024, 0.5827, 0.5817,  ..., 0.5567, 0.5285, 0.4509],
          [0.5031, 0.4163, 0.4942,  ..., 0.5270, 0.4723, 0.4370],
          [0.5750, 0.4843, 0.6641,  ..., 0.5432, 0.4090, 0.7033]],

         ...,

         [[0.5531, 0.5084, 0.5074,  ..., 0.4362, 0.6084, 0.5233],
          [0.3916, 0.5195, 0.5053,  ..., 0.6384, 0.5442, 0.5699],
          [0.2814, 0.4790, 0.5869,  ..., 0.4408, 0.4636, 0.4049],
          [0.5559, 0.3969, 0.5831,  ..., 0.5877, 0.4151, 0.5437]],

         [[0.5764, 0.3749, 0.5522,  ..., 0.4848, 0.5819, 0.5740],
          [0.4483, 0.5531, 0.5675,  ..., 0.5500, 0.5177, 0.5277],
          [0.3080, 0.4240, 0.5161,  ..., 0.5816, 0.4750, 0.3565],
          [0.4861, 0.4880, 0.5746,  ..., 0.5835, 0.4621, 0.3870]],

         [[0.5764, 0.4182, 0.5038,  ..., 0.5702, 0.5131, 0.5996],
          [0.3897, 0.4879, 0.6422,  ..., 0.5117, 0.5957, 0.4658],
          [0.3474, 0.4149, 0.5874,  ..., 0.6456, 0.5476, 0.4709],
          [0.4253, 0.5062, 0.3657,  ..., 0.4965, 0.4809, 0.4974]]],


        [[[0.4882, 0.4596, 0.5560,  ..., 0.3745, 0.4988, 0.3618],
          [0.5062, 0.5661, 0.4493,  ..., 0.5692, 0.3955, 0.4552],
          [0.6689, 0.4755, 0.4365,  ..., 0.3581, 0.5924, 0.5120],
          [0.6008, 0.4984, 0.4913,  ..., 0.5098, 0.4484, 0.6342]],

         [[0.5263, 0.5746, 0.5029,  ..., 0.4536, 0.5370, 0.5053],
          [0.5147, 0.6414, 0.4634,  ..., 0.4853, 0.3969, 0.3770],
          [0.4914, 0.4785, 0.6502,  ..., 0.4052, 0.4364, 0.4227],
          [0.7066, 0.4341, 0.4851,  ..., 0.5579, 0.5814, 0.4653]],

         [[0.5877, 0.4068, 0.5575,  ..., 0.4118, 0.4933, 0.4133],
          [0.5607, 0.5187, 0.5897,  ..., 0.5668, 0.3440, 0.3417],
          [0.6259, 0.3926, 0.5163,  ..., 0.3851, 0.6183, 0.5243],
          [0.6105, 0.4120, 0.5784,  ..., 0.4628, 0.5529, 0.5658]],

         ...,

         [[0.6205, 0.4527, 0.5227,  ..., 0.5749, 0.3606, 0.5634],
          [0.5219, 0.5101, 0.4990,  ..., 0.5560, 0.6131, 0.5144],
          [0.5480, 0.4035, 0.5076,  ..., 0.4553, 0.4412, 0.5364],
          [0.2920, 0.5270, 0.6449,  ..., 0.5251, 0.4583, 0.6447]],

         [[0.5531, 0.4073, 0.4120,  ..., 0.5844, 0.4426, 0.7190],
          [0.4738, 0.5850, 0.5589,  ..., 0.6212, 0.5766, 0.6249],
          [0.5364, 0.3879, 0.5093,  ..., 0.5497, 0.3768, 0.5437],
          [0.4522, 0.6297, 0.5462,  ..., 0.5045, 0.4774, 0.4091]],

         [[0.4792, 0.4782, 0.5384,  ..., 0.6116, 0.3859, 0.6645],
          [0.5162, 0.5977, 0.4641,  ..., 0.3837, 0.5612, 0.5278],
          [0.4076, 0.4476, 0.4310,  ..., 0.4942, 0.5382, 0.3860],
          [0.4124, 0.5002, 0.4568,  ..., 0.3545, 0.5123, 0.4336]]]],
       device='cuda:0')
tensor([[[[0.5301, 0.4499, 0.4688,  ..., 0.5120, 0.3919, 0.6044],
          [0.5438, 0.7001, 0.3693,  ..., 0.5894, 0.4031, 0.5036],
          [0.5091, 0.4586, 0.6397,  ..., 0.5646, 0.4844, 0.3495],
          [0.6968, 0.5170, 0.5590,  ..., 0.5222, 0.4897, 0.5827]],

         [[0.5251, 0.5104, 0.4472,  ..., 0.5765, 0.5472, 0.5562],
          [0.3657, 0.5622, 0.4678,  ..., 0.5964, 0.5358, 0.4884],
          [0.5908, 0.5339, 0.5199,  ..., 0.4484, 0.5375, 0.3200],
          [0.4192, 0.5283, 0.5946,  ..., 0.4581, 0.4839, 0.6114]],

         [[0.4482, 0.5703, 0.6114,  ..., 0.5282, 0.4278, 0.5087],
          [0.5144, 0.5927, 0.5917,  ..., 0.5607, 0.5165, 0.4369],
          [0.5151, 0.4263, 0.5042,  ..., 0.5310, 0.4603, 0.4230],
          [0.5870, 0.4943, 0.6741,  ..., 0.5472, 0.3970, 0.6893]],

         ...,

         [[0.5651, 0.5184, 0.5174,  ..., 0.4402, 0.5964, 0.5093],
          [0.4036, 0.5295, 0.5153,  ..., 0.6424, 0.5322, 0.5559],
          [0.2934, 0.4890, 0.5969,  ..., 0.4448, 0.4516, 0.3909],
          [0.5679, 0.4069, 0.5931,  ..., 0.5917, 0.4031, 0.5297]],

         [[0.5884, 0.3849, 0.5622,  ..., 0.4888, 0.5699, 0.5600],
          [0.4603, 0.5631, 0.5775,  ..., 0.5540, 0.5057, 0.5137],
          [0.3200, 0.4340, 0.5261,  ..., 0.5856, 0.4630, 0.3425],
          [0.4981, 0.4980, 0.5846,  ..., 0.5875, 0.4501, 0.3730]],

         [[0.5884, 0.4282, 0.5138,  ..., 0.5742, 0.5011, 0.5856],
          [0.4017, 0.4979, 0.6522,  ..., 0.5157, 0.5837, 0.4518],
          [0.3594, 0.4249, 0.5974,  ..., 0.6496, 0.5356, 0.4569],
          [0.4373, 0.5162, 0.3757,  ..., 0.5005, 0.4689, 0.4834]]],


        [[[0.5002, 0.4696, 0.5660,  ..., 0.3785, 0.4868, 0.3478],
          [0.5182, 0.5761, 0.4593,  ..., 0.5732, 0.3835, 0.4412],
          [0.6809, 0.4855, 0.4465,  ..., 0.3621, 0.5804, 0.4980],
          [0.6128, 0.5084, 0.5013,  ..., 0.5138, 0.4364, 0.6202]],

         [[0.5383, 0.5846, 0.5129,  ..., 0.4576, 0.5250, 0.4913],
          [0.5267, 0.6514, 0.4734,  ..., 0.4893, 0.3849, 0.3630],
          [0.5034, 0.4885, 0.6602,  ..., 0.4092, 0.4244, 0.4087],
          [0.7186, 0.4441, 0.4951,  ..., 0.5619, 0.5694, 0.4513]],

         [[0.5997, 0.4168, 0.5675,  ..., 0.4158, 0.4813, 0.3993],
          [0.5727, 0.5287, 0.5997,  ..., 0.5708, 0.3320, 0.3277],
          [0.6379, 0.4026, 0.5263,  ..., 0.3891, 0.6063, 0.5103],
          [0.6225, 0.4220, 0.5884,  ..., 0.4668, 0.5409, 0.5518]],

         ...,

         [[0.6325, 0.4627, 0.5327,  ..., 0.5789, 0.3486, 0.5494],
          [0.5339, 0.5201, 0.5090,  ..., 0.5600, 0.6011, 0.5004],
          [0.5600, 0.4135, 0.5176,  ..., 0.4593, 0.4292, 0.5224],
          [0.3040, 0.5370, 0.6549,  ..., 0.5291, 0.4463, 0.6307]],

         [[0.5651, 0.4173, 0.4220,  ..., 0.5884, 0.4306, 0.7050],
          [0.4858, 0.5950, 0.5689,  ..., 0.6252, 0.5646, 0.6109],
          [0.5484, 0.3979, 0.5193,  ..., 0.5537, 0.3648, 0.5297],
          [0.4642, 0.6397, 0.5562,  ..., 0.5085, 0.4654, 0.3951]],

         [[0.4912, 0.4882, 0.5484,  ..., 0.6156, 0.3739, 0.6505],
          [0.5282, 0.6077, 0.4741,  ..., 0.3877, 0.5492, 0.5138],
          [0.4196, 0.4576, 0.4410,  ..., 0.4982, 0.5262, 0.3720],
          [0.4244, 0.5102, 0.4668,  ..., 0.3585, 0.5003, 0.4196]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0120, -0.0100, -0.0100,  0.0060, -0.0140,  0.0140, -0.0140, -0.0040,
         0.0120,  0.0140], device='cuda:0')
selected experts tensor([1650, 1843, 1572, 1659, 1430, 1118, 2216, 1783, 1313, 1800],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6135, 0.5170, 0.4999,  ..., 0.5513, 0.5498, 0.5122],
          [0.5671, 0.4342, 0.3450,  ..., 0.5807, 0.5067, 0.4751],
          [0.4645, 0.5382, 0.4152,  ..., 0.6054, 0.3997, 0.4891],
          [0.4962, 0.4733, 0.5821,  ..., 0.3602, 0.4952, 0.3855]],

         [[0.5739, 0.5754, 0.4171,  ..., 0.3513, 0.4162, 0.7081],
          [0.5815, 0.6024, 0.4129,  ..., 0.4296, 0.4237, 0.5087],
          [0.5427, 0.6071, 0.5939,  ..., 0.5323, 0.6618, 0.5095],
          [0.4294, 0.5035, 0.5610,  ..., 0.5035, 0.4997, 0.3417]],

         [[0.5055, 0.4866, 0.5230,  ..., 0.5526, 0.4728, 0.6073],
          [0.5430, 0.4559, 0.4568,  ..., 0.2785, 0.5127, 0.5670],
          [0.5001, 0.5231, 0.5040,  ..., 0.4315, 0.5213, 0.4518],
          [0.5376, 0.5080, 0.4752,  ..., 0.4659, 0.5443, 0.3575]],

         ...,

         [[0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5080],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5080],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5080],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5080]],

         [[0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5080],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5080],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5080],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5080]],

         [[0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5080],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5080],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5080],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5080]]],


        [[[0.4534, 0.4309, 0.5035,  ..., 0.3145, 0.3937, 0.5731],
          [0.4165, 0.3251, 0.4766,  ..., 0.4523, 0.5159, 0.4859],
          [0.4505, 0.6071, 0.5501,  ..., 0.4272, 0.4709, 0.3828],
          [0.5227, 0.4506, 0.5778,  ..., 0.6889, 0.5159, 0.5056]],

         [[0.5295, 0.4809, 0.4724,  ..., 0.4642, 0.5253, 0.5740],
          [0.4421, 0.5091, 0.4894,  ..., 0.3452, 0.5844, 0.4485],
          [0.5995, 0.4704, 0.5330,  ..., 0.4092, 0.3735, 0.4952],
          [0.4696, 0.5281, 0.5316,  ..., 0.5988, 0.4501, 0.4444]],

         [[0.4619, 0.6330, 0.4789,  ..., 0.3322, 0.4233, 0.5365],
          [0.4689, 0.4200, 0.3900,  ..., 0.4003, 0.5122, 0.6369],
          [0.5891, 0.4983, 0.4518,  ..., 0.5387, 0.5269, 0.4880],
          [0.6089, 0.5501, 0.4825,  ..., 0.4540, 0.5782, 0.4858]],

         ...,

         [[0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5080],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5080],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5080],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5080]],

         [[0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5080],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5080],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5080],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5080]],

         [[0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5080],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5080],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5080],
          [0.5040, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5080]]]],
       device='cuda:0')/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([[[[0.6095, 0.5110, 0.4939,  ..., 0.5433, 0.5438, 0.5042],
          [0.5631, 0.4282, 0.3390,  ..., 0.5727, 0.5007, 0.4671],
          [0.4605, 0.5322, 0.4092,  ..., 0.5974, 0.3937, 0.4811],
          [0.4922, 0.4673, 0.5761,  ..., 0.3522, 0.4892, 0.3775]],

         [[0.5699, 0.5694, 0.4111,  ..., 0.3433, 0.4102, 0.7001],
          [0.5775, 0.5964, 0.4069,  ..., 0.4216, 0.4177, 0.5007],
          [0.5387, 0.6011, 0.5879,  ..., 0.5243, 0.6558, 0.5015],
          [0.4254, 0.4975, 0.5550,  ..., 0.4955, 0.4937, 0.3337]],

         [[0.5015, 0.4806, 0.5170,  ..., 0.5446, 0.4668, 0.5993],
          [0.5390, 0.4499, 0.4508,  ..., 0.2705, 0.5067, 0.5590],
          [0.4961, 0.5171, 0.4980,  ..., 0.4235, 0.5153, 0.4438],
          [0.5336, 0.5020, 0.4692,  ..., 0.4579, 0.5383, 0.3495]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.4494, 0.4249, 0.4975,  ..., 0.3065, 0.3877, 0.5651],
          [0.4125, 0.3191, 0.4706,  ..., 0.4443, 0.5099, 0.4779],
          [0.4465, 0.6011, 0.5441,  ..., 0.4192, 0.4649, 0.3748],
          [0.5187, 0.4446, 0.5718,  ..., 0.6809, 0.5099, 0.4976]],

         [[0.5255, 0.4749, 0.4664,  ..., 0.4562, 0.5193, 0.5660],
          [0.4381, 0.5031, 0.4834,  ..., 0.3372, 0.5784, 0.4405],
          [0.5955, 0.4644, 0.5270,  ..., 0.4012, 0.3675, 0.4872],
          [0.4656, 0.5221, 0.5256,  ..., 0.5908, 0.4441, 0.4364]],

         [[0.4579, 0.6270, 0.4729,  ..., 0.3242, 0.4173, 0.5285],
          [0.4649, 0.4140, 0.3840,  ..., 0.3923, 0.5062, 0.6289],
          [0.5851, 0.4923, 0.4458,  ..., 0.5307, 0.5209, 0.4800],
          [0.6049, 0.5441, 0.4765,  ..., 0.4460, 0.5722, 0.4778]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0040,  0.0060,  0.0060,  0.0080, -0.0020,  0.0080,  0.0080,  0.0080,
         0.0060,  0.0080], device='cuda:0')
selected experts tensor([1685, 1515, 1633, 2248, 2242, 1982, 1082,  785, 1626, 1586],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1624., 1627., 1653., 1806., 1519., 1584., 1636., 1707., 1580., 1648.],
        [1685., 1515., 1633., 2248., 2242., 1982., 1082.,  785., 1626., 1586.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6180, 0.6674, 0.3260,  ..., 0.3977, 0.6169, 0.3865],
          [0.5639, 0.5180, 0.4921,  ..., 0.5205, 0.5651, 0.4215],
          [0.4014, 0.4032, 0.4794,  ..., 0.2966, 0.4535, 0.5235],
          [0.4435, 0.4413, 0.5046,  ..., 0.3917, 0.4064, 0.5032]],

         [[0.5990, 0.3288, 0.4555,  ..., 0.6037, 0.4354, 0.5526],
          [0.5019, 0.6336, 0.4908,  ..., 0.5667, 0.5557, 0.5632],
          [0.3527, 0.4841, 0.5222,  ..., 0.4633, 0.4254, 0.3989],
          [0.3902, 0.4716, 0.5291,  ..., 0.6061, 0.4393, 0.5019]],

         [[0.3500, 0.5258, 0.4628,  ..., 0.4987, 0.4988, 0.4177],
          [0.5976, 0.5311, 0.4256,  ..., 0.5367, 0.4144, 0.4625],
          [0.3635, 0.6226, 0.4398,  ..., 0.4175, 0.4187, 0.4884],
          [0.5985, 0.5598, 0.4766,  ..., 0.6925, 0.3886, 0.5436]],

         ...,

         [[0.6357, 0.3445, 0.5139,  ..., 0.4909, 0.5770, 0.3994],
          [0.4895, 0.5785, 0.4852,  ..., 0.5461, 0.5015, 0.5029],
          [0.5228, 0.5321, 0.4616,  ..., 0.3257, 0.3684, 0.4970],
          [0.4333, 0.4618, 0.3994,  ..., 0.6763, 0.5665, 0.5666]],

         [[0.3635, 0.4997, 0.6533,  ..., 0.3991, 0.6114, 0.5270],
          [0.4396, 0.4562, 0.5259,  ..., 0.4256, 0.4651, 0.4858],
          [0.5625, 0.6036, 0.3752,  ..., 0.4346, 0.5636, 0.4272],
          [0.5296, 0.3340, 0.4735,  ..., 0.4194, 0.4982, 0.4913]],

         [[0.4497, 0.4976, 0.3929,  ..., 0.6000, 0.6058, 0.5347],
          [0.3828, 0.4226, 0.4516,  ..., 0.4370, 0.6202, 0.5368],
          [0.4309, 0.4321, 0.4222,  ..., 0.5948, 0.4311, 0.6468],
          [0.3608, 0.5771, 0.6603,  ..., 0.4452, 0.5069, 0.4881]]],


        [[[0.6570, 0.4691, 0.4997,  ..., 0.6000, 0.5033, 0.4528],
          [0.3653, 0.5218, 0.5629,  ..., 0.4094, 0.5751, 0.4267],
          [0.4384, 0.5606, 0.4347,  ..., 0.4763, 0.4777, 0.4854],
          [0.6088, 0.4865, 0.3225,  ..., 0.5197, 0.4182, 0.4215]],

         [[0.5668, 0.4936, 0.4555,  ..., 0.5858, 0.5387, 0.6077],
          [0.4915, 0.5656, 0.5195,  ..., 0.6840, 0.4644, 0.5458],
          [0.4309, 0.5184, 0.5610,  ..., 0.4616, 0.4927, 0.5601],
          [0.4641, 0.5098, 0.4918,  ..., 0.6018, 0.4140, 0.5692]],

         [[0.6321, 0.5695, 0.5753,  ..., 0.5331, 0.3989, 0.5140],
          [0.5094, 0.4967, 0.5838,  ..., 0.4604, 0.5155, 0.4663],
          [0.3545, 0.5473, 0.4679,  ..., 0.5981, 0.3831, 0.5345],
          [0.5061, 0.5704, 0.5758,  ..., 0.5432, 0.5732, 0.4988]],

         ...,

         [[0.6116, 0.4966, 0.6054,  ..., 0.3482, 0.4258, 0.5424],
          [0.4257, 0.4905, 0.5600,  ..., 0.4873, 0.4373, 0.4377],
          [0.4095, 0.3305, 0.4270,  ..., 0.4459, 0.5301, 0.4906],
          [0.5184, 0.4941, 0.3822,  ..., 0.4104, 0.4441, 0.4848]],

         [[0.4560, 0.5188, 0.5180,  ..., 0.3894, 0.4249, 0.5917],
          [0.5493, 0.5766, 0.5286,  ..., 0.5113, 0.5112, 0.4671],
          [0.4650, 0.5241, 0.4218,  ..., 0.4227, 0.2539, 0.4586],
          [0.6013, 0.4079, 0.5378,  ..., 0.6455, 0.4871, 0.3999]],

         [[0.5948, 0.4533, 0.7312,  ..., 0.6554, 0.6039, 0.5257],
          [0.5995, 0.4577, 0.3217,  ..., 0.5882, 0.5576, 0.6655],
          [0.5249, 0.6199, 0.5412,  ..., 0.4791, 0.5494, 0.3443],
          [0.3846, 0.4903, 0.6229,  ..., 0.5824, 0.4424, 0.4410]]]],
       device='cuda:0')
tensor([[[[0.6220, 0.6654, 0.3320,  ..., 0.3937, 0.6169, 0.3785],
          [0.5679, 0.5160, 0.4981,  ..., 0.5165, 0.5651, 0.4135],
          [0.4054, 0.4012, 0.4854,  ..., 0.2926, 0.4535, 0.5155],
          [0.4475, 0.4393, 0.5106,  ..., 0.3877, 0.4064, 0.4952]],

         [[0.6030, 0.3268, 0.4615,  ..., 0.5997, 0.4354, 0.5446],
          [0.5059, 0.6316, 0.4968,  ..., 0.5627, 0.5557, 0.5552],
          [0.3567, 0.4821, 0.5282,  ..., 0.4593, 0.4254, 0.3909],
          [0.3942, 0.4696, 0.5351,  ..., 0.6021, 0.4393, 0.4939]],

         [[0.3540, 0.5238, 0.4688,  ..., 0.4947, 0.4988, 0.4097],
          [0.6016, 0.5291, 0.4316,  ..., 0.5327, 0.4144, 0.4545],
          [0.3675, 0.6206, 0.4458,  ..., 0.4135, 0.4187, 0.4804],
          [0.6025, 0.5578, 0.4826,  ..., 0.6885, 0.3886, 0.5356]],

         ...,

         [[0.6397, 0.3425, 0.5199,  ..., 0.4869, 0.5770, 0.3914],
          [0.4935, 0.5765, 0.4912,  ..., 0.5421, 0.5015, 0.4949],
          [0.5268, 0.5301, 0.4676,  ..., 0.3217, 0.3684, 0.4890],
          [0.4373, 0.4598, 0.4054,  ..., 0.6723, 0.5665, 0.5586]],

         [[0.3675, 0.4977, 0.6593,  ..., 0.3951, 0.6114, 0.5190],
          [0.4436, 0.4542, 0.5319,  ..., 0.4216, 0.4651, 0.4778],
          [0.5665, 0.6016, 0.3812,  ..., 0.4306, 0.5636, 0.4192],
          [0.5336, 0.3320, 0.4795,  ..., 0.4154, 0.4982, 0.4833]],

         [[0.4537, 0.4956, 0.3989,  ..., 0.5960, 0.6058, 0.5267],
          [0.3868, 0.4206, 0.4576,  ..., 0.4330, 0.6202, 0.5288],
          [0.4349, 0.4301, 0.4282,  ..., 0.5908, 0.4311, 0.6388],
          [0.3648, 0.5751, 0.6663,  ..., 0.4412, 0.5069, 0.4801]]],


        [[[0.6610, 0.4671, 0.5057,  ..., 0.5960, 0.5033, 0.4448],
          [0.3693, 0.5198, 0.5689,  ..., 0.4054, 0.5751, 0.4187],
          [0.4424, 0.5586, 0.4407,  ..., 0.4723, 0.4777, 0.4774],
          [0.6128, 0.4845, 0.3285,  ..., 0.5157, 0.4182, 0.4135]],

         [[0.5708, 0.4916, 0.4615,  ..., 0.5818, 0.5387, 0.5997],
          [0.4955, 0.5636, 0.5255,  ..., 0.6800, 0.4644, 0.5378],
          [0.4349, 0.5164, 0.5670,  ..., 0.4576, 0.4927, 0.5521],
          [0.4681, 0.5078, 0.4978,  ..., 0.5978, 0.4140, 0.5612]],

         [[0.6361, 0.5675, 0.5813,  ..., 0.5291, 0.3989, 0.5060],
          [0.5134, 0.4947, 0.5898,  ..., 0.4564, 0.5155, 0.4583],
          [0.3585, 0.5453, 0.4739,  ..., 0.5941, 0.3831, 0.5265],
          [0.5101, 0.5684, 0.5818,  ..., 0.5392, 0.5732, 0.4908]],

         ...,

         [[0.6156, 0.4946, 0.6114,  ..., 0.3442, 0.4258, 0.5344],
          [0.4297, 0.4885, 0.5660,  ..., 0.4833, 0.4373, 0.4297],
          [0.4135, 0.3285, 0.4330,  ..., 0.4419, 0.5301, 0.4826],
          [0.5224, 0.4921, 0.3882,  ..., 0.4064, 0.4441, 0.4768]],

         [[0.4600, 0.5168, 0.5240,  ..., 0.3854, 0.4249, 0.5837],
          [0.5533, 0.5746, 0.5346,  ..., 0.5073, 0.5112, 0.4591],
          [0.4690, 0.5221, 0.4278,  ..., 0.4187, 0.2539, 0.4506],
          [0.6053, 0.4059, 0.5438,  ..., 0.6415, 0.4871, 0.3919]],

         [[0.5988, 0.4513, 0.7372,  ..., 0.6514, 0.6039, 0.5177],
          [0.6035, 0.4557, 0.3277,  ..., 0.5842, 0.5576, 0.6575],
          [0.5289, 0.6179, 0.5472,  ..., 0.4751, 0.5494, 0.3363],
          [0.3886, 0.4883, 0.6289,  ..., 0.5784, 0.4424, 0.4330]]]],
       device='cuda:0', requires_grad=True)
tensor([-4.0000e-03,  2.0000e-03, -6.0000e-03, -2.0000e-03, -4.0000e-03,
        -4.0000e-03,  4.0000e-03,  4.0000e-03, -2.3283e-10,  8.0000e-03],
       device='cuda:0')
selected experts tensor([1584, 1647, 1619, 1638, 1677, 1636, 1649, 1608, 1586, 1740],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5621, 0.7347, 0.3206,  ..., 0.4692, 0.3325, 0.4961],
          [0.5204, 0.3438, 0.4611,  ..., 0.5081, 0.4798, 0.4625],
          [0.4916, 0.4551, 0.4777,  ..., 0.5142, 0.4800, 0.5574],
          [0.6617, 0.3563, 0.5386,  ..., 0.6069, 0.5037, 0.5489]],

         [[0.4315, 0.4920, 0.5839,  ..., 0.4276, 0.6242, 0.5713],
          [0.6377, 0.5630, 0.3616,  ..., 0.4454, 0.4696, 0.5499],
          [0.3987, 0.3967, 0.4337,  ..., 0.3581, 0.4894, 0.4821],
          [0.7030, 0.4636, 0.5401,  ..., 0.6009, 0.6798, 0.2991]],

         [[0.5051, 0.4033, 0.5275,  ..., 0.4295, 0.5519, 0.4455],
          [0.6260, 0.4329, 0.5413,  ..., 0.4488, 0.5045, 0.6270],
          [0.3737, 0.4199, 0.3456,  ..., 0.6631, 0.5563, 0.4711],
          [0.4053, 0.6579, 0.4184,  ..., 0.4161, 0.5303, 0.5689]],

         ...,

         [[0.7078, 0.6055, 0.7024,  ..., 0.3563, 0.2699, 0.4854],
          [0.5013, 0.4488, 0.4774,  ..., 0.5730, 0.6098, 0.4868],
          [0.5726, 0.4531, 0.4862,  ..., 0.5111, 0.4687, 0.4196],
          [0.5926, 0.4921, 0.3871,  ..., 0.6092, 0.4752, 0.5240]],

         [[0.5452, 0.5401, 0.4607,  ..., 0.5116, 0.4923, 0.5804],
          [0.5493, 0.5357, 0.4802,  ..., 0.3662, 0.7130, 0.4706],
          [0.6865, 0.6212, 0.3797,  ..., 0.4851, 0.4580, 0.5557],
          [0.5174, 0.4081, 0.4813,  ..., 0.6069, 0.5114, 0.4918]],

         [[0.4573, 0.4767, 0.5652,  ..., 0.5086, 0.5369, 0.5799],
          [0.4933, 0.4066, 0.4337,  ..., 0.4536, 0.4718, 0.4477],
          [0.4962, 0.4305, 0.5256,  ..., 0.5021, 0.6916, 0.5227],
          [0.4493, 0.4573, 0.5981,  ..., 0.6553, 0.6500, 0.4838]]],


        [[[0.5292, 0.6194, 0.3325,  ..., 0.4228, 0.4356, 0.6202],
          [0.4532, 0.3438, 0.4909,  ..., 0.5121, 0.4512, 0.4916],
          [0.5878, 0.4066, 0.4010,  ..., 0.4485, 0.5020, 0.4287],
          [0.3583, 0.6605, 0.3945,  ..., 0.3944, 0.6117, 0.5332]],

         [[0.5607, 0.5635, 0.5292,  ..., 0.4977, 0.4930, 0.5722],
          [0.6136, 0.4748, 0.6117,  ..., 0.5673, 0.6685, 0.5271],
          [0.3538, 0.4430, 0.3866,  ..., 0.3879, 0.5301, 0.5974],
          [0.4949, 0.6120, 0.4198,  ..., 0.5802, 0.5739, 0.4737]],

         [[0.5258, 0.5625, 0.4912,  ..., 0.5256, 0.5134, 0.4496],
          [0.5089, 0.5413, 0.5934,  ..., 0.4904, 0.5609, 0.3942],
          [0.4086, 0.5783, 0.5743,  ..., 0.4444, 0.5691, 0.4790],
          [0.3682, 0.4510, 0.5691,  ..., 0.5032, 0.4507, 0.5419]],

         ...,

         [[0.5702, 0.5345, 0.5211,  ..., 0.5522, 0.4005, 0.5627],
          [0.4785, 0.6285, 0.5051,  ..., 0.4645, 0.5971, 0.4664],
          [0.3601, 0.5754, 0.4906,  ..., 0.5296, 0.5447, 0.5336],
          [0.5239, 0.6027, 0.5934,  ..., 0.4636, 0.4747, 0.4521]],

         [[0.4786, 0.4218, 0.4604,  ..., 0.4839, 0.4322, 0.5775],
          [0.3265, 0.5820, 0.4251,  ..., 0.5456, 0.4792, 0.5622],
          [0.6467, 0.5202, 0.4812,  ..., 0.5279, 0.4570, 0.3693],
          [0.5399, 0.4768, 0.6347,  ..., 0.3991, 0.3248, 0.4450]],

         [[0.4498, 0.3939, 0.3317,  ..., 0.3916, 0.3231, 0.4235],
          [0.5585, 0.4807, 0.5063,  ..., 0.3818, 0.4510, 0.4130],
          [0.5930, 0.5171, 0.5427,  ..., 0.5820, 0.5352, 0.6137],
          [0.5042, 0.3823, 0.5488,  ..., 0.5350, 0.3571, 0.6077]]]],
       device='cuda:0')
tensor([[[[0.5641, 0.7387, 0.3166,  ..., 0.4732, 0.3285, 0.4961],
          [0.5224, 0.3478, 0.4571,  ..., 0.5121, 0.4758, 0.4625],
          [0.4936, 0.4591, 0.4737,  ..., 0.5182, 0.4760, 0.5574],
          [0.6637, 0.3603, 0.5346,  ..., 0.6109, 0.4997, 0.5489]],

         [[0.4335, 0.4960, 0.5799,  ..., 0.4316, 0.6202, 0.5713],
          [0.6397, 0.5670, 0.3576,  ..., 0.4494, 0.4656, 0.5499],
          [0.4007, 0.4007, 0.4297,  ..., 0.3621, 0.4854, 0.4821],
          [0.7050, 0.4676, 0.5361,  ..., 0.6049, 0.6758, 0.2991]],

         [[0.5071, 0.4073, 0.5235,  ..., 0.4335, 0.5479, 0.4455],
          [0.6280, 0.4369, 0.5373,  ..., 0.4528, 0.5005, 0.6270],
          [0.3757, 0.4239, 0.3416,  ..., 0.6671, 0.5523, 0.4711],
          [0.4073, 0.6619, 0.4144,  ..., 0.4201, 0.5263, 0.5689]],

         ...,

         [[0.7098, 0.6095, 0.6984,  ..., 0.3603, 0.2659, 0.4854],
          [0.5033, 0.4528, 0.4734,  ..., 0.5770, 0.6058, 0.4868],
          [0.5746, 0.4571, 0.4822,  ..., 0.5151, 0.4647, 0.4196],
          [0.5946, 0.4961, 0.3831,  ..., 0.6132, 0.4712, 0.5240]],

         [[0.5472, 0.5441, 0.4567,  ..., 0.5156, 0.4883, 0.5804],
          [0.5513, 0.5397, 0.4762,  ..., 0.3702, 0.7090, 0.4706],
          [0.6885, 0.6252, 0.3757,  ..., 0.4891, 0.4540, 0.5557],
          [0.5194, 0.4121, 0.4773,  ..., 0.6109, 0.5074, 0.4918]],

         [[0.4593, 0.4807, 0.5612,  ..., 0.5126, 0.5329, 0.5799],
          [0.4953, 0.4106, 0.4297,  ..., 0.4576, 0.4678, 0.4477],
          [0.4982, 0.4345, 0.5216,  ..., 0.5061, 0.6876, 0.5227],
          [0.4513, 0.4613, 0.5941,  ..., 0.6593, 0.6460, 0.4838]]],


        [[[0.5312, 0.6234, 0.3285,  ..., 0.4268, 0.4316, 0.6202],
          [0.4552, 0.3478, 0.4869,  ..., 0.5161, 0.4472, 0.4916],
          [0.5898, 0.4106, 0.3970,  ..., 0.4525, 0.4980, 0.4287],
          [0.3603, 0.6645, 0.3905,  ..., 0.3984, 0.6077, 0.5332]],

         [[0.5627, 0.5675, 0.5252,  ..., 0.5017, 0.4890, 0.5722],
          [0.6156, 0.4788, 0.6077,  ..., 0.5713, 0.6645, 0.5271],
          [0.3558, 0.4470, 0.3826,  ..., 0.3919, 0.5261, 0.5974],
          [0.4969, 0.6160, 0.4158,  ..., 0.5842, 0.5699, 0.4737]],

         [[0.5278, 0.5665, 0.4872,  ..., 0.5296, 0.5094, 0.4496],
          [0.5109, 0.5453, 0.5894,  ..., 0.4944, 0.5569, 0.3942],
          [0.4106, 0.5823, 0.5703,  ..., 0.4484, 0.5651, 0.4790],
          [0.3702, 0.4550, 0.5651,  ..., 0.5072, 0.4467, 0.5419]],

         ...,

         [[0.5722, 0.5385, 0.5171,  ..., 0.5562, 0.3965, 0.5627],
          [0.4805, 0.6325, 0.5011,  ..., 0.4685, 0.5931, 0.4664],
          [0.3621, 0.5794, 0.4866,  ..., 0.5336, 0.5407, 0.5336],
          [0.5259, 0.6067, 0.5894,  ..., 0.4676, 0.4707, 0.4521]],

         [[0.4806, 0.4258, 0.4564,  ..., 0.4879, 0.4282, 0.5775],
          [0.3285, 0.5860, 0.4211,  ..., 0.5496, 0.4752, 0.5622],
          [0.6487, 0.5242, 0.4772,  ..., 0.5319, 0.4530, 0.3693],
          [0.5419, 0.4808, 0.6307,  ..., 0.4031, 0.3208, 0.4450]],

         [[0.4518, 0.3979, 0.3277,  ..., 0.3956, 0.3191, 0.4235],
          [0.5605, 0.4847, 0.5023,  ..., 0.3858, 0.4470, 0.4130],
          [0.5950, 0.5211, 0.5387,  ..., 0.5860, 0.5312, 0.6137],
          [0.5062, 0.3863, 0.5448,  ..., 0.5390, 0.3531, 0.6077]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0020, -0.0040,  0.0040,  0.0100, -0.0020,  0.0100, -0.0100, -0.0040,
         0.0040,  0.0000], device='cuda:0')
selected experts tensor([1624, 1661, 1690, 1612, 1510, 1706, 1609, 1652, 1638, 1682],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5923, 0.5107, 0.5609,  ..., 0.3981, 0.6137, 0.4913],
          [0.4320, 0.4307, 0.6019,  ..., 0.4868, 0.5002, 0.4511],
          [0.5678, 0.6188, 0.5234,  ..., 0.4887, 0.6137, 0.5600],
          [0.3600, 0.5401, 0.5874,  ..., 0.5572, 0.5496, 0.6661]],

         [[0.5716, 0.6124, 0.5785,  ..., 0.5504, 0.4998, 0.6028],
          [0.5122, 0.5593, 0.3291,  ..., 0.4974, 0.4559, 0.5091],
          [0.5678, 0.5369, 0.5546,  ..., 0.6045, 0.3652, 0.3616],
          [0.6392, 0.6350, 0.6084,  ..., 0.6835, 0.5289, 0.4760]],

         [[0.5787, 0.5209, 0.5363,  ..., 0.4696, 0.4270, 0.6193],
          [0.4969, 0.5943, 0.4520,  ..., 0.6934, 0.4870, 0.5273],
          [0.5048, 0.3935, 0.4561,  ..., 0.4314, 0.5319, 0.3364],
          [0.6780, 0.5708, 0.4460,  ..., 0.5663, 0.4564, 0.4355]],

         ...,

         [[0.5621, 0.6395, 0.5623,  ..., 0.4849, 0.5348, 0.5585],
          [0.3164, 0.4910, 0.5162,  ..., 0.4550, 0.4641, 0.4431],
          [0.3545, 0.5046, 0.5317,  ..., 0.4897, 0.5368, 0.5250],
          [0.4255, 0.5915, 0.4616,  ..., 0.6142, 0.4403, 0.4663]],

         [[0.5683, 0.5774, 0.4259,  ..., 0.4904, 0.6964, 0.5852],
          [0.5480, 0.3973, 0.6042,  ..., 0.6374, 0.4436, 0.5417],
          [0.4405, 0.4706, 0.4846,  ..., 0.4601, 0.4850, 0.4607],
          [0.6085, 0.5641, 0.5500,  ..., 0.4463, 0.3751, 0.4829]],

         [[0.5154, 0.4662, 0.5742,  ..., 0.4999, 0.5266, 0.6336],
          [0.4119, 0.5646, 0.4991,  ..., 0.5540, 0.5169, 0.5602],
          [0.5248, 0.3098, 0.6459,  ..., 0.5211, 0.5761, 0.3984],
          [0.4780, 0.5560, 0.4574,  ..., 0.5010, 0.4331, 0.4588]]],


        [[[0.5077, 0.6457, 0.4416,  ..., 0.4280, 0.6253, 0.5537],
          [0.4531, 0.6179, 0.4884,  ..., 0.6673, 0.5240, 0.4236],
          [0.5427, 0.5911, 0.6503,  ..., 0.4132, 0.3528, 0.4251],
          [0.5900, 0.5326, 0.5016,  ..., 0.5787, 0.4580, 0.5260]],

         [[0.4296, 0.5971, 0.5232,  ..., 0.4766, 0.4166, 0.5211],
          [0.4133, 0.5971, 0.3833,  ..., 0.6239, 0.4288, 0.3988],
          [0.4877, 0.4191, 0.5865,  ..., 0.4185, 0.5356, 0.5766],
          [0.6258, 0.4544, 0.4527,  ..., 0.5777, 0.4815, 0.3742]],

         [[0.5573, 0.4833, 0.4435,  ..., 0.3735, 0.5124, 0.5476],
          [0.4219, 0.6430, 0.4305,  ..., 0.5601, 0.5843, 0.2610],
          [0.6062, 0.5135, 0.5237,  ..., 0.4437, 0.5563, 0.5153],
          [0.5101, 0.3193, 0.6253,  ..., 0.5068, 0.4012, 0.5886]],

         ...,

         [[0.5103, 0.5343, 0.4630,  ..., 0.4889, 0.4819, 0.5986],
          [0.5545, 0.5497, 0.4617,  ..., 0.4587, 0.5694, 0.4882],
          [0.5248, 0.5641, 0.5438,  ..., 0.4151, 0.4730, 0.4436],
          [0.5364, 0.3385, 0.5268,  ..., 0.4765, 0.3947, 0.4912]],

         [[0.4573, 0.4788, 0.5385,  ..., 0.4113, 0.5378, 0.4465],
          [0.4081, 0.6064, 0.5261,  ..., 0.5133, 0.5790, 0.6085],
          [0.5182, 0.3359, 0.5026,  ..., 0.4538, 0.4907, 0.4868],
          [0.4971, 0.5765, 0.6226,  ..., 0.5182, 0.4236, 0.6066]],

         [[0.4686, 0.3790, 0.5780,  ..., 0.5138, 0.5426, 0.6127],
          [0.5146, 0.4020, 0.4885,  ..., 0.5362, 0.4336, 0.4384],
          [0.4598, 0.4225, 0.5968,  ..., 0.5891, 0.6391, 0.4208],
          [0.6044, 0.5490, 0.4387,  ..., 0.5299, 0.4100, 0.4617]]]],
       device='cuda:0')
tensor([[[[0.6053, 0.5217, 0.5699,  ..., 0.4031, 0.6007, 0.4783],
          [0.4450, 0.4417, 0.6109,  ..., 0.4918, 0.4872, 0.4381],
          [0.5808, 0.6298, 0.5324,  ..., 0.4937, 0.6007, 0.5470],
          [0.3730, 0.5511, 0.5964,  ..., 0.5622, 0.5366, 0.6531]],

         [[0.5846, 0.6234, 0.5875,  ..., 0.5554, 0.4868, 0.5898],
          [0.5252, 0.5703, 0.3381,  ..., 0.5024, 0.4429, 0.4961],
          [0.5808, 0.5479, 0.5636,  ..., 0.6095, 0.3522, 0.3486],
          [0.6522, 0.6460, 0.6174,  ..., 0.6885, 0.5159, 0.4630]],

         [[0.5917, 0.5319, 0.5453,  ..., 0.4746, 0.4140, 0.6063],
          [0.5099, 0.6053, 0.4610,  ..., 0.6984, 0.4740, 0.5143],
          [0.5178, 0.4045, 0.4651,  ..., 0.4364, 0.5189, 0.3234],
          [0.6910, 0.5818, 0.4550,  ..., 0.5713, 0.4434, 0.4225]],

         ...,

         [[0.5751, 0.6505, 0.5713,  ..., 0.4899, 0.5218, 0.5455],
          [0.3294, 0.5020, 0.5252,  ..., 0.4600, 0.4511, 0.4301],
          [0.3675, 0.5156, 0.5407,  ..., 0.4947, 0.5238, 0.5120],
          [0.4385, 0.6025, 0.4706,  ..., 0.6192, 0.4273, 0.4533]],

         [[0.5813, 0.5884, 0.4349,  ..., 0.4954, 0.6834, 0.5722],
          [0.5610, 0.4083, 0.6132,  ..., 0.6424, 0.4306, 0.5287],
          [0.4535, 0.4816, 0.4936,  ..., 0.4651, 0.4720, 0.4477],
          [0.6215, 0.5751, 0.5590,  ..., 0.4513, 0.3621, 0.4699]],

         [[0.5284, 0.4772, 0.5832,  ..., 0.5049, 0.5136, 0.6206],
          [0.4249, 0.5756, 0.5081,  ..., 0.5590, 0.5039, 0.5472],
          [0.5378, 0.3208, 0.6549,  ..., 0.5261, 0.5631, 0.3854],
          [0.4910, 0.5670, 0.4664,  ..., 0.5060, 0.4201, 0.4458]]],


        [[[0.5207, 0.6567, 0.4506,  ..., 0.4330, 0.6123, 0.5407],
          [0.4661, 0.6289, 0.4974,  ..., 0.6723, 0.5110, 0.4106],
          [0.5557, 0.6021, 0.6593,  ..., 0.4182, 0.3398, 0.4121],
          [0.6030, 0.5436, 0.5106,  ..., 0.5837, 0.4450, 0.5130]],

         [[0.4426, 0.6081, 0.5322,  ..., 0.4816, 0.4036, 0.5081],
          [0.4263, 0.6081, 0.3923,  ..., 0.6289, 0.4158, 0.3858],
          [0.5007, 0.4301, 0.5955,  ..., 0.4235, 0.5226, 0.5636],
          [0.6388, 0.4654, 0.4617,  ..., 0.5827, 0.4685, 0.3612]],

         [[0.5703, 0.4943, 0.4525,  ..., 0.3785, 0.4994, 0.5346],
          [0.4349, 0.6540, 0.4395,  ..., 0.5651, 0.5713, 0.2480],
          [0.6192, 0.5245, 0.5327,  ..., 0.4487, 0.5433, 0.5023],
          [0.5231, 0.3303, 0.6343,  ..., 0.5118, 0.3882, 0.5756]],

         ...,

         [[0.5233, 0.5453, 0.4720,  ..., 0.4939, 0.4689, 0.5856],
          [0.5675, 0.5607, 0.4707,  ..., 0.4637, 0.5564, 0.4752],
          [0.5378, 0.5751, 0.5528,  ..., 0.4201, 0.4600, 0.4306],
          [0.5494, 0.3495, 0.5358,  ..., 0.4815, 0.3817, 0.4782]],

         [[0.4703, 0.4898, 0.5475,  ..., 0.4163, 0.5248, 0.4335],
          [0.4211, 0.6174, 0.5351,  ..., 0.5183, 0.5660, 0.5955],
          [0.5312, 0.3469, 0.5116,  ..., 0.4588, 0.4777, 0.4738],
          [0.5101, 0.5875, 0.6316,  ..., 0.5232, 0.4106, 0.5936]],

         [[0.4816, 0.3900, 0.5870,  ..., 0.5188, 0.5296, 0.5997],
          [0.5276, 0.4130, 0.4975,  ..., 0.5412, 0.4206, 0.4254],
          [0.4728, 0.4335, 0.6058,  ..., 0.5941, 0.6261, 0.4078],
          [0.6174, 0.5600, 0.4477,  ..., 0.5349, 0.3970, 0.4487]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0130, -0.0110, -0.0090,  0.0050, -0.0130,  0.0150, -0.0150, -0.0050,
         0.0130,  0.0130], device='cuda:0')
selected experts tensor([1776, 1677, 1442, 1974, 1687, 1115, 2330, 1620, 1537, 1226],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4784, 0.4482, 0.5716,  ..., 0.3856, 0.3736, 0.4449],
          [0.3367, 0.4878, 0.6147,  ..., 0.4919, 0.3910, 0.4244],
          [0.4943, 0.6313, 0.5620,  ..., 0.4459, 0.5788, 0.5277],
          [0.4241, 0.4964, 0.5648,  ..., 0.5218, 0.6011, 0.4023]],

         [[0.6552, 0.5375, 0.5087,  ..., 0.4940, 0.4338, 0.5591],
          [0.5242, 0.5436, 0.5138,  ..., 0.3576, 0.5983, 0.3995],
          [0.5473, 0.5489, 0.5788,  ..., 0.3541, 0.4167, 0.4468],
          [0.4529, 0.6494, 0.5217,  ..., 0.5625, 0.5549, 0.4973]],

         [[0.5320, 0.6539, 0.3896,  ..., 0.4306, 0.5930, 0.3953],
          [0.3651, 0.5759, 0.4824,  ..., 0.4771, 0.4612, 0.5244],
          [0.5582, 0.4854, 0.5025,  ..., 0.5125, 0.4668, 0.6148],
          [0.4279, 0.5316, 0.4593,  ..., 0.5808, 0.6067, 0.3410]],

         ...,

         [[0.5030, 0.5070, 0.5070,  ..., 0.5090, 0.5070, 0.5090],
          [0.5030, 0.5070, 0.5070,  ..., 0.5090, 0.5070, 0.5090],
          [0.5030, 0.5070, 0.5070,  ..., 0.5090, 0.5070, 0.5090],
          [0.5030, 0.5070, 0.5070,  ..., 0.5090, 0.5070, 0.5090]],

         [[0.5030, 0.5070, 0.5070,  ..., 0.5090, 0.5070, 0.5090],
          [0.5030, 0.5070, 0.5070,  ..., 0.5090, 0.5070, 0.5090],
          [0.5030, 0.5070, 0.5070,  ..., 0.5090, 0.5070, 0.5090],
          [0.5030, 0.5070, 0.5070,  ..., 0.5090, 0.5070, 0.5090]],

         [[0.5030, 0.5070, 0.5070,  ..., 0.5090, 0.5070, 0.5090],
          [0.5030, 0.5070, 0.5070,  ..., 0.5090, 0.5070, 0.5090],
          [0.5030, 0.5070, 0.5070,  ..., 0.5090, 0.5070, 0.5090],
          [0.5030, 0.5070, 0.5070,  ..., 0.5090, 0.5070, 0.5090]]],


        [[[0.4911, 0.4954, 0.4205,  ..., 0.5214, 0.3836, 0.3930],
          [0.4056, 0.4707, 0.5788,  ..., 0.4880, 0.5557, 0.6045],
          [0.5881, 0.4386, 0.5326,  ..., 0.5903, 0.6262, 0.6460],
          [0.5237, 0.5130, 0.5202,  ..., 0.5274, 0.6044, 0.3865]],

         [[0.5534, 0.4763, 0.3882,  ..., 0.3765, 0.5369, 0.4754],
          [0.5270, 0.4381, 0.4726,  ..., 0.5236, 0.6226, 0.4864],
          [0.6701, 0.4494, 0.4831,  ..., 0.5006, 0.4419, 0.6370],
          [0.5243, 0.5198, 0.4059,  ..., 0.5647, 0.7046, 0.3995]],

         [[0.4113, 0.5241, 0.4424,  ..., 0.3568, 0.3548, 0.5257],
          [0.4471, 0.5930, 0.5133,  ..., 0.3948, 0.3682, 0.4140],
          [0.5738, 0.5831, 0.5651,  ..., 0.3948, 0.3407, 0.5091],
          [0.4633, 0.5778, 0.4651,  ..., 0.4886, 0.5303, 0.3657]],

         ...,

         [[0.5030, 0.5070, 0.5070,  ..., 0.5090, 0.5070, 0.5090],
          [0.5030, 0.5070, 0.5070,  ..., 0.5090, 0.5070, 0.5090],
          [0.5030, 0.5070, 0.5070,  ..., 0.5090, 0.5070, 0.5090],
          [0.5030, 0.5070, 0.5070,  ..., 0.5090, 0.5070, 0.5090]],

         [[0.5030, 0.5070, 0.5070,  ..., 0.5090, 0.5070, 0.5090],
          [0.5030, 0.5070, 0.5070,  ..., 0.5090, 0.5070, 0.5090],
          [0.5030, 0.5070, 0.5070,  ..., 0.5090, 0.5070, 0.5090],
          [0.5030, 0.5070, 0.5070,  ..., 0.5090, 0.5070, 0.5090]],

         [[0.5030, 0.5070, 0.5070,  ..., 0.5090, 0.5070, 0.5090],
          [0.5030, 0.5070, 0.5070,  ..., 0.5090, 0.5070, 0.5090],
          [0.5030, 0.5070, 0.5070,  ..., 0.5090, 0.5070, 0.5090],
          [0.5030, 0.5070, 0.5070,  ..., 0.5090, 0.5070, 0.5090]]]],
       device='cuda:0')
tensor([[[[0.4754, 0.4412, 0.5646,  ..., 0.3766, 0.3666, 0.4359],
          [0.3337, 0.4808, 0.6077,  ..., 0.4829, 0.3840, 0.4154],
          [0.4913, 0.6243, 0.5550,  ..., 0.4369, 0.5718, 0.5187],
          [0.4211, 0.4894, 0.5578,  ..., 0.5128, 0.5941, 0.3933]],

         [[0.6522, 0.5305, 0.5017,  ..., 0.4850, 0.4268, 0.5501],
          [0.5212, 0.5366, 0.5068,  ..., 0.3486, 0.5913, 0.3905],
          [0.5443, 0.5419, 0.5718,  ..., 0.3451, 0.4097, 0.4378],
          [0.4499, 0.6424, 0.5147,  ..., 0.5535, 0.5479, 0.4883]],

         [[0.5290, 0.6469, 0.3826,  ..., 0.4216, 0.5860, 0.3863],
          [0.3621, 0.5689, 0.4754,  ..., 0.4681, 0.4542, 0.5154],
          [0.5552, 0.4784, 0.4955,  ..., 0.5035, 0.4598, 0.6058],
          [0.4249, 0.5246, 0.4523,  ..., 0.5718, 0.5997, 0.3320]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.4881, 0.4884, 0.4135,  ..., 0.5124, 0.3766, 0.3840],
          [0.4026, 0.4637, 0.5718,  ..., 0.4790, 0.5487, 0.5955],
          [0.5851, 0.4316, 0.5256,  ..., 0.5813, 0.6192, 0.6370],
          [0.5207, 0.5060, 0.5132,  ..., 0.5184, 0.5974, 0.3775]],

         [[0.5504, 0.4693, 0.3812,  ..., 0.3675, 0.5299, 0.4664],
          [0.5240, 0.4311, 0.4656,  ..., 0.5146, 0.6156, 0.4774],
          [0.6671, 0.4424, 0.4761,  ..., 0.4916, 0.4349, 0.6280],
          [0.5213, 0.5128, 0.3989,  ..., 0.5557, 0.6976, 0.3905]],

         [[0.4083, 0.5171, 0.4354,  ..., 0.3478, 0.3478, 0.5167],
          [0.4441, 0.5860, 0.5063,  ..., 0.3858, 0.3612, 0.4050],
          [0.5708, 0.5761, 0.5581,  ..., 0.3858, 0.3337, 0.5001],
          [0.4603, 0.5708, 0.4581,  ..., 0.4796, 0.5233, 0.3567]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([ 0.0030,  0.0070,  0.0070,  0.0070, -0.0030,  0.0070,  0.0090,  0.0090,
         0.0070,  0.0090], device='cuda:0')
selected experts tensor([1793, 1655, 1759,  944, 2395,  740, 2195, 1734, 1841, 1328],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1624., 1661., 1690., 1612., 1510., 1706., 1609., 1652., 1638., 1682.],
        [1793., 1655., 1759.,  944., 2395.,  740., 2195., 1734., 1841., 1328.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6367, 0.3435, 0.5146,  ..., 0.4918, 0.5780, 0.3984],
          [0.4905, 0.5771, 0.4863,  ..., 0.5469, 0.5025, 0.5018],
          [0.5237, 0.5311, 0.4626,  ..., 0.3267, 0.3694, 0.4959],
          [0.4343, 0.4608, 0.4004,  ..., 0.6773, 0.5675, 0.5656]],

         [[0.6770, 0.6532, 0.3962,  ..., 0.4600, 0.5946, 0.4639],
          [0.4281, 0.4415, 0.4396,  ..., 0.5076, 0.5713, 0.4134],
          [0.4067, 0.5918, 0.4517,  ..., 0.3725, 0.4613, 0.5730],
          [0.4257, 0.5856, 0.4398,  ..., 0.6302, 0.4434, 0.4003]],

         [[0.6126, 0.4956, 0.6064,  ..., 0.3492, 0.4268, 0.5411],
          [0.4267, 0.4896, 0.5610,  ..., 0.4882, 0.4383, 0.4367],
          [0.4105, 0.3295, 0.4280,  ..., 0.4469, 0.5311, 0.4896],
          [0.5194, 0.4932, 0.3832,  ..., 0.4114, 0.4451, 0.4839]],

         ...,

         [[0.6545, 0.4032, 0.4597,  ..., 0.4171, 0.4671, 0.4243],
          [0.5037, 0.4444, 0.5924,  ..., 0.3725, 0.5376, 0.5100],
          [0.6304, 0.3330, 0.3661,  ..., 0.3581, 0.5265, 0.5359],
          [0.3010, 0.4297, 0.5672,  ..., 0.4824, 0.4936, 0.5032]],

         [[0.4276, 0.5091, 0.4869,  ..., 0.4709, 0.4955, 0.5697],
          [0.5011, 0.3244, 0.2957,  ..., 0.5825, 0.5956, 0.2996],
          [0.4899, 0.3532, 0.4609,  ..., 0.4595, 0.4197, 0.5598],
          [0.4896, 0.4432, 0.5207,  ..., 0.5811, 0.4326, 0.5571]],

         [[0.6005, 0.5979, 0.5146,  ..., 0.4984, 0.6452, 0.4766],
          [0.5094, 0.4863, 0.6464,  ..., 0.4469, 0.4723, 0.5646],
          [0.5659, 0.5227, 0.4726,  ..., 0.4600, 0.6045, 0.4683],
          [0.4442, 0.5899, 0.6472,  ..., 0.3233, 0.3910, 0.4082]]],


        [[[0.5193, 0.4112, 0.6437,  ..., 0.3955, 0.4240, 0.4181],
          [0.5319, 0.5082, 0.4827,  ..., 0.4062, 0.5584, 0.5778],
          [0.5534, 0.5116, 0.4042,  ..., 0.3492, 0.5301, 0.6548],
          [0.5854, 0.4958, 0.4400,  ..., 0.5425, 0.4709, 0.4569]],

         [[0.5037, 0.5368, 0.6534,  ..., 0.5035, 0.4364, 0.3119],
          [0.5382, 0.5221, 0.6275,  ..., 0.3554, 0.5327, 0.6476],
          [0.5085, 0.5591, 0.4828,  ..., 0.5267, 0.6603, 0.4228],
          [0.5023, 0.4983, 0.3771,  ..., 0.5483, 0.5899, 0.5992]],

         [[0.4981, 0.3758, 0.3808,  ..., 0.4123, 0.5175, 0.4649],
          [0.4959, 0.3831, 0.5792,  ..., 0.4624, 0.5400, 0.4562],
          [0.6475, 0.5031, 0.4898,  ..., 0.5844, 0.4226, 0.5392],
          [0.5357, 0.4856, 0.3901,  ..., 0.5096, 0.3966, 0.6258]],

         ...,

         [[0.4568, 0.5180, 0.5192,  ..., 0.3899, 0.4259, 0.5902],
          [0.5500, 0.5756, 0.5296,  ..., 0.5122, 0.5121, 0.4663],
          [0.4663, 0.5232, 0.4228,  ..., 0.4242, 0.2549, 0.4574],
          [0.6023, 0.4069, 0.5386,  ..., 0.6465, 0.4879, 0.3989]],

         [[0.6545, 0.4032, 0.4597,  ..., 0.4171, 0.4671, 0.4243],
          [0.5037, 0.4444, 0.5924,  ..., 0.3725, 0.5376, 0.5100],
          [0.6304, 0.3330, 0.3661,  ..., 0.3581, 0.5265, 0.5359],
          [0.3010, 0.4297, 0.5672,  ..., 0.4824, 0.4936, 0.5032]],

         [[0.6545, 0.4032, 0.4597,  ..., 0.4171, 0.4671, 0.4243],
          [0.5037, 0.4444, 0.5924,  ..., 0.3725, 0.5376, 0.5100],
          [0.6304, 0.3330, 0.3661,  ..., 0.3581, 0.5265, 0.5359],
          [0.3010, 0.4297, 0.5672,  ..., 0.4824, 0.4936, 0.5032]]]],
       device='cuda:0')
tensor([[[[0.6397, 0.3425, 0.5196,  ..., 0.4868, 0.5770, 0.3914],
          [0.4935, 0.5761, 0.4913,  ..., 0.5419, 0.5015, 0.4948],
          [0.5267, 0.5301, 0.4676,  ..., 0.3217, 0.3684, 0.4889],
          [0.4373, 0.4598, 0.4054,  ..., 0.6723, 0.5665, 0.5586]],

         [[0.6800, 0.6522, 0.4012,  ..., 0.4550, 0.5936, 0.4569],
          [0.4311, 0.4405, 0.4446,  ..., 0.5026, 0.5703, 0.4064],
          [0.4097, 0.5908, 0.4567,  ..., 0.3675, 0.4603, 0.5660],
          [0.4287, 0.5846, 0.4448,  ..., 0.6252, 0.4424, 0.3933]],

         [[0.6156, 0.4946, 0.6114,  ..., 0.3442, 0.4258, 0.5341],
          [0.4297, 0.4886, 0.5660,  ..., 0.4832, 0.4373, 0.4297],
          [0.4135, 0.3285, 0.4330,  ..., 0.4419, 0.5301, 0.4826],
          [0.5224, 0.4922, 0.3882,  ..., 0.4064, 0.4441, 0.4769]],

         ...,

         [[0.6575, 0.4022, 0.4647,  ..., 0.4121, 0.4661, 0.4173],
          [0.5067, 0.4434, 0.5974,  ..., 0.3675, 0.5366, 0.5030],
          [0.6334, 0.3320, 0.3711,  ..., 0.3531, 0.5255, 0.5289],
          [0.3040, 0.4287, 0.5722,  ..., 0.4774, 0.4926, 0.4962]],

         [[0.4306, 0.5081, 0.4919,  ..., 0.4659, 0.4945, 0.5627],
          [0.5041, 0.3234, 0.3007,  ..., 0.5775, 0.5946, 0.2926],
          [0.4929, 0.3522, 0.4659,  ..., 0.4545, 0.4187, 0.5528],
          [0.4926, 0.4422, 0.5257,  ..., 0.5761, 0.4316, 0.5501]],

         [[0.6035, 0.5969, 0.5196,  ..., 0.4934, 0.6442, 0.4696],
          [0.5124, 0.4853, 0.6514,  ..., 0.4419, 0.4713, 0.5576],
          [0.5689, 0.5217, 0.4776,  ..., 0.4550, 0.6035, 0.4613],
          [0.4472, 0.5889, 0.6522,  ..., 0.3183, 0.3900, 0.4012]]],


        [[[0.5223, 0.4102, 0.6487,  ..., 0.3905, 0.4230, 0.4111],
          [0.5349, 0.5072, 0.4877,  ..., 0.4012, 0.5574, 0.5708],
          [0.5564, 0.5106, 0.4092,  ..., 0.3442, 0.5291, 0.6478],
          [0.5884, 0.4948, 0.4450,  ..., 0.5375, 0.4699, 0.4499]],

         [[0.5067, 0.5358, 0.6584,  ..., 0.4985, 0.4354, 0.3049],
          [0.5412, 0.5211, 0.6325,  ..., 0.3504, 0.5317, 0.6406],
          [0.5115, 0.5581, 0.4878,  ..., 0.5217, 0.6593, 0.4158],
          [0.5053, 0.4973, 0.3821,  ..., 0.5433, 0.5889, 0.5922]],

         [[0.5011, 0.3748, 0.3858,  ..., 0.4073, 0.5165, 0.4579],
          [0.4989, 0.3821, 0.5842,  ..., 0.4574, 0.5390, 0.4492],
          [0.6505, 0.5021, 0.4948,  ..., 0.5794, 0.4216, 0.5322],
          [0.5387, 0.4846, 0.3951,  ..., 0.5046, 0.3956, 0.6188]],

         ...,

         [[0.4598, 0.5170, 0.5242,  ..., 0.3849, 0.4249, 0.5832],
          [0.5530, 0.5746, 0.5346,  ..., 0.5072, 0.5111, 0.4593],
          [0.4693, 0.5222, 0.4278,  ..., 0.4192, 0.2539, 0.4504],
          [0.6053, 0.4059, 0.5436,  ..., 0.6415, 0.4869, 0.3919]],

         [[0.6575, 0.4022, 0.4647,  ..., 0.4121, 0.4661, 0.4173],
          [0.5067, 0.4434, 0.5974,  ..., 0.3675, 0.5366, 0.5030],
          [0.6334, 0.3320, 0.3711,  ..., 0.3531, 0.5255, 0.5289],
          [0.3040, 0.4287, 0.5722,  ..., 0.4774, 0.4926, 0.4962]],

         [[0.6575, 0.4022, 0.4647,  ..., 0.4121, 0.4661, 0.4173],
          [0.5067, 0.4434, 0.5974,  ..., 0.3675, 0.5366, 0.5030],
          [0.6334, 0.3320, 0.3711,  ..., 0.3531, 0.5255, 0.5289],
          [0.3040, 0.4287, 0.5722,  ..., 0.4774, 0.4926, 0.4962]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0030,  0.0010, -0.0050, -0.0010, -0.0050, -0.0030,  0.0030,  0.0050,
         0.0010,  0.0070], device='cuda:0')
selected experts tensor([1676, 1658, 1603, 1607, 1737, 1793, 1646, 1534, 1630, 1500],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.7088, 0.6045, 0.7014,  ..., 0.3562, 0.2709, 0.4845],
          [0.5024, 0.4478, 0.4764,  ..., 0.5720, 0.6113, 0.4858],
          [0.5736, 0.4521, 0.4852,  ..., 0.5101, 0.4697, 0.4186],
          [0.5931, 0.4912, 0.3861,  ..., 0.6078, 0.4761, 0.5229]],

         [[0.6369, 0.4066, 0.6111,  ..., 0.4570, 0.3258, 0.6039],
          [0.4387, 0.5596, 0.5985,  ..., 0.4970, 0.4546, 0.4906],
          [0.6696, 0.4747, 0.2980,  ..., 0.6742, 0.4251, 0.4870],
          [0.3941, 0.5196, 0.2805,  ..., 0.3544, 0.4946, 0.4433]],

         [[0.5708, 0.5330, 0.5201,  ..., 0.5509, 0.4015, 0.5617],
          [0.4796, 0.6275, 0.5041,  ..., 0.4633, 0.5981, 0.4651],
          [0.3611, 0.5744, 0.4896,  ..., 0.5289, 0.5457, 0.5326],
          [0.5247, 0.6017, 0.5919,  ..., 0.4628, 0.4759, 0.4511]],

         ...,

         [[0.3720, 0.4984, 0.3660,  ..., 0.4033, 0.5249, 0.5552],
          [0.4940, 0.5867, 0.4628,  ..., 0.3253, 0.4076, 0.5525],
          [0.5387, 0.4232, 0.4432,  ..., 0.5408, 0.4993, 0.5065],
          [0.6001, 0.4611, 0.5403,  ..., 0.4194, 0.3807, 0.4535]],

         [[0.3521, 0.4271, 0.4534,  ..., 0.3410, 0.4761, 0.3830],
          [0.5225, 0.5015, 0.6526,  ..., 0.5615, 0.4308, 0.6486],
          [0.4938, 0.3598, 0.3778,  ..., 0.3873, 0.5323, 0.2948],
          [0.5108, 0.7104, 0.4775,  ..., 0.4000, 0.4646, 0.4666]],

         [[0.3415, 0.3758, 0.5018,  ..., 0.3544, 0.4735, 0.4419],
          [0.3602, 0.5116, 0.5606,  ..., 0.4066, 0.5758, 0.4852],
          [0.3345, 0.5379, 0.4452,  ..., 0.4132, 0.4718, 0.4462],
          [0.4773, 0.5410, 0.6264,  ..., 0.5249, 0.4871, 0.5135]]],


        [[[0.7120, 0.5451, 0.5886,  ..., 0.5444, 0.4733, 0.5172],
          [0.3747, 0.4796, 0.5757,  ..., 0.4648, 0.4746, 0.4120],
          [0.6916, 0.5318, 0.5633,  ..., 0.5744, 0.6061, 0.5576],
          [0.5532, 0.5354, 0.5241,  ..., 0.4836, 0.5437, 0.4419]],

         [[0.4898, 0.5257, 0.5230,  ..., 0.5734, 0.4954, 0.5469],
          [0.4201, 0.6175, 0.6125,  ..., 0.5444, 0.4525, 0.3881],
          [0.6333, 0.5127, 0.4184,  ..., 0.5792, 0.4755, 0.5421],
          [0.4115, 0.4362, 0.5315,  ..., 0.5124, 0.4455, 0.6169]],

         [[0.6512, 0.4550, 0.4005,  ..., 0.4019, 0.5715, 0.4443],
          [0.5448, 0.3661, 0.5178,  ..., 0.5230, 0.5284, 0.5583],
          [0.3692, 0.4741, 0.5326,  ..., 0.5035, 0.5929, 0.3647],
          [0.3575, 0.4570, 0.4878,  ..., 0.6017, 0.6010, 0.4407]],

         ...,

         [[0.4796, 0.4208, 0.4594,  ..., 0.4831, 0.4332, 0.5765],
          [0.3275, 0.5810, 0.4241,  ..., 0.5449, 0.4804, 0.5612],
          [0.6477, 0.5194, 0.4802,  ..., 0.5269, 0.4583, 0.3683],
          [0.5409, 0.4756, 0.6337,  ..., 0.3981, 0.3258, 0.4438]],

         [[0.3720, 0.4984, 0.3660,  ..., 0.4033, 0.5249, 0.5552],
          [0.4940, 0.5867, 0.4628,  ..., 0.3253, 0.4076, 0.5525],
          [0.5387, 0.4232, 0.4432,  ..., 0.5408, 0.4993, 0.5065],
          [0.6001, 0.4611, 0.5403,  ..., 0.4194, 0.3807, 0.4535]],

         [[0.3720, 0.4984, 0.3660,  ..., 0.4033, 0.5249, 0.5552],
          [0.4940, 0.5867, 0.4628,  ..., 0.3253, 0.4076, 0.5525],
          [0.5387, 0.4232, 0.4432,  ..., 0.5408, 0.4993, 0.5065],
          [0.6001, 0.4611, 0.5403,  ..., 0.4194, 0.3807, 0.4535]]]],
       device='cuda:0')
tensor([[[[0.7098, 0.6095, 0.6984,  ..., 0.3612, 0.2659, 0.4855],
          [0.5034, 0.4528, 0.4734,  ..., 0.5770, 0.6063, 0.4868],
          [0.5746, 0.4571, 0.4822,  ..., 0.5151, 0.4647, 0.4196],
          [0.5941, 0.4962, 0.3831,  ..., 0.6128, 0.4711, 0.5239]],

         [[0.6379, 0.4116, 0.6081,  ..., 0.4620, 0.3208, 0.6049],
          [0.4397, 0.5646, 0.5955,  ..., 0.5020, 0.4496, 0.4916],
          [0.6706, 0.4797, 0.2950,  ..., 0.6792, 0.4201, 0.4880],
          [0.3951, 0.5246, 0.2775,  ..., 0.3594, 0.4896, 0.4443]],

         [[0.5718, 0.5380, 0.5171,  ..., 0.5559, 0.3965, 0.5627],
          [0.4806, 0.6325, 0.5011,  ..., 0.4683, 0.5931, 0.4661],
          [0.3621, 0.5794, 0.4866,  ..., 0.5339, 0.5407, 0.5336],
          [0.5257, 0.6067, 0.5889,  ..., 0.4678, 0.4709, 0.4521]],

         ...,

         [[0.3730, 0.5034, 0.3630,  ..., 0.4083, 0.5199, 0.5562],
          [0.4950, 0.5917, 0.4598,  ..., 0.3303, 0.4026, 0.5535],
          [0.5397, 0.4282, 0.4402,  ..., 0.5458, 0.4943, 0.5075],
          [0.6011, 0.4661, 0.5373,  ..., 0.4244, 0.3757, 0.4545]],

         [[0.3531, 0.4321, 0.4504,  ..., 0.3460, 0.4711, 0.3840],
          [0.5235, 0.5065, 0.6496,  ..., 0.5665, 0.4258, 0.6496],
          [0.4948, 0.3648, 0.3748,  ..., 0.3923, 0.5273, 0.2958],
          [0.5118, 0.7154, 0.4745,  ..., 0.4050, 0.4596, 0.4676]],

         [[0.3425, 0.3808, 0.4988,  ..., 0.3594, 0.4685, 0.4429],
          [0.3612, 0.5166, 0.5576,  ..., 0.4116, 0.5708, 0.4862],
          [0.3355, 0.5429, 0.4422,  ..., 0.4182, 0.4668, 0.4472],
          [0.4783, 0.5460, 0.6234,  ..., 0.5299, 0.4821, 0.5145]]],


        [[[0.7130, 0.5501, 0.5856,  ..., 0.5494, 0.4683, 0.5182],
          [0.3757, 0.4846, 0.5727,  ..., 0.4698, 0.4696, 0.4130],
          [0.6926, 0.5368, 0.5603,  ..., 0.5794, 0.6011, 0.5586],
          [0.5542, 0.5404, 0.5211,  ..., 0.4886, 0.5387, 0.4429]],

         [[0.4908, 0.5307, 0.5200,  ..., 0.5784, 0.4904, 0.5479],
          [0.4211, 0.6225, 0.6095,  ..., 0.5494, 0.4475, 0.3891],
          [0.6343, 0.5177, 0.4154,  ..., 0.5842, 0.4705, 0.5431],
          [0.4125, 0.4412, 0.5285,  ..., 0.5174, 0.4405, 0.6179]],

         [[0.6522, 0.4600, 0.3975,  ..., 0.4069, 0.5665, 0.4453],
          [0.5458, 0.3711, 0.5148,  ..., 0.5280, 0.5234, 0.5593],
          [0.3702, 0.4791, 0.5296,  ..., 0.5085, 0.5879, 0.3657],
          [0.3585, 0.4620, 0.4848,  ..., 0.6067, 0.5960, 0.4417]],

         ...,

         [[0.4806, 0.4258, 0.4564,  ..., 0.4881, 0.4282, 0.5775],
          [0.3285, 0.5860, 0.4211,  ..., 0.5499, 0.4754, 0.5622],
          [0.6487, 0.5244, 0.4772,  ..., 0.5319, 0.4533, 0.3693],
          [0.5419, 0.4806, 0.6307,  ..., 0.4031, 0.3208, 0.4448]],

         [[0.3730, 0.5034, 0.3630,  ..., 0.4083, 0.5199, 0.5562],
          [0.4950, 0.5917, 0.4598,  ..., 0.3303, 0.4026, 0.5535],
          [0.5397, 0.4282, 0.4402,  ..., 0.5458, 0.4943, 0.5075],
          [0.6011, 0.4661, 0.5373,  ..., 0.4244, 0.3757, 0.4545]],

         [[0.3730, 0.5034, 0.3630,  ..., 0.4083, 0.5199, 0.5562],
          [0.4950, 0.5917, 0.4598,  ..., 0.3303, 0.4026, 0.5535],
          [0.5397, 0.4282, 0.4402,  ..., 0.5458, 0.4943, 0.5075],
          [0.6011, 0.4661, 0.5373,  ..., 0.4244, 0.3757, 0.4545]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0010, -0.0050,  0.0030,  0.0110, -0.0010,  0.0090, -0.0090, -0.0050,
         0.0050, -0.0010], device='cuda:0')
selected experts tensor([1821, 1530, 1483, 1785, 1670, 1535, 1694, 1624, 1630, 1612],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4536, 0.5103, 0.5503,  ..., 0.5138, 0.5646, 0.6627],
          [0.2810, 0.5374, 0.5556,  ..., 0.4860, 0.5721, 0.4470],
          [0.5308, 0.4316, 0.5709,  ..., 0.5449, 0.5406, 0.4762],
          [0.4066, 0.5352, 0.3613,  ..., 0.5721, 0.4336, 0.5288]],

         [[0.4298, 0.5185, 0.5349,  ..., 0.5924, 0.5358, 0.5910],
          [0.3723, 0.5229, 0.4967,  ..., 0.5294, 0.5452, 0.5108],
          [0.5337, 0.4505, 0.4012,  ..., 0.5304, 0.5474, 0.4298],
          [0.4662, 0.6429, 0.4136,  ..., 0.5730, 0.4835, 0.5254]],

         [[0.4468, 0.5097, 0.4462,  ..., 0.4464, 0.4860, 0.4528],
          [0.4965, 0.6268, 0.5131,  ..., 0.6709, 0.4432, 0.5877],
          [0.5264, 0.3669, 0.3774,  ..., 0.4271, 0.5523, 0.5718],
          [0.5702, 0.4608, 0.2633,  ..., 0.5497, 0.4129, 0.5853]],

         ...,

         [[0.5501, 0.5485, 0.6583,  ..., 0.5117, 0.4446, 0.5525],
          [0.6052, 0.5569, 0.5138,  ..., 0.3851, 0.5023, 0.5313],
          [0.3858, 0.4939, 0.4761,  ..., 0.5529, 0.4813, 0.3761],
          [0.5354, 0.4995, 0.5556,  ..., 0.4721, 0.4232, 0.3172]],

         [[0.5871, 0.5265, 0.5027,  ..., 0.6083, 0.5716, 0.6179],
          [0.4166, 0.5470, 0.4807,  ..., 0.4580, 0.6226, 0.5446],
          [0.6130, 0.3200, 0.5997,  ..., 0.6230, 0.5605, 0.4437],
          [0.5597, 0.5698, 0.5491,  ..., 0.5082, 0.5024, 0.2907]],

         [[0.4688, 0.5669, 0.3094,  ..., 0.5432, 0.4896, 0.5920],
          [0.3742, 0.5304, 0.5595,  ..., 0.4997, 0.6203, 0.5726],
          [0.4836, 0.5185, 0.5025,  ..., 0.4461, 0.4934, 0.4966],
          [0.5621, 0.6036, 0.5987,  ..., 0.4047, 0.5011, 0.5045]]],


        [[[0.5390, 0.5535, 0.3778,  ..., 0.5408, 0.5706, 0.5800],
          [0.3751, 0.5241, 0.4535,  ..., 0.6735, 0.5716, 0.3323],
          [0.5568, 0.4756, 0.3951,  ..., 0.4324, 0.5771, 0.4646],
          [0.5431, 0.5028, 0.6103,  ..., 0.5418, 0.4813, 0.4757]],

         [[0.4730, 0.5275, 0.3815,  ..., 0.5825, 0.5036, 0.5682],
          [0.3877, 0.5759, 0.4250,  ..., 0.6936, 0.5241, 0.3644],
          [0.5027, 0.4595, 0.4907,  ..., 0.3455, 0.5459, 0.4509],
          [0.5706, 0.4100, 0.5222,  ..., 0.4868, 0.4950, 0.4223]],

         [[0.4963, 0.5118, 0.5704,  ..., 0.5517, 0.5082, 0.4690],
          [0.5578, 0.4568, 0.5704,  ..., 0.6166, 0.5362, 0.4881],
          [0.5749, 0.4593, 0.5571,  ..., 0.4946, 0.6193, 0.4375],
          [0.5810, 0.5849, 0.5809,  ..., 0.4558, 0.5408, 0.5215]],

         ...,

         [[0.5871, 0.5485, 0.5096,  ..., 0.3271, 0.4874, 0.4923],
          [0.3223, 0.6012, 0.4003,  ..., 0.6726, 0.5137, 0.5972],
          [0.5434, 0.5037, 0.4392,  ..., 0.4199, 0.5190, 0.5294],
          [0.5539, 0.3771, 0.3820,  ..., 0.5887, 0.3980, 0.5762]],

         [[0.5405, 0.6003, 0.5049,  ..., 0.4100, 0.6156, 0.4774],
          [0.3811, 0.5081, 0.4752,  ..., 0.6393, 0.5944, 0.5752],
          [0.4056, 0.5131, 0.6317,  ..., 0.3795, 0.4821, 0.4494],
          [0.5664, 0.3794, 0.5351,  ..., 0.5432, 0.3860, 0.6374]],

         [[0.6885, 0.5284, 0.5431,  ..., 0.5025, 0.6420, 0.7066],
          [0.3896, 0.6150, 0.6290,  ..., 0.6097, 0.4489, 0.4884],
          [0.4661, 0.4328, 0.5024,  ..., 0.5161, 0.4634, 0.4559],
          [0.6025, 0.6141, 0.5823,  ..., 0.4795, 0.4218, 0.5395]]]],
       device='cuda:0')
tensor([[[[0.4676, 0.5223, 0.5583,  ..., 0.5178, 0.5506, 0.6487],
          [0.2950, 0.5494, 0.5636,  ..., 0.4900, 0.5581, 0.4330],
          [0.5448, 0.4436, 0.5789,  ..., 0.5489, 0.5266, 0.4622],
          [0.4206, 0.5472, 0.3693,  ..., 0.5761, 0.4196, 0.5148]],

         [[0.4438, 0.5305, 0.5429,  ..., 0.5964, 0.5218, 0.5770],
          [0.3863, 0.5349, 0.5047,  ..., 0.5334, 0.5312, 0.4968],
          [0.5477, 0.4625, 0.4092,  ..., 0.5344, 0.5334, 0.4158],
          [0.4802, 0.6549, 0.4216,  ..., 0.5770, 0.4695, 0.5114]],

         [[0.4608, 0.5217, 0.4542,  ..., 0.4504, 0.4720, 0.4388],
          [0.5105, 0.6388, 0.5211,  ..., 0.6749, 0.4292, 0.5737],
          [0.5404, 0.3789, 0.3854,  ..., 0.4311, 0.5383, 0.5578],
          [0.5842, 0.4728, 0.2713,  ..., 0.5537, 0.3989, 0.5713]],

         ...,

         [[0.5641, 0.5605, 0.6663,  ..., 0.5157, 0.4306, 0.5385],
          [0.6192, 0.5689, 0.5218,  ..., 0.3891, 0.4883, 0.5173],
          [0.3998, 0.5059, 0.4841,  ..., 0.5569, 0.4673, 0.3621],
          [0.5494, 0.5115, 0.5636,  ..., 0.4761, 0.4092, 0.3032]],

         [[0.6011, 0.5385, 0.5107,  ..., 0.6123, 0.5576, 0.6039],
          [0.4306, 0.5590, 0.4887,  ..., 0.4620, 0.6086, 0.5306],
          [0.6270, 0.3320, 0.6077,  ..., 0.6270, 0.5465, 0.4297],
          [0.5737, 0.5818, 0.5571,  ..., 0.5122, 0.4884, 0.2767]],

         [[0.4828, 0.5789, 0.3174,  ..., 0.5472, 0.4756, 0.5780],
          [0.3882, 0.5424, 0.5675,  ..., 0.5037, 0.6063, 0.5586],
          [0.4976, 0.5305, 0.5105,  ..., 0.4501, 0.4794, 0.4826],
          [0.5761, 0.6156, 0.6067,  ..., 0.4087, 0.4871, 0.4905]]],


        [[[0.5530, 0.5655, 0.3858,  ..., 0.5448, 0.5566, 0.5660],
          [0.3891, 0.5361, 0.4615,  ..., 0.6775, 0.5576, 0.3183],
          [0.5708, 0.4876, 0.4031,  ..., 0.4364, 0.5631, 0.4506],
          [0.5571, 0.5148, 0.6183,  ..., 0.5458, 0.4673, 0.4617]],

         [[0.4870, 0.5395, 0.3895,  ..., 0.5865, 0.4896, 0.5542],
          [0.4017, 0.5879, 0.4330,  ..., 0.6976, 0.5101, 0.3504],
          [0.5167, 0.4715, 0.4987,  ..., 0.3495, 0.5319, 0.4369],
          [0.5846, 0.4220, 0.5302,  ..., 0.4908, 0.4810, 0.4083]],

         [[0.5103, 0.5238, 0.5784,  ..., 0.5557, 0.4942, 0.4550],
          [0.5718, 0.4688, 0.5784,  ..., 0.6206, 0.5222, 0.4741],
          [0.5889, 0.4713, 0.5651,  ..., 0.4986, 0.6053, 0.4235],
          [0.5950, 0.5969, 0.5889,  ..., 0.4598, 0.5268, 0.5075]],

         ...,

         [[0.6011, 0.5605, 0.5176,  ..., 0.3311, 0.4734, 0.4783],
          [0.3363, 0.6132, 0.4083,  ..., 0.6766, 0.4997, 0.5832],
          [0.5574, 0.5157, 0.4472,  ..., 0.4239, 0.5050, 0.5154],
          [0.5679, 0.3891, 0.3900,  ..., 0.5927, 0.3840, 0.5622]],

         [[0.5545, 0.6123, 0.5129,  ..., 0.4140, 0.6016, 0.4634],
          [0.3951, 0.5201, 0.4832,  ..., 0.6433, 0.5804, 0.5612],
          [0.4196, 0.5251, 0.6397,  ..., 0.3835, 0.4681, 0.4354],
          [0.5804, 0.3914, 0.5431,  ..., 0.5472, 0.3720, 0.6234]],

         [[0.7025, 0.5404, 0.5511,  ..., 0.5065, 0.6280, 0.6926],
          [0.4036, 0.6270, 0.6370,  ..., 0.6137, 0.4349, 0.4744],
          [0.4801, 0.4448, 0.5104,  ..., 0.5201, 0.4494, 0.4419],
          [0.6165, 0.6261, 0.5903,  ..., 0.4835, 0.4078, 0.5255]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([-0.0140, -0.0120, -0.0080,  0.0040, -0.0140,  0.0160, -0.0160, -0.0040,
         0.0140,  0.0140], device='cuda:0')
selected experts tensor([1756, 1717, 1220, 1783, 1904, 1307, 2326, 1722, 1128, 1521],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4174, 0.4247, 0.5074,  ..., 0.4220, 0.3817, 0.4932],
          [0.5545, 0.6439, 0.6294,  ..., 0.5740, 0.4965, 0.5219],
          [0.4422, 0.3942, 0.3886,  ..., 0.4751, 0.5299, 0.3802],
          [0.6152, 0.4605, 0.5887,  ..., 0.3901, 0.4817, 0.3224]],

         [[0.4553, 0.4295, 0.5735,  ..., 0.3055, 0.4496, 0.5775],
          [0.5492, 0.5151, 0.5701,  ..., 0.5103, 0.5720, 0.4944],
          [0.5175, 0.5377, 0.4185,  ..., 0.4125, 0.4743, 0.4051],
          [0.5499, 0.5152, 0.6024,  ..., 0.4210, 0.5491, 0.3820]],

         [[0.5235, 0.4641, 0.5503,  ..., 0.4555, 0.5566, 0.5808],
          [0.5364, 0.5597, 0.6118,  ..., 0.4849, 0.6113, 0.6434],
          [0.6092, 0.4271, 0.3192,  ..., 0.4756, 0.4627, 0.5089],
          [0.5369, 0.5044, 0.5527,  ..., 0.5292, 0.5573, 0.4908]],

         ...,

         [[0.5020, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5100],
          [0.5020, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5100],
          [0.5020, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5100],
          [0.5020, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5100]],

         [[0.5020, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5100],
          [0.5020, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5100],
          [0.5020, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5100],
          [0.5020, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5100]],

         [[0.5020, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5100],
          [0.5020, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5100],
          [0.5020, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5100],
          [0.5020, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5100]]],


        [[[0.5572, 0.5018, 0.4607,  ..., 0.4069, 0.6006, 0.4837],
          [0.5035, 0.4400, 0.4697,  ..., 0.5475, 0.5340, 0.5784],
          [0.5666, 0.4995, 0.4166,  ..., 0.5301, 0.4660, 0.4449],
          [0.5002, 0.4687, 0.5359,  ..., 0.5707, 0.5079, 0.3586]],

         [[0.5699, 0.6001, 0.5266,  ..., 0.4853, 0.6165, 0.4882],
          [0.3349, 0.5125, 0.4433,  ..., 0.3966, 0.4171, 0.4273],
          [0.4507, 0.4333, 0.4460,  ..., 0.4603, 0.4053, 0.5461],
          [0.5451, 0.3771, 0.6403,  ..., 0.4353, 0.5058, 0.3542]],

         [[0.4413, 0.4110, 0.4597,  ..., 0.5161, 0.3872, 0.4406],
          [0.3713, 0.5902, 0.5250,  ..., 0.5311, 0.5090, 0.4382],
          [0.5317, 0.3983, 0.4699,  ..., 0.4982, 0.4682, 0.5592],
          [0.6381, 0.6024, 0.5612,  ..., 0.5533, 0.4825, 0.4306]],

         ...,

         [[0.5020, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5100],
          [0.5020, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5100],
          [0.5020, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5100],
          [0.5020, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5100]],

         [[0.5020, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5100],
          [0.5020, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5100],
          [0.5020, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5100],
          [0.5020, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5100]],

         [[0.5020, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5100],
          [0.5020, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5100],
          [0.5020, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5100],
          [0.5020, 0.5060, 0.5060,  ..., 0.5080, 0.5060, 0.5100]]]],
       device='cuda:0')
tensor([[[[0.4154, 0.4187, 0.5014,  ..., 0.4140, 0.3757, 0.4832],
          [0.5525, 0.6379, 0.6234,  ..., 0.5660, 0.4905, 0.5119],
          [0.4402, 0.3882, 0.3826,  ..., 0.4671, 0.5239, 0.3702],
          [0.6132, 0.4545, 0.5827,  ..., 0.3821, 0.4757, 0.3124]],

         [[0.4533, 0.4235, 0.5675,  ..., 0.2975, 0.4436, 0.5675],
          [0.5472, 0.5091, 0.5641,  ..., 0.5023, 0.5660, 0.4844],
          [0.5155, 0.5317, 0.4125,  ..., 0.4045, 0.4683, 0.3951],
          [0.5479, 0.5092, 0.5964,  ..., 0.4130, 0.5431, 0.3720]],

         [[0.5215, 0.4581, 0.5443,  ..., 0.4475, 0.5506, 0.5708],
          [0.5344, 0.5537, 0.6058,  ..., 0.4769, 0.6053, 0.6334],
          [0.6072, 0.4211, 0.3132,  ..., 0.4676, 0.4567, 0.4989],
          [0.5349, 0.4984, 0.5467,  ..., 0.5212, 0.5513, 0.4808]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5552, 0.4958, 0.4547,  ..., 0.3989, 0.5946, 0.4737],
          [0.5015, 0.4340, 0.4637,  ..., 0.5395, 0.5280, 0.5684],
          [0.5646, 0.4935, 0.4106,  ..., 0.5221, 0.4600, 0.4349],
          [0.4982, 0.4627, 0.5299,  ..., 0.5627, 0.5019, 0.3486]],

         [[0.5679, 0.5941, 0.5206,  ..., 0.4773, 0.6105, 0.4782],
          [0.3329, 0.5065, 0.4373,  ..., 0.3886, 0.4111, 0.4173],
          [0.4487, 0.4273, 0.4400,  ..., 0.4523, 0.3993, 0.5361],
          [0.5431, 0.3711, 0.6343,  ..., 0.4273, 0.4998, 0.3442]],

         [[0.4393, 0.4050, 0.4537,  ..., 0.5081, 0.3812, 0.4306],
          [0.3693, 0.5842, 0.5190,  ..., 0.5231, 0.5030, 0.4282],
          [0.5297, 0.3923, 0.4639,  ..., 0.4902, 0.4622, 0.5492],
          [0.6361, 0.5964, 0.5552,  ..., 0.5453, 0.4765, 0.4206]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0020,  0.0060,  0.0060,  0.0080, -0.0040,  0.0080,  0.0080,  0.0080,
         0.0060,  0.0100], device='cuda:0')
selected experts tensor([1823, 1591, 1800, 2399, 1882,  872,  946,  773, 1898, 2400],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1821., 1530., 1483., 1785., 1670., 1535., 1694., 1624., 1630., 1612.],
        [1823., 1591., 1800., 2399., 1882.,  872.,  946.,  773., 1898., 2400.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5299, 0.4216, 0.4483,  ..., 0.4501, 0.5010, 0.4791],
          [0.4114, 0.5322, 0.5064,  ..., 0.4959, 0.6435, 0.5225],
          [0.6069, 0.6926, 0.4223,  ..., 0.5754, 0.5148, 0.4338],
          [0.5193, 0.4741, 0.3680,  ..., 0.6340, 0.4775, 0.5455]],

         [[0.3902, 0.4581, 0.4794,  ..., 0.3582, 0.5308, 0.3558],
          [0.5282, 0.6202, 0.6249,  ..., 0.6285, 0.4741, 0.6147],
          [0.4877, 0.4007, 0.4948,  ..., 0.2986, 0.4650, 0.4276],
          [0.6240, 0.4716, 0.5068,  ..., 0.5146, 0.4977, 0.5978]],

         [[0.4786, 0.3886, 0.5990,  ..., 0.5578, 0.4302, 0.6513],
          [0.6194, 0.5222, 0.6853,  ..., 0.6104, 0.4009, 0.4964],
          [0.4628, 0.4622, 0.5408,  ..., 0.5614, 0.3759, 0.5680],
          [0.6683, 0.5617, 0.5692,  ..., 0.6358, 0.6045, 0.6002]],

         ...,

         [[0.4794, 0.5703, 0.4897,  ..., 0.5022, 0.4951, 0.3782],
          [0.5526, 0.3540, 0.5721,  ..., 0.6662, 0.4577, 0.6082],
          [0.3860, 0.5513, 0.3925,  ..., 0.4138, 0.5224, 0.6812],
          [0.4790, 0.3789, 0.4938,  ..., 0.4566, 0.4027, 0.4444]],

         [[0.5418, 0.4790, 0.4152,  ..., 0.5354, 0.4288, 0.5860],
          [0.5101, 0.5429, 0.2927,  ..., 0.5825, 0.4212, 0.5154],
          [0.5137, 0.4927, 0.4100,  ..., 0.4361, 0.4897, 0.5191],
          [0.5197, 0.4140, 0.6330,  ..., 0.4152, 0.4499, 0.5526]],

         [[0.5687, 0.4140, 0.3795,  ..., 0.6466, 0.5164, 0.5042],
          [0.5056, 0.4206, 0.4543,  ..., 0.4376, 0.4519, 0.4995],
          [0.5891, 0.3675, 0.5620,  ..., 0.4961, 0.4819, 0.3860],
          [0.5335, 0.4726, 0.3745,  ..., 0.4844, 0.5104, 0.4262]]],


        [[[0.5015, 0.4111, 0.5012,  ..., 0.4995, 0.4136, 0.7297],
          [0.4684, 0.5046, 0.5127,  ..., 0.4590, 0.5814, 0.4960],
          [0.5048, 0.3675, 0.5778,  ..., 0.5345, 0.4473, 0.5745],
          [0.4897, 0.5302, 0.3491,  ..., 0.3645, 0.5143, 0.5673]],

         [[0.3653, 0.5088, 0.4809,  ..., 0.4675, 0.5321, 0.3791],
          [0.6240, 0.2942, 0.5792,  ..., 0.5537, 0.5031, 0.5950],
          [0.5976, 0.6169, 0.4374,  ..., 0.6493, 0.5728, 0.4952],
          [0.5938, 0.5761, 0.6303,  ..., 0.5148, 0.3211, 0.5184]],

         [[0.5601, 0.4216, 0.5668,  ..., 0.4539, 0.3786, 0.4059],
          [0.4611, 0.3576, 0.4818,  ..., 0.4304, 0.4828, 0.5375],
          [0.3809, 0.6039, 0.4233,  ..., 0.5150, 0.5723, 0.5051],
          [0.5534, 0.3567, 0.5519,  ..., 0.5129, 0.4630, 0.4758]],

         ...,

         [[0.6570, 0.4649, 0.5020,  ..., 0.6024, 0.5052, 0.4528],
          [0.3653, 0.5198, 0.5649,  ..., 0.4114, 0.5771, 0.4267],
          [0.4386, 0.5586, 0.4367,  ..., 0.4784, 0.4794, 0.4854],
          [0.6088, 0.4845, 0.3245,  ..., 0.5216, 0.4207, 0.4215]],

         [[0.5117, 0.5665, 0.5663,  ..., 0.5033, 0.4932, 0.4869],
          [0.4592, 0.6072, 0.4493,  ..., 0.4021, 0.4843, 0.4372],
          [0.4449, 0.5036, 0.4147,  ..., 0.4445, 0.5914, 0.4983],
          [0.4365, 0.4069, 0.5345,  ..., 0.5672, 0.6290, 0.4017]],

         [[0.4614, 0.5080, 0.4485,  ..., 0.5868, 0.6471, 0.4465],
          [0.5882, 0.5329, 0.4133,  ..., 0.5038, 0.6027, 0.5630],
          [0.4658, 0.4912, 0.4580,  ..., 0.4002, 0.3418, 0.5232],
          [0.6212, 0.3909, 0.7316,  ..., 0.6740, 0.4379, 0.3575]]]],
       device='cuda:0')
tensor([[[[0.5339, 0.4216, 0.4523,  ..., 0.4441, 0.4990, 0.4711],
          [0.4154, 0.5322, 0.5104,  ..., 0.4899, 0.6415, 0.5145],
          [0.6109, 0.6926, 0.4263,  ..., 0.5694, 0.5128, 0.4258],
          [0.5233, 0.4741, 0.3720,  ..., 0.6280, 0.4755, 0.5375]],

         [[0.3942, 0.4581, 0.4834,  ..., 0.3522, 0.5288, 0.3478],
          [0.5322, 0.6202, 0.6289,  ..., 0.6225, 0.4721, 0.6067],
          [0.4917, 0.4007, 0.4988,  ..., 0.2926, 0.4630, 0.4196],
          [0.6280, 0.4716, 0.5108,  ..., 0.5086, 0.4957, 0.5898]],

         [[0.4826, 0.3886, 0.6030,  ..., 0.5518, 0.4282, 0.6433],
          [0.6234, 0.5222, 0.6893,  ..., 0.6044, 0.3989, 0.4884],
          [0.4668, 0.4622, 0.5448,  ..., 0.5554, 0.3739, 0.5600],
          [0.6723, 0.5617, 0.5732,  ..., 0.6298, 0.6025, 0.5922]],

         ...,

         [[0.4834, 0.5703, 0.4937,  ..., 0.4962, 0.4931, 0.3702],
          [0.5566, 0.3540, 0.5761,  ..., 0.6602, 0.4557, 0.6002],
          [0.3900, 0.5513, 0.3965,  ..., 0.4078, 0.5204, 0.6732],
          [0.4830, 0.3789, 0.4978,  ..., 0.4506, 0.4007, 0.4364]],

         [[0.5458, 0.4790, 0.4192,  ..., 0.5294, 0.4268, 0.5780],
          [0.5141, 0.5429, 0.2967,  ..., 0.5765, 0.4192, 0.5074],
          [0.5177, 0.4927, 0.4140,  ..., 0.4301, 0.4877, 0.5111],
          [0.5237, 0.4140, 0.6370,  ..., 0.4092, 0.4479, 0.5446]],

         [[0.5727, 0.4140, 0.3835,  ..., 0.6406, 0.5144, 0.4962],
          [0.5096, 0.4206, 0.4583,  ..., 0.4316, 0.4499, 0.4915],
          [0.5931, 0.3675, 0.5660,  ..., 0.4901, 0.4799, 0.3780],
          [0.5375, 0.4726, 0.3785,  ..., 0.4784, 0.5084, 0.4182]]],


        [[[0.5055, 0.4111, 0.5052,  ..., 0.4935, 0.4116, 0.7217],
          [0.4724, 0.5046, 0.5167,  ..., 0.4530, 0.5794, 0.4880],
          [0.5088, 0.3675, 0.5818,  ..., 0.5285, 0.4453, 0.5665],
          [0.4937, 0.5302, 0.3531,  ..., 0.3585, 0.5123, 0.5593]],

         [[0.3693, 0.5088, 0.4849,  ..., 0.4615, 0.5301, 0.3711],
          [0.6280, 0.2942, 0.5832,  ..., 0.5477, 0.5011, 0.5870],
          [0.6016, 0.6169, 0.4414,  ..., 0.6433, 0.5708, 0.4872],
          [0.5978, 0.5761, 0.6343,  ..., 0.5088, 0.3191, 0.5104]],

         [[0.5641, 0.4216, 0.5708,  ..., 0.4479, 0.3766, 0.3979],
          [0.4651, 0.3576, 0.4858,  ..., 0.4244, 0.4808, 0.5295],
          [0.3849, 0.6039, 0.4273,  ..., 0.5090, 0.5703, 0.4971],
          [0.5574, 0.3567, 0.5559,  ..., 0.5069, 0.4610, 0.4678]],

         ...,

         [[0.6610, 0.4649, 0.5060,  ..., 0.5964, 0.5032, 0.4448],
          [0.3693, 0.5198, 0.5689,  ..., 0.4054, 0.5751, 0.4187],
          [0.4426, 0.5586, 0.4407,  ..., 0.4724, 0.4774, 0.4774],
          [0.6128, 0.4845, 0.3285,  ..., 0.5156, 0.4187, 0.4135]],

         [[0.5157, 0.5665, 0.5703,  ..., 0.4973, 0.4912, 0.4789],
          [0.4632, 0.6072, 0.4533,  ..., 0.3961, 0.4823, 0.4292],
          [0.4489, 0.5036, 0.4187,  ..., 0.4385, 0.5894, 0.4903],
          [0.4405, 0.4069, 0.5385,  ..., 0.5612, 0.6270, 0.3937]],

         [[0.4654, 0.5080, 0.4525,  ..., 0.5808, 0.6451, 0.4385],
          [0.5922, 0.5329, 0.4173,  ..., 0.4978, 0.6007, 0.5550],
          [0.4698, 0.4912, 0.4620,  ..., 0.3942, 0.3398, 0.5152],
          [0.6252, 0.3909, 0.7356,  ..., 0.6680, 0.4359, 0.3495]]]],
       device='cuda:0', requires_grad=True)
tensor([-4.0000e-03, -2.3283e-10, -4.0000e-03,  0.0000e+00, -6.0000e-03,
        -4.0000e-03,  2.0000e-03,  6.0000e-03,  2.0000e-03,  8.0000e-03],
       device='cuda:0')
selected experts tensor([1547, 1544, 1761, 1565, 1560, 1785, 1523, 1629, 1711, 1759],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4243, 0.4703, 0.5201,  ..., 0.5587, 0.3059, 0.5144],
          [0.5479, 0.5156, 0.5748,  ..., 0.4517, 0.4549, 0.4547],
          [0.4955, 0.6294, 0.4701,  ..., 0.5318, 0.4844, 0.4848],
          [0.5418, 0.4641, 0.3643,  ..., 0.4142, 0.6127, 0.5694]],

         [[0.4917, 0.6743, 0.5478,  ..., 0.5668, 0.5802, 0.6025],
          [0.4296, 0.5454, 0.5166,  ..., 0.4939, 0.3277, 0.5641],
          [0.4167, 0.5999, 0.5372,  ..., 0.5343, 0.3555, 0.5708],
          [0.4053, 0.4759, 0.4947,  ..., 0.4996, 0.5288, 0.6298]],

         [[0.5864, 0.4047, 0.5247,  ..., 0.3745, 0.4419, 0.5310],
          [0.3950, 0.4979, 0.6214,  ..., 0.5133, 0.5868, 0.4676],
          [0.5103, 0.4437, 0.4251,  ..., 0.4062, 0.6340, 0.6007],
          [0.5455, 0.5844, 0.5099,  ..., 0.6384, 0.5457, 0.3149]],

         ...,

         [[0.4974, 0.4835, 0.5527,  ..., 0.4903, 0.3771, 0.5960],
          [0.5578, 0.5011, 0.6781,  ..., 0.4636, 0.5255, 0.5946],
          [0.4921, 0.4309, 0.3917,  ..., 0.5721, 0.4658, 0.5860],
          [0.6940, 0.6544, 0.5263,  ..., 0.4890, 0.5203, 0.5603]],

         [[0.4668, 0.6819, 0.3760,  ..., 0.5410, 0.5014, 0.6689],
          [0.4423, 0.6303, 0.3898,  ..., 0.3925, 0.5542, 0.5375],
          [0.5764, 0.6230, 0.5801,  ..., 0.6152, 0.5958, 0.4321],
          [0.4971, 0.5250, 0.5981,  ..., 0.4834, 0.4658, 0.6077]],

         [[0.4559, 0.5232, 0.4580,  ..., 0.5697, 0.6385, 0.4695],
          [0.3664, 0.3846, 0.4062,  ..., 0.5408, 0.6394, 0.3567],
          [0.4573, 0.4505, 0.5319,  ..., 0.4585, 0.6953, 0.5799],
          [0.3992, 0.4711, 0.6746,  ..., 0.6321, 0.4067, 0.5332]]],


        [[[0.5897, 0.4539, 0.5618,  ..., 0.4483, 0.4866, 0.6234],
          [0.5028, 0.4745, 0.6464,  ..., 0.5526, 0.5025, 0.3522],
          [0.4301, 0.5119, 0.7024,  ..., 0.4624, 0.4656, 0.6415],
          [0.5774, 0.4152, 0.7016,  ..., 0.4062, 0.4884, 0.4417]],

         [[0.4039, 0.5116, 0.4933,  ..., 0.4437, 0.6015, 0.4887],
          [0.4181, 0.5596, 0.4380,  ..., 0.4609, 0.3979, 0.4273],
          [0.4921, 0.5999, 0.5266,  ..., 0.5020, 0.4443, 0.5392],
          [0.5551, 0.4410, 0.5590,  ..., 0.3791, 0.4433, 0.4116]],

         [[0.3719, 0.6074, 0.5355,  ..., 0.5663, 0.4646, 0.4448],
          [0.3466, 0.5452, 0.5628,  ..., 0.5759, 0.5367, 0.5600],
          [0.4186, 0.3581, 0.4430,  ..., 0.5050, 0.5622, 0.6487],
          [0.4832, 0.5260, 0.5070,  ..., 0.5481, 0.4328, 0.7249]],

         ...,

         [[0.5276, 0.6203, 0.3334,  ..., 0.4233, 0.4352, 0.6215],
          [0.4532, 0.3438, 0.4909,  ..., 0.5120, 0.4532, 0.4918],
          [0.5878, 0.4071, 0.4010,  ..., 0.4485, 0.5041, 0.4287],
          [0.3583, 0.6605, 0.3945,  ..., 0.3944, 0.6137, 0.5334]],

         [[0.4763, 0.4979, 0.4823,  ..., 0.5275, 0.5198, 0.5542],
          [0.5079, 0.6152, 0.4265,  ..., 0.6078, 0.4405, 0.6316],
          [0.5285, 0.4285, 0.5900,  ..., 0.5284, 0.5344, 0.2830],
          [0.4738, 0.7050, 0.5350,  ..., 0.5915, 0.4971, 0.4059]],

         [[0.6172, 0.5184, 0.5498,  ..., 0.6152, 0.4166, 0.5297],
          [0.4673, 0.4266, 0.6098,  ..., 0.4266, 0.5093, 0.4158],
          [0.5236, 0.6420, 0.5558,  ..., 0.2814, 0.5031, 0.5140],
          [0.4363, 0.3464, 0.3697,  ..., 0.5635, 0.3988, 0.5383]]]],
       device='cuda:0')
tensor([[[[0.4263, 0.4743, 0.5161,  ..., 0.5627, 0.2999, 0.5144],
          [0.5499, 0.5196, 0.5708,  ..., 0.4557, 0.4489, 0.4547],
          [0.4975, 0.6334, 0.4661,  ..., 0.5358, 0.4784, 0.4848],
          [0.5438, 0.4681, 0.3603,  ..., 0.4182, 0.6067, 0.5694]],

         [[0.4937, 0.6783, 0.5438,  ..., 0.5708, 0.5742, 0.6025],
          [0.4316, 0.5494, 0.5126,  ..., 0.4979, 0.3217, 0.5641],
          [0.4187, 0.6039, 0.5332,  ..., 0.5383, 0.3495, 0.5708],
          [0.4073, 0.4799, 0.4907,  ..., 0.5036, 0.5228, 0.6298]],

         [[0.5884, 0.4087, 0.5207,  ..., 0.3785, 0.4359, 0.5310],
          [0.3970, 0.5019, 0.6174,  ..., 0.5173, 0.5808, 0.4676],
          [0.5123, 0.4477, 0.4211,  ..., 0.4102, 0.6280, 0.6007],
          [0.5475, 0.5884, 0.5059,  ..., 0.6424, 0.5397, 0.3149]],

         ...,

         [[0.4994, 0.4875, 0.5487,  ..., 0.4943, 0.3711, 0.5960],
          [0.5598, 0.5051, 0.6741,  ..., 0.4676, 0.5195, 0.5946],
          [0.4941, 0.4349, 0.3877,  ..., 0.5761, 0.4598, 0.5860],
          [0.6960, 0.6584, 0.5223,  ..., 0.4930, 0.5143, 0.5603]],

         [[0.4688, 0.6859, 0.3720,  ..., 0.5450, 0.4954, 0.6689],
          [0.4443, 0.6343, 0.3858,  ..., 0.3965, 0.5482, 0.5375],
          [0.5784, 0.6270, 0.5761,  ..., 0.6192, 0.5898, 0.4321],
          [0.4991, 0.5290, 0.5941,  ..., 0.4874, 0.4598, 0.6077]],

         [[0.4579, 0.5272, 0.4540,  ..., 0.5737, 0.6325, 0.4695],
          [0.3684, 0.3886, 0.4022,  ..., 0.5448, 0.6334, 0.3567],
          [0.4593, 0.4545, 0.5279,  ..., 0.4625, 0.6893, 0.5799],
          [0.4012, 0.4751, 0.6706,  ..., 0.6361, 0.4007, 0.5332]]],


        [[[0.5917, 0.4579, 0.5578,  ..., 0.4523, 0.4806, 0.6234],
          [0.5048, 0.4785, 0.6424,  ..., 0.5566, 0.4965, 0.3522],
          [0.4321, 0.5159, 0.6984,  ..., 0.4664, 0.4596, 0.6415],
          [0.5794, 0.4192, 0.6976,  ..., 0.4102, 0.4824, 0.4417]],

         [[0.4059, 0.5156, 0.4893,  ..., 0.4477, 0.5955, 0.4887],
          [0.4201, 0.5636, 0.4340,  ..., 0.4649, 0.3919, 0.4273],
          [0.4941, 0.6039, 0.5226,  ..., 0.5060, 0.4383, 0.5392],
          [0.5571, 0.4450, 0.5550,  ..., 0.3831, 0.4373, 0.4116]],

         [[0.3739, 0.6114, 0.5315,  ..., 0.5703, 0.4586, 0.4448],
          [0.3486, 0.5492, 0.5588,  ..., 0.5799, 0.5307, 0.5600],
          [0.4206, 0.3621, 0.4390,  ..., 0.5090, 0.5562, 0.6487],
          [0.4852, 0.5300, 0.5030,  ..., 0.5521, 0.4268, 0.7249]],

         ...,

         [[0.5296, 0.6243, 0.3294,  ..., 0.4273, 0.4292, 0.6215],
          [0.4552, 0.3478, 0.4869,  ..., 0.5160, 0.4472, 0.4918],
          [0.5898, 0.4111, 0.3970,  ..., 0.4525, 0.4981, 0.4287],
          [0.3603, 0.6645, 0.3905,  ..., 0.3984, 0.6077, 0.5334]],

         [[0.4783, 0.5019, 0.4783,  ..., 0.5315, 0.5138, 0.5542],
          [0.5099, 0.6192, 0.4225,  ..., 0.6118, 0.4345, 0.6316],
          [0.5305, 0.4325, 0.5860,  ..., 0.5324, 0.5284, 0.2830],
          [0.4758, 0.7090, 0.5310,  ..., 0.5955, 0.4911, 0.4059]],

         [[0.6192, 0.5224, 0.5458,  ..., 0.6192, 0.4106, 0.5297],
          [0.4693, 0.4306, 0.6058,  ..., 0.4306, 0.5033, 0.4158],
          [0.5256, 0.6460, 0.5518,  ..., 0.2854, 0.4971, 0.5140],
          [0.4383, 0.3504, 0.3657,  ..., 0.5675, 0.3928, 0.5383]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0020, -0.0040,  0.0040,  0.0100, -0.0020,  0.0100, -0.0100, -0.0040,
         0.0060,  0.0000], device='cuda:0')
selected experts tensor([1618, 1624, 1701, 1605, 1402, 1718, 1669, 1747, 1686, 1614],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4765, 0.4463, 0.5334,  ..., 0.5011, 0.4666, 0.5234],
          [0.5148, 0.4795, 0.5286,  ..., 0.5715, 0.6124, 0.4186],
          [0.6859, 0.3728, 0.4697,  ..., 0.4572, 0.4413, 0.3744],
          [0.4735, 0.5816, 0.5054,  ..., 0.5391, 0.5450, 0.5231]],

         [[0.4446, 0.6240, 0.5400,  ..., 0.3261, 0.4965, 0.6375],
          [0.3881, 0.5650, 0.5252,  ..., 0.5581, 0.5415, 0.3479],
          [0.6434, 0.6095, 0.5298,  ..., 0.5519, 0.4299, 0.5078],
          [0.3876, 0.4553, 0.5325,  ..., 0.4737, 0.4519, 0.7264]],

         [[0.4066, 0.5362, 0.4696,  ..., 0.5166, 0.4767, 0.5552],
          [0.4945, 0.5333, 0.5513,  ..., 0.7160, 0.5324, 0.4661],
          [0.4018, 0.5279, 0.4364,  ..., 0.5028, 0.5429, 0.5479],
          [0.5358, 0.4181, 0.4994,  ..., 0.5425, 0.4285, 0.5345]],

         ...,

         [[0.5987, 0.4378, 0.5552,  ..., 0.4372, 0.5158, 0.5339],
          [0.4672, 0.6076, 0.5300,  ..., 0.4151, 0.6565, 0.5047],
          [0.4390, 0.3948, 0.6426,  ..., 0.4483, 0.6899, 0.5062],
          [0.4628, 0.4427, 0.5494,  ..., 0.4604, 0.6708, 0.5125]],

         [[0.5308, 0.4057, 0.4858,  ..., 0.3209, 0.4649, 0.5547],
          [0.3966, 0.6445, 0.4909,  ..., 0.5277, 0.5366, 0.4656],
          [0.5051, 0.3268, 0.5368,  ..., 0.5420, 0.6430, 0.4008],
          [0.3890, 0.4296, 0.5619,  ..., 0.5478, 0.5334, 0.4370]],

         [[0.5481, 0.5313, 0.4574,  ..., 0.4550, 0.3939, 0.6760],
          [0.4700, 0.6312, 0.4041,  ..., 0.5386, 0.7118, 0.4870],
          [0.4867, 0.4086, 0.6532,  ..., 0.5567, 0.4041, 0.5228],
          [0.5534, 0.3627, 0.5073,  ..., 0.4965, 0.5958, 0.4404]]],


        [[[0.5083, 0.4800, 0.5002,  ..., 0.5806, 0.5542, 0.6029],
          [0.5472, 0.5616, 0.5477,  ..., 0.4997, 0.6147, 0.4271],
          [0.4204, 0.5311, 0.5286,  ..., 0.3428, 0.4673, 0.4746],
          [0.5048, 0.4248, 0.3569,  ..., 0.5148, 0.4247, 0.6502]],

         [[0.5189, 0.5853, 0.3909,  ..., 0.4052, 0.4821, 0.5644],
          [0.4511, 0.5881, 0.4160,  ..., 0.6068, 0.5116, 0.3583],
          [0.5354, 0.4468, 0.5974,  ..., 0.4483, 0.5390, 0.3215],
          [0.6776, 0.3859, 0.5024,  ..., 0.5625, 0.5452, 0.6245]],

         [[0.5922, 0.5172, 0.3542,  ..., 0.3244, 0.5253, 0.5896],
          [0.4929, 0.3509, 0.3756,  ..., 0.5896, 0.5728, 0.3780],
          [0.4745, 0.4328, 0.6372,  ..., 0.4123, 0.6039, 0.5148],
          [0.6735, 0.4439, 0.6497,  ..., 0.4374, 0.4233, 0.6637]],

         ...,

         [[0.5271, 0.6294, 0.4751,  ..., 0.6959, 0.5147, 0.5755],
          [0.4814, 0.4451, 0.4838,  ..., 0.4127, 0.5598, 0.4129],
          [0.5496, 0.4666, 0.4169,  ..., 0.5787, 0.4856, 0.6185],
          [0.4351, 0.5127, 0.4774,  ..., 0.6275, 0.4692, 0.4385]],

         [[0.6301, 0.4195, 0.4915,  ..., 0.2999, 0.4947, 0.5825],
          [0.4223, 0.6330, 0.5145,  ..., 0.5036, 0.4995, 0.4569],
          [0.4075, 0.3374, 0.5861,  ..., 0.5569, 0.4490, 0.5421],
          [0.5298, 0.6375, 0.3696,  ..., 0.6022, 0.6058, 0.3852]],

         [[0.6818, 0.4239, 0.5065,  ..., 0.3799, 0.5482, 0.4418],
          [0.4857, 0.5194, 0.5226,  ..., 0.5509, 0.6034, 0.4428],
          [0.4538, 0.4128, 0.5371,  ..., 0.4531, 0.5977, 0.4642],
          [0.4976, 0.4253, 0.5530,  ..., 0.5734, 0.4313, 0.4627]]]],
       device='cuda:0')
tensor([[[[0.4915, 0.4593, 0.5404,  ..., 0.5061, 0.4516, 0.5084],
          [0.5297, 0.4925, 0.5356,  ..., 0.5765, 0.5974, 0.4036],
          [0.7009, 0.3858, 0.4767,  ..., 0.4622, 0.4263, 0.3594],
          [0.4885, 0.5946, 0.5124,  ..., 0.5441, 0.5300, 0.5081]],

         [[0.4596, 0.6370, 0.5470,  ..., 0.3311, 0.4815, 0.6225],
          [0.4031, 0.5780, 0.5322,  ..., 0.5631, 0.5265, 0.3329],
          [0.6584, 0.6225, 0.5368,  ..., 0.5569, 0.4149, 0.4928],
          [0.4026, 0.4683, 0.5395,  ..., 0.4787, 0.4369, 0.7114]],

         [[0.4216, 0.5492, 0.4766,  ..., 0.5216, 0.4617, 0.5402],
          [0.5095, 0.5463, 0.5583,  ..., 0.7210, 0.5174, 0.4511],
          [0.4168, 0.5409, 0.4434,  ..., 0.5078, 0.5279, 0.5329],
          [0.5508, 0.4311, 0.5064,  ..., 0.5475, 0.4135, 0.5195]],

         ...,

         [[0.6137, 0.4508, 0.5622,  ..., 0.4422, 0.5008, 0.5189],
          [0.4822, 0.6206, 0.5370,  ..., 0.4201, 0.6415, 0.4897],
          [0.4540, 0.4078, 0.6496,  ..., 0.4533, 0.6749, 0.4912],
          [0.4778, 0.4557, 0.5564,  ..., 0.4654, 0.6558, 0.4975]],

         [[0.5458, 0.4187, 0.4928,  ..., 0.3259, 0.4499, 0.5397],
          [0.4116, 0.6575, 0.4979,  ..., 0.5327, 0.5216, 0.4506],
          [0.5201, 0.3398, 0.5438,  ..., 0.5470, 0.6280, 0.3858],
          [0.4040, 0.4426, 0.5689,  ..., 0.5528, 0.5184, 0.4220]],

         [[0.5631, 0.5443, 0.4644,  ..., 0.4600, 0.3789, 0.6610],
          [0.4850, 0.6442, 0.4111,  ..., 0.5436, 0.6968, 0.4720],
          [0.5017, 0.4216, 0.6602,  ..., 0.5617, 0.3891, 0.5078],
          [0.5684, 0.3757, 0.5143,  ..., 0.5015, 0.5808, 0.4254]]],


        [[[0.5233, 0.4930, 0.5072,  ..., 0.5856, 0.5392, 0.5879],
          [0.5622, 0.5746, 0.5547,  ..., 0.5047, 0.5997, 0.4121],
          [0.4354, 0.5441, 0.5356,  ..., 0.3478, 0.4523, 0.4596],
          [0.5198, 0.4378, 0.3639,  ..., 0.5198, 0.4097, 0.6352]],

         [[0.5339, 0.5983, 0.3979,  ..., 0.4102, 0.4671, 0.5494],
          [0.4661, 0.6011, 0.4230,  ..., 0.6118, 0.4966, 0.3433],
          [0.5504, 0.4598, 0.6044,  ..., 0.4533, 0.5240, 0.3065],
          [0.6926, 0.3989, 0.5094,  ..., 0.5675, 0.5302, 0.6095]],

         [[0.6072, 0.5302, 0.3612,  ..., 0.3294, 0.5103, 0.5746],
          [0.5079, 0.3639, 0.3826,  ..., 0.5946, 0.5578, 0.3630],
          [0.4895, 0.4458, 0.6442,  ..., 0.4173, 0.5889, 0.4998],
          [0.6885, 0.4569, 0.6567,  ..., 0.4424, 0.4083, 0.6487]],

         ...,

         [[0.5421, 0.6424, 0.4821,  ..., 0.7009, 0.4997, 0.5605],
          [0.4964, 0.4581, 0.4908,  ..., 0.4177, 0.5448, 0.3979],
          [0.5646, 0.4796, 0.4239,  ..., 0.5837, 0.4706, 0.6035],
          [0.4501, 0.5257, 0.4844,  ..., 0.6325, 0.4542, 0.4235]],

         [[0.6451, 0.4325, 0.4985,  ..., 0.3049, 0.4797, 0.5675],
          [0.4373, 0.6460, 0.5215,  ..., 0.5086, 0.4845, 0.4419],
          [0.4225, 0.3504, 0.5931,  ..., 0.5619, 0.4340, 0.5271],
          [0.5448, 0.6505, 0.3766,  ..., 0.6072, 0.5908, 0.3702]],

         [[0.6968, 0.4369, 0.5135,  ..., 0.3849, 0.5332, 0.4268],
          [0.5007, 0.5324, 0.5296,  ..., 0.5559, 0.5884, 0.4278],
          [0.4688, 0.4258, 0.5441,  ..., 0.4581, 0.5827, 0.4492],
          [0.5126, 0.4383, 0.5600,  ..., 0.5784, 0.4163, 0.4477]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0150, -0.0130, -0.0070,  0.0030, -0.0150,  0.0170, -0.0170, -0.0050,
         0.0150,  0.0150], device='cuda:0')
selected experts tensor([1670, 1293, 1385, 1799, 1649, 1480, 2283, 1558, 1815, 1452],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5149, 0.6879, 0.4261,  ..., 0.3205, 0.3440, 0.4678],
          [0.4335, 0.4755, 0.5811,  ..., 0.4608, 0.3635, 0.4858],
          [0.5842, 0.5070, 0.3825,  ..., 0.2960, 0.5156, 0.6134],
          [0.5504, 0.4386, 0.5149,  ..., 0.4004, 0.6375, 0.4055]],

         [[0.5019, 0.3855, 0.3501,  ..., 0.5409, 0.4595, 0.3907],
          [0.4871, 0.4101, 0.6678,  ..., 0.4377, 0.5563, 0.5124],
          [0.5025, 0.5240, 0.4100,  ..., 0.4857, 0.6201, 0.6143],
          [0.5292, 0.5954, 0.5197,  ..., 0.5279, 0.6660, 0.3801]],

         [[0.6133, 0.4537, 0.4860,  ..., 0.4859, 0.6113, 0.4611],
          [0.5295, 0.3924, 0.5655,  ..., 0.4459, 0.6108, 0.5183],
          [0.5143, 0.4238, 0.4337,  ..., 0.5664, 0.5255, 0.5398],
          [0.5332, 0.5930, 0.5171,  ..., 0.4391, 0.5686, 0.3256]],

         ...,

         [[0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5050, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5050, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5050, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5050, 0.5090]],

         [[0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5050, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5050, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5050, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5050, 0.5090]],

         [[0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5050, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5050, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5050, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5050, 0.5090]]],


        [[[0.5842, 0.3993, 0.4142,  ..., 0.4861, 0.4199, 0.5002],
          [0.4302, 0.4617, 0.6320,  ..., 0.5139, 0.4987, 0.3810],
          [0.6497, 0.5193, 0.3839,  ..., 0.4425, 0.5739, 0.4784],
          [0.5547, 0.5494, 0.5346,  ..., 0.4267, 0.6375, 0.5191]],

         [[0.5927, 0.5501, 0.5083,  ..., 0.3612, 0.5476, 0.4698],
          [0.4635, 0.5251, 0.5403,  ..., 0.4471, 0.5758, 0.3792],
          [0.6497, 0.3347, 0.4464,  ..., 0.4598, 0.5544, 0.5984],
          [0.5151, 0.5935, 0.5332,  ..., 0.5274, 0.5532, 0.4372]],

         [[0.5376, 0.6230, 0.5653,  ..., 0.4013, 0.3922, 0.4391],
          [0.4782, 0.4400, 0.4714,  ..., 0.4358, 0.5787, 0.4538],
          [0.5620, 0.4243, 0.5251,  ..., 0.4946, 0.5849, 0.4955],
          [0.5009, 0.6741, 0.4549,  ..., 0.3981, 0.5362, 0.4037]],

         ...,

         [[0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5050, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5050, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5050, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5050, 0.5090]],

         [[0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5050, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5050, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5050, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5050, 0.5090]],

         [[0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5050, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5050, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5050, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5050, 0.5090]]]],
       device='cuda:0')/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([[[[0.5139, 0.6809, 0.4211,  ..., 0.3115, 0.3390, 0.4588],
          [0.4325, 0.4685, 0.5761,  ..., 0.4518, 0.3585, 0.4768],
          [0.5832, 0.5000, 0.3775,  ..., 0.2870, 0.5106, 0.6044],
          [0.5494, 0.4316, 0.5099,  ..., 0.3914, 0.6325, 0.3965]],

         [[0.5009, 0.3785, 0.3451,  ..., 0.5319, 0.4545, 0.3817],
          [0.4861, 0.4031, 0.6628,  ..., 0.4287, 0.5513, 0.5034],
          [0.5015, 0.5170, 0.4050,  ..., 0.4767, 0.6151, 0.6053],
          [0.5282, 0.5884, 0.5147,  ..., 0.5189, 0.6610, 0.3711]],

         [[0.6123, 0.4467, 0.4810,  ..., 0.4769, 0.6063, 0.4521],
          [0.5285, 0.3854, 0.5605,  ..., 0.4369, 0.6058, 0.5093],
          [0.5133, 0.4168, 0.4287,  ..., 0.5574, 0.5205, 0.5308],
          [0.5322, 0.5860, 0.5121,  ..., 0.4301, 0.5636, 0.3166]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5832, 0.3923, 0.4092,  ..., 0.4771, 0.4149, 0.4912],
          [0.4292, 0.4547, 0.6270,  ..., 0.5049, 0.4937, 0.3720],
          [0.6487, 0.5123, 0.3789,  ..., 0.4335, 0.5689, 0.4694],
          [0.5537, 0.5424, 0.5296,  ..., 0.4177, 0.6325, 0.5101]],

         [[0.5917, 0.5431, 0.5033,  ..., 0.3522, 0.5426, 0.4608],
          [0.4625, 0.5181, 0.5353,  ..., 0.4381, 0.5708, 0.3702],
          [0.6487, 0.3277, 0.4414,  ..., 0.4508, 0.5494, 0.5894],
          [0.5141, 0.5865, 0.5282,  ..., 0.5184, 0.5482, 0.4282]],

         [[0.5366, 0.6160, 0.5603,  ..., 0.3923, 0.3872, 0.4301],
          [0.4772, 0.4330, 0.4664,  ..., 0.4268, 0.5737, 0.4448],
          [0.5610, 0.4173, 0.5201,  ..., 0.4856, 0.5799, 0.4865],
          [0.4999, 0.6671, 0.4499,  ..., 0.3891, 0.5312, 0.3947]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0010,  0.0070,  0.0050,  0.0070, -0.0050,  0.0090,  0.0090,  0.0090,
         0.0050,  0.0090], device='cuda:0')
selected experts tensor([1737, 1635, 2004, 1176, 2111, 1947, 2355,  571, 1785, 1063],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1618., 1624., 1701., 1605., 1402., 1718., 1669., 1747., 1686., 1614.],
        [1737., 1635., 2004., 1176., 2111., 1947., 2355.,  571., 1785., 1063.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.2714, 0.5189, 0.6220,  ..., 0.5162, 0.6096, 0.4451],
          [0.4176, 0.4523, 0.5825,  ..., 0.5745, 0.5775, 0.4687],
          [0.5208, 0.5833, 0.5359,  ..., 0.6785, 0.5134, 0.5243],
          [0.4977, 0.3703, 0.4975,  ..., 0.5797, 0.5311, 0.3160]],

         [[0.6580, 0.4647, 0.5010,  ..., 0.6034, 0.5043, 0.4518],
          [0.3663, 0.5208, 0.5639,  ..., 0.4129, 0.5761, 0.4257],
          [0.4396, 0.5596, 0.4355,  ..., 0.4796, 0.4783, 0.4844],
          [0.6098, 0.4855, 0.3235,  ..., 0.5226, 0.4192, 0.4205]],

         [[0.6158, 0.5361, 0.5174,  ..., 0.5354, 0.3622, 0.4743],
          [0.4433, 0.5923, 0.4546,  ..., 0.6689, 0.4793, 0.5523],
          [0.4641, 0.4885, 0.3934,  ..., 0.4881, 0.4340, 0.3924],
          [0.5394, 0.6878, 0.4839,  ..., 0.6575, 0.5276, 0.5089]],

         ...,

         [[0.5048, 0.5804, 0.3986,  ..., 0.7208, 0.4485, 0.5660],
          [0.4510, 0.5128, 0.4981,  ..., 0.6619, 0.4730, 0.5745],
          [0.4902, 0.5078, 0.4902,  ..., 0.4309, 0.4383, 0.4328],
          [0.6121, 0.7011, 0.4533,  ..., 0.4995, 0.6470, 0.6272]],

         [[0.6358, 0.3435, 0.5148,  ..., 0.4940, 0.5775, 0.3984],
          [0.4905, 0.5771, 0.4866,  ..., 0.5489, 0.5026, 0.5017],
          [0.5240, 0.5310, 0.4628,  ..., 0.3287, 0.3694, 0.4960],
          [0.4343, 0.4610, 0.4004,  ..., 0.6793, 0.5675, 0.5656]],

         [[0.2920, 0.4434, 0.5536,  ..., 0.4266, 0.4288, 0.3952],
          [0.3600, 0.5632, 0.4415,  ..., 0.6340, 0.5492, 0.5745],
          [0.6385, 0.4436, 0.3771,  ..., 0.6592, 0.5823, 0.4588],
          [0.6796, 0.4535, 0.5519,  ..., 0.6485, 0.6895, 0.5462]]],


        [[[0.4271, 0.5723, 0.6967,  ..., 0.5018, 0.4999, 0.3655],
          [0.5906, 0.3685, 0.4543,  ..., 0.4680, 0.5227, 0.4788],
          [0.5394, 0.5349, 0.5244,  ..., 0.5244, 0.4856, 0.5094],
          [0.3700, 0.3841, 0.4841,  ..., 0.5893, 0.4826, 0.4247]],

         [[0.2553, 0.4535, 0.3869,  ..., 0.4176, 0.6003, 0.5812],
          [0.6421, 0.5875, 0.4261,  ..., 0.5992, 0.4734, 0.5513],
          [0.5353, 0.5572, 0.6508,  ..., 0.4035, 0.5284, 0.5039],
          [0.4507, 0.3330, 0.5301,  ..., 0.5166, 0.5322, 0.3718]],

         [[0.5621, 0.5243, 0.5557,  ..., 0.7350, 0.5543, 0.5130],
          [0.4457, 0.6225, 0.5966,  ..., 0.3800, 0.6115, 0.5921],
          [0.3187, 0.3532, 0.5782,  ..., 0.4576, 0.5181, 0.4482],
          [0.4826, 0.5665, 0.3235,  ..., 0.4838, 0.3924, 0.5959]],

         ...,

         [[0.4310, 0.4669, 0.3024,  ..., 0.4381, 0.6326, 0.4622],
          [0.5901, 0.4497, 0.5634,  ..., 0.5508, 0.4439, 0.4328],
          [0.5046, 0.4832, 0.5792,  ..., 0.5878, 0.5504, 0.5021],
          [0.5878, 0.5625, 0.5653,  ..., 0.4709, 0.5960, 0.5663]],

         [[0.4486, 0.5526, 0.4717,  ..., 0.5888, 0.6156, 0.4632],
          [0.3618, 0.5175, 0.3201,  ..., 0.3961, 0.6244, 0.5583],
          [0.4507, 0.5625, 0.6220,  ..., 0.4921, 0.6317, 0.3521],
          [0.4537, 0.5368, 0.4918,  ..., 0.3864, 0.4914, 0.3993]],

         [[0.4690, 0.6308, 0.4384,  ..., 0.4405, 0.5713, 0.5330],
          [0.4257, 0.3924, 0.5260,  ..., 0.4247, 0.5327, 0.5090],
          [0.4850, 0.4533, 0.2990,  ..., 0.3836, 0.4664, 0.4096],
          [0.4833, 0.4216, 0.5810,  ..., 0.4576, 0.5232, 0.5005]]]],
       device='cuda:0')
tensor([[[[0.2744, 0.5179, 0.6270,  ..., 0.5092, 0.6086, 0.4381],
          [0.4206, 0.4513, 0.5875,  ..., 0.5675, 0.5765, 0.4617],
          [0.5238, 0.5823, 0.5409,  ..., 0.6715, 0.5124, 0.5173],
          [0.5007, 0.3693, 0.5025,  ..., 0.5727, 0.5301, 0.3090]],

         [[0.6610, 0.4637, 0.5060,  ..., 0.5964, 0.5033, 0.4448],
          [0.3693, 0.5198, 0.5689,  ..., 0.4059, 0.5751, 0.4187],
          [0.4426, 0.5586, 0.4405,  ..., 0.4726, 0.4773, 0.4774],
          [0.6128, 0.4845, 0.3285,  ..., 0.5156, 0.4182, 0.4135]],

         [[0.6188, 0.5351, 0.5224,  ..., 0.5284, 0.3612, 0.4673],
          [0.4463, 0.5913, 0.4596,  ..., 0.6619, 0.4783, 0.5453],
          [0.4671, 0.4875, 0.3984,  ..., 0.4811, 0.4330, 0.3854],
          [0.5424, 0.6868, 0.4889,  ..., 0.6505, 0.5266, 0.5019]],

         ...,

         [[0.5078, 0.5794, 0.4036,  ..., 0.7138, 0.4475, 0.5590],
          [0.4540, 0.5118, 0.5031,  ..., 0.6549, 0.4720, 0.5675],
          [0.4932, 0.5068, 0.4952,  ..., 0.4239, 0.4373, 0.4258],
          [0.6151, 0.7001, 0.4583,  ..., 0.4925, 0.6460, 0.6202]],

         [[0.6388, 0.3425, 0.5198,  ..., 0.4870, 0.5765, 0.3914],
          [0.4935, 0.5761, 0.4916,  ..., 0.5419, 0.5016, 0.4947],
          [0.5270, 0.5300, 0.4678,  ..., 0.3217, 0.3684, 0.4890],
          [0.4373, 0.4600, 0.4054,  ..., 0.6723, 0.5665, 0.5586]],

         [[0.2950, 0.4424, 0.5586,  ..., 0.4196, 0.4278, 0.3882],
          [0.3630, 0.5622, 0.4465,  ..., 0.6270, 0.5482, 0.5675],
          [0.6415, 0.4426, 0.3821,  ..., 0.6522, 0.5813, 0.4518],
          [0.6826, 0.4525, 0.5569,  ..., 0.6415, 0.6885, 0.5392]]],


        [[[0.4301, 0.5713, 0.7017,  ..., 0.4948, 0.4989, 0.3585],
          [0.5936, 0.3675, 0.4593,  ..., 0.4610, 0.5217, 0.4718],
          [0.5424, 0.5339, 0.5294,  ..., 0.5174, 0.4846, 0.5024],
          [0.3730, 0.3831, 0.4891,  ..., 0.5823, 0.4816, 0.4177]],

         [[0.2583, 0.4525, 0.3919,  ..., 0.4106, 0.5993, 0.5742],
          [0.6451, 0.5865, 0.4311,  ..., 0.5922, 0.4724, 0.5443],
          [0.5383, 0.5562, 0.6558,  ..., 0.3965, 0.5274, 0.4969],
          [0.4537, 0.3320, 0.5351,  ..., 0.5096, 0.5312, 0.3648]],

         [[0.5651, 0.5233, 0.5607,  ..., 0.7280, 0.5533, 0.5060],
          [0.4487, 0.6215, 0.6016,  ..., 0.3730, 0.6105, 0.5851],
          [0.3217, 0.3522, 0.5832,  ..., 0.4506, 0.5171, 0.4412],
          [0.4856, 0.5655, 0.3285,  ..., 0.4768, 0.3914, 0.5889]],

         ...,

         [[0.4340, 0.4659, 0.3074,  ..., 0.4311, 0.6316, 0.4552],
          [0.5931, 0.4487, 0.5684,  ..., 0.5438, 0.4429, 0.4258],
          [0.5076, 0.4822, 0.5842,  ..., 0.5808, 0.5494, 0.4951],
          [0.5908, 0.5615, 0.5703,  ..., 0.4639, 0.5950, 0.5593]],

         [[0.4516, 0.5516, 0.4767,  ..., 0.5818, 0.6146, 0.4562],
          [0.3648, 0.5165, 0.3251,  ..., 0.3891, 0.6234, 0.5513],
          [0.4537, 0.5615, 0.6270,  ..., 0.4851, 0.6307, 0.3451],
          [0.4567, 0.5358, 0.4968,  ..., 0.3794, 0.4904, 0.3923]],

         [[0.4720, 0.6298, 0.4434,  ..., 0.4335, 0.5703, 0.5260],
          [0.4287, 0.3914, 0.5310,  ..., 0.4177, 0.5317, 0.5020],
          [0.4880, 0.4523, 0.3040,  ..., 0.3766, 0.4654, 0.4026],
          [0.4863, 0.4206, 0.5860,  ..., 0.4506, 0.5222, 0.4935]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0030,  0.0010, -0.0050,  0.0010, -0.0050, -0.0050,  0.0030,  0.0070,
         0.0010,  0.0070], device='cuda:0')
selected experts tensor([1633, 1694, 1732, 1603, 1577, 1665, 1537, 1620, 1584, 1739],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5218, 0.4024, 0.6864,  ..., 0.6124, 0.4939, 0.5217],
          [0.5167, 0.5830, 0.5053,  ..., 0.4966, 0.6168, 0.4704],
          [0.4448, 0.4233, 0.4973,  ..., 0.5403, 0.5428, 0.6012],
          [0.5945, 0.4794, 0.5608,  ..., 0.5867, 0.5715, 0.4187]],

         [[0.5280, 0.6222, 0.3324,  ..., 0.4223, 0.4332, 0.6235],
          [0.4542, 0.3448, 0.4898,  ..., 0.5109, 0.4525, 0.4928],
          [0.5888, 0.4081, 0.4000,  ..., 0.4475, 0.5031, 0.4297],
          [0.3593, 0.6615, 0.3935,  ..., 0.3934, 0.6127, 0.5344]],

         [[0.4770, 0.4495, 0.6813,  ..., 0.5625, 0.6456, 0.5927],
          [0.5968, 0.4935, 0.6391,  ..., 0.5087, 0.4232, 0.6026],
          [0.4457, 0.6888, 0.6032,  ..., 0.4533, 0.5626, 0.5222],
          [0.3793, 0.4181, 0.5767,  ..., 0.4323, 0.5901, 0.5473]],

         ...,

         [[0.4988, 0.4810, 0.6158,  ..., 0.5159, 0.5669, 0.6115],
          [0.4945, 0.5423, 0.5292,  ..., 0.5192, 0.6402, 0.5927],
          [0.5263, 0.5257, 0.5524,  ..., 0.4473, 0.5160, 0.4610],
          [0.4537, 0.3824, 0.5400,  ..., 0.5730, 0.5268, 0.3785]],

         [[0.7080, 0.6070, 0.7014,  ..., 0.3571, 0.2709, 0.4865],
          [0.5024, 0.4500, 0.4763,  ..., 0.5720, 0.6113, 0.4878],
          [0.5736, 0.4541, 0.4851,  ..., 0.5100, 0.4697, 0.4206],
          [0.5931, 0.4931, 0.3861,  ..., 0.6078, 0.4761, 0.5249]],

         [[0.6043, 0.4860, 0.6499,  ..., 0.5815, 0.6901, 0.5088],
          [0.4720, 0.4887, 0.6074,  ..., 0.4707, 0.4863, 0.5128],
          [0.6925, 0.5481, 0.6055,  ..., 0.6275, 0.4462, 0.3749],
          [0.6224, 0.4643, 0.4490,  ..., 0.4587, 0.3165, 0.6119]]],


        [[[0.5765, 0.3144, 0.6337,  ..., 0.4475, 0.3448, 0.4758],
          [0.5794, 0.3102, 0.5444,  ..., 0.3670, 0.4862, 0.5184],
          [0.6530, 0.4619, 0.4746,  ..., 0.4037, 0.6660, 0.4630],
          [0.6405, 0.4998, 0.4764,  ..., 0.4921, 0.6085, 0.5956]],

         [[0.5552, 0.5396, 0.5572,  ..., 0.3535, 0.5440, 0.5287],
          [0.4148, 0.5835, 0.5425,  ..., 0.5562, 0.7108, 0.4844],
          [0.5358, 0.3537, 0.3385,  ..., 0.5333, 0.3689, 0.4773],
          [0.4282, 0.5807, 0.4384,  ..., 0.5914, 0.5199, 0.3391]],

         [[0.4770, 0.6259, 0.5567,  ..., 0.5352, 0.5527, 0.5598],
          [0.3450, 0.6376, 0.3385,  ..., 0.5095, 0.3492, 0.6994],
          [0.5665, 0.5845, 0.5060,  ..., 0.4509, 0.4356, 0.3822],
          [0.6351, 0.4558, 0.3778,  ..., 0.4799, 0.6330, 0.6207]],

         ...,

         [[0.4253, 0.3810, 0.3723,  ..., 0.5528, 0.7188, 0.4168],
          [0.5679, 0.4977, 0.4982,  ..., 0.4028, 0.5094, 0.3461],
          [0.3798, 0.4355, 0.6060,  ..., 0.5872, 0.5318, 0.3631],
          [0.3923, 0.3727, 0.4623,  ..., 0.4075, 0.3716, 0.5039]],

         [[0.4769, 0.5098, 0.4483,  ..., 0.4685, 0.7116, 0.3523],
          [0.3923, 0.4587, 0.3760,  ..., 0.5044, 0.6859, 0.4844],
          [0.5703, 0.4495, 0.4490,  ..., 0.5858, 0.4655, 0.5487],
          [0.4153, 0.4765, 0.4394,  ..., 0.6374, 0.3881, 0.5172]],

         [[0.6006, 0.5205, 0.6111,  ..., 0.5049, 0.3707, 0.5380],
          [0.5324, 0.4683, 0.6847,  ..., 0.5829, 0.6150, 0.5567],
          [0.4315, 0.5503, 0.5786,  ..., 0.4004, 0.4275, 0.5622],
          [0.3647, 0.4819, 0.3534,  ..., 0.5320, 0.4137, 0.6532]]]],
       device='cuda:0')
tensor([[[[0.5228, 0.4054, 0.6834,  ..., 0.6174, 0.4889, 0.5207],
          [0.5177, 0.5860, 0.5023,  ..., 0.5016, 0.6118, 0.4694],
          [0.4458, 0.4263, 0.4943,  ..., 0.5453, 0.5378, 0.6002],
          [0.5955, 0.4824, 0.5578,  ..., 0.5917, 0.5665, 0.4177]],

         [[0.5290, 0.6252, 0.3294,  ..., 0.4273, 0.4282, 0.6225],
          [0.4552, 0.3478, 0.4868,  ..., 0.5159, 0.4475, 0.4918],
          [0.5898, 0.4111, 0.3970,  ..., 0.4525, 0.4981, 0.4287],
          [0.3603, 0.6645, 0.3905,  ..., 0.3984, 0.6077, 0.5334]],

         [[0.4780, 0.4525, 0.6783,  ..., 0.5675, 0.6406, 0.5917],
          [0.5978, 0.4965, 0.6361,  ..., 0.5137, 0.4182, 0.6016],
          [0.4467, 0.6918, 0.6002,  ..., 0.4583, 0.5576, 0.5212],
          [0.3803, 0.4211, 0.5737,  ..., 0.4373, 0.5851, 0.5463]],

         ...,

         [[0.4998, 0.4840, 0.6128,  ..., 0.5209, 0.5619, 0.6105],
          [0.4955, 0.5453, 0.5262,  ..., 0.5242, 0.6352, 0.5917],
          [0.5273, 0.5287, 0.5494,  ..., 0.4523, 0.5110, 0.4600],
          [0.4547, 0.3854, 0.5370,  ..., 0.5780, 0.5218, 0.3775]],

         [[0.7090, 0.6100, 0.6984,  ..., 0.3621, 0.2659, 0.4855],
          [0.5034, 0.4530, 0.4733,  ..., 0.5770, 0.6063, 0.4868],
          [0.5746, 0.4571, 0.4821,  ..., 0.5150, 0.4647, 0.4196],
          [0.5941, 0.4961, 0.3831,  ..., 0.6128, 0.4711, 0.5239]],

         [[0.6053, 0.4890, 0.6469,  ..., 0.5865, 0.6851, 0.5078],
          [0.4730, 0.4917, 0.6044,  ..., 0.4757, 0.4813, 0.5118],
          [0.6935, 0.5511, 0.6025,  ..., 0.6325, 0.4412, 0.3739],
          [0.6234, 0.4673, 0.4460,  ..., 0.4637, 0.3115, 0.6109]]],


        [[[0.5775, 0.3174, 0.6307,  ..., 0.4525, 0.3398, 0.4748],
          [0.5804, 0.3132, 0.5414,  ..., 0.3720, 0.4812, 0.5174],
          [0.6540, 0.4649, 0.4716,  ..., 0.4087, 0.6610, 0.4620],
          [0.6415, 0.5028, 0.4734,  ..., 0.4971, 0.6035, 0.5946]],

         [[0.5562, 0.5426, 0.5542,  ..., 0.3585, 0.5390, 0.5277],
          [0.4158, 0.5865, 0.5395,  ..., 0.5612, 0.7058, 0.4834],
          [0.5368, 0.3567, 0.3355,  ..., 0.5383, 0.3639, 0.4763],
          [0.4292, 0.5837, 0.4354,  ..., 0.5964, 0.5149, 0.3381]],

         [[0.4780, 0.6289, 0.5537,  ..., 0.5402, 0.5477, 0.5588],
          [0.3460, 0.6406, 0.3355,  ..., 0.5145, 0.3442, 0.6984],
          [0.5675, 0.5875, 0.5030,  ..., 0.4559, 0.4306, 0.3812],
          [0.6361, 0.4588, 0.3748,  ..., 0.4849, 0.6280, 0.6197]],

         ...,

         [[0.4263, 0.3840, 0.3693,  ..., 0.5578, 0.7138, 0.4158],
          [0.5689, 0.5007, 0.4952,  ..., 0.4078, 0.5044, 0.3451],
          [0.3808, 0.4385, 0.6030,  ..., 0.5922, 0.5268, 0.3621],
          [0.3933, 0.3757, 0.4593,  ..., 0.4125, 0.3666, 0.5029]],

         [[0.4779, 0.5128, 0.4453,  ..., 0.4735, 0.7066, 0.3513],
          [0.3933, 0.4617, 0.3730,  ..., 0.5094, 0.6809, 0.4834],
          [0.5713, 0.4525, 0.4460,  ..., 0.5908, 0.4605, 0.5477],
          [0.4163, 0.4795, 0.4364,  ..., 0.6424, 0.3831, 0.5162]],

         [[0.6016, 0.5235, 0.6081,  ..., 0.5099, 0.3657, 0.5370],
          [0.5334, 0.4713, 0.6817,  ..., 0.5879, 0.6100, 0.5557],
          [0.4325, 0.5533, 0.5756,  ..., 0.4054, 0.4225, 0.5612],
          [0.3657, 0.4849, 0.3504,  ..., 0.5370, 0.4087, 0.6522]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0010, -0.0030,  0.0030,  0.0110, -0.0010,  0.0090, -0.0110, -0.0050,
         0.0050,  0.0010], device='cuda:0')
selected experts tensor([1741, 1687, 1577, 1629, 1607, 1658, 1591, 1610, 1652, 1632],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5300, 0.5299, 0.4936,  ..., 0.4827, 0.5583, 0.7028],
          [0.4137, 0.5750, 0.4198,  ..., 0.6666, 0.5249, 0.4083],
          [0.4411, 0.5550, 0.4991,  ..., 0.5398, 0.4456, 0.4782],
          [0.5377, 0.6214, 0.5405,  ..., 0.5437, 0.5007, 0.4177]],

         [[0.5748, 0.6646, 0.5123,  ..., 0.4038, 0.6291, 0.4743],
          [0.4402, 0.5612, 0.5441,  ..., 0.6348, 0.4322, 0.4380],
          [0.5682, 0.5938, 0.6077,  ..., 0.4252, 0.4280, 0.5137],
          [0.5131, 0.4115, 0.4779,  ..., 0.5534, 0.4171, 0.4756]],

         [[0.5931, 0.5396, 0.6292,  ..., 0.3893, 0.5733, 0.7078],
          [0.4741, 0.6114, 0.5497,  ..., 0.5711, 0.5615, 0.4529],
          [0.4467, 0.4804, 0.5320,  ..., 0.4252, 0.4180, 0.3326],
          [0.4645, 0.4829, 0.4189,  ..., 0.6845, 0.4133, 0.4567]],

         ...,

         [[0.3597, 0.5147, 0.6063,  ..., 0.3237, 0.5282, 0.5489],
          [0.4528, 0.6250, 0.5257,  ..., 0.4818, 0.5191, 0.4533],
          [0.3722, 0.5350, 0.5956,  ..., 0.5868, 0.5972, 0.6320],
          [0.5119, 0.5148, 0.5615,  ..., 0.3745, 0.5066, 0.5811]],

         [[0.5334, 0.4225, 0.5710,  ..., 0.4718, 0.5262, 0.6394],
          [0.3497, 0.5952, 0.6096,  ..., 0.4261, 0.3512, 0.5126],
          [0.4568, 0.3883, 0.4792,  ..., 0.5415, 0.6249, 0.5034],
          [0.3638, 0.5260, 0.4762,  ..., 0.5500, 0.6118, 0.5044]],

         [[0.5181, 0.3004, 0.4937,  ..., 0.4744, 0.5886, 0.6718],
          [0.5139, 0.5598, 0.6858,  ..., 0.5896, 0.6492, 0.4102],
          [0.5148, 0.4148, 0.5605,  ..., 0.3883, 0.5542, 0.4920],
          [0.5467, 0.4413, 0.4649,  ..., 0.5606, 0.5007, 0.4637]]],


        [[[0.5014, 0.4887, 0.5018,  ..., 0.4233, 0.3888, 0.6190],
          [0.4290, 0.6031, 0.5114,  ..., 0.5230, 0.6207, 0.4926],
          [0.4271, 0.4593, 0.5658,  ..., 0.4681, 0.3833, 0.4756],
          [0.5610, 0.6455, 0.4950,  ..., 0.6786, 0.5340, 0.3567]],

         [[0.4329, 0.5468, 0.5567,  ..., 0.3781, 0.5013, 0.5344],
          [0.4300, 0.5816, 0.4084,  ..., 0.5401, 0.4603, 0.3899],
          [0.5476, 0.3953, 0.6471,  ..., 0.4624, 0.5834, 0.4855],
          [0.5634, 0.5238, 0.4265,  ..., 0.5350, 0.5338, 0.4219]],

         [[0.4723, 0.6045, 0.5979,  ..., 0.4916, 0.5344, 0.5830],
          [0.5078, 0.6286, 0.5403,  ..., 0.4142, 0.5571, 0.3899],
          [0.4790, 0.5092, 0.4237,  ..., 0.4118, 0.4745, 0.3515],
          [0.4247, 0.6349, 0.4742,  ..., 0.5420, 0.5583, 0.4929]],

         ...,

         [[0.6537, 0.5650, 0.6026,  ..., 0.4195, 0.4494, 0.4356],
          [0.5068, 0.5405, 0.5900,  ..., 0.5284, 0.5296, 0.5152],
          [0.3638, 0.3474, 0.5257,  ..., 0.4432, 0.5133, 0.4177],
          [0.4925, 0.3771, 0.5216,  ..., 0.5754, 0.3626, 0.5354]],

         [[0.3380, 0.5458, 0.5629,  ..., 0.4010, 0.5148, 0.5835],
          [0.3434, 0.5214, 0.4629,  ..., 0.4252, 0.5977, 0.4671],
          [0.3416, 0.5054, 0.6715,  ..., 0.4271, 0.5177, 0.4929],
          [0.4722, 0.6831, 0.4878,  ..., 0.4276, 0.4711, 0.4623]],

         [[0.5057, 0.5612, 0.4492,  ..., 0.3563, 0.4658, 0.5415],
          [0.4501, 0.5947, 0.5027,  ..., 0.5830, 0.5800, 0.6770],
          [0.4680, 0.4957, 0.5161,  ..., 0.5976, 0.4791, 0.5111],
          [0.4484, 0.5309, 0.5526,  ..., 0.5057, 0.3348, 0.5521]]]],
       device='cuda:0')
tensor([[[[0.5460, 0.5419, 0.4996,  ..., 0.4867, 0.5443, 0.6868],
          [0.4297, 0.5870, 0.4258,  ..., 0.6706, 0.5109, 0.3923],
          [0.4571, 0.5670, 0.5051,  ..., 0.5438, 0.4316, 0.4622],
          [0.5537, 0.6334, 0.5465,  ..., 0.5477, 0.4867, 0.4017]],

         [[0.5908, 0.6766, 0.5183,  ..., 0.4078, 0.6151, 0.4583],
          [0.4562, 0.5732, 0.5501,  ..., 0.6388, 0.4182, 0.4220],
          [0.5842, 0.6058, 0.6137,  ..., 0.4292, 0.4140, 0.4977],
          [0.5291, 0.4235, 0.4839,  ..., 0.5574, 0.4031, 0.4596]],

         [[0.6091, 0.5516, 0.6352,  ..., 0.3933, 0.5593, 0.6918],
          [0.4901, 0.6234, 0.5557,  ..., 0.5751, 0.5475, 0.4369],
          [0.4627, 0.4924, 0.5380,  ..., 0.4292, 0.4040, 0.3166],
          [0.4805, 0.4949, 0.4249,  ..., 0.6885, 0.3993, 0.4407]],

         ...,

         [[0.3757, 0.5267, 0.6123,  ..., 0.3277, 0.5142, 0.5329],
          [0.4688, 0.6370, 0.5317,  ..., 0.4858, 0.5051, 0.4373],
          [0.3882, 0.5470, 0.6016,  ..., 0.5908, 0.5832, 0.6160],
          [0.5279, 0.5268, 0.5675,  ..., 0.3785, 0.4926, 0.5651]],

         [[0.5494, 0.4345, 0.5770,  ..., 0.4758, 0.5122, 0.6234],
          [0.3657, 0.6072, 0.6156,  ..., 0.4301, 0.3372, 0.4966],
          [0.4728, 0.4003, 0.4852,  ..., 0.5455, 0.6109, 0.4874],
          [0.3798, 0.5380, 0.4822,  ..., 0.5540, 0.5978, 0.4884]],

         [[0.5341, 0.3124, 0.4997,  ..., 0.4784, 0.5746, 0.6558],
          [0.5299, 0.5718, 0.6918,  ..., 0.5936, 0.6352, 0.3942],
          [0.5308, 0.4268, 0.5665,  ..., 0.3923, 0.5402, 0.4760],
          [0.5627, 0.4533, 0.4709,  ..., 0.5646, 0.4867, 0.4477]]],


        [[[0.5174, 0.5007, 0.5078,  ..., 0.4273, 0.3748, 0.6030],
          [0.4450, 0.6151, 0.5174,  ..., 0.5270, 0.6067, 0.4766],
          [0.4431, 0.4713, 0.5718,  ..., 0.4721, 0.3693, 0.4596],
          [0.5770, 0.6575, 0.5010,  ..., 0.6826, 0.5200, 0.3407]],

         [[0.4489, 0.5588, 0.5627,  ..., 0.3821, 0.4873, 0.5184],
          [0.4460, 0.5936, 0.4144,  ..., 0.5441, 0.4463, 0.3739],
          [0.5636, 0.4073, 0.6531,  ..., 0.4664, 0.5694, 0.4695],
          [0.5794, 0.5358, 0.4325,  ..., 0.5390, 0.5198, 0.4059]],

         [[0.4883, 0.6165, 0.6039,  ..., 0.4956, 0.5204, 0.5670],
          [0.5238, 0.6406, 0.5463,  ..., 0.4182, 0.5431, 0.3739],
          [0.4950, 0.5212, 0.4297,  ..., 0.4158, 0.4605, 0.3355],
          [0.4407, 0.6469, 0.4802,  ..., 0.5460, 0.5443, 0.4769]],

         ...,

         [[0.6697, 0.5770, 0.6086,  ..., 0.4235, 0.4354, 0.4196],
          [0.5228, 0.5525, 0.5960,  ..., 0.5324, 0.5156, 0.4992],
          [0.3798, 0.3594, 0.5317,  ..., 0.4472, 0.4993, 0.4017],
          [0.5085, 0.3891, 0.5276,  ..., 0.5794, 0.3486, 0.5194]],

         [[0.3540, 0.5578, 0.5689,  ..., 0.4050, 0.5008, 0.5675],
          [0.3594, 0.5334, 0.4689,  ..., 0.4292, 0.5837, 0.4511],
          [0.3576, 0.5174, 0.6775,  ..., 0.4311, 0.5037, 0.4769],
          [0.4882, 0.6951, 0.4938,  ..., 0.4316, 0.4571, 0.4463]],

         [[0.5217, 0.5732, 0.4552,  ..., 0.3603, 0.4518, 0.5255],
          [0.4661, 0.6067, 0.5087,  ..., 0.5870, 0.5660, 0.6610],
          [0.4840, 0.5077, 0.5221,  ..., 0.6016, 0.4651, 0.4951],
          [0.4644, 0.5429, 0.5586,  ..., 0.5097, 0.3208, 0.5361]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0160, -0.0120, -0.0060,  0.0020, -0.0160,  0.0180, -0.0180, -0.0040,
         0.0140,  0.0160], device='cuda:0')
selected experts tensor([1417, 1682, 1631, 1915, 1639, 1270, 2265, 1497, 1566, 1502],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5684, 0.5670, 0.4024,  ..., 0.4282, 0.4848, 0.5115],
          [0.5832, 0.5601, 0.5294,  ..., 0.4961, 0.5618, 0.4258],
          [0.4735, 0.3339, 0.4592,  ..., 0.4676, 0.4640, 0.5312],
          [0.5511, 0.5646, 0.6033,  ..., 0.4676, 0.5568, 0.3516]],

         [[0.4741, 0.5021, 0.5003,  ..., 0.5273, 0.4303, 0.4541],
          [0.5304, 0.4710, 0.5515,  ..., 0.4659, 0.6536, 0.6497],
          [0.5537, 0.5248, 0.5498,  ..., 0.5960, 0.5891, 0.5529],
          [0.5742, 0.5298, 0.5229,  ..., 0.5524, 0.5541, 0.3658]],

         [[0.5409, 0.5151, 0.5590,  ..., 0.3507, 0.6383, 0.5832],
          [0.4644, 0.5472, 0.7000,  ..., 0.4563, 0.4805, 0.4009],
          [0.4017, 0.5489, 0.5154,  ..., 0.3935, 0.4546, 0.5251],
          [0.4696, 0.4785, 0.5645,  ..., 0.5808, 0.6473, 0.3604]],

         ...,

         [[0.5000, 0.5080, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5000, 0.5080, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5000, 0.5080, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5000, 0.5080, 0.5040,  ..., 0.5100, 0.5040, 0.5100]],

         [[0.5000, 0.5080, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5000, 0.5080, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5000, 0.5080, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5000, 0.5080, 0.5040,  ..., 0.5100, 0.5040, 0.5100]],

         [[0.5000, 0.5080, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5000, 0.5080, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5000, 0.5080, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5000, 0.5080, 0.5040,  ..., 0.5100, 0.5040, 0.5100]]],


        [[[0.6307, 0.5244, 0.4638,  ..., 0.3968, 0.3589, 0.5582],
          [0.4685, 0.5031, 0.6075,  ..., 0.3802, 0.5891, 0.3542],
          [0.4637, 0.4310, 0.4687,  ..., 0.4818, 0.4965, 0.5070],
          [0.5327, 0.6194, 0.5995,  ..., 0.3490, 0.6037, 0.4112]],

         [[0.4487, 0.4540, 0.5039,  ..., 0.4335, 0.4577, 0.4235],
          [0.4149, 0.5516, 0.5159,  ..., 0.5166, 0.5249, 0.5304],
          [0.3844, 0.3971, 0.4085,  ..., 0.6008, 0.4915, 0.5594],
          [0.4943, 0.5441, 0.6365,  ..., 0.5960, 0.4607, 0.3560]],

         [[0.5617, 0.3773, 0.4151,  ..., 0.4572, 0.3935, 0.5111],
          [0.3817, 0.5774, 0.6163,  ..., 0.3903, 0.5037, 0.5125],
          [0.6086, 0.5550, 0.4232,  ..., 0.4292, 0.5029, 0.5292],
          [0.4975, 0.4576, 0.6401,  ..., 0.5604, 0.5548, 0.4363]],

         ...,

         [[0.5000, 0.5080, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5000, 0.5080, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5000, 0.5080, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5000, 0.5080, 0.5040,  ..., 0.5100, 0.5040, 0.5100]],

         [[0.5000, 0.5080, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5000, 0.5080, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5000, 0.5080, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5000, 0.5080, 0.5040,  ..., 0.5100, 0.5040, 0.5100]],

         [[0.5000, 0.5080, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5000, 0.5080, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5000, 0.5080, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5000, 0.5080, 0.5040,  ..., 0.5100, 0.5040, 0.5100]]]],
       device='cuda:0')
tensor([[[[0.5684, 0.5590, 0.3984,  ..., 0.4182, 0.4808, 0.5015],
          [0.5832, 0.5521, 0.5254,  ..., 0.4861, 0.5578, 0.4158],
          [0.4735, 0.3259, 0.4552,  ..., 0.4576, 0.4600, 0.5212],
          [0.5511, 0.5566, 0.5993,  ..., 0.4576, 0.5528, 0.3416]],

         [[0.4741, 0.4941, 0.4963,  ..., 0.5173, 0.4263, 0.4441],
          [0.5304, 0.4630, 0.5475,  ..., 0.4559, 0.6496, 0.6397],
          [0.5537, 0.5168, 0.5458,  ..., 0.5860, 0.5851, 0.5429],
          [0.5742, 0.5218, 0.5189,  ..., 0.5424, 0.5501, 0.3558]],

         [[0.5409, 0.5071, 0.5550,  ..., 0.3407, 0.6343, 0.5732],
          [0.4644, 0.5392, 0.6960,  ..., 0.4463, 0.4765, 0.3909],
          [0.4017, 0.5409, 0.5114,  ..., 0.3835, 0.4506, 0.5151],
          [0.4696, 0.4705, 0.5605,  ..., 0.5708, 0.6433, 0.3504]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.6307, 0.5164, 0.4598,  ..., 0.3868, 0.3549, 0.5482],
          [0.4685, 0.4951, 0.6035,  ..., 0.3702, 0.5851, 0.3442],
          [0.4637, 0.4230, 0.4647,  ..., 0.4718, 0.4925, 0.4970],
          [0.5327, 0.6114, 0.5955,  ..., 0.3390, 0.5997, 0.4012]],

         [[0.4487, 0.4460, 0.4999,  ..., 0.4235, 0.4537, 0.4135],
          [0.4149, 0.5436, 0.5119,  ..., 0.5066, 0.5209, 0.5204],
          [0.3844, 0.3891, 0.4045,  ..., 0.5908, 0.4875, 0.5494],
          [0.4943, 0.5361, 0.6325,  ..., 0.5860, 0.4567, 0.3460]],

         [[0.5617, 0.3693, 0.4111,  ..., 0.4472, 0.3895, 0.5011],
          [0.3817, 0.5694, 0.6123,  ..., 0.3803, 0.4997, 0.5025],
          [0.6086, 0.5470, 0.4192,  ..., 0.4192, 0.4989, 0.5192],
          [0.4975, 0.4496, 0.6361,  ..., 0.5504, 0.5508, 0.4263]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)[batch=20/40]:
	 Train time/batch: 19
	 Train time/sample: 38
	 Train time/batch_in_epoch: 19
	 Train time/sample_in_epoch: 38
	 Train time/token: 38912
	 Train time/token_in_epoch: 38912
	 Train memory/current_allocated_mem: 1.1423
	 Train memory/current_active_mem: 1.1423
	 Train memory/current_inactive_mem: 0.7409
	 Train memory/current_reserved_mem: 3.7979
	 Train memory/peak_allocated_mem: 2.7534
	 Train memory/peak_active_mem: 2.7534
	 Train memory/peak_inactive_mem: 0.8590
	 Train memory/peak_reserved_mem: 3.7979
	 Train memory/alloc_retries: 0
	 Train trainer/device_train_microbatch_size: 10
	 Train loss/train/total: 0.0050
	 Train metrics/train/LanguageCrossEntropy: 10.2293
	 Train metrics/train/LanguagePerplexity: 27702.3848
	 Train metrics/train/TokenAccuracy: 0.1255
	 Train throughput/batches_per_sec: 0.4249
	 Train throughput/samples_per_sec: 0.8497
	 Train throughput/device/batches_per_sec: 0.4249
	 Train throughput/device/samples_per_sec: 0.8497
	 Train throughput/tokens_per_sec: 870.1345
	 Train throughput/device/tokens_per_sec: 870.1345
	 Train time/train: 0.0212
	 Train time/val: 0.0000
	 Train time/total: 0.0212
	 Train lr-DecoupledAdamW/group0: 0.0000
	 Train time/remaining_estimate: 0.0158
	 Train metrics/shannon_entropy: 10.6343
	 Train metrics/batch_shannon_entropy: <wandb.sdk.data_types.table.Table object at 0x77b707caac90>
	 Train metrics/seq_shannon_entropy: CustomChart(table=<wandb.sdk.data_types.table.Table object at 0x77b99fb154f0>, spec=CustomChartSpec(spec_name='wandb/lineseries/v0', fields={'step': 'step', 'lineKey': 'lineKey', 'lineVal': 'lineVal'}, string_fields={'title': 'Per-Layer Shannon Entropy', 'xname': 'Block Index'}, key='', panel_type='Vega2', split_table=False))
	 Train metrics/exit_entropy: 0.6641
	 Train metrics/batch_exit_entropy: <wandb.sdk.data_types.table.Table object at 0x77b9a1b2f8f0>
	 Train metrics/seq_exit_entropy: CustomChart(table=<wandb.sdk.data_types.table.Table object at 0x77b99f873020>, spec=CustomChartSpec(spec_name='wandb/lineseries/v0', fields={'step': 'step', 'lineKey': 'lineKey', 'lineVal': 'lineVal'}, string_fields={'title': 'Per-Layer Exit Entropy', 'xname': 'Block Index'}, key='', panel_type='Vega2', split_table=False))
	 Train expert_selection/ffn_layer: <wandb.sdk.data_types.image.Image object at 0x77b7069b3920>
	 Train expert_selection/attn_o_layer: <wandb.sdk.data_types.image.Image object at 0x77b99f6a0230>
	 Train expert_selection/attn_v_layer: <wandb.sdk.data_types.image.Image object at 0x77b9a181be30>
	 Train l2_norm/moment/model.transformer.router: 0.0000
	 Train l2_norm/param/model.transformer.router: 0.3612
	 Train l2_norm/update/model.transformer.router: 0.0004
	 Train l2_norm/grad/model.transformer.router: 0.0000
	 Train l2_norm/moment/model.transformer.tau: 0.0000
	 Train l2_norm/param/model.transformer.tau: 1.0002
	 Train l2_norm/update/model.transformer.tau: 0.0000
	 Train l2_norm/grad/model.transformer.tau: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.attention.v: 0.0005
	 Train l2_norm/param/model.transformer.layers.0.attention.v: 20.4798
	 Train l2_norm/update/model.transformer.layers.0.attention.v: 0.0129
	 Train l2_norm/grad/model.transformer.layers.0.attention.v: 0.0006
	 Train l2_norm/moment/model.transformer.layers.0.attention.o: 0.0005
	 Train l2_norm/param/model.transformer.layers.0.attention.o: 22.6873
	 Train l2_norm/update/model.transformer.layers.0.attention.o: 0.0138
	 Train l2_norm/grad/model.transformer.layers.0.attention.o: 0.0006
	 Train l2_norm/moment/model.transformer.layers.0.attention.sel_v: 0.0001
	 Train l2_norm/param/model.transformer.layers.0.attention.sel_v: 2.2599
	 Train l2_norm/update/model.transformer.layers.0.attention.sel_v: 0.0008
	 Train l2_norm/grad/model.transformer.layers.0.attention.sel_v: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.attention.sel_o: 0.0001
	 Train l2_norm/param/model.transformer.layers.0.attention.sel_o: 2.2418
	 Train l2_norm/update/model.transformer.layers.0.attention.sel_o: 0.0009
	 Train l2_norm/grad/model.transformer.layers.0.attention.sel_o: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.attention.q.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attention.q.weight: 6.4836
	 Train l2_norm/update/model.transformer.layers.0.attention.q.weight: 0.0044
	 Train l2_norm/grad/model.transformer.layers.0.attention.q.weight: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.attention.k.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attention.k.weight: 6.4741
	 Train l2_norm/update/model.transformer.layers.0.attention.k.weight: 0.0044
	 Train l2_norm/grad/model.transformer.layers.0.attention.k.weight: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.ffn.keys: 0.0004
	 Train l2_norm/param/model.transformer.layers.0.ffn.keys: 14.9988
	 Train l2_norm/update/model.transformer.layers.0.ffn.keys: 0.0110
	 Train l2_norm/grad/model.transformer.layers.0.ffn.keys: 0.0005
	 Train l2_norm/moment/model.transformer.layers.0.ffn.values: 0.0010
	 Train l2_norm/param/model.transformer.layers.0.ffn.values: 7.1781
	 Train l2_norm/update/model.transformer.layers.0.ffn.values: 0.0117
	 Train l2_norm/grad/model.transformer.layers.0.ffn.values: 0.0011
	 Train l2_norm/moment/model.transformer.layers.0.ffn.expert_sel: 0.0001
	 Train l2_norm/param/model.transformer.layers.0.ffn.expert_sel: 4.7495
	 Train l2_norm/update/model.transformer.layers.0.ffn.expert_sel: 0.0035
	 Train l2_norm/grad/model.transformer.layers.0.ffn.expert_sel: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.attn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attn_pre.weight: 20.2977
	 Train l2_norm/update/model.transformer.layers.0.attn_pre.weight: 0.0003
	 Train l2_norm/grad/model.transformer.layers.0.attn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.attn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attn_post.weight: 20.2977
	 Train l2_norm/update/model.transformer.layers.0.attn_post.weight: 0.0003
	 Train l2_norm/grad/model.transformer.layers.0.attn_post.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.ffn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.ffn_pre.weight: 20.2977
	 Train l2_norm/update/model.transformer.layers.0.ffn_pre.weight: 0.0003
	 Train l2_norm/grad/model.transformer.layers.0.ffn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.ffn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.ffn_post.weight: 20.2980
	 Train l2_norm/update/model.transformer.layers.0.ffn_post.weight: 0.0003
	 Train l2_norm/grad/model.transformer.layers.0.ffn_post.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.v: 0.0003
	 Train l2_norm/param/model.transformer.layers.1.attention.v: 20.4915
	 Train l2_norm/update/model.transformer.layers.1.attention.v: 0.0124
	 Train l2_norm/grad/model.transformer.layers.1.attention.v: 0.0004
	 Train l2_norm/moment/model.transformer.layers.1.attention.o: 0.0003
	 Train l2_norm/param/model.transformer.layers.1.attention.o: 22.6906
	 Train l2_norm/update/model.transformer.layers.1.attention.o: 0.0125
	 Train l2_norm/grad/model.transformer.layers.1.attention.o: 0.0004
	 Train l2_norm/moment/model.transformer.layers.1.attention.sel_v: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.sel_v: 2.2543
	 Train l2_norm/update/model.transformer.layers.1.attention.sel_v: 0.0008
	 Train l2_norm/grad/model.transformer.layers.1.attention.sel_v: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.sel_o: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.sel_o: 2.2406
	 Train l2_norm/update/model.transformer.layers.1.attention.sel_o: 0.0007
	 Train l2_norm/grad/model.transformer.layers.1.attention.sel_o: 0.0001
	 Train l2_norm/moment/model.transformer.layers.1.attention.q.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.q.weight: 6.4905
	 Train l2_norm/update/model.transformer.layers.1.attention.q.weight: 0.0025
	 Train l2_norm/grad/model.transformer.layers.1.attention.q.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.k.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.k.weight: 6.4852
	 Train l2_norm/update/model.transformer.layers.1.attention.k.weight: 0.0026
	 Train l2_norm/grad/model.transformer.layers.1.attention.k.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn.keys: 0.0002
	 Train l2_norm/param/model.transformer.layers.1.ffn.keys: 15.0042
	 Train l2_norm/update/model.transformer.layers.1.ffn.keys: 0.0103
	 Train l2_norm/grad/model.transformer.layers.1.ffn.keys: 0.0004
	 Train l2_norm/moment/model.transformer.layers.1.ffn.values: 0.0007
	 Train l2_norm/param/model.transformer.layers.1.ffn.values: 7.1780
	 Train l2_norm/update/model.transformer.layers.1.ffn.values: 0.0128
	 Train l2_norm/grad/model.transformer.layers.1.ffn.values: 0.0009
	 Train l2_norm/moment/model.transformer.layers.1.ffn.expert_sel: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.ffn.expert_sel: 4.7526
	 Train l2_norm/update/model.transformer.layers.1.ffn.expert_sel: 0.0032
	 Train l2_norm/grad/model.transformer.layers.1.ffn.expert_sel: 0.0001
	 Train l2_norm/moment/model.transformer.layers.1.attn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attn_pre.weight: 20.2978
	 Train l2_norm/update/model.transformer.layers.1.attn_pre.weight: 0.0002
	 Train l2_norm/grad/model.transformer.layers.1.attn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attn_post.weight: 20.2974
	 Train l2_norm/update/model.transformer.layers.1.attn_post.weight: 0.0002
	 Train l2_norm/grad/model.transformer.layers.1.attn_post.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.ffn_pre.weight: 20.2976
	 Train l2_norm/update/model.transformer.layers.1.ffn_pre.weight: 0.0002
	 Train l2_norm/grad/model.transformer.layers.1.ffn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.ffn_post.weight: 20.2977
	 Train l2_norm/update/model.transformer.layers.1.ffn_post.weight: 0.0003
	 Train l2_norm/grad/model.transformer.layers.1.ffn_post.weight: 0.0000
	 Train l2_norm/moment/model.embedding.weight: 0.0002
	 Train l2_norm/param/model.embedding.weight: 221.7079
	 Train l2_norm/update/model.embedding.weight: 0.0082
	 Train l2_norm/grad/model.embedding.weight: 0.0002
	 Train l2_norm/moment/model.lm_head.weight: 0.0005
	 Train l2_norm/param/model.lm_head.weight: 127.9877
	 Train l2_norm/update/model.lm_head.weight: 0.0305
	 Train l2_norm/grad/model.lm_head.weight: 0.0007
	 Train l2_norm/moment/model.lm_head.bias: 0.0000
	 Train l2_norm/param/model.lm_head.bias: 6.3164
	 Train l2_norm/update/model.lm_head.bias: 0.0025
	 Train l2_norm/grad/model.lm_head.bias: 0.0000
	 Train l2_norm/moment/model.out_norm.weight: 0.0000
	 Train l2_norm/param/model.out_norm.weight: 20.2971
	 Train l2_norm/update/model.out_norm.weight: 0.0003
	 Train l2_norm/grad/model.out_norm.weight: 0.0000
	 Train l2_norm/grad/global: 0.0020
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([-2.3283e-10,  8.0000e-03,  4.0000e-03,  8.0000e-03, -6.0000e-03,
         8.0000e-03,  8.0000e-03,  1.0000e-02,  4.0000e-03,  1.0000e-02],
       device='cuda:0')
selected experts tensor([1379, 1660, 1397, 1372, 1768,  723, 1299, 2205, 1956, 2625],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1741., 1687., 1577., 1629., 1607., 1658., 1591., 1610., 1652., 1632.],
        [1379., 1660., 1397., 1372., 1768.,  723., 1299., 2205., 1956., 2625.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5626, 0.5351, 0.5748,  ..., 0.5279, 0.5723, 0.5297],
          [0.3087, 0.5492, 0.4352,  ..., 0.6414, 0.4004, 0.3726],
          [0.6159, 0.5387, 0.5947,  ..., 0.3957, 0.3533, 0.4218],
          [0.4086, 0.6002, 0.5429,  ..., 0.4627, 0.6408, 0.3753]],

         [[0.4809, 0.4441, 0.6319,  ..., 0.5860, 0.6390, 0.3564],
          [0.3945, 0.4800, 0.5648,  ..., 0.4929, 0.4317, 0.6118],
          [0.5389, 0.6715, 0.4270,  ..., 0.5170, 0.4293, 0.7580],
          [0.5911, 0.7233, 0.3462,  ..., 0.6002, 0.4365, 0.5811]],

         [[0.4672, 0.4724, 0.5124,  ..., 0.6423, 0.5407, 0.6894],
          [0.5392, 0.4854, 0.4477,  ..., 0.5697, 0.4374, 0.4237],
          [0.4484, 0.4899, 0.5951,  ..., 0.6295, 0.4545, 0.5396],
          [0.6278, 0.4886, 0.5233,  ..., 0.5470, 0.5352, 0.5204]],

         ...,

         [[0.3824, 0.5689, 0.5682,  ..., 0.3237, 0.5733, 0.5968],
          [0.4416, 0.4973, 0.6382,  ..., 0.4429, 0.5514, 0.5406],
          [0.5262, 0.3657, 0.5257,  ..., 0.4714, 0.5984, 0.5163],
          [0.5438, 0.5827, 0.6749,  ..., 0.4970, 0.4760, 0.3900]],

         [[0.5358, 0.6706, 0.3670,  ..., 0.5203, 0.5914, 0.5125],
          [0.3871, 0.4173, 0.5373,  ..., 0.4477, 0.6435, 0.3636],
          [0.4044, 0.5251, 0.4342,  ..., 0.3915, 0.5071, 0.4453],
          [0.6089, 0.3886, 0.3277,  ..., 0.5841, 0.5531, 0.4508]],

         [[0.4219, 0.4688, 0.5753,  ..., 0.4439, 0.4449, 0.4968],
          [0.4443, 0.5955, 0.6445,  ..., 0.6332, 0.4202, 0.5327],
          [0.6599, 0.5679, 0.3199,  ..., 0.5412, 0.3713, 0.6062],
          [0.5126, 0.4450, 0.3808,  ..., 0.5499, 0.6354, 0.4675]]],


        [[[0.5044, 0.5363, 0.6550,  ..., 0.5058, 0.4417, 0.3092],
          [0.5392, 0.5210, 0.6265,  ..., 0.3584, 0.5337, 0.6466],
          [0.5095, 0.5583, 0.4819,  ..., 0.5300, 0.6622, 0.4214],
          [0.5032, 0.4973, 0.3761,  ..., 0.5513, 0.5914, 0.5982]],

         [[0.3765, 0.3817, 0.5014,  ..., 0.3683, 0.5088, 0.4011],
          [0.5248, 0.5252, 0.4366,  ..., 0.5817, 0.4894, 0.4952],
          [0.6010, 0.4949, 0.4504,  ..., 0.6110, 0.4898, 0.4501],
          [0.4196, 0.4395, 0.5562,  ..., 0.7258, 0.3349, 0.5682]],

         [[0.4148, 0.4335, 0.3734,  ..., 0.3755, 0.5800, 0.4658],
          [0.3592, 0.4895, 0.4265,  ..., 0.5601, 0.5236, 0.4942],
          [0.3973, 0.5188, 0.5829,  ..., 0.5627, 0.5021, 0.4252],
          [0.4409, 0.6792, 0.6799,  ..., 0.6332, 0.5155, 0.5337]],

         ...,

         [[0.5688, 0.4908, 0.5729,  ..., 0.5656, 0.3614, 0.4460],
          [0.4717, 0.4130, 0.3962,  ..., 0.6880, 0.5620, 0.5725],
          [0.4690, 0.6058, 0.5342,  ..., 0.5248, 0.4369, 0.5122],
          [0.5280, 0.5156, 0.4974,  ..., 0.4837, 0.5557, 0.5806]],

         [[0.5363, 0.6243, 0.4046,  ..., 0.6673, 0.7467, 0.4280],
          [0.5595, 0.6105, 0.5829,  ..., 0.4736, 0.5339, 0.4285],
          [0.5063, 0.5578, 0.5547,  ..., 0.5974, 0.3731, 0.5024],
          [0.6052, 0.4779, 0.2676,  ..., 0.5097, 0.6363, 0.5302]],

         [[0.5134, 0.4211, 0.2508,  ..., 0.3461, 0.5242, 0.5778],
          [0.5193, 0.5179, 0.6418,  ..., 0.5922, 0.5247, 0.5701],
          [0.4547, 0.5528, 0.4439,  ..., 0.4386, 0.5814, 0.4887],
          [0.4981, 0.5465, 0.5815,  ..., 0.5550, 0.4701, 0.4687]]]],
       device='cuda:0')
tensor([[[[0.5646, 0.5351, 0.5808,  ..., 0.5199, 0.5703, 0.5237],
          [0.3107, 0.5492, 0.4412,  ..., 0.6334, 0.3984, 0.3666],
          [0.6179, 0.5387, 0.6007,  ..., 0.3877, 0.3513, 0.4158],
          [0.4106, 0.6002, 0.5489,  ..., 0.4547, 0.6388, 0.3693]],

         [[0.4829, 0.4441, 0.6379,  ..., 0.5780, 0.6370, 0.3504],
          [0.3965, 0.4800, 0.5708,  ..., 0.4849, 0.4297, 0.6058],
          [0.5409, 0.6715, 0.4330,  ..., 0.5090, 0.4273, 0.7520],
          [0.5931, 0.7233, 0.3522,  ..., 0.5922, 0.4345, 0.5751]],

         [[0.4692, 0.4724, 0.5184,  ..., 0.6343, 0.5387, 0.6834],
          [0.5412, 0.4854, 0.4537,  ..., 0.5617, 0.4354, 0.4177],
          [0.4504, 0.4899, 0.6011,  ..., 0.6215, 0.4525, 0.5336],
          [0.6298, 0.4886, 0.5293,  ..., 0.5390, 0.5332, 0.5144]],

         ...,

         [[0.3844, 0.5689, 0.5742,  ..., 0.3157, 0.5713, 0.5908],
          [0.4436, 0.4973, 0.6442,  ..., 0.4349, 0.5494, 0.5346],
          [0.5282, 0.3657, 0.5317,  ..., 0.4634, 0.5964, 0.5103],
          [0.5458, 0.5827, 0.6809,  ..., 0.4890, 0.4740, 0.3840]],

         [[0.5378, 0.6706, 0.3730,  ..., 0.5123, 0.5894, 0.5065],
          [0.3891, 0.4173, 0.5433,  ..., 0.4397, 0.6415, 0.3576],
          [0.4064, 0.5251, 0.4402,  ..., 0.3835, 0.5051, 0.4393],
          [0.6109, 0.3886, 0.3337,  ..., 0.5761, 0.5511, 0.4448]],

         [[0.4239, 0.4688, 0.5813,  ..., 0.4359, 0.4429, 0.4908],
          [0.4463, 0.5955, 0.6505,  ..., 0.6252, 0.4182, 0.5267],
          [0.6619, 0.5679, 0.3259,  ..., 0.5332, 0.3693, 0.6002],
          [0.5146, 0.4450, 0.3868,  ..., 0.5419, 0.6334, 0.4615]]],


        [[[0.5064, 0.5363, 0.6610,  ..., 0.4978, 0.4397, 0.3032],
          [0.5412, 0.5210, 0.6325,  ..., 0.3504, 0.5317, 0.6406],
          [0.5115, 0.5583, 0.4879,  ..., 0.5220, 0.6602, 0.4154],
          [0.5052, 0.4973, 0.3821,  ..., 0.5433, 0.5894, 0.5922]],

         [[0.3785, 0.3817, 0.5074,  ..., 0.3603, 0.5068, 0.3951],
          [0.5268, 0.5252, 0.4426,  ..., 0.5737, 0.4874, 0.4892],
          [0.6030, 0.4949, 0.4564,  ..., 0.6030, 0.4878, 0.4441],
          [0.4216, 0.4395, 0.5622,  ..., 0.7178, 0.3329, 0.5622]],

         [[0.4168, 0.4335, 0.3794,  ..., 0.3675, 0.5780, 0.4598],
          [0.3612, 0.4895, 0.4325,  ..., 0.5521, 0.5216, 0.4882],
          [0.3993, 0.5188, 0.5889,  ..., 0.5547, 0.5001, 0.4192],
          [0.4429, 0.6792, 0.6859,  ..., 0.6252, 0.5135, 0.5277]],

         ...,

         [[0.5708, 0.4908, 0.5789,  ..., 0.5576, 0.3594, 0.4400],
          [0.4737, 0.4130, 0.4022,  ..., 0.6800, 0.5600, 0.5665],
          [0.4710, 0.6058, 0.5402,  ..., 0.5168, 0.4349, 0.5062],
          [0.5300, 0.5156, 0.5034,  ..., 0.4757, 0.5537, 0.5746]],

         [[0.5383, 0.6243, 0.4106,  ..., 0.6593, 0.7447, 0.4220],
          [0.5615, 0.6105, 0.5889,  ..., 0.4656, 0.5319, 0.4225],
          [0.5083, 0.5578, 0.5607,  ..., 0.5894, 0.3711, 0.4964],
          [0.6072, 0.4779, 0.2736,  ..., 0.5017, 0.6343, 0.5242]],

         [[0.5154, 0.4211, 0.2568,  ..., 0.3381, 0.5222, 0.5718],
          [0.5213, 0.5179, 0.6478,  ..., 0.5842, 0.5227, 0.5641],
          [0.4567, 0.5528, 0.4499,  ..., 0.4306, 0.5794, 0.4827],
          [0.5001, 0.5465, 0.5875,  ..., 0.5470, 0.4681, 0.4627]]]],
       device='cuda:0', requires_grad=True)
tensor([-2.0000e-03, -2.3283e-10, -6.0000e-03,  2.0000e-03, -4.0000e-03,
        -6.0000e-03,  4.0000e-03,  8.0000e-03,  2.0000e-03,  6.0000e-03],
       device='cuda:0')
selected experts tensor([1737, 1552, 1640, 1807, 1676, 1555, 1568, 1645, 1641, 1563],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4588, 0.5534, 0.5249,  ..., 0.3911, 0.6737, 0.5485],
          [0.5551, 0.5548, 0.6338,  ..., 0.4209, 0.3996, 0.5857],
          [0.5302, 0.4415, 0.5886,  ..., 0.4925, 0.4556, 0.3795],
          [0.3848, 0.4602, 0.5001,  ..., 0.4641, 0.5269, 0.5473]],

         [[0.5260, 0.3134, 0.4104,  ..., 0.4737, 0.5647, 0.6595],
          [0.5741, 0.5048, 0.2608,  ..., 0.4066, 0.5609, 0.5147],
          [0.5225, 0.5644, 0.6205,  ..., 0.3527, 0.5425, 0.5757],
          [0.4759, 0.5367, 0.4916,  ..., 0.3067, 0.6172, 0.4374]],

         [[0.4243, 0.5345, 0.6004,  ..., 0.4527, 0.5729, 0.4117],
          [0.4960, 0.3758, 0.2815,  ..., 0.4890, 0.4780, 0.3462],
          [0.4816, 0.4631, 0.4706,  ..., 0.4616, 0.5041, 0.3902],
          [0.5211, 0.5204, 0.5143,  ..., 0.6312, 0.5031, 0.5287]],

         ...,

         [[0.6814, 0.5953, 0.6650,  ..., 0.5505, 0.3903, 0.4454],
          [0.3112, 0.6357, 0.5910,  ..., 0.4783, 0.5891, 0.4541],
          [0.4129, 0.5764, 0.6338,  ..., 0.4776, 0.5645, 0.5352],
          [0.5290, 0.4981, 0.6042,  ..., 0.6570, 0.5618, 0.3677]],

         [[0.5822, 0.6009, 0.3931,  ..., 0.5591, 0.4824, 0.6120],
          [0.4981, 0.4066, 0.6028,  ..., 0.4819, 0.5459, 0.6709],
          [0.5878, 0.3726, 0.5032,  ..., 0.5054, 0.5074, 0.3578],
          [0.4253, 0.5178, 0.5986,  ..., 0.3289, 0.4493, 0.6717]],

         [[0.6651, 0.4633, 0.4284,  ..., 0.4047, 0.3926, 0.6083],
          [0.5254, 0.5910, 0.5786,  ..., 0.5306, 0.3670, 0.7142],
          [0.3987, 0.5096, 0.6182,  ..., 0.4247, 0.4071, 0.4355],
          [0.4503, 0.5362, 0.5452,  ..., 0.3768, 0.5457, 0.5214]]],


        [[[0.4881, 0.5251, 0.5258,  ..., 0.5735, 0.4944, 0.5502],
          [0.4191, 0.6185, 0.6135,  ..., 0.5454, 0.4517, 0.3915],
          [0.6323, 0.5136, 0.4194,  ..., 0.5802, 0.4746, 0.5451],
          [0.4105, 0.4372, 0.5324,  ..., 0.5134, 0.4442, 0.6199]],

         [[0.3673, 0.5128, 0.3589,  ..., 0.4024, 0.4332, 0.4526],
          [0.4843, 0.5306, 0.5858,  ..., 0.4488, 0.4696, 0.6700],
          [0.6080, 0.4379, 0.5662,  ..., 0.5231, 0.4270, 0.5584],
          [0.4941, 0.5439, 0.4156,  ..., 0.3491, 0.3334, 0.5378]],

         [[0.4775, 0.3315, 0.5171,  ..., 0.5113, 0.6615, 0.2962],
          [0.6038, 0.5716, 0.5182,  ..., 0.4185, 0.5582, 0.5757],
          [0.4751, 0.3832, 0.5286,  ..., 0.5251, 0.4614, 0.4331],
          [0.3857, 0.4981, 0.6500,  ..., 0.4923, 0.3884, 0.2771]],

         ...,

         [[0.5447, 0.4233, 0.4175,  ..., 0.5372, 0.5575, 0.4216],
          [0.5341, 0.6166, 0.4903,  ..., 0.3271, 0.4643, 0.5277],
          [0.5068, 0.4123, 0.3334,  ..., 0.5124, 0.5507, 0.5325],
          [0.5129, 0.4867, 0.4643,  ..., 0.5630, 0.5271, 0.3897]],

         [[0.5170, 0.4660, 0.5153,  ..., 0.4104, 0.5376, 0.5620],
          [0.4286, 0.5579, 0.4862,  ..., 0.5330, 0.4577, 0.5342],
          [0.4896, 0.6429, 0.5681,  ..., 0.4029, 0.3788, 0.6752],
          [0.4229, 0.5673, 0.4294,  ..., 0.4100, 0.5662, 0.4808]],

         [[0.6019, 0.4333, 0.4687,  ..., 0.4643, 0.4260, 0.4902],
          [0.5222, 0.4686, 0.6320,  ..., 0.5265, 0.3797, 0.3605],
          [0.6332, 0.4883, 0.5507,  ..., 0.5282, 0.4123, 0.5225],
          [0.3903, 0.5697, 0.6650,  ..., 0.5510, 0.4677, 0.4420]]]],
       device='cuda:0')
tensor([[[[0.4608, 0.5574, 0.5209,  ..., 0.3951, 0.6697, 0.5465],
          [0.5571, 0.5588, 0.6298,  ..., 0.4249, 0.3956, 0.5837],
          [0.5322, 0.4455, 0.5846,  ..., 0.4965, 0.4516, 0.3775],
          [0.3868, 0.4642, 0.4961,  ..., 0.4681, 0.5229, 0.5453]],

         [[0.5280, 0.3174, 0.4064,  ..., 0.4777, 0.5607, 0.6575],
          [0.5761, 0.5088, 0.2568,  ..., 0.4106, 0.5569, 0.5127],
          [0.5245, 0.5684, 0.6165,  ..., 0.3567, 0.5385, 0.5737],
          [0.4779, 0.5407, 0.4876,  ..., 0.3107, 0.6132, 0.4354]],

         [[0.4263, 0.5385, 0.5964,  ..., 0.4567, 0.5689, 0.4097],
          [0.4980, 0.3798, 0.2775,  ..., 0.4930, 0.4740, 0.3442],
          [0.4836, 0.4671, 0.4666,  ..., 0.4656, 0.5001, 0.3882],
          [0.5231, 0.5244, 0.5103,  ..., 0.6352, 0.4991, 0.5267]],

         ...,

         [[0.6834, 0.5993, 0.6610,  ..., 0.5545, 0.3863, 0.4434],
          [0.3132, 0.6397, 0.5870,  ..., 0.4823, 0.5851, 0.4521],
          [0.4149, 0.5804, 0.6298,  ..., 0.4816, 0.5605, 0.5332],
          [0.5310, 0.5021, 0.6002,  ..., 0.6610, 0.5578, 0.3657]],

         [[0.5842, 0.6049, 0.3891,  ..., 0.5631, 0.4784, 0.6100],
          [0.5001, 0.4106, 0.5988,  ..., 0.4859, 0.5419, 0.6689],
          [0.5898, 0.3766, 0.4992,  ..., 0.5094, 0.5034, 0.3558],
          [0.4273, 0.5218, 0.5946,  ..., 0.3329, 0.4453, 0.6697]],

         [[0.6671, 0.4673, 0.4244,  ..., 0.4087, 0.3886, 0.6063],
          [0.5274, 0.5950, 0.5746,  ..., 0.5346, 0.3630, 0.7122],
          [0.4007, 0.5136, 0.6142,  ..., 0.4287, 0.4031, 0.4335],
          [0.4523, 0.5402, 0.5412,  ..., 0.3808, 0.5417, 0.5194]]],


        [[[0.4901, 0.5291, 0.5218,  ..., 0.5775, 0.4904, 0.5482],
          [0.4211, 0.6225, 0.6095,  ..., 0.5494, 0.4477, 0.3895],
          [0.6343, 0.5176, 0.4154,  ..., 0.5842, 0.4706, 0.5431],
          [0.4125, 0.4412, 0.5284,  ..., 0.5174, 0.4402, 0.6179]],

         [[0.3693, 0.5168, 0.3549,  ..., 0.4064, 0.4292, 0.4506],
          [0.4863, 0.5346, 0.5818,  ..., 0.4528, 0.4656, 0.6680],
          [0.6100, 0.4419, 0.5622,  ..., 0.5271, 0.4230, 0.5564],
          [0.4961, 0.5479, 0.4116,  ..., 0.3531, 0.3294, 0.5358]],

         [[0.4795, 0.3355, 0.5131,  ..., 0.5153, 0.6575, 0.2942],
          [0.6058, 0.5756, 0.5142,  ..., 0.4225, 0.5542, 0.5737],
          [0.4771, 0.3872, 0.5246,  ..., 0.5291, 0.4574, 0.4311],
          [0.3877, 0.5021, 0.6460,  ..., 0.4963, 0.3844, 0.2751]],

         ...,

         [[0.5467, 0.4273, 0.4135,  ..., 0.5412, 0.5535, 0.4196],
          [0.5361, 0.6206, 0.4863,  ..., 0.3311, 0.4603, 0.5257],
          [0.5088, 0.4163, 0.3294,  ..., 0.5164, 0.5467, 0.5305],
          [0.5149, 0.4907, 0.4603,  ..., 0.5670, 0.5231, 0.3877]],

         [[0.5190, 0.4700, 0.5113,  ..., 0.4144, 0.5336, 0.5600],
          [0.4306, 0.5619, 0.4822,  ..., 0.5370, 0.4537, 0.5322],
          [0.4916, 0.6469, 0.5641,  ..., 0.4069, 0.3748, 0.6732],
          [0.4249, 0.5713, 0.4254,  ..., 0.4140, 0.5622, 0.4788]],

         [[0.6039, 0.4373, 0.4647,  ..., 0.4683, 0.4220, 0.4882],
          [0.5242, 0.4726, 0.6280,  ..., 0.5305, 0.3757, 0.3585],
          [0.6352, 0.4923, 0.5467,  ..., 0.5322, 0.4083, 0.5205],
          [0.3923, 0.5737, 0.6610,  ..., 0.5550, 0.4637, 0.4400]]]],
       device='cuda:0', requires_grad=True)
tensor([-2.0000e-03, -4.0000e-03,  4.0000e-03,  1.2000e-02,  2.3283e-10,
         8.0000e-03, -1.0000e-02, -4.0000e-03,  4.0000e-03,  2.0000e-03],
       device='cuda:0')
selected experts tensor([1620, 1579, 1643, 1841, 1644, 1461, 1740, 1568, 1587, 1701],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4798, 0.6081, 0.5739,  ..., 0.4428, 0.4332, 0.6522],
          [0.4395, 0.6755, 0.4280,  ..., 0.5231, 0.5523, 0.5482],
          [0.4828, 0.4891, 0.5362,  ..., 0.5299, 0.4724, 0.4272],
          [0.5692, 0.5040, 0.6165,  ..., 0.5740, 0.4428, 0.5343]],

         [[0.4504, 0.5635, 0.4651,  ..., 0.4682, 0.5513, 0.4879],
          [0.4315, 0.5797, 0.4802,  ..., 0.5440, 0.5099, 0.4168],
          [0.4736, 0.4929, 0.5461,  ..., 0.3528, 0.3690, 0.5519],
          [0.6033, 0.4393, 0.6767,  ..., 0.6204, 0.3717, 0.5591]],

         [[0.4574, 0.4949, 0.6311,  ..., 0.5488, 0.4967, 0.5892],
          [0.4898, 0.4613, 0.4618,  ..., 0.5735, 0.5098, 0.6307],
          [0.5021, 0.4987, 0.4954,  ..., 0.3755, 0.4755, 0.5230],
          [0.4930, 0.5740, 0.5966,  ..., 0.4804, 0.4994, 0.4117]],

         ...,

         [[0.6056, 0.5919, 0.2982,  ..., 0.3828, 0.4702, 0.7572],
          [0.4795, 0.4076, 0.5914,  ..., 0.6135, 0.5343, 0.5321],
          [0.6075, 0.4757, 0.4085,  ..., 0.3439, 0.6241, 0.7046],
          [0.5515, 0.5616, 0.3873,  ..., 0.4510, 0.4600, 0.5082]],

         [[0.4233, 0.4148, 0.5601,  ..., 0.4329, 0.4792, 0.6657],
          [0.5572, 0.5415, 0.5357,  ..., 0.5088, 0.6329, 0.5390],
          [0.4037, 0.3609, 0.6302,  ..., 0.4471, 0.5271, 0.5835],
          [0.5973, 0.4919, 0.3689,  ..., 0.5066, 0.4167, 0.4993]],

         [[0.6184, 0.4325, 0.4333,  ..., 0.5972, 0.6058, 0.6486],
          [0.5024, 0.4944, 0.5250,  ..., 0.5606, 0.5724, 0.6228],
          [0.3708, 0.2621, 0.3707,  ..., 0.4166, 0.2917, 0.4352],
          [0.6139, 0.5754, 0.3962,  ..., 0.4907, 0.5318, 0.4010]]],


        [[[0.4922, 0.5616, 0.4137,  ..., 0.5005, 0.5139, 0.6073],
          [0.3872, 0.5417, 0.4555,  ..., 0.6770, 0.4831, 0.3900],
          [0.4986, 0.4205, 0.5071,  ..., 0.3360, 0.5542, 0.4168],
          [0.5696, 0.4403, 0.4673,  ..., 0.4899, 0.5318, 0.4565]],

         [[0.6425, 0.4738, 0.5758,  ..., 0.4195, 0.5511, 0.6195],
          [0.4713, 0.5792, 0.4538,  ..., 0.5630, 0.5133, 0.6404],
          [0.5313, 0.5417, 0.6211,  ..., 0.4954, 0.5276, 0.3473],
          [0.4858, 0.5573, 0.5596,  ..., 0.5127, 0.5440, 0.3881]],

         [[0.4776, 0.6445, 0.4864,  ..., 0.4880, 0.4608, 0.5138],
          [0.4721, 0.5303, 0.3227,  ..., 0.4524, 0.5303, 0.5121],
          [0.4689, 0.4461, 0.5545,  ..., 0.5272, 0.4055, 0.3464],
          [0.6964, 0.5787, 0.6142,  ..., 0.5551, 0.4957, 0.4987]],

         ...,

         [[0.5978, 0.5233, 0.5274,  ..., 0.4954, 0.6282, 0.5138],
          [0.4305, 0.5678, 0.4894,  ..., 0.3912, 0.5915, 0.5202],
          [0.5558, 0.4205, 0.6843,  ..., 0.4909, 0.4101, 0.5606],
          [0.4108, 0.5167, 0.4004,  ..., 0.3273, 0.5359, 0.5768]],

         [[0.5606, 0.4366, 0.4543,  ..., 0.5387, 0.3907, 0.6540],
          [0.5070, 0.5410, 0.4930,  ..., 0.4233, 0.6601, 0.4376],
          [0.4694, 0.5041, 0.5644,  ..., 0.5124, 0.5024, 0.3665],
          [0.4061, 0.6679, 0.5062,  ..., 0.3861, 0.4380, 0.4291]],

         [[0.3561, 0.4839, 0.3762,  ..., 0.5673, 0.4318, 0.7195],
          [0.4004, 0.4954, 0.4891,  ..., 0.3982, 0.5093, 0.5864],
          [0.4373, 0.4995, 0.5451,  ..., 0.3982, 0.4545, 0.3473],
          [0.5118, 0.5863, 0.6202,  ..., 0.5401, 0.3461, 0.6270]]]],
       device='cuda:0')
tensor([[[[0.4948, 0.6211, 0.5789,  ..., 0.4458, 0.4182, 0.6352],
          [0.4545, 0.6885, 0.4330,  ..., 0.5261, 0.5373, 0.5312],
          [0.4978, 0.5021, 0.5412,  ..., 0.5329, 0.4574, 0.4102],
          [0.5842, 0.5170, 0.6215,  ..., 0.5770, 0.4278, 0.5173]],

         [[0.4654, 0.5765, 0.4701,  ..., 0.4712, 0.5363, 0.4709],
          [0.4465, 0.5927, 0.4852,  ..., 0.5470, 0.4949, 0.3998],
          [0.4886, 0.5059, 0.5511,  ..., 0.3558, 0.3540, 0.5349],
          [0.6183, 0.4523, 0.6817,  ..., 0.6234, 0.3567, 0.5421]],

         [[0.4724, 0.5079, 0.6361,  ..., 0.5518, 0.4817, 0.5722],
          [0.5048, 0.4743, 0.4668,  ..., 0.5765, 0.4948, 0.6137],
          [0.5171, 0.5117, 0.5004,  ..., 0.3785, 0.4605, 0.5060],
          [0.5080, 0.5870, 0.6016,  ..., 0.4834, 0.4844, 0.3947]],

         ...,

         [[0.6206, 0.6049, 0.3032,  ..., 0.3858, 0.4552, 0.7402],
          [0.4945, 0.4206, 0.5964,  ..., 0.6165, 0.5193, 0.5151],
          [0.6225, 0.4887, 0.4135,  ..., 0.3469, 0.6091, 0.6876],
          [0.5665, 0.5746, 0.3923,  ..., 0.4540, 0.4450, 0.4912]],

         [[0.4383, 0.4278, 0.5651,  ..., 0.4359, 0.4642, 0.6487],
          [0.5722, 0.5545, 0.5407,  ..., 0.5118, 0.6179, 0.5220],
          [0.4187, 0.3739, 0.6352,  ..., 0.4501, 0.5121, 0.5665],
          [0.6123, 0.5049, 0.3739,  ..., 0.5096, 0.4017, 0.4823]],

         [[0.6334, 0.4455, 0.4383,  ..., 0.6002, 0.5908, 0.6316],
          [0.5174, 0.5074, 0.5300,  ..., 0.5636, 0.5574, 0.6058],
          [0.3858, 0.2751, 0.3757,  ..., 0.4196, 0.2767, 0.4182],
          [0.6289, 0.5884, 0.4012,  ..., 0.4937, 0.5168, 0.3840]]],


        [[[0.5072, 0.5746, 0.4187,  ..., 0.5035, 0.4989, 0.5903],
          [0.4022, 0.5547, 0.4605,  ..., 0.6800, 0.4681, 0.3730],
          [0.5136, 0.4335, 0.5121,  ..., 0.3390, 0.5392, 0.3998],
          [0.5846, 0.4533, 0.4723,  ..., 0.4929, 0.5168, 0.4395]],

         [[0.6575, 0.4868, 0.5808,  ..., 0.4225, 0.5361, 0.6025],
          [0.4863, 0.5922, 0.4588,  ..., 0.5660, 0.4983, 0.6234],
          [0.5463, 0.5547, 0.6261,  ..., 0.4984, 0.5126, 0.3303],
          [0.5008, 0.5703, 0.5646,  ..., 0.5157, 0.5290, 0.3711]],

         [[0.4926, 0.6575, 0.4914,  ..., 0.4910, 0.4458, 0.4968],
          [0.4871, 0.5433, 0.3277,  ..., 0.4554, 0.5153, 0.4951],
          [0.4839, 0.4591, 0.5595,  ..., 0.5302, 0.3905, 0.3294],
          [0.7114, 0.5917, 0.6192,  ..., 0.5581, 0.4807, 0.4817]],

         ...,

         [[0.6128, 0.5363, 0.5324,  ..., 0.4984, 0.6132, 0.4968],
          [0.4455, 0.5808, 0.4944,  ..., 0.3942, 0.5765, 0.5032],
          [0.5708, 0.4335, 0.6893,  ..., 0.4939, 0.3951, 0.5436],
          [0.4258, 0.5297, 0.4054,  ..., 0.3303, 0.5209, 0.5598]],

         [[0.5756, 0.4496, 0.4593,  ..., 0.5417, 0.3757, 0.6370],
          [0.5220, 0.5540, 0.4980,  ..., 0.4263, 0.6451, 0.4206],
          [0.4844, 0.5171, 0.5694,  ..., 0.5154, 0.4874, 0.3495],
          [0.4211, 0.6809, 0.5112,  ..., 0.3891, 0.4230, 0.4121]],

         [[0.3711, 0.4969, 0.3812,  ..., 0.5703, 0.4168, 0.7025],
          [0.4154, 0.5084, 0.4941,  ..., 0.4012, 0.4943, 0.5694],
          [0.4523, 0.5125, 0.5501,  ..., 0.4012, 0.4395, 0.3303],
          [0.5268, 0.5993, 0.6252,  ..., 0.5431, 0.3311, 0.6100]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([-0.0150, -0.0130, -0.0050,  0.0010, -0.0170,  0.0190, -0.0190, -0.0030,
         0.0150,  0.0170], device='cuda:0')
selected experts tensor([1455, 1929, 1340, 1695, 1681, 1487, 2234, 1733, 1350, 1480],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6202, 0.4504, 0.5403,  ..., 0.4032, 0.3828, 0.6433],
          [0.5071, 0.5450, 0.4911,  ..., 0.4301, 0.5524, 0.5707],
          [0.6443, 0.5163, 0.4943,  ..., 0.5635, 0.4927, 0.5357],
          [0.6193, 0.5725, 0.4409,  ..., 0.4377, 0.5625, 0.4277]],

         [[0.4661, 0.5477, 0.6117,  ..., 0.3247, 0.2852, 0.5485],
          [0.5181, 0.3682, 0.5145,  ..., 0.4348, 0.4718, 0.4107],
          [0.4383, 0.5773, 0.4347,  ..., 0.4187, 0.5577, 0.2779],
          [0.4302, 0.6350, 0.6206,  ..., 0.3810, 0.5608, 0.5307]],

         [[0.4538, 0.4434, 0.4237,  ..., 0.4329, 0.4870, 0.6424],
          [0.4164, 0.5404, 0.5232,  ..., 0.3471, 0.5042, 0.4348],
          [0.5988, 0.5450, 0.4128,  ..., 0.4009, 0.3760, 0.4818],
          [0.4776, 0.5874, 0.5544,  ..., 0.5244, 0.5237, 0.4768]],

         ...,

         [[0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5030, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5030, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5030, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5030, 0.5090]],

         [[0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5030, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5030, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5030, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5030, 0.5090]],

         [[0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5030, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5030, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5030, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5030, 0.5090]]],


        [[[0.5261, 0.5296, 0.4256,  ..., 0.3568, 0.6204, 0.4964],
          [0.4485, 0.4986, 0.4971,  ..., 0.4784, 0.5652, 0.5700],
          [0.5557, 0.4796, 0.3955,  ..., 0.5328, 0.5488, 0.4553],
          [0.5243, 0.4451, 0.5117,  ..., 0.5290, 0.4752, 0.3603]],

         [[0.3994, 0.5321, 0.4034,  ..., 0.3471, 0.4279, 0.4859],
          [0.4659, 0.6175, 0.5991,  ..., 0.4296, 0.4509, 0.5189],
          [0.4934, 0.6467, 0.4404,  ..., 0.3675, 0.4028, 0.5380],
          [0.4036, 0.6610, 0.5115,  ..., 0.4785, 0.5352, 0.4454]],

         [[0.5704, 0.6016, 0.4447,  ..., 0.3648, 0.4555, 0.4027],
          [0.4666, 0.4931, 0.5825,  ..., 0.4215, 0.6463, 0.4540],
          [0.5271, 0.4695, 0.5944,  ..., 0.4955, 0.5468, 0.4773],
          [0.3933, 0.6170, 0.4609,  ..., 0.5314, 0.5320, 0.3756]],

         ...,

         [[0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5030, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5030, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5030, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5030, 0.5090]],

         [[0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5030, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5030, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5030, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5030, 0.5090]],

         [[0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5030, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5030, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5030, 0.5090],
          [0.5010, 0.5070, 0.5050,  ..., 0.5090, 0.5030, 0.5090]]]],
       device='cuda:0')
tensor([[[[0.6192, 0.4434, 0.5353,  ..., 0.3942, 0.3798, 0.6343],
          [0.5061, 0.5380, 0.4861,  ..., 0.4211, 0.5494, 0.5617],
          [0.6433, 0.5093, 0.4893,  ..., 0.5545, 0.4897, 0.5267],
          [0.6183, 0.5655, 0.4359,  ..., 0.4287, 0.5595, 0.4187]],

         [[0.4651, 0.5407, 0.6067,  ..., 0.3157, 0.2822, 0.5395],
          [0.5171, 0.3612, 0.5095,  ..., 0.4258, 0.4688, 0.4017],
          [0.4373, 0.5703, 0.4297,  ..., 0.4097, 0.5547, 0.2689],
          [0.4292, 0.6280, 0.6156,  ..., 0.3720, 0.5578, 0.5217]],

         [[0.4528, 0.4364, 0.4187,  ..., 0.4239, 0.4840, 0.6334],
          [0.4154, 0.5334, 0.5182,  ..., 0.3381, 0.5012, 0.4258],
          [0.5978, 0.5380, 0.4078,  ..., 0.3919, 0.3730, 0.4728],
          [0.4766, 0.5804, 0.5494,  ..., 0.5154, 0.5207, 0.4678]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5251, 0.5226, 0.4206,  ..., 0.3478, 0.6174, 0.4874],
          [0.4475, 0.4916, 0.4921,  ..., 0.4694, 0.5622, 0.5610],
          [0.5547, 0.4726, 0.3905,  ..., 0.5238, 0.5458, 0.4463],
          [0.5233, 0.4381, 0.5067,  ..., 0.5200, 0.4722, 0.3513]],

         [[0.3984, 0.5251, 0.3984,  ..., 0.3381, 0.4249, 0.4769],
          [0.4649, 0.6105, 0.5941,  ..., 0.4206, 0.4479, 0.5099],
          [0.4924, 0.6397, 0.4354,  ..., 0.3585, 0.3998, 0.5290],
          [0.4026, 0.6540, 0.5065,  ..., 0.4695, 0.5322, 0.4364]],

         [[0.5694, 0.5946, 0.4397,  ..., 0.3558, 0.4525, 0.3937],
          [0.4656, 0.4861, 0.5775,  ..., 0.4125, 0.6433, 0.4450],
          [0.5261, 0.4625, 0.5894,  ..., 0.4865, 0.5438, 0.4683],
          [0.3923, 0.6100, 0.4559,  ..., 0.5224, 0.5290, 0.3666]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0010,  0.0070,  0.0050,  0.0090, -0.0070,  0.0090,  0.0090,  0.0090,
         0.0030,  0.0090], device='cuda:0')
selected experts tensor([1450, 2076, 1698, 2445, 2029, 1986, 1291,  659, 1583, 1167],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1620., 1579., 1643., 1841., 1644., 1461., 1740., 1568., 1587., 1701.],
        [1450., 2076., 1698., 2445., 2029., 1986., 1291.,  659., 1583., 1167.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5255, 0.4226, 0.4640,  ..., 0.5850, 0.4379, 0.5416],
          [0.6079, 0.5131, 0.5059,  ..., 0.4940, 0.4798, 0.4314],
          [0.4810, 0.5422, 0.6327,  ..., 0.3673, 0.4391, 0.4707],
          [0.4855, 0.5584, 0.4861,  ..., 0.4328, 0.4918, 0.6175]],

         [[0.4551, 0.4890, 0.5281,  ..., 0.5902, 0.5001, 0.6476],
          [0.5408, 0.4463, 0.5209,  ..., 0.5540, 0.7889, 0.6476],
          [0.6139, 0.6335, 0.6062,  ..., 0.5115, 0.5294, 0.4763],
          [0.5802, 0.5395, 0.3928,  ..., 0.4501, 0.3694, 0.4863]],

         [[0.6846, 0.5234, 0.4826,  ..., 0.6548, 0.5197, 0.5257],
          [0.4171, 0.5870, 0.5724,  ..., 0.5518, 0.3749, 0.5651],
          [0.4319, 0.4987, 0.5089,  ..., 0.6304, 0.4954, 0.5234],
          [0.4643, 0.4671, 0.4808,  ..., 0.6904, 0.5431, 0.4930]],

         ...,

         [[0.3842, 0.6244, 0.4698,  ..., 0.5916, 0.5661, 0.4082],
          [0.6403, 0.3694, 0.3770,  ..., 0.5382, 0.5814, 0.5050],
          [0.3750, 0.4550, 0.4986,  ..., 0.4063, 0.5334, 0.4139],
          [0.4481, 0.4836, 0.4969,  ..., 0.4704, 0.3694, 0.4247]],

         [[0.6088, 0.4036, 0.5330,  ..., 0.4889, 0.4717, 0.5428],
          [0.3889, 0.5737, 0.4208,  ..., 0.4939, 0.5213, 0.5620],
          [0.5348, 0.6110, 0.5383,  ..., 0.6485, 0.6532, 0.4615],
          [0.4370, 0.5613, 0.5186,  ..., 0.5227, 0.5198, 0.4843]],

         [[0.4992, 0.5771, 0.2681,  ..., 0.4588, 0.6133, 0.4972],
          [0.4418, 0.5125, 0.4460,  ..., 0.4049, 0.6096, 0.5173],
          [0.5892, 0.3740, 0.4695,  ..., 0.6548, 0.3749, 0.5620],
          [0.4978, 0.3873, 0.4289,  ..., 0.3859, 0.4131, 0.4124]]],


        [[[0.4658, 0.2993, 0.5264,  ..., 0.5119, 0.5402, 0.6091],
          [0.3992, 0.2985, 0.5050,  ..., 0.5651, 0.5014, 0.5333],
          [0.5654, 0.5301, 0.5838,  ..., 0.5759, 0.5412, 0.5773],
          [0.4967, 0.5083, 0.4008,  ..., 0.5745, 0.4316, 0.4371]],

         [[0.3528, 0.3252, 0.5472,  ..., 0.4386, 0.3577, 0.4139],
          [0.3865, 0.4420, 0.4873,  ..., 0.5545, 0.4706, 0.5151],
          [0.4343, 0.6550, 0.5767,  ..., 0.6156, 0.6461, 0.6467],
          [0.6711, 0.5833, 0.5633,  ..., 0.6619, 0.5861, 0.6058]],

         [[0.4602, 0.5704, 0.5000,  ..., 0.4124, 0.4060, 0.4707],
          [0.4741, 0.5266, 0.5932,  ..., 0.4896, 0.4003, 0.5506],
          [0.4979, 0.4154, 0.5087,  ..., 0.7381, 0.5085, 0.4984],
          [0.4675, 0.5356, 0.3966,  ..., 0.4904, 0.6040, 0.4832]],

         ...,

         [[0.3889, 0.4737, 0.4017,  ..., 0.4738, 0.3532, 0.5416],
          [0.6014, 0.5441, 0.4758,  ..., 0.4714, 0.3339, 0.3261],
          [0.5260, 0.6371, 0.5681,  ..., 0.5545, 0.6179, 0.3521],
          [0.4814, 0.5152, 0.4873,  ..., 0.6494, 0.5170, 0.5978]],

         [[0.5491, 0.5007, 0.4572,  ..., 0.5854, 0.5245, 0.7719],
          [0.6107, 0.4934, 0.4484,  ..., 0.5540, 0.5024, 0.3924],
          [0.4648, 0.5932, 0.6873,  ..., 0.4026, 0.4046, 0.3709],
          [0.4224, 0.5622, 0.6461,  ..., 0.5220, 0.6425, 0.4017]],

         [[0.4737, 0.4703, 0.5095,  ..., 0.4651, 0.4772, 0.5433],
          [0.6607, 0.4465, 0.6141,  ..., 0.5101, 0.4552, 0.4966],
          [0.4351, 0.6230, 0.4513,  ..., 0.4649, 0.5932, 0.6285],
          [0.4568, 0.4652, 0.5330,  ..., 0.6628, 0.4359, 0.5554]]]],
       device='cuda:0')
tensor([[[[0.5285, 0.4216, 0.4710,  ..., 0.5780, 0.4369, 0.5346],
          [0.6109, 0.5121, 0.5129,  ..., 0.4870, 0.4788, 0.4244],
          [0.4840, 0.5412, 0.6397,  ..., 0.3603, 0.4381, 0.4637],
          [0.4885, 0.5574, 0.4931,  ..., 0.4258, 0.4908, 0.6105]],

         [[0.4581, 0.4880, 0.5351,  ..., 0.5832, 0.4991, 0.6406],
          [0.5438, 0.4453, 0.5279,  ..., 0.5470, 0.7879, 0.6406],
          [0.6169, 0.6325, 0.6132,  ..., 0.5045, 0.5284, 0.4693],
          [0.5832, 0.5385, 0.3998,  ..., 0.4431, 0.3684, 0.4793]],

         [[0.6876, 0.5224, 0.4896,  ..., 0.6478, 0.5187, 0.5187],
          [0.4201, 0.5860, 0.5794,  ..., 0.5448, 0.3739, 0.5581],
          [0.4349, 0.4977, 0.5159,  ..., 0.6234, 0.4944, 0.5164],
          [0.4673, 0.4661, 0.4878,  ..., 0.6834, 0.5421, 0.4860]],

         ...,

         [[0.3872, 0.6234, 0.4768,  ..., 0.5846, 0.5651, 0.4012],
          [0.6433, 0.3684, 0.3840,  ..., 0.5312, 0.5804, 0.4980],
          [0.3780, 0.4540, 0.5056,  ..., 0.3993, 0.5324, 0.4069],
          [0.4511, 0.4826, 0.5039,  ..., 0.4634, 0.3684, 0.4177]],

         [[0.6118, 0.4026, 0.5400,  ..., 0.4819, 0.4707, 0.5358],
          [0.3919, 0.5727, 0.4278,  ..., 0.4869, 0.5203, 0.5550],
          [0.5378, 0.6100, 0.5453,  ..., 0.6415, 0.6522, 0.4545],
          [0.4400, 0.5603, 0.5256,  ..., 0.5157, 0.5188, 0.4773]],

         [[0.5022, 0.5761, 0.2751,  ..., 0.4518, 0.6123, 0.4902],
          [0.4448, 0.5115, 0.4530,  ..., 0.3979, 0.6086, 0.5103],
          [0.5922, 0.3730, 0.4765,  ..., 0.6478, 0.3739, 0.5550],
          [0.5008, 0.3863, 0.4359,  ..., 0.3789, 0.4121, 0.4054]]],


        [[[0.4688, 0.2983, 0.5334,  ..., 0.5049, 0.5392, 0.6021],
          [0.4022, 0.2975, 0.5120,  ..., 0.5581, 0.5004, 0.5263],
          [0.5684, 0.5291, 0.5908,  ..., 0.5689, 0.5402, 0.5703],
          [0.4997, 0.5073, 0.4078,  ..., 0.5675, 0.4306, 0.4301]],

         [[0.3558, 0.3242, 0.5542,  ..., 0.4316, 0.3567, 0.4069],
          [0.3895, 0.4410, 0.4943,  ..., 0.5475, 0.4696, 0.5081],
          [0.4373, 0.6540, 0.5837,  ..., 0.6086, 0.6451, 0.6397],
          [0.6741, 0.5823, 0.5703,  ..., 0.6549, 0.5851, 0.5988]],

         [[0.4632, 0.5694, 0.5070,  ..., 0.4054, 0.4050, 0.4637],
          [0.4771, 0.5256, 0.6002,  ..., 0.4826, 0.3993, 0.5436],
          [0.5009, 0.4144, 0.5157,  ..., 0.7311, 0.5075, 0.4914],
          [0.4705, 0.5346, 0.4036,  ..., 0.4834, 0.6030, 0.4762]],

         ...,

         [[0.3919, 0.4727, 0.4087,  ..., 0.4668, 0.3522, 0.5346],
          [0.6044, 0.5431, 0.4828,  ..., 0.4644, 0.3329, 0.3191],
          [0.5290, 0.6361, 0.5751,  ..., 0.5475, 0.6169, 0.3451],
          [0.4844, 0.5142, 0.4943,  ..., 0.6424, 0.5160, 0.5908]],

         [[0.5521, 0.4997, 0.4642,  ..., 0.5784, 0.5235, 0.7649],
          [0.6137, 0.4924, 0.4554,  ..., 0.5470, 0.5014, 0.3854],
          [0.4678, 0.5922, 0.6943,  ..., 0.3956, 0.4036, 0.3639],
          [0.4254, 0.5612, 0.6531,  ..., 0.5150, 0.6415, 0.3947]],

         [[0.4767, 0.4693, 0.5165,  ..., 0.4581, 0.4762, 0.5363],
          [0.6637, 0.4455, 0.6211,  ..., 0.5031, 0.4542, 0.4896],
          [0.4381, 0.6220, 0.4583,  ..., 0.4579, 0.5922, 0.6215],
          [0.4598, 0.4642, 0.5400,  ..., 0.6558, 0.4349, 0.5484]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0030,  0.0010, -0.0070,  0.0010, -0.0050, -0.0050,  0.0050,  0.0070,
         0.0010,  0.0070], device='cuda:0')
selected experts tensor([1702, 1581, 1589, 1671, 1737, 1756, 1582, 1576, 1635, 1555],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5997, 0.4815, 0.5437,  ..., 0.5338, 0.5583, 0.5809],
          [0.4646, 0.5374, 0.4198,  ..., 0.6102, 0.5203, 0.5463],
          [0.4695, 0.4839, 0.5824,  ..., 0.5013, 0.5476, 0.5289],
          [0.3485, 0.5678, 0.5315,  ..., 0.3537, 0.5616, 0.4560]],

         [[0.4201, 0.4411, 0.5202,  ..., 0.4541, 0.4983, 0.6317],
          [0.6001, 0.5418, 0.4852,  ..., 0.4927, 0.5749, 0.5617],
          [0.6306, 0.4377, 0.6190,  ..., 0.4851, 0.5369, 0.4904],
          [0.4818, 0.3805, 0.4599,  ..., 0.4566, 0.3581, 0.6690]],

         [[0.5698, 0.5712, 0.5838,  ..., 0.3094, 0.5725, 0.6225],
          [0.5419, 0.5060, 0.5649,  ..., 0.3791, 0.6206, 0.5540],
          [0.5457, 0.5437, 0.4976,  ..., 0.4271, 0.3890, 0.4526],
          [0.6369, 0.4982, 0.5776,  ..., 0.6719, 0.4505, 0.4779]],

         ...,

         [[0.5008, 0.4631, 0.5229,  ..., 0.4918, 0.3945, 0.5603],
          [0.4077, 0.5314, 0.4742,  ..., 0.5469, 0.5014, 0.5378],
          [0.5059, 0.6484, 0.6319,  ..., 0.5224, 0.6438, 0.5231],
          [0.3946, 0.5859, 0.4912,  ..., 0.4896, 0.5093, 0.5305]],

         [[0.3557, 0.5399, 0.5497,  ..., 0.3992, 0.4772, 0.4589],
          [0.6169, 0.5498, 0.4056,  ..., 0.4707, 0.5017, 0.2880],
          [0.4907, 0.5783, 0.6083,  ..., 0.5433, 0.4619, 0.5723],
          [0.3207, 0.4566, 0.5105,  ..., 0.5524, 0.6108, 0.5506]],

         [[0.3293, 0.5688, 0.6427,  ..., 0.5350, 0.4976, 0.6443],
          [0.4789, 0.6000, 0.3741,  ..., 0.4755, 0.4807, 0.4533],
          [0.4506, 0.5868, 0.5485,  ..., 0.4786, 0.4972, 0.5723],
          [0.5073, 0.5716, 0.6195,  ..., 0.5411, 0.4733, 0.5252]]],


        [[[0.4407, 0.5454, 0.3472,  ..., 0.5254, 0.4646, 0.4975],
          [0.4163, 0.4138, 0.4019,  ..., 0.6367, 0.5222, 0.5270],
          [0.5708, 0.3968, 0.5180,  ..., 0.5659, 0.3662, 0.5150],
          [0.5169, 0.5474, 0.5772,  ..., 0.4454, 0.4731, 0.6026]],

         [[0.5898, 0.3801, 0.4846,  ..., 0.5906, 0.6057, 0.3479],
          [0.3181, 0.4086, 0.3805,  ..., 0.3992, 0.4270, 0.6230],
          [0.4689, 0.4483, 0.3579,  ..., 0.4638, 0.6311, 0.5875],
          [0.5339, 0.6492, 0.4815,  ..., 0.3750, 0.4799, 0.5946]],

         [[0.5472, 0.5503, 0.4454,  ..., 0.5536, 0.4542, 0.6953],
          [0.3807, 0.5343, 0.4454,  ..., 0.5745, 0.4034, 0.6156],
          [0.5708, 0.5227, 0.5748,  ..., 0.4478, 0.4954, 0.5641],
          [0.4561, 0.4394, 0.5690,  ..., 0.3718, 0.4907, 0.3595]],

         ...,

         [[0.4244, 0.5673, 0.5264,  ..., 0.4541, 0.4015, 0.4112],
          [0.3756, 0.6135, 0.4802,  ..., 0.4963, 0.5294, 0.3142],
          [0.4359, 0.4761, 0.5476,  ..., 0.4905, 0.5282, 0.3859],
          [0.5693, 0.4673, 0.5078,  ..., 0.4413, 0.3115, 0.4849]],

         [[0.4537, 0.4133, 0.5140,  ..., 0.5621, 0.4486, 0.4439],
          [0.4311, 0.5237, 0.5914,  ..., 0.5148, 0.3318, 0.4788],
          [0.6991, 0.3921, 0.4948,  ..., 0.5859, 0.5249, 0.4211],
          [0.5414, 0.3700, 0.5180,  ..., 0.4941, 0.6474, 0.5376]],

         [[0.4443, 0.5196, 0.5957,  ..., 0.5797, 0.6765, 0.5497],
          [0.6136, 0.4329, 0.5490,  ..., 0.6167, 0.5600, 0.5036],
          [0.4966, 0.4747, 0.5642,  ..., 0.6448, 0.3936, 0.5002],
          [0.5370, 0.6250, 0.5676,  ..., 0.4147, 0.5022, 0.3559]]]],
       device='cuda:0')
tensor([[[[0.6007, 0.4845, 0.5407,  ..., 0.5368, 0.5533, 0.5799],
          [0.4656, 0.5404, 0.4168,  ..., 0.6132, 0.5153, 0.5453],
          [0.4705, 0.4869, 0.5794,  ..., 0.5043, 0.5426, 0.5279],
          [0.3495, 0.5708, 0.5285,  ..., 0.3567, 0.5566, 0.4550]],

         [[0.4211, 0.4441, 0.5172,  ..., 0.4571, 0.4933, 0.6307],
          [0.6011, 0.5448, 0.4822,  ..., 0.4957, 0.5699, 0.5607],
          [0.6316, 0.4407, 0.6160,  ..., 0.4881, 0.5319, 0.4894],
          [0.4828, 0.3835, 0.4569,  ..., 0.4596, 0.3531, 0.6680]],

         [[0.5708, 0.5742, 0.5808,  ..., 0.3124, 0.5675, 0.6215],
          [0.5429, 0.5090, 0.5619,  ..., 0.3821, 0.6156, 0.5530],
          [0.5467, 0.5467, 0.4946,  ..., 0.4301, 0.3840, 0.4516],
          [0.6379, 0.5012, 0.5746,  ..., 0.6749, 0.4455, 0.4769]],

         ...,

         [[0.5018, 0.4661, 0.5199,  ..., 0.4948, 0.3895, 0.5593],
          [0.4087, 0.5344, 0.4712,  ..., 0.5499, 0.4964, 0.5368],
          [0.5069, 0.6514, 0.6289,  ..., 0.5254, 0.6388, 0.5221],
          [0.3956, 0.5889, 0.4882,  ..., 0.4926, 0.5043, 0.5295]],

         [[0.3567, 0.5429, 0.5467,  ..., 0.4022, 0.4722, 0.4579],
          [0.6179, 0.5528, 0.4026,  ..., 0.4737, 0.4967, 0.2870],
          [0.4917, 0.5813, 0.6053,  ..., 0.5463, 0.4569, 0.5713],
          [0.3217, 0.4596, 0.5075,  ..., 0.5554, 0.6058, 0.5496]],

         [[0.3303, 0.5718, 0.6397,  ..., 0.5380, 0.4926, 0.6433],
          [0.4799, 0.6030, 0.3711,  ..., 0.4785, 0.4757, 0.4523],
          [0.4516, 0.5898, 0.5455,  ..., 0.4816, 0.4922, 0.5713],
          [0.5083, 0.5746, 0.6165,  ..., 0.5441, 0.4683, 0.5242]]],


        [[[0.4417, 0.5484, 0.3442,  ..., 0.5284, 0.4596, 0.4965],
          [0.4173, 0.4168, 0.3989,  ..., 0.6397, 0.5172, 0.5260],
          [0.5718, 0.3998, 0.5150,  ..., 0.5689, 0.3612, 0.5140],
          [0.5179, 0.5504, 0.5742,  ..., 0.4484, 0.4681, 0.6016]],

         [[0.5908, 0.3831, 0.4816,  ..., 0.5936, 0.6007, 0.3469],
          [0.3191, 0.4116, 0.3775,  ..., 0.4022, 0.4220, 0.6220],
          [0.4699, 0.4513, 0.3549,  ..., 0.4668, 0.6261, 0.5865],
          [0.5349, 0.6522, 0.4785,  ..., 0.3780, 0.4749, 0.5936]],

         [[0.5482, 0.5533, 0.4424,  ..., 0.5566, 0.4492, 0.6943],
          [0.3817, 0.5373, 0.4424,  ..., 0.5775, 0.3984, 0.6146],
          [0.5718, 0.5257, 0.5718,  ..., 0.4508, 0.4904, 0.5631],
          [0.4571, 0.4424, 0.5660,  ..., 0.3748, 0.4857, 0.3585]],

         ...,

         [[0.4254, 0.5703, 0.5234,  ..., 0.4571, 0.3965, 0.4102],
          [0.3766, 0.6165, 0.4772,  ..., 0.4993, 0.5244, 0.3132],
          [0.4369, 0.4791, 0.5446,  ..., 0.4935, 0.5232, 0.3849],
          [0.5703, 0.4703, 0.5048,  ..., 0.4443, 0.3065, 0.4839]],

         [[0.4547, 0.4163, 0.5110,  ..., 0.5651, 0.4436, 0.4429],
          [0.4321, 0.5267, 0.5884,  ..., 0.5178, 0.3268, 0.4778],
          [0.7001, 0.3951, 0.4918,  ..., 0.5889, 0.5199, 0.4201],
          [0.5424, 0.3730, 0.5150,  ..., 0.4971, 0.6424, 0.5366]],

         [[0.4453, 0.5226, 0.5927,  ..., 0.5827, 0.6715, 0.5487],
          [0.6146, 0.4359, 0.5460,  ..., 0.6197, 0.5550, 0.5026],
          [0.4976, 0.4777, 0.5612,  ..., 0.6478, 0.3886, 0.4992],
          [0.5380, 0.6280, 0.5646,  ..., 0.4177, 0.4972, 0.3549]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0010, -0.0030,  0.0030,  0.0110, -0.0010,  0.0090, -0.0110, -0.0030,
         0.0050,  0.0010], device='cuda:0')
selected experts tensor([1699, 1743, 1505, 1670, 1526, 1647, 1553, 1701, 1650, 1690],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4915, 0.6071, 0.4899,  ..., 0.3855, 0.5174, 0.6182],
          [0.4578, 0.4138, 0.4403,  ..., 0.5920, 0.5526, 0.4467],
          [0.3962, 0.5028, 0.6384,  ..., 0.4883, 0.5656, 0.5125],
          [0.6085, 0.4009, 0.5971,  ..., 0.4875, 0.3763, 0.4391]],

         [[0.3677, 0.6112, 0.5052,  ..., 0.4570, 0.4831, 0.5974],
          [0.5109, 0.5306, 0.4995,  ..., 0.5816, 0.3611, 0.3711],
          [0.5611, 0.4667, 0.5534,  ..., 0.3411, 0.5406, 0.5770],
          [0.6592, 0.3490, 0.4290,  ..., 0.4731, 0.3682, 0.5392]],

         [[0.5194, 0.4762, 0.3944,  ..., 0.5282, 0.5560, 0.4510],
          [0.5482, 0.4436, 0.5721,  ..., 0.5260, 0.4505, 0.4089],
          [0.5395, 0.4964, 0.5744,  ..., 0.4628, 0.4243, 0.4686],
          [0.6103, 0.4930, 0.4580,  ..., 0.5759, 0.5447, 0.4995]],

         ...,

         [[0.4443, 0.5189, 0.4988,  ..., 0.3902, 0.5211, 0.5317],
          [0.5136, 0.5126, 0.3572,  ..., 0.4423, 0.6002, 0.5826],
          [0.5332, 0.3995, 0.5901,  ..., 0.5216, 0.6011, 0.6078],
          [0.5568, 0.6266, 0.4701,  ..., 0.5379, 0.5143, 0.4890]],

         [[0.5787, 0.4857, 0.3491,  ..., 0.5682, 0.6002, 0.5879],
          [0.6311, 0.5177, 0.4394,  ..., 0.5164, 0.5574, 0.5110],
          [0.6302, 0.3535, 0.5430,  ..., 0.5181, 0.6918, 0.5381],
          [0.3535, 0.5061, 0.5502,  ..., 0.4038, 0.5513, 0.4568]],

         [[0.4791, 0.6382, 0.4252,  ..., 0.4468, 0.4366, 0.5640],
          [0.3626, 0.4938, 0.3671,  ..., 0.3916, 0.6143, 0.5604],
          [0.6302, 0.3293, 0.5306,  ..., 0.5999, 0.4995, 0.4011],
          [0.4772, 0.5330, 0.5061,  ..., 0.4884, 0.4833, 0.3919]]],


        [[[0.5578, 0.5006, 0.4353,  ..., 0.3245, 0.4485, 0.7056],
          [0.3825, 0.5194, 0.3935,  ..., 0.6456, 0.5521, 0.3960],
          [0.5787, 0.4793, 0.6752,  ..., 0.4643, 0.5688, 0.3756],
          [0.7380, 0.4014, 0.6384,  ..., 0.4413, 0.4760, 0.5993]],

         [[0.5453, 0.5749, 0.5335,  ..., 0.5195, 0.5121, 0.5761],
          [0.4147, 0.6479, 0.4427,  ..., 0.6936, 0.5075, 0.5567],
          [0.3668, 0.4588, 0.5215,  ..., 0.5152, 0.2242, 0.3992],
          [0.5272, 0.5424, 0.5034,  ..., 0.5749, 0.3835, 0.5068]],

         [[0.4555, 0.5649, 0.4656,  ..., 0.5096, 0.4824, 0.4386],
          [0.5640, 0.5175, 0.4519,  ..., 0.4934, 0.5451, 0.4472],
          [0.3668, 0.5501, 0.5199,  ..., 0.5044, 0.3292, 0.4131],
          [0.5824, 0.4410, 0.6069,  ..., 0.5601, 0.4965, 0.5879]],

         ...,

         [[0.4618, 0.4395, 0.4507,  ..., 0.3367, 0.5700, 0.7567],
          [0.4359, 0.4686, 0.5264,  ..., 0.5773, 0.5849, 0.6441],
          [0.4482, 0.3760, 0.6143,  ..., 0.4585, 0.3772, 0.4386],
          [0.7093, 0.4550, 0.4468,  ..., 0.5136, 0.4833, 0.4570]],

         [[0.5306, 0.4647, 0.6294,  ..., 0.3411, 0.4065, 0.6622],
          [0.4770, 0.6557, 0.3572,  ..., 0.4771, 0.6421, 0.6834],
          [0.5997, 0.4638, 0.5716,  ..., 0.4076, 0.5283, 0.3992],
          [0.4373, 0.5235, 0.5678,  ..., 0.5096, 0.4671, 0.3570]],

         [[0.5383, 0.5625, 0.5209,  ..., 0.4781, 0.4428, 0.6177],
          [0.3746, 0.4743, 0.6312,  ..., 0.4922, 0.5430, 0.5955],
          [0.5119, 0.4352, 0.4803,  ..., 0.4933, 0.4550, 0.4362],
          [0.5055, 0.5184, 0.5456,  ..., 0.4645, 0.5572, 0.5621]]]],
       device='cuda:0')
tensor([[[[0.5055, 0.6211, 0.4939,  ..., 0.3895, 0.5014, 0.6002],
          [0.4718, 0.4278, 0.4443,  ..., 0.5960, 0.5366, 0.4287],
          [0.4102, 0.5168, 0.6424,  ..., 0.4923, 0.5496, 0.4945],
          [0.6225, 0.4149, 0.6011,  ..., 0.4915, 0.3603, 0.4211]],

         [[0.3817, 0.6252, 0.5092,  ..., 0.4610, 0.4671, 0.5794],
          [0.5249, 0.5446, 0.5035,  ..., 0.5856, 0.3451, 0.3531],
          [0.5751, 0.4807, 0.5574,  ..., 0.3451, 0.5246, 0.5590],
          [0.6732, 0.3630, 0.4330,  ..., 0.4771, 0.3522, 0.5212]],

         [[0.5334, 0.4902, 0.3984,  ..., 0.5322, 0.5400, 0.4330],
          [0.5622, 0.4576, 0.5761,  ..., 0.5300, 0.4345, 0.3909],
          [0.5535, 0.5104, 0.5784,  ..., 0.4668, 0.4083, 0.4506],
          [0.6243, 0.5070, 0.4620,  ..., 0.5799, 0.5287, 0.4815]],

         ...,

         [[0.4583, 0.5329, 0.5028,  ..., 0.3942, 0.5051, 0.5137],
          [0.5276, 0.5266, 0.3612,  ..., 0.4463, 0.5842, 0.5646],
          [0.5472, 0.4135, 0.5941,  ..., 0.5256, 0.5851, 0.5898],
          [0.5708, 0.6406, 0.4741,  ..., 0.5419, 0.4983, 0.4710]],

         [[0.5927, 0.4997, 0.3531,  ..., 0.5722, 0.5842, 0.5699],
          [0.6451, 0.5317, 0.4434,  ..., 0.5204, 0.5414, 0.4930],
          [0.6442, 0.3675, 0.5470,  ..., 0.5221, 0.6758, 0.5201],
          [0.3675, 0.5201, 0.5542,  ..., 0.4078, 0.5353, 0.4388]],

         [[0.4931, 0.6522, 0.4292,  ..., 0.4508, 0.4206, 0.5460],
          [0.3766, 0.5078, 0.3711,  ..., 0.3956, 0.5983, 0.5424],
          [0.6442, 0.3433, 0.5346,  ..., 0.6039, 0.4835, 0.3831],
          [0.4912, 0.5470, 0.5101,  ..., 0.4924, 0.4673, 0.3739]]],


        [[[0.5718, 0.5146, 0.4393,  ..., 0.3285, 0.4325, 0.6876],
          [0.3965, 0.5334, 0.3975,  ..., 0.6496, 0.5361, 0.3780],
          [0.5927, 0.4933, 0.6792,  ..., 0.4683, 0.5528, 0.3576],
          [0.7520, 0.4154, 0.6424,  ..., 0.4453, 0.4600, 0.5813]],

         [[0.5593, 0.5889, 0.5375,  ..., 0.5235, 0.4961, 0.5581],
          [0.4287, 0.6619, 0.4467,  ..., 0.6976, 0.4915, 0.5387],
          [0.3808, 0.4728, 0.5255,  ..., 0.5192, 0.2082, 0.3812],
          [0.5412, 0.5564, 0.5074,  ..., 0.5789, 0.3675, 0.4888]],

         [[0.4695, 0.5789, 0.4696,  ..., 0.5136, 0.4664, 0.4206],
          [0.5780, 0.5315, 0.4559,  ..., 0.4974, 0.5291, 0.4292],
          [0.3808, 0.5641, 0.5239,  ..., 0.5084, 0.3132, 0.3951],
          [0.5964, 0.4550, 0.6109,  ..., 0.5641, 0.4805, 0.5699]],

         ...,

         [[0.4758, 0.4535, 0.4547,  ..., 0.3407, 0.5540, 0.7387],
          [0.4499, 0.4826, 0.5304,  ..., 0.5813, 0.5689, 0.6261],
          [0.4622, 0.3900, 0.6183,  ..., 0.4625, 0.3612, 0.4206],
          [0.7233, 0.4690, 0.4508,  ..., 0.5176, 0.4673, 0.4390]],

         [[0.5446, 0.4787, 0.6334,  ..., 0.3451, 0.3905, 0.6442],
          [0.4910, 0.6697, 0.3612,  ..., 0.4811, 0.6261, 0.6654],
          [0.6137, 0.4778, 0.5756,  ..., 0.4116, 0.5123, 0.3812],
          [0.4513, 0.5375, 0.5718,  ..., 0.5136, 0.4511, 0.3390]],

         [[0.5523, 0.5765, 0.5249,  ..., 0.4821, 0.4268, 0.5997],
          [0.3886, 0.4883, 0.6352,  ..., 0.4962, 0.5270, 0.5775],
          [0.5259, 0.4492, 0.4843,  ..., 0.4973, 0.4390, 0.4182],
          [0.5195, 0.5324, 0.5496,  ..., 0.4685, 0.5412, 0.5441]]]],
       device='cuda:0', requires_grad=True)
tensor([-1.4000e-02, -1.4000e-02, -4.0000e-03, -2.3283e-10, -1.8000e-02,
         2.0000e-02, -2.0000e-02, -4.0000e-03,  1.6000e-02,  1.8000e-02],
       device='cuda:0')
selected experts tensor([1365, 1482, 1730, 1920, 1836, 1363, 2205, 1665, 1362, 1456],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5572, 0.5014, 0.5824,  ..., 0.4392, 0.4298, 0.5616],
          [0.4703, 0.5590, 0.5858,  ..., 0.4883, 0.6320, 0.4263],
          [0.5089, 0.4956, 0.3715,  ..., 0.4983, 0.3491, 0.5875],
          [0.4245, 0.5958, 0.4830,  ..., 0.4930, 0.6650, 0.4449]],

         [[0.6059, 0.5183, 0.4303,  ..., 0.3848, 0.4684, 0.3830],
          [0.4574, 0.4634, 0.4524,  ..., 0.5242, 0.5953, 0.4440],
          [0.6489, 0.6137, 0.4442,  ..., 0.5727, 0.5327, 0.4683],
          [0.5742, 0.5897, 0.4807,  ..., 0.6177, 0.5686, 0.4548]],

         [[0.5061, 0.5816, 0.4132,  ..., 0.4079, 0.4718, 0.4311],
          [0.5359, 0.5554, 0.5820,  ..., 0.4968, 0.4674, 0.4899],
          [0.5309, 0.4827, 0.5262,  ..., 0.4722, 0.4791, 0.4354],
          [0.5904, 0.5585, 0.4784,  ..., 0.4730, 0.5314, 0.4047]],

         ...,

         [[0.5020, 0.5060, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5020, 0.5060, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5020, 0.5060, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5020, 0.5060, 0.5040,  ..., 0.5100, 0.5040, 0.5100]],

         [[0.5020, 0.5060, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5020, 0.5060, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5020, 0.5060, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5020, 0.5060, 0.5040,  ..., 0.5100, 0.5040, 0.5100]],

         [[0.5020, 0.5060, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5020, 0.5060, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5020, 0.5060, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5020, 0.5060, 0.5040,  ..., 0.5100, 0.5040, 0.5100]]],


        [[[0.4716, 0.3771, 0.5734,  ..., 0.3640, 0.4611, 0.4306],
          [0.4894, 0.5118, 0.5408,  ..., 0.4387, 0.5437, 0.4126],
          [0.4350, 0.4405, 0.4184,  ..., 0.4725, 0.4880, 0.5923],
          [0.5598, 0.6216, 0.5250,  ..., 0.5502, 0.5805, 0.3839]],

         [[0.6534, 0.5530, 0.4137,  ..., 0.4879, 0.4488, 0.4221],
          [0.5553, 0.5835, 0.6018,  ..., 0.5625, 0.5218, 0.4406],
          [0.5495, 0.4347, 0.4447,  ..., 0.5181, 0.4308, 0.6976],
          [0.4504, 0.5359, 0.4694,  ..., 0.5098, 0.4423, 0.3748]],

         [[0.4819, 0.5506, 0.4409,  ..., 0.4023, 0.3959, 0.5705],
          [0.4730, 0.6635, 0.5168,  ..., 0.3224, 0.4203, 0.4705],
          [0.5814, 0.5266, 0.3779,  ..., 0.4406, 0.4816, 0.6050],
          [0.4993, 0.5778, 0.4536,  ..., 0.4107, 0.5355, 0.4244]],

         ...,

         [[0.5020, 0.5060, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5020, 0.5060, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5020, 0.5060, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5020, 0.5060, 0.5040,  ..., 0.5100, 0.5040, 0.5100]],

         [[0.5020, 0.5060, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5020, 0.5060, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5020, 0.5060, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5020, 0.5060, 0.5040,  ..., 0.5100, 0.5040, 0.5100]],

         [[0.5020, 0.5060, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5020, 0.5060, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5020, 0.5060, 0.5040,  ..., 0.5100, 0.5040, 0.5100],
          [0.5020, 0.5060, 0.5040,  ..., 0.5100, 0.5040, 0.5100]]]],
       device='cuda:0')/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([[[[0.5552, 0.4954, 0.5784,  ..., 0.4292, 0.4258, 0.5516],
          [0.4683, 0.5530, 0.5818,  ..., 0.4783, 0.6280, 0.4163],
          [0.5069, 0.4896, 0.3675,  ..., 0.4883, 0.3451, 0.5775],
          [0.4225, 0.5898, 0.4790,  ..., 0.4830, 0.6610, 0.4349]],

         [[0.6039, 0.5123, 0.4263,  ..., 0.3748, 0.4644, 0.3730],
          [0.4554, 0.4574, 0.4484,  ..., 0.5142, 0.5913, 0.4340],
          [0.6469, 0.6077, 0.4402,  ..., 0.5627, 0.5287, 0.4583],
          [0.5722, 0.5837, 0.4767,  ..., 0.6077, 0.5646, 0.4448]],

         [[0.5041, 0.5756, 0.4092,  ..., 0.3979, 0.4678, 0.4211],
          [0.5339, 0.5494, 0.5780,  ..., 0.4868, 0.4634, 0.4799],
          [0.5289, 0.4767, 0.5222,  ..., 0.4622, 0.4751, 0.4254],
          [0.5884, 0.5525, 0.4744,  ..., 0.4630, 0.5274, 0.3947]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.4696, 0.3711, 0.5694,  ..., 0.3540, 0.4571, 0.4206],
          [0.4874, 0.5058, 0.5368,  ..., 0.4287, 0.5397, 0.4026],
          [0.4330, 0.4345, 0.4144,  ..., 0.4625, 0.4840, 0.5823],
          [0.5578, 0.6156, 0.5210,  ..., 0.5402, 0.5765, 0.3739]],

         [[0.6514, 0.5470, 0.4097,  ..., 0.4779, 0.4448, 0.4121],
          [0.5533, 0.5775, 0.5978,  ..., 0.5525, 0.5178, 0.4306],
          [0.5475, 0.4287, 0.4407,  ..., 0.5081, 0.4268, 0.6876],
          [0.4484, 0.5299, 0.4654,  ..., 0.4998, 0.4383, 0.3648]],

         [[0.4799, 0.5446, 0.4369,  ..., 0.3923, 0.3919, 0.5605],
          [0.4710, 0.6575, 0.5128,  ..., 0.3124, 0.4163, 0.4605],
          [0.5794, 0.5206, 0.3739,  ..., 0.4306, 0.4776, 0.5950],
          [0.4973, 0.5718, 0.4496,  ..., 0.4007, 0.5315, 0.4144]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0020,  0.0060,  0.0040,  0.0080, -0.0080,  0.0080,  0.0100,  0.0100,
         0.0040,  0.0100], device='cuda:0')
selected experts tensor([1578, 2303, 1750, 1774, 1690,  667, 2044, 1478, 1755, 1345],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1699., 1743., 1505., 1670., 1526., 1647., 1553., 1701., 1650., 1690.],
        [1578., 2303., 1750., 1774., 1690.,  667., 2044., 1478., 1755., 1345.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.3653, 0.5112, 0.4800,  ..., 0.4702, 0.5316, 0.3791],
          [0.6240, 0.2962, 0.5772,  ..., 0.5562, 0.5032, 0.5950],
          [0.5981, 0.6189, 0.4357,  ..., 0.6513, 0.5728, 0.4952],
          [0.5938, 0.5781, 0.6292,  ..., 0.5167, 0.3211, 0.5183]],

         [[0.5531, 0.5432, 0.7094,  ..., 0.4073, 0.5742, 0.5061],
          [0.5587, 0.4693, 0.7054,  ..., 0.5685, 0.2485, 0.4439],
          [0.5430, 0.5275, 0.6498,  ..., 0.5969, 0.5156, 0.7056],
          [0.6375, 0.5395, 0.3859,  ..., 0.6459, 0.3731, 0.4324]],

         [[0.5024, 0.5390, 0.6577,  ..., 0.5057, 0.4444, 0.3104],
          [0.5374, 0.5230, 0.6265,  ..., 0.3584, 0.5337, 0.6486],
          [0.5077, 0.5601, 0.4820,  ..., 0.5301, 0.6622, 0.4234],
          [0.5010, 0.4995, 0.3766,  ..., 0.5511, 0.5914, 0.6002]],

         ...,

         [[0.4365, 0.4917, 0.4591,  ..., 0.4856, 0.6462, 0.4191],
          [0.4410, 0.5100, 0.4797,  ..., 0.4705, 0.4032, 0.4163],
          [0.5432, 0.5393, 0.6210,  ..., 0.5031, 0.3650, 0.4695],
          [0.4529, 0.4963, 0.3766,  ..., 0.5845, 0.5248, 0.4834]],

         [[0.6588, 0.4623, 0.4999,  ..., 0.6049, 0.5053, 0.4530],
          [0.3653, 0.5216, 0.5629,  ..., 0.4139, 0.5771, 0.4272],
          [0.4384, 0.5606, 0.4347,  ..., 0.4804, 0.4792, 0.4854],
          [0.6088, 0.4867, 0.3225,  ..., 0.5239, 0.4202, 0.4215]],

         [[0.4152, 0.4103, 0.5218,  ..., 0.4819, 0.5049, 0.4429],
          [0.4661, 0.4084, 0.4639,  ..., 0.4069, 0.5166, 0.4632],
          [0.4029, 0.7323, 0.5407,  ..., 0.4449, 0.5410, 0.5357],
          [0.5901, 0.5115, 0.4888,  ..., 0.4981, 0.6162, 0.5997]]],


        [[[0.4806, 0.6480, 0.3976,  ..., 0.5332, 0.5019, 0.4410],
          [0.4029, 0.3957, 0.4909,  ..., 0.4362, 0.4278, 0.5368],
          [0.3814, 0.5880, 0.4836,  ..., 0.4998, 0.5045, 0.4736],
          [0.5587, 0.3883, 0.6471,  ..., 0.3204, 0.4599, 0.5779]],

         [[0.4319, 0.6489, 0.4655,  ..., 0.6855, 0.5322, 0.4639],
          [0.4257, 0.4662, 0.3435,  ..., 0.5429, 0.4074, 0.4984],
          [0.4806, 0.4965, 0.6382,  ..., 0.4951, 0.5723, 0.4756],
          [0.5783, 0.5199, 0.5786,  ..., 0.5324, 0.3357, 0.4992]],

         [[0.5425, 0.5574, 0.3896,  ..., 0.4981, 0.7376, 0.4215],
          [0.5321, 0.6272, 0.4736,  ..., 0.5755, 0.5790, 0.5014],
          [0.4585, 0.4401, 0.5648,  ..., 0.5472, 0.5322, 0.5812],
          [0.5056, 0.5359, 0.3882,  ..., 0.4951, 0.3686, 0.4229]],

         ...,

         [[0.4052, 0.4103, 0.4089,  ..., 0.5596, 0.4674, 0.4572],
          [0.7421, 0.4705, 0.5605,  ..., 0.5063, 0.5582, 0.5699],
          [0.4374, 0.4645, 0.4866,  ..., 0.4714, 0.4710, 0.6629],
          [0.4947, 0.4987, 0.3887,  ..., 0.6138, 0.4278, 0.5683]],

         [[0.4510, 0.5637, 0.4709,  ..., 0.4904, 0.4023, 0.5264],
          [0.5444, 0.3929, 0.5076,  ..., 0.4925, 0.4882, 0.6821],
          [0.5227, 0.5824, 0.5456,  ..., 0.5386, 0.5021, 0.6305],
          [0.5301, 0.5090, 0.4434,  ..., 0.5219, 0.5847, 0.4688]],

         [[0.4800, 0.5904, 0.4625,  ..., 0.6725, 0.4971, 0.6087],
          [0.5058, 0.3668, 0.5195,  ..., 0.5412, 0.5424, 0.5458],
          [0.5563, 0.5466, 0.5667,  ..., 0.6171, 0.5762, 0.5069],
          [0.5333, 0.5313, 0.6146,  ..., 0.3638, 0.3795, 0.4528]]]],
       device='cuda:0')
tensor([[[[0.3693, 0.5092, 0.4860,  ..., 0.4622, 0.5296, 0.3711],
          [0.6280, 0.2942, 0.5832,  ..., 0.5482, 0.5012, 0.5870],
          [0.6021, 0.6169, 0.4417,  ..., 0.6433, 0.5708, 0.4872],
          [0.5978, 0.5761, 0.6352,  ..., 0.5087, 0.3191, 0.5103]],

         [[0.5571, 0.5412, 0.7154,  ..., 0.3993, 0.5722, 0.4981],
          [0.5627, 0.4673, 0.7114,  ..., 0.5605, 0.2465, 0.4359],
          [0.5470, 0.5255, 0.6558,  ..., 0.5889, 0.5136, 0.6976],
          [0.6415, 0.5375, 0.3919,  ..., 0.6379, 0.3711, 0.4244]],

         [[0.5064, 0.5370, 0.6637,  ..., 0.4977, 0.4424, 0.3024],
          [0.5414, 0.5210, 0.6325,  ..., 0.3504, 0.5317, 0.6406],
          [0.5117, 0.5581, 0.4880,  ..., 0.5221, 0.6602, 0.4154],
          [0.5050, 0.4975, 0.3826,  ..., 0.5431, 0.5894, 0.5922]],

         ...,

         [[0.4405, 0.4897, 0.4651,  ..., 0.4776, 0.6442, 0.4111],
          [0.4450, 0.5080, 0.4857,  ..., 0.4625, 0.4012, 0.4083],
          [0.5472, 0.5373, 0.6270,  ..., 0.4951, 0.3630, 0.4615],
          [0.4569, 0.4943, 0.3826,  ..., 0.5765, 0.5228, 0.4754]],

         [[0.6628, 0.4603, 0.5059,  ..., 0.5969, 0.5033, 0.4450],
          [0.3693, 0.5196, 0.5689,  ..., 0.4059, 0.5751, 0.4192],
          [0.4424, 0.5586, 0.4407,  ..., 0.4724, 0.4772, 0.4774],
          [0.6128, 0.4847, 0.3285,  ..., 0.5159, 0.4182, 0.4135]],

         [[0.4192, 0.4083, 0.5278,  ..., 0.4739, 0.5029, 0.4349],
          [0.4701, 0.4064, 0.4699,  ..., 0.3989, 0.5146, 0.4552],
          [0.4069, 0.7303, 0.5467,  ..., 0.4369, 0.5390, 0.5277],
          [0.5941, 0.5095, 0.4948,  ..., 0.4901, 0.6142, 0.5917]]],


        [[[0.4846, 0.6460, 0.4036,  ..., 0.5252, 0.4999, 0.4330],
          [0.4069, 0.3937, 0.4969,  ..., 0.4282, 0.4258, 0.5288],
          [0.3854, 0.5860, 0.4896,  ..., 0.4918, 0.5025, 0.4656],
          [0.5627, 0.3863, 0.6531,  ..., 0.3124, 0.4579, 0.5699]],

         [[0.4359, 0.6469, 0.4715,  ..., 0.6775, 0.5302, 0.4559],
          [0.4297, 0.4642, 0.3495,  ..., 0.5349, 0.4054, 0.4904],
          [0.4846, 0.4945, 0.6442,  ..., 0.4871, 0.5703, 0.4676],
          [0.5823, 0.5179, 0.5846,  ..., 0.5244, 0.3337, 0.4912]],

         [[0.5465, 0.5554, 0.3956,  ..., 0.4901, 0.7356, 0.4135],
          [0.5361, 0.6252, 0.4796,  ..., 0.5675, 0.5770, 0.4934],
          [0.4625, 0.4381, 0.5708,  ..., 0.5392, 0.5302, 0.5732],
          [0.5096, 0.5339, 0.3942,  ..., 0.4871, 0.3666, 0.4149]],

         ...,

         [[0.4092, 0.4083, 0.4149,  ..., 0.5516, 0.4654, 0.4492],
          [0.7461, 0.4685, 0.5665,  ..., 0.4983, 0.5562, 0.5619],
          [0.4414, 0.4625, 0.4926,  ..., 0.4634, 0.4690, 0.6549],
          [0.4987, 0.4967, 0.3947,  ..., 0.6058, 0.4258, 0.5603]],

         [[0.4550, 0.5617, 0.4769,  ..., 0.4824, 0.4003, 0.5184],
          [0.5484, 0.3909, 0.5136,  ..., 0.4845, 0.4862, 0.6741],
          [0.5267, 0.5804, 0.5516,  ..., 0.5306, 0.5001, 0.6225],
          [0.5341, 0.5070, 0.4494,  ..., 0.5139, 0.5827, 0.4608]],

         [[0.4840, 0.5884, 0.4685,  ..., 0.6645, 0.4951, 0.6007],
          [0.5098, 0.3648, 0.5255,  ..., 0.5332, 0.5404, 0.5378],
          [0.5603, 0.5446, 0.5727,  ..., 0.6091, 0.5742, 0.4989],
          [0.5373, 0.5293, 0.6206,  ..., 0.3558, 0.3775, 0.4448]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0040,  0.0020, -0.0060,  0.0000, -0.0060, -0.0060,  0.0060,  0.0080,
         0.0020,  0.0080], device='cuda:0')
selected experts tensor([1630, 1603, 1738, 1523, 1579, 1534, 1601, 1744, 1668, 1764],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4053, 0.5121, 0.4928,  ..., 0.4439, 0.6000, 0.4879],
          [0.4181, 0.5596, 0.4375,  ..., 0.4609, 0.3959, 0.4273],
          [0.4920, 0.5999, 0.5261,  ..., 0.5016, 0.4423, 0.5392],
          [0.5554, 0.4406, 0.5590,  ..., 0.3791, 0.4413, 0.4116]],

         [[0.5508, 0.5882, 0.4294,  ..., 0.5178, 0.3996, 0.4125],
          [0.4720, 0.5910, 0.5510,  ..., 0.4995, 0.4565, 0.5101],
          [0.5522, 0.5203, 0.4599,  ..., 0.4123, 0.4029, 0.4325],
          [0.4842, 0.4982, 0.3861,  ..., 0.2311, 0.4175, 0.4706]],

         [[0.4875, 0.5240, 0.5274,  ..., 0.5730, 0.4942, 0.5479],
          [0.4191, 0.6185, 0.6131,  ..., 0.5454, 0.4517, 0.3891],
          [0.6323, 0.5138, 0.4194,  ..., 0.5802, 0.4745, 0.5433],
          [0.4105, 0.4370, 0.5324,  ..., 0.5132, 0.4440, 0.6179]],

         ...,

         [[0.4788, 0.3902, 0.4818,  ..., 0.6078, 0.4236, 0.5542],
          [0.5175, 0.5034, 0.4113,  ..., 0.6527, 0.5311, 0.4946],
          [0.4747, 0.4718, 0.3922,  ..., 0.4607, 0.5210, 0.5114],
          [0.2359, 0.4888, 0.5602,  ..., 0.4490, 0.5473, 0.5157]],

         [[0.5237, 0.6221, 0.3334,  ..., 0.4228, 0.4284, 0.6252],
          [0.4532, 0.3438, 0.4906,  ..., 0.5120, 0.4515, 0.4921],
          [0.5878, 0.4066, 0.4010,  ..., 0.4483, 0.5021, 0.4282],
          [0.3583, 0.6605, 0.3945,  ..., 0.3944, 0.6117, 0.5334]],

         [[0.5831, 0.4736, 0.5466,  ..., 0.4962, 0.5112, 0.5978],
          [0.4841, 0.4014, 0.4356,  ..., 0.4420, 0.6891, 0.5356],
          [0.4556, 0.4529, 0.5805,  ..., 0.4276, 0.5406, 0.4528],
          [0.5798, 0.4524, 0.4241,  ..., 0.2959, 0.5058, 0.4988]]],


        [[[0.4597, 0.5702, 0.4820,  ..., 0.7050, 0.4440, 0.5349],
          [0.3682, 0.5386, 0.6356,  ..., 0.5250, 0.3866, 0.5884],
          [0.5101, 0.6474, 0.5561,  ..., 0.5481, 0.6866, 0.4794],
          [0.4115, 0.3749, 0.5437,  ..., 0.5016, 0.4551, 0.6183]],

         [[0.4868, 0.4743, 0.4891,  ..., 0.4423, 0.3889, 0.5988],
          [0.6066, 0.5408, 0.5633,  ..., 0.6106, 0.3852, 0.4381],
          [0.4157, 0.5289, 0.4019,  ..., 0.5268, 0.6255, 0.3141],
          [0.5295, 0.3921, 0.5082,  ..., 0.4343, 0.5367, 0.3849]],

         [[0.4986, 0.6474, 0.4260,  ..., 0.4468, 0.4165, 0.3956],
          [0.5459, 0.5299, 0.3926,  ..., 0.5002, 0.4998, 0.4239],
          [0.7142, 0.4933, 0.3688,  ..., 0.3874, 0.5753, 0.5655],
          [0.4091, 0.4478, 0.6070,  ..., 0.5217, 0.5346, 0.3844]],

         ...,

         [[0.5736, 0.4558, 0.6242,  ..., 0.5142, 0.4883, 0.6901],
          [0.5245, 0.5084, 0.5577,  ..., 0.4779, 0.5329, 0.4763],
          [0.4663, 0.5570, 0.7033,  ..., 0.5219, 0.5483, 0.5421],
          [0.4501, 0.5901, 0.5839,  ..., 0.5113, 0.5507, 0.4487]],

         [[0.4752, 0.4686, 0.5762,  ..., 0.6803, 0.5582, 0.2379],
          [0.5333, 0.4575, 0.4990,  ..., 0.3781, 0.4563, 0.5554],
          [0.4760, 0.3653, 0.5357,  ..., 0.4464, 0.4251, 0.4220],
          [0.5674, 0.5396, 0.5810,  ..., 0.5995, 0.4471, 0.3657]],

         [[0.4426, 0.6366, 0.4517,  ..., 0.3754, 0.4156, 0.6469],
          [0.4404, 0.5111, 0.5406,  ..., 0.4716, 0.6365, 0.6379],
          [0.5021, 0.5159, 0.5587,  ..., 0.6402, 0.3733, 0.4467],
          [0.5336, 0.4568, 0.4820,  ..., 0.5606, 0.4142, 0.4983]]]],
       device='cuda:0')
tensor([[[[0.4073, 0.5161, 0.4888,  ..., 0.4479, 0.5960, 0.4879],
          [0.4201, 0.5636, 0.4335,  ..., 0.4649, 0.3919, 0.4273],
          [0.4940, 0.6039, 0.5221,  ..., 0.5056, 0.4383, 0.5392],
          [0.5574, 0.4446, 0.5550,  ..., 0.3831, 0.4373, 0.4116]],

         [[0.5528, 0.5922, 0.4254,  ..., 0.5218, 0.3956, 0.4125],
          [0.4740, 0.5950, 0.5470,  ..., 0.5035, 0.4525, 0.5101],
          [0.5542, 0.5243, 0.4559,  ..., 0.4163, 0.3989, 0.4325],
          [0.4862, 0.5022, 0.3821,  ..., 0.2351, 0.4135, 0.4706]],

         [[0.4895, 0.5280, 0.5234,  ..., 0.5770, 0.4902, 0.5479],
          [0.4211, 0.6225, 0.6091,  ..., 0.5494, 0.4477, 0.3891],
          [0.6343, 0.5178, 0.4154,  ..., 0.5842, 0.4705, 0.5433],
          [0.4125, 0.4410, 0.5284,  ..., 0.5172, 0.4400, 0.6179]],

         ...,

         [[0.4808, 0.3942, 0.4778,  ..., 0.6118, 0.4196, 0.5542],
          [0.5195, 0.5074, 0.4073,  ..., 0.6567, 0.5271, 0.4946],
          [0.4767, 0.4758, 0.3882,  ..., 0.4647, 0.5170, 0.5114],
          [0.2379, 0.4928, 0.5562,  ..., 0.4530, 0.5433, 0.5157]],

         [[0.5257, 0.6261, 0.3294,  ..., 0.4268, 0.4244, 0.6252],
          [0.4552, 0.3478, 0.4866,  ..., 0.5160, 0.4475, 0.4921],
          [0.5898, 0.4106, 0.3970,  ..., 0.4523, 0.4981, 0.4282],
          [0.3603, 0.6645, 0.3905,  ..., 0.3984, 0.6077, 0.5334]],

         [[0.5851, 0.4776, 0.5426,  ..., 0.5002, 0.5072, 0.5978],
          [0.4861, 0.4054, 0.4316,  ..., 0.4460, 0.6851, 0.5356],
          [0.4576, 0.4569, 0.5765,  ..., 0.4316, 0.5366, 0.4528],
          [0.5818, 0.4564, 0.4201,  ..., 0.2999, 0.5018, 0.4988]]],


        [[[0.4617, 0.5742, 0.4780,  ..., 0.7090, 0.4400, 0.5349],
          [0.3702, 0.5426, 0.6316,  ..., 0.5290, 0.3826, 0.5884],
          [0.5121, 0.6514, 0.5521,  ..., 0.5521, 0.6826, 0.4794],
          [0.4135, 0.3789, 0.5397,  ..., 0.5056, 0.4511, 0.6183]],

         [[0.4888, 0.4783, 0.4851,  ..., 0.4463, 0.3849, 0.5988],
          [0.6086, 0.5448, 0.5593,  ..., 0.6146, 0.3812, 0.4381],
          [0.4177, 0.5329, 0.3979,  ..., 0.5308, 0.6215, 0.3141],
          [0.5315, 0.3961, 0.5042,  ..., 0.4383, 0.5327, 0.3849]],

         [[0.5006, 0.6514, 0.4220,  ..., 0.4508, 0.4125, 0.3956],
          [0.5479, 0.5339, 0.3886,  ..., 0.5042, 0.4958, 0.4239],
          [0.7162, 0.4973, 0.3648,  ..., 0.3914, 0.5713, 0.5655],
          [0.4111, 0.4518, 0.6030,  ..., 0.5257, 0.5306, 0.3844]],

         ...,

         [[0.5756, 0.4598, 0.6202,  ..., 0.5182, 0.4843, 0.6901],
          [0.5265, 0.5124, 0.5537,  ..., 0.4819, 0.5289, 0.4763],
          [0.4683, 0.5610, 0.6993,  ..., 0.5259, 0.5443, 0.5421],
          [0.4521, 0.5941, 0.5799,  ..., 0.5153, 0.5467, 0.4487]],

         [[0.4772, 0.4726, 0.5722,  ..., 0.6843, 0.5542, 0.2379],
          [0.5353, 0.4615, 0.4950,  ..., 0.3821, 0.4523, 0.5554],
          [0.4780, 0.3693, 0.5317,  ..., 0.4504, 0.4211, 0.4220],
          [0.5694, 0.5436, 0.5770,  ..., 0.6035, 0.4431, 0.3657]],

         [[0.4446, 0.6406, 0.4477,  ..., 0.3794, 0.4116, 0.6469],
          [0.4424, 0.5151, 0.5366,  ..., 0.4756, 0.6325, 0.6379],
          [0.5041, 0.5199, 0.5547,  ..., 0.6442, 0.3693, 0.4467],
          [0.5356, 0.4608, 0.4780,  ..., 0.5646, 0.4102, 0.4983]]]],
       device='cuda:0', requires_grad=True)
tensor([-2.0000e-03, -4.0000e-03,  4.0000e-03,  1.0000e-02,  2.3283e-10,
         8.0000e-03, -1.0000e-02, -4.0000e-03,  4.0000e-03,  0.0000e+00],
       device='cuda:0')
selected experts tensor([1585, 1521, 1677, 1592, 1658, 1747, 1673, 1709, 1614, 1608],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5013, 0.5984, 0.4360,  ..., 0.3616, 0.5273, 0.5590],
          [0.4487, 0.5573, 0.4577,  ..., 0.6013, 0.5174, 0.3970],
          [0.5223, 0.4152, 0.5299,  ..., 0.5032, 0.5840, 0.3784],
          [0.6410, 0.4239, 0.4266,  ..., 0.5279, 0.4866, 0.6341]],

         [[0.4519, 0.5349, 0.3906,  ..., 0.4166, 0.5509, 0.5955],
          [0.4195, 0.4340, 0.5239,  ..., 0.6767, 0.6841, 0.4496],
          [0.5877, 0.5085, 0.4657,  ..., 0.5062, 0.4575, 0.5903],
          [0.4512, 0.4714, 0.5485,  ..., 0.4628, 0.4988, 0.6424]],

         [[0.4675, 0.5308, 0.4381,  ..., 0.5134, 0.5061, 0.5531],
          [0.3618, 0.5616, 0.4166,  ..., 0.7337, 0.5782, 0.4386],
          [0.4422, 0.4308, 0.5188,  ..., 0.3634, 0.5352, 0.4793],
          [0.5141, 0.3775, 0.5284,  ..., 0.5265, 0.5100, 0.5016]],

         ...,

         [[0.4736, 0.5919, 0.3869,  ..., 0.5239, 0.5074, 0.6008],
          [0.3019, 0.5664, 0.2725,  ..., 0.5825, 0.4173, 0.4487],
          [0.4949, 0.4962, 0.5475,  ..., 0.4776, 0.3791, 0.4155],
          [0.4879, 0.4512, 0.5658,  ..., 0.4028, 0.4347, 0.5336]],

         [[0.5070, 0.5881, 0.4473,  ..., 0.3680, 0.5153, 0.6350],
          [0.3719, 0.5919, 0.5985,  ..., 0.4889, 0.5717, 0.4535],
          [0.4708, 0.3609, 0.5434,  ..., 0.4033, 0.5642, 0.5313],
          [0.3887, 0.5177, 0.5233,  ..., 0.4889, 0.5357, 0.6229]],

         [[0.4859, 0.4449, 0.3804,  ..., 0.4461, 0.4998, 0.5599],
          [0.4391, 0.4950, 0.5905,  ..., 0.4319, 0.6177, 0.5470],
          [0.4294, 0.5170, 0.5071,  ..., 0.5284, 0.4652, 0.5176],
          [0.4028, 0.5112, 0.4684,  ..., 0.4218, 0.3982, 0.5350]]],


        [[[0.4761, 0.4558, 0.5463,  ..., 0.4728, 0.5325, 0.5288],
          [0.4821, 0.6222, 0.4572,  ..., 0.4650, 0.5840, 0.5526],
          [0.4531, 0.5573, 0.4633,  ..., 0.4335, 0.4201, 0.3580],
          [0.3849, 0.4982, 0.5610,  ..., 0.4808, 0.3881, 0.5744]],

         [[0.4229, 0.5420, 0.5677,  ..., 0.3901, 0.4739, 0.5429],
          [0.5053, 0.5279, 0.5815,  ..., 0.5744, 0.4836, 0.5327],
          [0.5937, 0.4785, 0.5877,  ..., 0.5425, 0.5002, 0.6336],
          [0.4417, 0.3943, 0.5241,  ..., 0.4618, 0.3863, 0.5807]],

         [[0.5086, 0.4820, 0.3607,  ..., 0.4379, 0.5801, 0.6470],
          [0.3392, 0.5468, 0.5853,  ..., 0.5734, 0.6876, 0.3784],
          [0.5477, 0.5583, 0.5289,  ..., 0.4319, 0.3525, 0.4174],
          [0.5313, 0.5417, 0.5151,  ..., 0.4810, 0.4529, 0.5397]],

         ...,

         [[0.5136, 0.5359, 0.4946,  ..., 0.4597, 0.6092, 0.5955],
          [0.6437, 0.4844, 0.4487,  ..., 0.4983, 0.5065, 0.5898],
          [0.5764, 0.4559, 0.5687,  ..., 0.5677, 0.6172, 0.4127],
          [0.3817, 0.4119, 0.4185,  ..., 0.5252, 0.2875, 0.4025]],

         [[0.5357, 0.4802, 0.6124,  ..., 0.4417, 0.4173, 0.6098],
          [0.4649, 0.5745, 0.5449,  ..., 0.4340, 0.4519, 0.5653],
          [0.6303, 0.2788, 0.5296,  ..., 0.5910, 0.5570, 0.4458],
          [0.4010, 0.6533, 0.5132,  ..., 0.5071, 0.3959, 0.6470]],

         [[0.5169, 0.5153, 0.5758,  ..., 0.4403, 0.5405, 0.4530],
          [0.5525, 0.5569, 0.5839,  ..., 0.5342, 0.6495, 0.6355],
          [0.5415, 0.4957, 0.6220,  ..., 0.4850, 0.5057, 0.4749],
          [0.4675, 0.4813, 0.4808,  ..., 0.4705, 0.4343, 0.4858]]]],
       device='cuda:0')
tensor([[[[0.5143, 0.6114, 0.4410,  ..., 0.3666, 0.5103, 0.5400],
          [0.4617, 0.5703, 0.4627,  ..., 0.6063, 0.5004, 0.3780],
          [0.5353, 0.4282, 0.5349,  ..., 0.5082, 0.5670, 0.3594],
          [0.6540, 0.4369, 0.4316,  ..., 0.5329, 0.4696, 0.6151]],

         [[0.4649, 0.5479, 0.3956,  ..., 0.4216, 0.5339, 0.5765],
          [0.4325, 0.4470, 0.5289,  ..., 0.6817, 0.6671, 0.4306],
          [0.6007, 0.5215, 0.4707,  ..., 0.5112, 0.4405, 0.5713],
          [0.4642, 0.4844, 0.5535,  ..., 0.4678, 0.4818, 0.6234]],

         [[0.4805, 0.5438, 0.4431,  ..., 0.5184, 0.4891, 0.5341],
          [0.3748, 0.5746, 0.4216,  ..., 0.7387, 0.5612, 0.4196],
          [0.4552, 0.4438, 0.5238,  ..., 0.3684, 0.5182, 0.4603],
          [0.5271, 0.3905, 0.5334,  ..., 0.5315, 0.4930, 0.4826]],

         ...,

         [[0.4866, 0.6049, 0.3919,  ..., 0.5289, 0.4904, 0.5818],
          [0.3149, 0.5794, 0.2775,  ..., 0.5875, 0.4003, 0.4297],
          [0.5079, 0.5092, 0.5525,  ..., 0.4826, 0.3621, 0.3965],
          [0.5009, 0.4642, 0.5708,  ..., 0.4078, 0.4177, 0.5146]],

         [[0.5200, 0.6011, 0.4523,  ..., 0.3730, 0.4983, 0.6160],
          [0.3849, 0.6049, 0.6035,  ..., 0.4939, 0.5547, 0.4345],
          [0.4838, 0.3739, 0.5484,  ..., 0.4083, 0.5472, 0.5123],
          [0.4017, 0.5307, 0.5283,  ..., 0.4939, 0.5187, 0.6039]],

         [[0.4989, 0.4579, 0.3854,  ..., 0.4511, 0.4828, 0.5409],
          [0.4521, 0.5080, 0.5955,  ..., 0.4369, 0.6007, 0.5280],
          [0.4424, 0.5300, 0.5121,  ..., 0.5334, 0.4482, 0.4986],
          [0.4158, 0.5242, 0.4734,  ..., 0.4268, 0.3812, 0.5160]]],


        [[[0.4891, 0.4688, 0.5513,  ..., 0.4778, 0.5155, 0.5098],
          [0.4951, 0.6352, 0.4622,  ..., 0.4700, 0.5670, 0.5336],
          [0.4661, 0.5703, 0.4683,  ..., 0.4385, 0.4031, 0.3390],
          [0.3979, 0.5112, 0.5660,  ..., 0.4858, 0.3711, 0.5554]],

         [[0.4359, 0.5550, 0.5727,  ..., 0.3951, 0.4569, 0.5239],
          [0.5183, 0.5409, 0.5865,  ..., 0.5794, 0.4666, 0.5137],
          [0.6067, 0.4915, 0.5927,  ..., 0.5475, 0.4832, 0.6146],
          [0.4547, 0.4073, 0.5291,  ..., 0.4668, 0.3693, 0.5617]],

         [[0.5216, 0.4950, 0.3657,  ..., 0.4429, 0.5631, 0.6280],
          [0.3522, 0.5598, 0.5903,  ..., 0.5784, 0.6706, 0.3594],
          [0.5607, 0.5713, 0.5339,  ..., 0.4369, 0.3355, 0.3984],
          [0.5443, 0.5547, 0.5201,  ..., 0.4860, 0.4359, 0.5207]],

         ...,

         [[0.5266, 0.5489, 0.4996,  ..., 0.4647, 0.5922, 0.5765],
          [0.6567, 0.4974, 0.4537,  ..., 0.5033, 0.4895, 0.5708],
          [0.5894, 0.4689, 0.5737,  ..., 0.5727, 0.6002, 0.3937],
          [0.3947, 0.4249, 0.4235,  ..., 0.5302, 0.2705, 0.3835]],

         [[0.5487, 0.4932, 0.6174,  ..., 0.4467, 0.4003, 0.5908],
          [0.4779, 0.5875, 0.5499,  ..., 0.4390, 0.4349, 0.5463],
          [0.6433, 0.2918, 0.5346,  ..., 0.5960, 0.5400, 0.4268],
          [0.4140, 0.6663, 0.5182,  ..., 0.5121, 0.3789, 0.6280]],

         [[0.5299, 0.5283, 0.5808,  ..., 0.4453, 0.5235, 0.4340],
          [0.5655, 0.5699, 0.5889,  ..., 0.5392, 0.6325, 0.6165],
          [0.5545, 0.5087, 0.6270,  ..., 0.4900, 0.4887, 0.4559],
          [0.4805, 0.4943, 0.4858,  ..., 0.4755, 0.4173, 0.4668]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0130, -0.0130, -0.0050, -0.0010, -0.0190,  0.0210, -0.0210, -0.0050,
         0.0170,  0.0190], device='cuda:0')
selected experts tensor([1270, 1444, 1228, 2296, 1526, 1602, 2349, 1464, 1510, 1695],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5649, 0.5825, 0.5388,  ..., 0.3605, 0.6037, 0.5045],
          [0.4370, 0.5329, 0.4974,  ..., 0.5180, 0.5642, 0.3668],
          [0.5994, 0.3881, 0.4246,  ..., 0.4570, 0.5449, 0.5947],
          [0.5620, 0.5498, 0.4780,  ..., 0.5415, 0.5189, 0.4197]],

         [[0.5268, 0.5027, 0.4958,  ..., 0.3276, 0.6232, 0.5234],
          [0.4748, 0.4479, 0.4647,  ..., 0.5720, 0.5129, 0.5141],
          [0.5286, 0.5093, 0.3714,  ..., 0.4956, 0.5558, 0.5263],
          [0.5055, 0.4375, 0.5724,  ..., 0.5103, 0.4346, 0.4383]],

         [[0.4858, 0.4390, 0.4558,  ..., 0.3973, 0.4435, 0.4268],
          [0.4713, 0.4896, 0.7014,  ..., 0.5756, 0.4331, 0.4640],
          [0.4604, 0.4854, 0.3696,  ..., 0.4840, 0.5053, 0.4795],
          [0.4802, 0.5645, 0.5534,  ..., 0.4292, 0.6195, 0.4520]],

         ...,

         [[0.5030, 0.5050, 0.5030,  ..., 0.5110, 0.5030, 0.5110],
          [0.5030, 0.5050, 0.5030,  ..., 0.5110, 0.5030, 0.5110],
          [0.5030, 0.5050, 0.5030,  ..., 0.5110, 0.5030, 0.5110],
          [0.5030, 0.5050, 0.5030,  ..., 0.5110, 0.5030, 0.5110]],

         [[0.5030, 0.5050, 0.5030,  ..., 0.5110, 0.5030, 0.5110],
          [0.5030, 0.5050, 0.5030,  ..., 0.5110, 0.5030, 0.5110],
          [0.5030, 0.5050, 0.5030,  ..., 0.5110, 0.5030, 0.5110],
          [0.5030, 0.5050, 0.5030,  ..., 0.5110, 0.5030, 0.5110]],

         [[0.5030, 0.5050, 0.5030,  ..., 0.5110, 0.5030, 0.5110],
          [0.5030, 0.5050, 0.5030,  ..., 0.5110, 0.5030, 0.5110],
          [0.5030, 0.5050, 0.5030,  ..., 0.5110, 0.5030, 0.5110],
          [0.5030, 0.5050, 0.5030,  ..., 0.5110, 0.5030, 0.5110]]],


        [[[0.5106, 0.6127, 0.3095,  ..., 0.4335, 0.4577, 0.3175],
          [0.4985, 0.4180, 0.6300,  ..., 0.5366, 0.4468, 0.4879],
          [0.5264, 0.6261, 0.3606,  ..., 0.3200, 0.4413, 0.5514],
          [0.4597, 0.4147, 0.5049,  ..., 0.4589, 0.5228, 0.4558]],

         [[0.5990, 0.4949, 0.5524,  ..., 0.4883, 0.5154, 0.4245],
          [0.5876, 0.4742, 0.4633,  ..., 0.4019, 0.5666, 0.5756],
          [0.5172, 0.5624, 0.4868,  ..., 0.4848, 0.5671, 0.4426],
          [0.5952, 0.5575, 0.3769,  ..., 0.4570, 0.4246, 0.4174]],

         [[0.5003, 0.4590, 0.5476,  ..., 0.3978, 0.3481, 0.5397],
          [0.4538, 0.5072, 0.6544,  ..., 0.5885, 0.4193, 0.4188],
          [0.5471, 0.5428, 0.5179,  ..., 0.4066, 0.6623, 0.5710],
          [0.4459, 0.4858, 0.5081,  ..., 0.5083, 0.5493, 0.4169]],

         ...,

         [[0.5030, 0.5050, 0.5030,  ..., 0.5110, 0.5030, 0.5110],
          [0.5030, 0.5050, 0.5030,  ..., 0.5110, 0.5030, 0.5110],
          [0.5030, 0.5050, 0.5030,  ..., 0.5110, 0.5030, 0.5110],
          [0.5030, 0.5050, 0.5030,  ..., 0.5110, 0.5030, 0.5110]],

         [[0.5030, 0.5050, 0.5030,  ..., 0.5110, 0.5030, 0.5110],
          [0.5030, 0.5050, 0.5030,  ..., 0.5110, 0.5030, 0.5110],
          [0.5030, 0.5050, 0.5030,  ..., 0.5110, 0.5030, 0.5110],
          [0.5030, 0.5050, 0.5030,  ..., 0.5110, 0.5030, 0.5110]],

         [[0.5030, 0.5050, 0.5030,  ..., 0.5110, 0.5030, 0.5110],
          [0.5030, 0.5050, 0.5030,  ..., 0.5110, 0.5030, 0.5110],
          [0.5030, 0.5050, 0.5030,  ..., 0.5110, 0.5030, 0.5110],
          [0.5030, 0.5050, 0.5030,  ..., 0.5110, 0.5030, 0.5110]]]],
       device='cuda:0')
tensor([[[[0.5619, 0.5775, 0.5358,  ..., 0.3495, 0.6007, 0.4935],
          [0.4340, 0.5279, 0.4944,  ..., 0.5070, 0.5612, 0.3558],
          [0.5964, 0.3831, 0.4216,  ..., 0.4460, 0.5419, 0.5837],
          [0.5590, 0.5448, 0.4750,  ..., 0.5305, 0.5159, 0.4087]],

         [[0.5238, 0.4977, 0.4928,  ..., 0.3166, 0.6202, 0.5124],
          [0.4718, 0.4429, 0.4617,  ..., 0.5610, 0.5099, 0.5031],
          [0.5256, 0.5043, 0.3684,  ..., 0.4846, 0.5528, 0.5153],
          [0.5025, 0.4325, 0.5694,  ..., 0.4993, 0.4316, 0.4273]],

         [[0.4828, 0.4340, 0.4528,  ..., 0.3863, 0.4405, 0.4158],
          [0.4683, 0.4846, 0.6984,  ..., 0.5646, 0.4301, 0.4530],
          [0.4574, 0.4804, 0.3666,  ..., 0.4730, 0.5023, 0.4685],
          [0.4772, 0.5595, 0.5504,  ..., 0.4182, 0.6165, 0.4410]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5076, 0.6077, 0.3065,  ..., 0.4225, 0.4547, 0.3065],
          [0.4955, 0.4130, 0.6270,  ..., 0.5256, 0.4438, 0.4769],
          [0.5234, 0.6211, 0.3576,  ..., 0.3090, 0.4383, 0.5404],
          [0.4567, 0.4097, 0.5019,  ..., 0.4479, 0.5198, 0.4448]],

         [[0.5960, 0.4899, 0.5494,  ..., 0.4773, 0.5124, 0.4135],
          [0.5846, 0.4692, 0.4603,  ..., 0.3909, 0.5636, 0.5646],
          [0.5142, 0.5574, 0.4838,  ..., 0.4738, 0.5641, 0.4316],
          [0.5922, 0.5525, 0.3739,  ..., 0.4460, 0.4216, 0.4064]],

         [[0.4973, 0.4540, 0.5446,  ..., 0.3868, 0.3451, 0.5287],
          [0.4508, 0.5022, 0.6514,  ..., 0.5775, 0.4163, 0.4078],
          [0.5441, 0.5378, 0.5149,  ..., 0.3956, 0.6593, 0.5600],
          [0.4429, 0.4808, 0.5051,  ..., 0.4973, 0.5463, 0.4059]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([ 0.0030,  0.0050,  0.0030,  0.0070, -0.0090,  0.0090,  0.0090,  0.0110,
         0.0030,  0.0110], device='cuda:0')
selected experts tensor([1710, 2096, 1593, 1172, 1659,  664, 1151, 2017, 1869, 2453],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1585., 1521., 1677., 1592., 1658., 1747., 1673., 1709., 1614., 1608.],
        [1710., 2096., 1593., 1172., 1659.,  664., 1151., 2017., 1869., 2453.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5897, 0.4928, 0.4055,  ..., 0.4228, 0.4235, 0.4864],
          [0.3879, 0.5724, 0.5436,  ..., 0.5369, 0.4552, 0.4205],
          [0.5963, 0.5114, 0.6963,  ..., 0.5419, 0.4164, 0.4443],
          [0.4110, 0.5843, 0.6865,  ..., 0.4741, 0.6096, 0.4410]],

         [[0.4634, 0.4113, 0.5368,  ..., 0.5274, 0.5067, 0.5778],
          [0.6084, 0.4226, 0.3839,  ..., 0.7342, 0.5689, 0.4649],
          [0.4721, 0.5095, 0.6627,  ..., 0.5246, 0.3435, 0.4819],
          [0.5793, 0.5980, 0.5535,  ..., 0.3947, 0.6045, 0.5528]],

         [[0.5125, 0.4725, 0.4987,  ..., 0.5346, 0.5641, 0.4741],
          [0.4903, 0.6121, 0.4613,  ..., 0.5605, 0.4097, 0.4646],
          [0.2721, 0.5053, 0.3623,  ..., 0.3836, 0.5197, 0.5797],
          [0.5428, 0.4298, 0.5236,  ..., 0.4746, 0.4436, 0.3556]],

         ...,

         [[0.5121, 0.5714, 0.5638,  ..., 0.5050, 0.4934, 0.4861],
          [0.4604, 0.6102, 0.4458,  ..., 0.4026, 0.4832, 0.4362],
          [0.4462, 0.5063, 0.4117,  ..., 0.4455, 0.5908, 0.4975],
          [0.4375, 0.4103, 0.5315,  ..., 0.5682, 0.6280, 0.4007]],

         [[0.4694, 0.4611, 0.4727,  ..., 0.5023, 0.5608, 0.6637],
          [0.4715, 0.4587, 0.5135,  ..., 0.3574, 0.4564, 0.5058],
          [0.5688, 0.3769, 0.3533,  ..., 0.5610, 0.4150, 0.3503],
          [0.4949, 0.5146, 0.4088,  ..., 0.6193, 0.5436, 0.5503]],

         [[0.5149, 0.5173, 0.3054,  ..., 0.3882, 0.4374, 0.4305],
          [0.5204, 0.5095, 0.3728,  ..., 0.4925, 0.4398, 0.7087],
          [0.4692, 0.4103, 0.4533,  ..., 0.5571, 0.3067, 0.5883],
          [0.4119, 0.5480, 0.4840,  ..., 0.4738, 0.4235, 0.4921]]],


        [[[0.5428, 0.4815, 0.4117,  ..., 0.5374, 0.4268, 0.5840],
          [0.5110, 0.5456, 0.2897,  ..., 0.5835, 0.4206, 0.5140],
          [0.5149, 0.4959, 0.4070,  ..., 0.4371, 0.4886, 0.5180],
          [0.5203, 0.4170, 0.6300,  ..., 0.4167, 0.4494, 0.5518]],

         [[0.4646, 0.4174, 0.5390,  ..., 0.4922, 0.5723, 0.5859],
          [0.5740, 0.4327, 0.5405,  ..., 0.5930, 0.4526, 0.5174],
          [0.4874, 0.5047, 0.3390,  ..., 0.2892, 0.6443, 0.5997],
          [0.5503, 0.3861, 0.3515,  ..., 0.5740, 0.5625, 0.7256]],

         [[0.5517, 0.5089, 0.6972,  ..., 0.5266, 0.5031, 0.5025],
          [0.5911, 0.6209, 0.5310,  ..., 0.5874, 0.4916, 0.4564],
          [0.4300, 0.5714, 0.4117,  ..., 0.4928, 0.5511, 0.4938],
          [0.6693, 0.5814, 0.4596,  ..., 0.4598, 0.4216, 0.4909]],

         ...,

         [[0.5034, 0.5403, 0.6575,  ..., 0.5047, 0.4451, 0.3094],
          [0.5384, 0.5240, 0.6255,  ..., 0.3574, 0.5329, 0.6476],
          [0.5088, 0.5611, 0.4809,  ..., 0.5290, 0.6612, 0.4224],
          [0.5020, 0.5004, 0.3756,  ..., 0.5501, 0.5904, 0.5987]],

         [[0.6075, 0.5035, 0.5724,  ..., 0.5888, 0.5061, 0.4224],
          [0.4674, 0.6213, 0.6452,  ..., 0.3763, 0.4879, 0.5316],
          [0.4020, 0.5661, 0.5262,  ..., 0.4798, 0.6353, 0.5897],
          [0.4348, 0.3428, 0.4795,  ..., 0.4697, 0.3961, 0.4811]],

         [[0.5991, 0.5700, 0.5535,  ..., 0.4357, 0.2920, 0.5440],
          [0.4219, 0.3669, 0.5378,  ..., 0.3530, 0.5543, 0.4309],
          [0.4062, 0.5471, 0.5540,  ..., 0.5831, 0.3452, 0.6063],
          [0.4486, 0.5127, 0.4801,  ..., 0.5564, 0.3924, 0.5039]]]],
       device='cuda:0')
tensor([[[[0.5927, 0.4898, 0.4125,  ..., 0.4158, 0.4225, 0.4794],
          [0.3909, 0.5694, 0.5506,  ..., 0.5299, 0.4542, 0.4135],
          [0.5993, 0.5084, 0.7033,  ..., 0.5349, 0.4154, 0.4373],
          [0.4140, 0.5813, 0.6935,  ..., 0.4671, 0.6086, 0.4340]],

         [[0.4664, 0.4083, 0.5438,  ..., 0.5204, 0.5057, 0.5708],
          [0.6114, 0.4196, 0.3909,  ..., 0.7272, 0.5679, 0.4579],
          [0.4751, 0.5065, 0.6697,  ..., 0.5176, 0.3425, 0.4749],
          [0.5823, 0.5950, 0.5605,  ..., 0.3877, 0.6035, 0.5458]],

         [[0.5155, 0.4695, 0.5057,  ..., 0.5276, 0.5631, 0.4671],
          [0.4933, 0.6091, 0.4683,  ..., 0.5535, 0.4087, 0.4576],
          [0.2751, 0.5023, 0.3693,  ..., 0.3766, 0.5187, 0.5727],
          [0.5458, 0.4268, 0.5306,  ..., 0.4676, 0.4426, 0.3486]],

         ...,

         [[0.5151, 0.5684, 0.5708,  ..., 0.4980, 0.4924, 0.4791],
          [0.4634, 0.6072, 0.4528,  ..., 0.3956, 0.4822, 0.4292],
          [0.4492, 0.5033, 0.4187,  ..., 0.4385, 0.5898, 0.4905],
          [0.4405, 0.4073, 0.5385,  ..., 0.5612, 0.6270, 0.3937]],

         [[0.4724, 0.4581, 0.4797,  ..., 0.4953, 0.5598, 0.6567],
          [0.4745, 0.4557, 0.5205,  ..., 0.3504, 0.4554, 0.4988],
          [0.5718, 0.3739, 0.3603,  ..., 0.5540, 0.4140, 0.3433],
          [0.4979, 0.5116, 0.4158,  ..., 0.6123, 0.5426, 0.5433]],

         [[0.5179, 0.5143, 0.3124,  ..., 0.3812, 0.4364, 0.4235],
          [0.5234, 0.5065, 0.3798,  ..., 0.4855, 0.4388, 0.7017],
          [0.4722, 0.4073, 0.4603,  ..., 0.5501, 0.3057, 0.5813],
          [0.4149, 0.5450, 0.4910,  ..., 0.4668, 0.4225, 0.4851]]],


        [[[0.5458, 0.4785, 0.4187,  ..., 0.5304, 0.4258, 0.5770],
          [0.5140, 0.5426, 0.2967,  ..., 0.5765, 0.4196, 0.5070],
          [0.5179, 0.4929, 0.4140,  ..., 0.4301, 0.4876, 0.5110],
          [0.5233, 0.4140, 0.6370,  ..., 0.4097, 0.4484, 0.5448]],

         [[0.4676, 0.4144, 0.5460,  ..., 0.4852, 0.5713, 0.5789],
          [0.5770, 0.4297, 0.5475,  ..., 0.5860, 0.4516, 0.5104],
          [0.4904, 0.5017, 0.3460,  ..., 0.2822, 0.6433, 0.5927],
          [0.5533, 0.3831, 0.3585,  ..., 0.5670, 0.5615, 0.7186]],

         [[0.5547, 0.5059, 0.7042,  ..., 0.5196, 0.5021, 0.4955],
          [0.5941, 0.6179, 0.5380,  ..., 0.5804, 0.4906, 0.4494],
          [0.4330, 0.5684, 0.4187,  ..., 0.4858, 0.5501, 0.4868],
          [0.6723, 0.5784, 0.4666,  ..., 0.4528, 0.4206, 0.4839]],

         ...,

         [[0.5064, 0.5373, 0.6645,  ..., 0.4977, 0.4441, 0.3024],
          [0.5414, 0.5210, 0.6325,  ..., 0.3504, 0.5319, 0.6406],
          [0.5118, 0.5581, 0.4879,  ..., 0.5220, 0.6602, 0.4154],
          [0.5050, 0.4974, 0.3826,  ..., 0.5431, 0.5894, 0.5917]],

         [[0.6105, 0.5005, 0.5794,  ..., 0.5818, 0.5051, 0.4154],
          [0.4704, 0.6183, 0.6522,  ..., 0.3693, 0.4869, 0.5246],
          [0.4050, 0.5631, 0.5332,  ..., 0.4728, 0.6343, 0.5827],
          [0.4378, 0.3398, 0.4865,  ..., 0.4627, 0.3951, 0.4741]],

         [[0.6021, 0.5670, 0.5605,  ..., 0.4287, 0.2910, 0.5370],
          [0.4249, 0.3639, 0.5448,  ..., 0.3460, 0.5533, 0.4239],
          [0.4092, 0.5441, 0.5610,  ..., 0.5761, 0.3442, 0.5993],
          [0.4516, 0.5097, 0.4871,  ..., 0.5494, 0.3914, 0.4969]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0030,  0.0030, -0.0070,  0.0010, -0.0050, -0.0050,  0.0070,  0.0070,
         0.0010,  0.0070], device='cuda:0')
selected experts tensor([1735, 1638, 1696, 1661, 1655, 1544, 1633, 1615, 1572, 1635],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4073, 0.4742, 0.5181,  ..., 0.6560, 0.5074, 0.4027],
          [0.4991, 0.4483, 0.3561,  ..., 0.5235, 0.5691, 0.4465],
          [0.5660, 0.4767, 0.5843,  ..., 0.4727, 0.5332, 0.4825],
          [0.5573, 0.4624, 0.6436,  ..., 0.4582, 0.4998, 0.6115]],

         [[0.5655, 0.5702, 0.5318,  ..., 0.5782, 0.5157, 0.4410],
          [0.4571, 0.4602, 0.4888,  ..., 0.4729, 0.4878, 0.6612],
          [0.5013, 0.4343, 0.4870,  ..., 0.4771, 0.5583, 0.4259],
          [0.5197, 0.4964, 0.5743,  ..., 0.4653, 0.6411, 0.4899]],

         [[0.6773, 0.4437, 0.4449,  ..., 0.4071, 0.6943, 0.4832],
          [0.4680, 0.5044, 0.6579,  ..., 0.3962, 0.4692, 0.5497],
          [0.5002, 0.4816, 0.3633,  ..., 0.6543, 0.6537, 0.3252],
          [0.5138, 0.4658, 0.3597,  ..., 0.6604, 0.6010, 0.5017]],

         ...,

         [[0.4772, 0.4997, 0.4849,  ..., 0.5291, 0.5182, 0.5545],
          [0.5091, 0.6162, 0.4255,  ..., 0.6068, 0.4390, 0.6335],
          [0.5296, 0.4300, 0.5890,  ..., 0.5274, 0.5330, 0.2840],
          [0.4745, 0.7060, 0.5341,  ..., 0.5905, 0.4959, 0.4069]],

         [[0.4673, 0.5411, 0.5009,  ..., 0.6078, 0.3653, 0.4088],
          [0.4874, 0.5191, 0.6107,  ..., 0.4497, 0.5505, 0.6629],
          [0.4757, 0.6037, 0.6158,  ..., 0.4911, 0.5616, 0.4112],
          [0.3173, 0.3421, 0.5534,  ..., 0.6302, 0.4704, 0.3712]],

         [[0.4939, 0.3264, 0.4246,  ..., 0.6742, 0.5207, 0.4379],
          [0.4059, 0.6023, 0.5548,  ..., 0.4846, 0.4902, 0.3878],
          [0.4803, 0.3921, 0.5454,  ..., 0.3607, 0.3867, 0.4112],
          [0.5698, 0.5731, 0.6981,  ..., 0.6534, 0.3057, 0.5837]]],


        [[[0.4675, 0.6838, 0.3750,  ..., 0.5396, 0.5002, 0.6690],
          [0.4428, 0.6313, 0.3888,  ..., 0.3915, 0.5534, 0.5385],
          [0.5774, 0.6240, 0.5791,  ..., 0.6138, 0.5944, 0.4326],
          [0.4983, 0.5263, 0.5976,  ..., 0.4822, 0.4648, 0.6091]],

         [[0.4892, 0.5575, 0.5381,  ..., 0.3643, 0.5948, 0.6577],
          [0.4523, 0.4100, 0.6856,  ..., 0.5677, 0.5944, 0.5156],
          [0.5114, 0.3264, 0.6227,  ..., 0.6767, 0.5597, 0.3559],
          [0.6324, 0.3027, 0.5524,  ..., 0.5425, 0.4590, 0.5680]],

         [[0.4968, 0.5093, 0.3307,  ..., 0.4531, 0.4871, 0.5591],
          [0.5564, 0.5476, 0.4611,  ..., 0.4885, 0.5435, 0.3845],
          [0.5072, 0.4859, 0.3633,  ..., 0.5204, 0.4185, 0.5312],
          [0.5552, 0.5045, 0.4336,  ..., 0.3972, 0.4731, 0.4811]],

         ...,

         [[0.4881, 0.5241, 0.5275,  ..., 0.5715, 0.4949, 0.5492],
          [0.4201, 0.6195, 0.6125,  ..., 0.5444, 0.4527, 0.3901],
          [0.6333, 0.5148, 0.4184,  ..., 0.5792, 0.4755, 0.5443],
          [0.4115, 0.4380, 0.5313,  ..., 0.5121, 0.4452, 0.6193]],

         [[0.4603, 0.5026, 0.4892,  ..., 0.3057, 0.5729, 0.3919],
          [0.4438, 0.4181, 0.3732,  ..., 0.4228, 0.3698, 0.4977],
          [0.4049, 0.6014, 0.5145,  ..., 0.5033, 0.4697, 0.5195],
          [0.4718, 0.6195, 0.6822,  ..., 0.4381, 0.4877, 0.5152]],

         [[0.3927, 0.4281, 0.5369,  ..., 0.5971, 0.5300, 0.6603],
          [0.4702, 0.2880, 0.2980,  ..., 0.5701, 0.5016, 0.4369],
          [0.4586, 0.4851, 0.5295,  ..., 0.3508, 0.5180, 0.4302],
          [0.5007, 0.5548, 0.5872,  ..., 0.5232, 0.4697, 0.4937]]]],
       device='cuda:0')
tensor([[[[0.4083, 0.4772, 0.5151,  ..., 0.6610, 0.5024, 0.4017],
          [0.5001, 0.4513, 0.3531,  ..., 0.5285, 0.5641, 0.4455],
          [0.5670, 0.4797, 0.5813,  ..., 0.4777, 0.5282, 0.4815],
          [0.5583, 0.4654, 0.6406,  ..., 0.4632, 0.4948, 0.6105]],

         [[0.5665, 0.5732, 0.5288,  ..., 0.5832, 0.5107, 0.4400],
          [0.4581, 0.4632, 0.4858,  ..., 0.4779, 0.4828, 0.6602],
          [0.5023, 0.4373, 0.4840,  ..., 0.4821, 0.5533, 0.4249],
          [0.5207, 0.4994, 0.5713,  ..., 0.4703, 0.6361, 0.4889]],

         [[0.6783, 0.4467, 0.4419,  ..., 0.4121, 0.6893, 0.4822],
          [0.4690, 0.5074, 0.6549,  ..., 0.4012, 0.4642, 0.5487],
          [0.5012, 0.4846, 0.3603,  ..., 0.6593, 0.6487, 0.3242],
          [0.5148, 0.4688, 0.3567,  ..., 0.6654, 0.5960, 0.5007]],

         ...,

         [[0.4782, 0.5027, 0.4819,  ..., 0.5341, 0.5132, 0.5535],
          [0.5101, 0.6192, 0.4225,  ..., 0.6118, 0.4340, 0.6325],
          [0.5306, 0.4330, 0.5860,  ..., 0.5324, 0.5280, 0.2830],
          [0.4755, 0.7090, 0.5311,  ..., 0.5955, 0.4909, 0.4059]],

         [[0.4683, 0.5441, 0.4979,  ..., 0.6128, 0.3603, 0.4078],
          [0.4884, 0.5221, 0.6077,  ..., 0.4547, 0.5455, 0.6619],
          [0.4767, 0.6067, 0.6128,  ..., 0.4961, 0.5566, 0.4102],
          [0.3183, 0.3451, 0.5504,  ..., 0.6352, 0.4654, 0.3702]],

         [[0.4949, 0.3294, 0.4216,  ..., 0.6792, 0.5157, 0.4369],
          [0.4069, 0.6053, 0.5518,  ..., 0.4896, 0.4852, 0.3868],
          [0.4813, 0.3951, 0.5424,  ..., 0.3657, 0.3817, 0.4102],
          [0.5708, 0.5761, 0.6951,  ..., 0.6584, 0.3007, 0.5827]]],


        [[[0.4685, 0.6868, 0.3720,  ..., 0.5446, 0.4952, 0.6680],
          [0.4438, 0.6343, 0.3858,  ..., 0.3965, 0.5484, 0.5375],
          [0.5784, 0.6270, 0.5761,  ..., 0.6188, 0.5894, 0.4316],
          [0.4993, 0.5293, 0.5946,  ..., 0.4872, 0.4598, 0.6081]],

         [[0.4902, 0.5605, 0.5351,  ..., 0.3693, 0.5898, 0.6567],
          [0.4533, 0.4130, 0.6826,  ..., 0.5727, 0.5894, 0.5146],
          [0.5124, 0.3294, 0.6197,  ..., 0.6817, 0.5547, 0.3549],
          [0.6334, 0.3057, 0.5494,  ..., 0.5475, 0.4540, 0.5670]],

         [[0.4978, 0.5123, 0.3277,  ..., 0.4581, 0.4821, 0.5581],
          [0.5574, 0.5506, 0.4581,  ..., 0.4935, 0.5385, 0.3835],
          [0.5082, 0.4889, 0.3603,  ..., 0.5254, 0.4135, 0.5302],
          [0.5562, 0.5075, 0.4306,  ..., 0.4022, 0.4681, 0.4801]],

         ...,

         [[0.4891, 0.5271, 0.5245,  ..., 0.5765, 0.4899, 0.5482],
          [0.4211, 0.6225, 0.6095,  ..., 0.5494, 0.4477, 0.3891],
          [0.6343, 0.5178, 0.4154,  ..., 0.5842, 0.4705, 0.5433],
          [0.4125, 0.4410, 0.5283,  ..., 0.5171, 0.4402, 0.6183]],

         [[0.4613, 0.5056, 0.4862,  ..., 0.3107, 0.5679, 0.3909],
          [0.4448, 0.4211, 0.3702,  ..., 0.4278, 0.3648, 0.4967],
          [0.4059, 0.6044, 0.5115,  ..., 0.5083, 0.4647, 0.5185],
          [0.4728, 0.6225, 0.6792,  ..., 0.4431, 0.4827, 0.5142]],

         [[0.3937, 0.4311, 0.5339,  ..., 0.6021, 0.5250, 0.6593],
          [0.4712, 0.2910, 0.2950,  ..., 0.5751, 0.4966, 0.4359],
          [0.4596, 0.4881, 0.5265,  ..., 0.3558, 0.5130, 0.4292],
          [0.5017, 0.5578, 0.5842,  ..., 0.5282, 0.4647, 0.4927]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0010, -0.0030,  0.0030,  0.0110, -0.0010,  0.0070, -0.0110, -0.0050,
         0.0050,  0.0010], device='cuda:0')
selected experts tensor([1667, 1609, 1627, 1686, 1684, 1572, 1653, 1571, 1669, 1646],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4568, 0.4191, 0.4744,  ..., 0.4594, 0.4853, 0.5341],
          [0.4297, 0.5219, 0.3883,  ..., 0.6116, 0.6097, 0.4831],
          [0.4210, 0.4587, 0.5284,  ..., 0.4019, 0.3312, 0.5169],
          [0.5096, 0.4617, 0.6032,  ..., 0.5778, 0.4544, 0.5720]],

         [[0.4432, 0.5957, 0.5430,  ..., 0.5299, 0.5122, 0.5859],
          [0.4234, 0.5417, 0.4762,  ..., 0.6009, 0.6577, 0.4935],
          [0.5050, 0.6881, 0.4733,  ..., 0.5702, 0.3106, 0.4977],
          [0.3887, 0.6322, 0.5531,  ..., 0.5976, 0.5369, 0.5277]],

         [[0.5340, 0.5952, 0.4717,  ..., 0.4979, 0.4367, 0.6289],
          [0.4062, 0.5755, 0.6895,  ..., 0.6482, 0.5669, 0.5840],
          [0.5091, 0.4595, 0.3991,  ..., 0.4990, 0.5108, 0.3960],
          [0.5376, 0.6367, 0.5242,  ..., 0.5330, 0.4844, 0.5446]],

         ...,

         [[0.5583, 0.5750, 0.5122,  ..., 0.3708, 0.5362, 0.6322],
          [0.4328, 0.5158, 0.4830,  ..., 0.5711, 0.5075, 0.5659],
          [0.5379, 0.5473, 0.5091,  ..., 0.5032, 0.5074, 0.6764],
          [0.6232, 0.4925, 0.4580,  ..., 0.4636, 0.4372, 0.4117]],

         [[0.4777, 0.6072, 0.4728,  ..., 0.5783, 0.5128, 0.4952],
          [0.3591, 0.5574, 0.4772,  ..., 0.6041, 0.6363, 0.4553],
          [0.6160, 0.5087, 0.5943,  ..., 0.4701, 0.5332, 0.4965],
          [0.4900, 0.4615, 0.5287,  ..., 0.6116, 0.3287, 0.5065]],

         [[0.4210, 0.5550, 0.5318,  ..., 0.3402, 0.4539, 0.6012],
          [0.3148, 0.6160, 0.5003,  ..., 0.5555, 0.4597, 0.6514],
          [0.3456, 0.4234, 0.6194,  ..., 0.5347, 0.5484, 0.4405],
          [0.5129, 0.5835, 0.6065,  ..., 0.4447, 0.4353, 0.5507]]],


        [[[0.3555, 0.5712, 0.4768,  ..., 0.3690, 0.6097, 0.5945],
          [0.4602, 0.5947, 0.5070,  ..., 0.5896, 0.5898, 0.4376],
          [0.5707, 0.5313, 0.5008,  ..., 0.4795, 0.4691, 0.4664],
          [0.5135, 0.4129, 0.3855,  ..., 0.4247, 0.4301, 0.6441]],

         [[0.4787, 0.5186, 0.4980,  ..., 0.5692, 0.4908, 0.4496],
          [0.6123, 0.5388, 0.6518,  ..., 0.4874, 0.3819, 0.5065],
          [0.5260, 0.5216, 0.4675,  ..., 0.3385, 0.5136, 0.3978],
          [0.6376, 0.4601, 0.4587,  ..., 0.4958, 0.4946, 0.6308]],

         [[0.4502, 0.5053, 0.5287,  ..., 0.4790, 0.4778, 0.6045],
          [0.4806, 0.5106, 0.4415,  ..., 0.3939, 0.5974, 0.5189],
          [0.5362, 0.4975, 0.5522,  ..., 0.4878, 0.5567, 0.4773],
          [0.6091, 0.5830, 0.5081,  ..., 0.4762, 0.5693, 0.5154]],

         ...,

         [[0.3564, 0.6123, 0.3680,  ..., 0.4811, 0.4812, 0.5254],
          [0.5109, 0.5340, 0.5301,  ..., 0.6221, 0.4606, 0.6074],
          [0.5626, 0.4379, 0.6657,  ..., 0.4718, 0.4609, 0.4732],
          [0.4986, 0.4273, 0.5084,  ..., 0.5524, 0.4357, 0.4800]],

         [[0.4410, 0.5318, 0.4661,  ..., 0.4384, 0.5307, 0.6191],
          [0.4493, 0.4001, 0.5188,  ..., 0.5165, 0.5988, 0.5267],
          [0.6447, 0.3977, 0.4794,  ..., 0.5938, 0.5888, 0.4348],
          [0.4393, 0.6259, 0.4300,  ..., 0.4776, 0.4957, 0.6158]],

         [[0.4890, 0.5335, 0.5560,  ..., 0.4329, 0.5269, 0.6140],
          [0.4434, 0.5075, 0.4529,  ..., 0.5854, 0.4652, 0.4108],
          [0.5207, 0.3688, 0.5668,  ..., 0.6339, 0.4491, 0.4501],
          [0.4724, 0.4833, 0.4916,  ..., 0.4166, 0.4225, 0.5285]]]],
       device='cuda:0')
tensor([[[[0.4688, 0.4311, 0.4784,  ..., 0.4634, 0.4673, 0.5161],
          [0.4417, 0.5339, 0.3923,  ..., 0.6156, 0.5917, 0.4651],
          [0.4330, 0.4707, 0.5324,  ..., 0.4059, 0.3132, 0.4989],
          [0.5216, 0.4737, 0.6072,  ..., 0.5818, 0.4364, 0.5540]],

         [[0.4552, 0.6077, 0.5470,  ..., 0.5339, 0.4942, 0.5679],
          [0.4354, 0.5537, 0.4802,  ..., 0.6049, 0.6397, 0.4755],
          [0.5170, 0.7001, 0.4773,  ..., 0.5742, 0.2926, 0.4797],
          [0.4007, 0.6442, 0.5571,  ..., 0.6016, 0.5189, 0.5097]],

         [[0.5460, 0.6072, 0.4757,  ..., 0.5019, 0.4187, 0.6109],
          [0.4182, 0.5875, 0.6935,  ..., 0.6522, 0.5489, 0.5660],
          [0.5211, 0.4715, 0.4031,  ..., 0.5030, 0.4928, 0.3780],
          [0.5496, 0.6487, 0.5282,  ..., 0.5370, 0.4664, 0.5266]],

         ...,

         [[0.5703, 0.5870, 0.5162,  ..., 0.3748, 0.5182, 0.6142],
          [0.4448, 0.5278, 0.4870,  ..., 0.5751, 0.4895, 0.5479],
          [0.5499, 0.5593, 0.5131,  ..., 0.5072, 0.4894, 0.6584],
          [0.6352, 0.5045, 0.4620,  ..., 0.4676, 0.4192, 0.3937]],

         [[0.4897, 0.6192, 0.4768,  ..., 0.5823, 0.4948, 0.4772],
          [0.3711, 0.5694, 0.4812,  ..., 0.6081, 0.6183, 0.4373],
          [0.6280, 0.5207, 0.5983,  ..., 0.4741, 0.5152, 0.4785],
          [0.5020, 0.4735, 0.5327,  ..., 0.6156, 0.3107, 0.4885]],

         [[0.4330, 0.5670, 0.5358,  ..., 0.3442, 0.4359, 0.5832],
          [0.3268, 0.6280, 0.5043,  ..., 0.5595, 0.4417, 0.6334],
          [0.3576, 0.4354, 0.6234,  ..., 0.5387, 0.5304, 0.4225],
          [0.5249, 0.5955, 0.6105,  ..., 0.4487, 0.4173, 0.5327]]],


        [[[0.3675, 0.5832, 0.4808,  ..., 0.3730, 0.5917, 0.5765],
          [0.4722, 0.6067, 0.5110,  ..., 0.5936, 0.5718, 0.4196],
          [0.5827, 0.5433, 0.5048,  ..., 0.4835, 0.4511, 0.4484],
          [0.5255, 0.4249, 0.3895,  ..., 0.4287, 0.4121, 0.6261]],

         [[0.4907, 0.5306, 0.5020,  ..., 0.5732, 0.4728, 0.4316],
          [0.6243, 0.5508, 0.6558,  ..., 0.4914, 0.3639, 0.4885],
          [0.5380, 0.5336, 0.4715,  ..., 0.3425, 0.4956, 0.3798],
          [0.6496, 0.4721, 0.4627,  ..., 0.4998, 0.4766, 0.6128]],

         [[0.4622, 0.5173, 0.5327,  ..., 0.4830, 0.4598, 0.5865],
          [0.4926, 0.5226, 0.4455,  ..., 0.3979, 0.5794, 0.5009],
          [0.5482, 0.5095, 0.5562,  ..., 0.4918, 0.5387, 0.4593],
          [0.6211, 0.5950, 0.5121,  ..., 0.4802, 0.5513, 0.4974]],

         ...,

         [[0.3684, 0.6243, 0.3720,  ..., 0.4851, 0.4632, 0.5074],
          [0.5229, 0.5460, 0.5341,  ..., 0.6261, 0.4426, 0.5894],
          [0.5746, 0.4499, 0.6697,  ..., 0.4758, 0.4429, 0.4552],
          [0.5106, 0.4393, 0.5124,  ..., 0.5564, 0.4177, 0.4620]],

         [[0.4530, 0.5438, 0.4701,  ..., 0.4424, 0.5127, 0.6011],
          [0.4613, 0.4121, 0.5228,  ..., 0.5205, 0.5808, 0.5087],
          [0.6567, 0.4097, 0.4834,  ..., 0.5978, 0.5708, 0.4168],
          [0.4513, 0.6379, 0.4340,  ..., 0.4816, 0.4777, 0.5978]],

         [[0.5010, 0.5455, 0.5600,  ..., 0.4369, 0.5089, 0.5960],
          [0.4554, 0.5195, 0.4569,  ..., 0.5894, 0.4472, 0.3928],
          [0.5327, 0.3808, 0.5708,  ..., 0.6379, 0.4311, 0.4321],
          [0.4844, 0.4953, 0.4956,  ..., 0.4206, 0.4045, 0.5105]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([-0.0120, -0.0120, -0.0040, -0.0020, -0.0180,  0.0220, -0.0220, -0.0040,
         0.0180,  0.0180], device='cuda:0')
selected experts tensor([1504, 1902, 1441, 1746, 1451, 1377, 2051, 1672, 1509, 1731],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6203, 0.5877, 0.5647,  ..., 0.4401, 0.4065, 0.5288],
          [0.5051, 0.5464, 0.6075,  ..., 0.4240, 0.5410, 0.5703],
          [0.5052, 0.5071, 0.3949,  ..., 0.4877, 0.4698, 0.5647],
          [0.4480, 0.5141, 0.4621,  ..., 0.3830, 0.4512, 0.4435]],

         [[0.5257, 0.5035, 0.4236,  ..., 0.4406, 0.4701, 0.4710],
          [0.3704, 0.5934, 0.6823,  ..., 0.5148, 0.5274, 0.4852],
          [0.4817, 0.5440, 0.4928,  ..., 0.4051, 0.4816, 0.6046],
          [0.4259, 0.4222, 0.6320,  ..., 0.3963, 0.5814, 0.4587]],

         [[0.5270, 0.6070, 0.5314,  ..., 0.3977, 0.5403, 0.4103],
          [0.4207, 0.3797, 0.6084,  ..., 0.5373, 0.4599, 0.4159],
          [0.4814, 0.4476, 0.4189,  ..., 0.5356, 0.5116, 0.6797],
          [0.6235, 0.5088, 0.4990,  ..., 0.3972, 0.5998, 0.3775]],

         ...,

         [[0.5020, 0.5040, 0.5040,  ..., 0.5100, 0.5020, 0.5100],
          [0.5020, 0.5040, 0.5040,  ..., 0.5100, 0.5020, 0.5100],
          [0.5020, 0.5040, 0.5040,  ..., 0.5100, 0.5020, 0.5100],
          [0.5020, 0.5040, 0.5040,  ..., 0.5100, 0.5020, 0.5100]],

         [[0.5020, 0.5040, 0.5040,  ..., 0.5100, 0.5020, 0.5100],
          [0.5020, 0.5040, 0.5040,  ..., 0.5100, 0.5020, 0.5100],
          [0.5020, 0.5040, 0.5040,  ..., 0.5100, 0.5020, 0.5100],
          [0.5020, 0.5040, 0.5040,  ..., 0.5100, 0.5020, 0.5100]],

         [[0.5020, 0.5040, 0.5040,  ..., 0.5100, 0.5020, 0.5100],
          [0.5020, 0.5040, 0.5040,  ..., 0.5100, 0.5020, 0.5100],
          [0.5020, 0.5040, 0.5040,  ..., 0.5100, 0.5020, 0.5100],
          [0.5020, 0.5040, 0.5040,  ..., 0.5100, 0.5020, 0.5100]]],


        [[[0.5062, 0.4898, 0.4241,  ..., 0.4244, 0.3498, 0.4070],
          [0.4495, 0.5437, 0.6365,  ..., 0.5412, 0.4623, 0.4803],
          [0.5642, 0.4057, 0.4275,  ..., 0.4761, 0.6613, 0.6461],
          [0.4664, 0.5351, 0.5319,  ..., 0.4715, 0.5157, 0.4028]],

         [[0.4446, 0.5493, 0.4445,  ..., 0.3935, 0.5050, 0.5870],
          [0.4288, 0.3866, 0.5758,  ..., 0.5647, 0.6111, 0.4589],
          [0.6542, 0.5839, 0.3987,  ..., 0.4522, 0.5852, 0.4263],
          [0.5023, 0.5129, 0.4988,  ..., 0.4268, 0.6281, 0.5363]],

         [[0.5507, 0.4785, 0.4844,  ..., 0.3099, 0.4226, 0.6209],
          [0.4674, 0.5202, 0.5801,  ..., 0.4202, 0.4509, 0.2875],
          [0.5499, 0.6023, 0.5049,  ..., 0.3658, 0.4979, 0.4145],
          [0.4835, 0.5068, 0.4927,  ..., 0.4459, 0.6426, 0.5521]],

         ...,

         [[0.5020, 0.5040, 0.5040,  ..., 0.5100, 0.5020, 0.5100],
          [0.5020, 0.5040, 0.5040,  ..., 0.5100, 0.5020, 0.5100],
          [0.5020, 0.5040, 0.5040,  ..., 0.5100, 0.5020, 0.5100],
          [0.5020, 0.5040, 0.5040,  ..., 0.5100, 0.5020, 0.5100]],

         [[0.5020, 0.5040, 0.5040,  ..., 0.5100, 0.5020, 0.5100],
          [0.5020, 0.5040, 0.5040,  ..., 0.5100, 0.5020, 0.5100],
          [0.5020, 0.5040, 0.5040,  ..., 0.5100, 0.5020, 0.5100],
          [0.5020, 0.5040, 0.5040,  ..., 0.5100, 0.5020, 0.5100]],

         [[0.5020, 0.5040, 0.5040,  ..., 0.5100, 0.5020, 0.5100],
          [0.5020, 0.5040, 0.5040,  ..., 0.5100, 0.5020, 0.5100],
          [0.5020, 0.5040, 0.5040,  ..., 0.5100, 0.5020, 0.5100],
          [0.5020, 0.5040, 0.5040,  ..., 0.5100, 0.5020, 0.5100]]]],
       device='cuda:0')
tensor([[[[0.6183, 0.5837, 0.5607,  ..., 0.4301, 0.4045, 0.5188],
          [0.5031, 0.5424, 0.6035,  ..., 0.4140, 0.5390, 0.5603],
          [0.5032, 0.5031, 0.3909,  ..., 0.4777, 0.4678, 0.5547],
          [0.4460, 0.5101, 0.4581,  ..., 0.3730, 0.4492, 0.4335]],

         [[0.5237, 0.4995, 0.4196,  ..., 0.4306, 0.4681, 0.4610],
          [0.3684, 0.5894, 0.6783,  ..., 0.5048, 0.5254, 0.4752],
          [0.4797, 0.5400, 0.4888,  ..., 0.3951, 0.4796, 0.5946],
          [0.4239, 0.4182, 0.6280,  ..., 0.3863, 0.5794, 0.4487]],

         [[0.5250, 0.6030, 0.5274,  ..., 0.3877, 0.5383, 0.4003],
          [0.4187, 0.3757, 0.6044,  ..., 0.5273, 0.4579, 0.4059],
          [0.4794, 0.4436, 0.4149,  ..., 0.5256, 0.5096, 0.6697],
          [0.6215, 0.5048, 0.4950,  ..., 0.3872, 0.5978, 0.3675]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5042, 0.4858, 0.4201,  ..., 0.4144, 0.3478, 0.3970],
          [0.4475, 0.5397, 0.6325,  ..., 0.5312, 0.4603, 0.4703],
          [0.5622, 0.4017, 0.4235,  ..., 0.4661, 0.6593, 0.6361],
          [0.4644, 0.5311, 0.5279,  ..., 0.4615, 0.5137, 0.3928]],

         [[0.4426, 0.5453, 0.4405,  ..., 0.3835, 0.5030, 0.5770],
          [0.4268, 0.3826, 0.5718,  ..., 0.5547, 0.6091, 0.4489],
          [0.6522, 0.5799, 0.3947,  ..., 0.4422, 0.5832, 0.4163],
          [0.5003, 0.5089, 0.4948,  ..., 0.4168, 0.6261, 0.5263]],

         [[0.5487, 0.4745, 0.4804,  ..., 0.2999, 0.4206, 0.6109],
          [0.4654, 0.5162, 0.5761,  ..., 0.4102, 0.4489, 0.2775],
          [0.5479, 0.5983, 0.5009,  ..., 0.3558, 0.4959, 0.4045],
          [0.4815, 0.5028, 0.4887,  ..., 0.4359, 0.6406, 0.5421]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0020,  0.0040,  0.0040,  0.0080, -0.0100,  0.0100,  0.0100,  0.0100,
         0.0020,  0.0100], device='cuda:0')
selected experts tensor([1750, 2265, 1530, 1411, 1721, 1424, 2246,  650, 2178, 1209],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1667., 1609., 1627., 1686., 1684., 1572., 1653., 1571., 1669., 1646.],
        [1750., 2265., 1530., 1411., 1721., 1424., 2246.,  650., 2178., 1209.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6482, 0.4890, 0.5652,  ..., 0.5102, 0.5208, 0.4712],
          [0.6675, 0.4794, 0.6043,  ..., 0.4712, 0.5079, 0.7305],
          [0.5406, 0.4423, 0.4683,  ..., 0.4693, 0.6055, 0.3952],
          [0.3991, 0.7487, 0.3960,  ..., 0.4923, 0.5504, 0.4627]],

         [[0.6129, 0.3282, 0.5482,  ..., 0.6194, 0.6390, 0.4610],
          [0.3818, 0.4807, 0.4140,  ..., 0.6291, 0.5317, 0.5290],
          [0.4319, 0.5919, 0.4640,  ..., 0.4781, 0.4250, 0.4593],
          [0.4195, 0.5141, 0.5823,  ..., 0.6872, 0.4630, 0.5438]],

         [[0.5418, 0.5452, 0.5120,  ..., 0.5596, 0.4867, 0.4901],
          [0.5971, 0.5495, 0.3686,  ..., 0.5581, 0.3828, 0.5817],
          [0.5418, 0.3875, 0.3523,  ..., 0.6602, 0.5137, 0.3746],
          [0.5362, 0.4866, 0.5094,  ..., 0.4888, 0.4635, 0.4836]],

         ...,

         [[0.6597, 0.4621, 0.4981,  ..., 0.6054, 0.5057, 0.4526],
          [0.3662, 0.5236, 0.5614,  ..., 0.4139, 0.5766, 0.4272],
          [0.4384, 0.5626, 0.4327,  ..., 0.4803, 0.4791, 0.4857],
          [0.6088, 0.4886, 0.3197,  ..., 0.5241, 0.4202, 0.4215]],

         [[0.4057, 0.4915, 0.6779,  ..., 0.4367, 0.3211, 0.5000],
          [0.4558, 0.3697, 0.4121,  ..., 0.5458, 0.6390, 0.5049],
          [0.5210, 0.5681, 0.5837,  ..., 0.4953, 0.3967, 0.4177],
          [0.5333, 0.6455, 0.5020,  ..., 0.6396, 0.5666, 0.4526]],

         [[0.5304, 0.4522, 0.6048,  ..., 0.5926, 0.4514, 0.4396],
          [0.4616, 0.3715, 0.3640,  ..., 0.4027, 0.5463, 0.5407],
          [0.5582, 0.5432, 0.4017,  ..., 0.5200, 0.6300, 0.6396],
          [0.4176, 0.4766, 0.6548,  ..., 0.5526, 0.4986, 0.6157]]],


        [[[0.5024, 0.4757, 0.3514,  ..., 0.5993, 0.4475, 0.5564],
          [0.5915, 0.5662, 0.5733,  ..., 0.4601, 0.3981, 0.6305],
          [0.4587, 0.3697, 0.4126,  ..., 0.5518, 0.3436, 0.5950],
          [0.4819, 0.4928, 0.5762,  ..., 0.6751, 0.5680, 0.4257]],

         [[0.6013, 0.4001, 0.4351,  ..., 0.3531, 0.5199, 0.4774],
          [0.4329, 0.6009, 0.6001,  ..., 0.4243, 0.5108, 0.4909],
          [0.5507, 0.3248, 0.4332,  ..., 0.3426, 0.4384, 0.5274],
          [0.5601, 0.4741, 0.5828,  ..., 0.6105, 0.4079, 0.5521]],

         [[0.4826, 0.6051, 0.4921,  ..., 0.6332, 0.5570, 0.4952],
          [0.3745, 0.4222, 0.4975,  ..., 0.4552, 0.3832, 0.4820],
          [0.4000, 0.4469, 0.4911,  ..., 0.4201, 0.4618, 0.4903],
          [0.4932, 0.5539, 0.5412,  ..., 0.5011, 0.4393, 0.6629]],

         ...,

         [[0.5024, 0.5418, 0.6583,  ..., 0.5055, 0.4478, 0.3104],
          [0.5374, 0.5250, 0.6245,  ..., 0.3584, 0.5337, 0.6486],
          [0.5078, 0.5621, 0.4799,  ..., 0.5300, 0.6622, 0.4234],
          [0.5009, 0.5014, 0.3746,  ..., 0.5513, 0.5914, 0.5997]],

         [[0.5304, 0.4682, 0.4457,  ..., 0.4324, 0.4727, 0.4920],
          [0.4401, 0.3580, 0.2473,  ..., 0.2998, 0.4521, 0.5218],
          [0.5027, 0.5490, 0.5349,  ..., 0.4579, 0.5192, 0.6777],
          [0.3109, 0.4996, 0.5363,  ..., 0.5610, 0.4084, 0.5254]],

         [[0.6553, 0.6554, 0.4339,  ..., 0.4625, 0.4278, 0.4888],
          [0.6294, 0.2549, 0.5479,  ..., 0.4215, 0.3498, 0.4286],
          [0.4171, 0.5241, 0.5286,  ..., 0.4338, 0.5998, 0.6965],
          [0.4947, 0.6527, 0.4775,  ..., 0.6956, 0.4727, 0.5557]]]],
       device='cuda:0')
tensor([[[[0.6522, 0.4850, 0.5732,  ..., 0.5022, 0.5188, 0.4632],
          [0.6715, 0.4754, 0.6123,  ..., 0.4632, 0.5059, 0.7225],
          [0.5446, 0.4383, 0.4763,  ..., 0.4613, 0.6035, 0.3872],
          [0.4031, 0.7447, 0.4040,  ..., 0.4843, 0.5484, 0.4547]],

         [[0.6169, 0.3242, 0.5562,  ..., 0.6114, 0.6370, 0.4530],
          [0.3858, 0.4767, 0.4220,  ..., 0.6211, 0.5297, 0.5210],
          [0.4359, 0.5879, 0.4720,  ..., 0.4701, 0.4230, 0.4513],
          [0.4235, 0.5101, 0.5903,  ..., 0.6792, 0.4610, 0.5358]],

         [[0.5458, 0.5412, 0.5200,  ..., 0.5516, 0.4847, 0.4821],
          [0.6011, 0.5455, 0.3766,  ..., 0.5501, 0.3808, 0.5737],
          [0.5458, 0.3835, 0.3603,  ..., 0.6522, 0.5117, 0.3666],
          [0.5402, 0.4826, 0.5174,  ..., 0.4808, 0.4615, 0.4756]],

         ...,

         [[0.6637, 0.4581, 0.5061,  ..., 0.5974, 0.5037, 0.4446],
          [0.3702, 0.5196, 0.5694,  ..., 0.4059, 0.5746, 0.4192],
          [0.4424, 0.5586, 0.4407,  ..., 0.4723, 0.4771, 0.4777],
          [0.6128, 0.4846, 0.3277,  ..., 0.5161, 0.4182, 0.4135]],

         [[0.4097, 0.4875, 0.6859,  ..., 0.4287, 0.3191, 0.4920],
          [0.4598, 0.3657, 0.4201,  ..., 0.5378, 0.6370, 0.4969],
          [0.5250, 0.5641, 0.5917,  ..., 0.4873, 0.3947, 0.4097],
          [0.5373, 0.6415, 0.5100,  ..., 0.6316, 0.5646, 0.4446]],

         [[0.5344, 0.4482, 0.6128,  ..., 0.5846, 0.4494, 0.4316],
          [0.4656, 0.3675, 0.3720,  ..., 0.3947, 0.5443, 0.5327],
          [0.5622, 0.5392, 0.4097,  ..., 0.5120, 0.6280, 0.6316],
          [0.4216, 0.4726, 0.6628,  ..., 0.5446, 0.4966, 0.6077]]],


        [[[0.5064, 0.4717, 0.3594,  ..., 0.5913, 0.4455, 0.5484],
          [0.5955, 0.5622, 0.5813,  ..., 0.4521, 0.3961, 0.6225],
          [0.4627, 0.3657, 0.4206,  ..., 0.5438, 0.3416, 0.5870],
          [0.4859, 0.4888, 0.5842,  ..., 0.6671, 0.5660, 0.4177]],

         [[0.6053, 0.3961, 0.4431,  ..., 0.3451, 0.5179, 0.4694],
          [0.4369, 0.5969, 0.6081,  ..., 0.4163, 0.5088, 0.4829],
          [0.5547, 0.3208, 0.4412,  ..., 0.3346, 0.4364, 0.5194],
          [0.5641, 0.4701, 0.5908,  ..., 0.6025, 0.4059, 0.5441]],

         [[0.4866, 0.6011, 0.5001,  ..., 0.6252, 0.5550, 0.4872],
          [0.3785, 0.4182, 0.5055,  ..., 0.4472, 0.3812, 0.4740],
          [0.4040, 0.4429, 0.4991,  ..., 0.4121, 0.4598, 0.4823],
          [0.4972, 0.5499, 0.5492,  ..., 0.4931, 0.4373, 0.6549]],

         ...,

         [[0.5064, 0.5378, 0.6663,  ..., 0.4975, 0.4458, 0.3024],
          [0.5414, 0.5210, 0.6325,  ..., 0.3504, 0.5317, 0.6406],
          [0.5118, 0.5581, 0.4879,  ..., 0.5220, 0.6602, 0.4154],
          [0.5049, 0.4974, 0.3826,  ..., 0.5433, 0.5894, 0.5917]],

         [[0.5344, 0.4642, 0.4537,  ..., 0.4244, 0.4707, 0.4840],
          [0.4441, 0.3540, 0.2553,  ..., 0.2918, 0.4501, 0.5138],
          [0.5067, 0.5450, 0.5429,  ..., 0.4499, 0.5172, 0.6697],
          [0.3149, 0.4956, 0.5443,  ..., 0.5530, 0.4064, 0.5174]],

         [[0.6593, 0.6514, 0.4419,  ..., 0.4545, 0.4258, 0.4808],
          [0.6334, 0.2509, 0.5559,  ..., 0.4135, 0.3478, 0.4206],
          [0.4211, 0.5201, 0.5366,  ..., 0.4258, 0.5978, 0.6885],
          [0.4987, 0.6487, 0.4855,  ..., 0.6876, 0.4707, 0.5477]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0040,  0.0040, -0.0080,  0.0000, -0.0060, -0.0040,  0.0080,  0.0080,
         0.0020,  0.0080], device='cuda:0')
selected experts tensor([1722, 1665, 1568, 1623, 1600, 1605, 1537, 1755, 1584, 1725],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4464, 0.4520, 0.7049,  ..., 0.6339, 0.3706, 0.4728],
          [0.5346, 0.4588, 0.4866,  ..., 0.5459, 0.6571, 0.5055],
          [0.4086, 0.5127, 0.5282,  ..., 0.5377, 0.4289, 0.6316],
          [0.5784, 0.5113, 0.6347,  ..., 0.4185, 0.6070, 0.6680]],

         [[0.6377, 0.5151, 0.5019,  ..., 0.4497, 0.4076, 0.5370],
          [0.5679, 0.3746, 0.5048,  ..., 0.4754, 0.6292, 0.5373],
          [0.4935, 0.5336, 0.4780,  ..., 0.5009, 0.5262, 0.4297],
          [0.5260, 0.6080, 0.4151,  ..., 0.4427, 0.3518, 0.5689]],

         [[0.3857, 0.5717, 0.3820,  ..., 0.5873, 0.3820, 0.6531],
          [0.6163, 0.4286, 0.4831,  ..., 0.5236, 0.5064, 0.6179],
          [0.6232, 0.4788, 0.6925,  ..., 0.5759, 0.6392, 0.6234],
          [0.5336, 0.4406, 0.4047,  ..., 0.4863, 0.4616, 0.5799]],

         ...,

         [[0.5215, 0.6250, 0.3334,  ..., 0.4223, 0.4260, 0.6261],
          [0.4534, 0.3458, 0.4906,  ..., 0.5117, 0.4512, 0.4921],
          [0.5878, 0.4086, 0.4010,  ..., 0.4483, 0.5020, 0.4282],
          [0.3583, 0.6625, 0.3949,  ..., 0.3944, 0.6117, 0.5336]],

         [[0.6057, 0.4181, 0.5313,  ..., 0.5452, 0.4563, 0.5162],
          [0.4593, 0.4760, 0.5522,  ..., 0.5873, 0.6301, 0.4412],
          [0.4656, 0.4433, 0.4546,  ..., 0.5369, 0.5372, 0.4443],
          [0.6377, 0.5911, 0.4906,  ..., 0.4827, 0.5748, 0.5084]],

         [[0.4375, 0.4631, 0.4346,  ..., 0.5377, 0.4910, 0.4954],
          [0.5132, 0.4782, 0.5253,  ..., 0.3662, 0.3421, 0.6352],
          [0.4902, 0.5467, 0.6301,  ..., 0.4837, 0.3935, 0.6901],
          [0.6296, 0.6404, 0.3257,  ..., 0.6116, 0.5585, 0.5521]]],


        [[[0.5134, 0.4651, 0.4760,  ..., 0.5768, 0.4556, 0.4506],
          [0.3087, 0.5370, 0.5767,  ..., 0.4010, 0.3403, 0.4815],
          [0.3788, 0.4874, 0.6347,  ..., 0.3545, 0.4394, 0.4297],
          [0.4602, 0.4910, 0.5541,  ..., 0.5967, 0.4413, 0.5746]],

         [[0.5013, 0.5991, 0.4512,  ..., 0.5563, 0.6251, 0.5884],
          [0.6746, 0.4086, 0.5858,  ..., 0.4748, 0.6292, 0.4282],
          [0.5554, 0.5037, 0.5667,  ..., 0.5682, 0.6163, 0.4659],
          [0.6651, 0.5505, 0.5376,  ..., 0.4718, 0.4873, 0.5182]],

         [[0.4843, 0.4353, 0.3788,  ..., 0.4882, 0.4486, 0.4596],
          [0.4532, 0.4353, 0.6562,  ..., 0.4901, 0.5877, 0.5329],
          [0.4871, 0.4819, 0.5218,  ..., 0.5587, 0.5014, 0.6123],
          [0.4876, 0.3079, 0.3535,  ..., 0.4764, 0.6089, 0.6002]],

         ...,

         [[0.4870, 0.5237, 0.5301,  ..., 0.5725, 0.4937, 0.5482],
          [0.4191, 0.6205, 0.6135,  ..., 0.5454, 0.4517, 0.3891],
          [0.6323, 0.5159, 0.4194,  ..., 0.5802, 0.4746, 0.5433],
          [0.4101, 0.4390, 0.5324,  ..., 0.5128, 0.4445, 0.6183]],

         [[0.5384, 0.5148, 0.2668,  ..., 0.4161, 0.5915, 0.5152],
          [0.5196, 0.4748, 0.6703,  ..., 0.5891, 0.3940, 0.4586],
          [0.4973, 0.4072, 0.4294,  ..., 0.6303, 0.4640, 0.4896],
          [0.4866, 0.5225, 0.6009,  ..., 0.4652, 0.5604, 0.5409]],

         [[0.5132, 0.5958, 0.3257,  ..., 0.6116, 0.5010, 0.5964],
          [0.4049, 0.3950, 0.4515,  ..., 0.5754, 0.5777, 0.5224],
          [0.5336, 0.5377, 0.5452,  ..., 0.6018, 0.5415, 0.5299],
          [0.4305, 0.5045, 0.6509,  ..., 0.4837, 0.4753, 0.5332]]]],
       device='cuda:0')
tensor([[[[0.4484, 0.4540, 0.7009,  ..., 0.6379, 0.3666, 0.4728],
          [0.5366, 0.4608, 0.4826,  ..., 0.5499, 0.6531, 0.5055],
          [0.4106, 0.5147, 0.5242,  ..., 0.5417, 0.4249, 0.6316],
          [0.5804, 0.5133, 0.6307,  ..., 0.4225, 0.6030, 0.6680]],

         [[0.6397, 0.5171, 0.4979,  ..., 0.4537, 0.4036, 0.5370],
          [0.5699, 0.3766, 0.5008,  ..., 0.4794, 0.6252, 0.5373],
          [0.4955, 0.5356, 0.4740,  ..., 0.5049, 0.5222, 0.4297],
          [0.5280, 0.6100, 0.4111,  ..., 0.4467, 0.3478, 0.5689]],

         [[0.3877, 0.5737, 0.3780,  ..., 0.5913, 0.3780, 0.6531],
          [0.6183, 0.4306, 0.4791,  ..., 0.5276, 0.5024, 0.6179],
          [0.6252, 0.4808, 0.6885,  ..., 0.5799, 0.6352, 0.6234],
          [0.5356, 0.4426, 0.4007,  ..., 0.4903, 0.4576, 0.5799]],

         ...,

         [[0.5235, 0.6270, 0.3294,  ..., 0.4263, 0.4220, 0.6261],
          [0.4554, 0.3478, 0.4866,  ..., 0.5157, 0.4472, 0.4921],
          [0.5898, 0.4106, 0.3970,  ..., 0.4523, 0.4980, 0.4282],
          [0.3603, 0.6645, 0.3909,  ..., 0.3984, 0.6077, 0.5336]],

         [[0.6077, 0.4201, 0.5273,  ..., 0.5492, 0.4523, 0.5162],
          [0.4613, 0.4780, 0.5482,  ..., 0.5913, 0.6261, 0.4412],
          [0.4676, 0.4453, 0.4506,  ..., 0.5409, 0.5332, 0.4443],
          [0.6397, 0.5931, 0.4866,  ..., 0.4867, 0.5708, 0.5084]],

         [[0.4395, 0.4651, 0.4306,  ..., 0.5417, 0.4870, 0.4954],
          [0.5152, 0.4802, 0.5213,  ..., 0.3702, 0.3381, 0.6352],
          [0.4922, 0.5487, 0.6261,  ..., 0.4877, 0.3895, 0.6901],
          [0.6316, 0.6424, 0.3217,  ..., 0.6156, 0.5545, 0.5521]]],


        [[[0.5154, 0.4671, 0.4720,  ..., 0.5808, 0.4516, 0.4506],
          [0.3107, 0.5390, 0.5727,  ..., 0.4050, 0.3363, 0.4815],
          [0.3808, 0.4894, 0.6307,  ..., 0.3585, 0.4354, 0.4297],
          [0.4622, 0.4930, 0.5501,  ..., 0.6007, 0.4373, 0.5746]],

         [[0.5033, 0.6011, 0.4472,  ..., 0.5603, 0.6211, 0.5884],
          [0.6766, 0.4106, 0.5818,  ..., 0.4788, 0.6252, 0.4282],
          [0.5574, 0.5057, 0.5627,  ..., 0.5722, 0.6123, 0.4659],
          [0.6671, 0.5525, 0.5336,  ..., 0.4758, 0.4833, 0.5182]],

         [[0.4863, 0.4373, 0.3748,  ..., 0.4922, 0.4446, 0.4596],
          [0.4552, 0.4373, 0.6522,  ..., 0.4941, 0.5837, 0.5329],
          [0.4891, 0.4839, 0.5178,  ..., 0.5627, 0.4974, 0.6123],
          [0.4896, 0.3099, 0.3495,  ..., 0.4804, 0.6049, 0.6002]],

         ...,

         [[0.4890, 0.5257, 0.5261,  ..., 0.5765, 0.4897, 0.5482],
          [0.4211, 0.6225, 0.6095,  ..., 0.5494, 0.4477, 0.3891],
          [0.6343, 0.5179, 0.4154,  ..., 0.5842, 0.4706, 0.5433],
          [0.4121, 0.4410, 0.5284,  ..., 0.5168, 0.4405, 0.6183]],

         [[0.5404, 0.5168, 0.2628,  ..., 0.4201, 0.5875, 0.5152],
          [0.5216, 0.4768, 0.6663,  ..., 0.5931, 0.3900, 0.4586],
          [0.4993, 0.4092, 0.4254,  ..., 0.6343, 0.4600, 0.4896],
          [0.4886, 0.5245, 0.5969,  ..., 0.4692, 0.5564, 0.5409]],

         [[0.5152, 0.5978, 0.3217,  ..., 0.6156, 0.4970, 0.5964],
          [0.4069, 0.3970, 0.4475,  ..., 0.5794, 0.5737, 0.5224],
          [0.5356, 0.5397, 0.5412,  ..., 0.6058, 0.5375, 0.5299],
          [0.4325, 0.5065, 0.6469,  ..., 0.4877, 0.4713, 0.5332]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0020, -0.0020,  0.0040,  0.0100, -0.0020,  0.0080, -0.0120, -0.0040,
         0.0040,  0.0000], device='cuda:0')
selected experts tensor([1617, 1654, 1641, 1640, 1498, 1661, 1697, 1771, 1660, 1545],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4850, 0.4727, 0.4305,  ..., 0.3616, 0.4732, 0.7138],
          [0.4642, 0.6062, 0.3255,  ..., 0.6239, 0.6332, 0.4864],
          [0.4768, 0.5306, 0.5433,  ..., 0.4912, 0.4660, 0.4807],
          [0.4730, 0.5032, 0.3963,  ..., 0.5117, 0.4665, 0.4898]],

         [[0.4263, 0.6267, 0.4334,  ..., 0.6003, 0.3623, 0.5459],
          [0.4889, 0.5005, 0.4568,  ..., 0.5473, 0.6131, 0.5717],
          [0.5632, 0.4090, 0.5582,  ..., 0.4953, 0.5081, 0.4145],
          [0.5803, 0.6085, 0.4401,  ..., 0.5082, 0.5009, 0.4582]],

         [[0.4153, 0.5881, 0.5102,  ..., 0.5687, 0.5123, 0.5608],
          [0.5239, 0.5044, 0.4291,  ..., 0.4487, 0.5577, 0.4553],
          [0.3982, 0.5456, 0.5510,  ..., 0.4071, 0.4179, 0.4084],
          [0.5925, 0.5223, 0.6277,  ..., 0.4916, 0.3838, 0.5233]],

         ...,

         [[0.4457, 0.5010, 0.4339,  ..., 0.3813, 0.5879, 0.6158],
          [0.4362, 0.6131, 0.5821,  ..., 0.4444, 0.6704, 0.4975],
          [0.5010, 0.4033, 0.5336,  ..., 0.4075, 0.4453, 0.6242],
          [0.5094, 0.3924, 0.5175,  ..., 0.4408, 0.5001, 0.3764]],

         [[0.4677, 0.5202, 0.4466,  ..., 0.4558, 0.5428, 0.4896],
          [0.4333, 0.5489, 0.4224,  ..., 0.3735, 0.5210, 0.4880],
          [0.4999, 0.4152, 0.5573,  ..., 0.5475, 0.5669, 0.5441],
          [0.5057, 0.3821, 0.4785,  ..., 0.6419, 0.4571, 0.3755]],

         [[0.5784, 0.5501, 0.5272,  ..., 0.4880, 0.6145, 0.5739],
          [0.4464, 0.5797, 0.6412,  ..., 0.5311, 0.6396, 0.6106],
          [0.5864, 0.4176, 0.5826,  ..., 0.6115, 0.5585, 0.5382],
          [0.4651, 0.4340, 0.3755,  ..., 0.6750, 0.4146, 0.4201]]],


        [[[0.5175, 0.4734, 0.5165,  ..., 0.4708, 0.4592, 0.5599],
          [0.4644, 0.6463, 0.3982,  ..., 0.5333, 0.5831, 0.4664],
          [0.4377, 0.5420, 0.4797,  ..., 0.5012, 0.3910, 0.4768],
          [0.5046, 0.4887, 0.5353,  ..., 0.5610, 0.3398, 0.5182]],

         [[0.4876, 0.5169, 0.5500,  ..., 0.5543, 0.5657, 0.7113],
          [0.4384, 0.5103, 0.5995,  ..., 0.4577, 0.5865, 0.4395],
          [0.3977, 0.3877, 0.4786,  ..., 0.5193, 0.4880, 0.5249],
          [0.6404, 0.7434, 0.4243,  ..., 0.4866, 0.4879, 0.4173]],

         [[0.5411, 0.4095, 0.4943,  ..., 0.4478, 0.4563, 0.4737],
          [0.5372, 0.4444, 0.5408,  ..., 0.6248, 0.5153, 0.4305],
          [0.6430, 0.3835, 0.6107,  ..., 0.4621, 0.4053, 0.6097],
          [0.5452, 0.4200, 0.5466,  ..., 0.4290, 0.4401, 0.4780]],

         ...,

         [[0.5017, 0.4914, 0.3945,  ..., 0.4161, 0.4614, 0.6120],
          [0.4776, 0.5199, 0.5883,  ..., 0.5249, 0.4996, 0.5741],
          [0.4695, 0.4809, 0.6762,  ..., 0.5526, 0.4947, 0.3319],
          [0.5032, 0.4229, 0.4940,  ..., 0.5461, 0.3475, 0.4386]],

         [[0.6055, 0.5040, 0.3945,  ..., 0.4372, 0.5496, 0.5489],
          [0.4235, 0.4284, 0.6047,  ..., 0.6161, 0.5343, 0.6540],
          [0.4755, 0.4299, 0.5241,  ..., 0.5437, 0.3975, 0.5273],
          [0.4887, 0.4715, 0.4664,  ..., 0.5461, 0.5405, 0.4768]],

         [[0.5641, 0.3401, 0.5265,  ..., 0.3616, 0.5343, 0.6431],
          [0.4607, 0.5993, 0.5635,  ..., 0.6472, 0.6136, 0.4843],
          [0.4639, 0.3929, 0.5864,  ..., 0.5100, 0.5449, 0.4958],
          [0.4759, 0.5073, 0.4619,  ..., 0.5308, 0.4554, 0.4893]]]],
       device='cuda:0')
tensor([[[[0.4960, 0.4857, 0.4335,  ..., 0.3666, 0.4542, 0.6968],
          [0.4752, 0.6192, 0.3285,  ..., 0.6289, 0.6142, 0.4694],
          [0.4878, 0.5436, 0.5463,  ..., 0.4962, 0.4470, 0.4637],
          [0.4840, 0.5162, 0.3993,  ..., 0.5167, 0.4475, 0.4728]],

         [[0.4373, 0.6397, 0.4364,  ..., 0.6053, 0.3433, 0.5289],
          [0.4999, 0.5135, 0.4598,  ..., 0.5523, 0.5941, 0.5547],
          [0.5742, 0.4220, 0.5612,  ..., 0.5003, 0.4891, 0.3975],
          [0.5913, 0.6215, 0.4431,  ..., 0.5132, 0.4819, 0.4412]],

         [[0.4263, 0.6011, 0.5132,  ..., 0.5737, 0.4933, 0.5438],
          [0.5349, 0.5174, 0.4321,  ..., 0.4537, 0.5387, 0.4383],
          [0.4092, 0.5586, 0.5540,  ..., 0.4121, 0.3989, 0.3914],
          [0.6035, 0.5353, 0.6307,  ..., 0.4966, 0.3648, 0.5063]],

         ...,

         [[0.4567, 0.5140, 0.4369,  ..., 0.3863, 0.5689, 0.5988],
          [0.4472, 0.6261, 0.5851,  ..., 0.4494, 0.6514, 0.4805],
          [0.5120, 0.4163, 0.5366,  ..., 0.4125, 0.4263, 0.6072],
          [0.5204, 0.4054, 0.5205,  ..., 0.4458, 0.4811, 0.3594]],

         [[0.4787, 0.5332, 0.4496,  ..., 0.4608, 0.5238, 0.4726],
          [0.4443, 0.5619, 0.4254,  ..., 0.3785, 0.5020, 0.4710],
          [0.5109, 0.4282, 0.5603,  ..., 0.5525, 0.5479, 0.5271],
          [0.5167, 0.3951, 0.4815,  ..., 0.6469, 0.4381, 0.3585]],

         [[0.5894, 0.5631, 0.5302,  ..., 0.4930, 0.5955, 0.5569],
          [0.4574, 0.5927, 0.6442,  ..., 0.5361, 0.6206, 0.5936],
          [0.5974, 0.4306, 0.5856,  ..., 0.6165, 0.5395, 0.5212],
          [0.4761, 0.4470, 0.3785,  ..., 0.6800, 0.3956, 0.4031]]],


        [[[0.5285, 0.4864, 0.5195,  ..., 0.4758, 0.4402, 0.5429],
          [0.4754, 0.6593, 0.4012,  ..., 0.5383, 0.5641, 0.4494],
          [0.4487, 0.5550, 0.4827,  ..., 0.5062, 0.3720, 0.4598],
          [0.5156, 0.5017, 0.5383,  ..., 0.5660, 0.3208, 0.5012]],

         [[0.4986, 0.5299, 0.5530,  ..., 0.5593, 0.5467, 0.6943],
          [0.4494, 0.5233, 0.6025,  ..., 0.4627, 0.5675, 0.4225],
          [0.4087, 0.4007, 0.4816,  ..., 0.5243, 0.4690, 0.5079],
          [0.6514, 0.7564, 0.4273,  ..., 0.4916, 0.4689, 0.4003]],

         [[0.5521, 0.4225, 0.4973,  ..., 0.4528, 0.4373, 0.4567],
          [0.5482, 0.4574, 0.5438,  ..., 0.6298, 0.4963, 0.4135],
          [0.6540, 0.3965, 0.6137,  ..., 0.4671, 0.3863, 0.5927],
          [0.5562, 0.4330, 0.5496,  ..., 0.4340, 0.4211, 0.4610]],

         ...,

         [[0.5127, 0.5044, 0.3975,  ..., 0.4211, 0.4424, 0.5950],
          [0.4886, 0.5329, 0.5913,  ..., 0.5299, 0.4806, 0.5571],
          [0.4805, 0.4939, 0.6792,  ..., 0.5576, 0.4757, 0.3149],
          [0.5142, 0.4359, 0.4970,  ..., 0.5511, 0.3285, 0.4216]],

         [[0.6165, 0.5170, 0.3975,  ..., 0.4422, 0.5306, 0.5319],
          [0.4345, 0.4414, 0.6077,  ..., 0.6211, 0.5153, 0.6370],
          [0.4865, 0.4429, 0.5271,  ..., 0.5487, 0.3785, 0.5103],
          [0.4997, 0.4845, 0.4694,  ..., 0.5511, 0.5215, 0.4598]],

         [[0.5751, 0.3531, 0.5295,  ..., 0.3666, 0.5153, 0.6261],
          [0.4717, 0.6123, 0.5665,  ..., 0.6522, 0.5946, 0.4673],
          [0.4749, 0.4059, 0.5894,  ..., 0.5150, 0.5259, 0.4788],
          [0.4869, 0.5203, 0.4649,  ..., 0.5358, 0.4364, 0.4723]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0110, -0.0130, -0.0030, -0.0030, -0.0170,  0.0230, -0.0230, -0.0050,
         0.0190,  0.0170], device='cuda:0')
selected experts tensor([1386, 1716, 1677, 1851, 1191, 1765, 2285, 1278, 1531, 1704],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4896, 0.5329, 0.4782,  ..., 0.4311, 0.5402, 0.4761],
          [0.5516, 0.4601, 0.6215,  ..., 0.6126, 0.5094, 0.4929],
          [0.4497, 0.3298, 0.3978,  ..., 0.4212, 0.5280, 0.6543],
          [0.5154, 0.5584, 0.5221,  ..., 0.5005, 0.5304, 0.4024]],

         [[0.4975, 0.5661, 0.5233,  ..., 0.4221, 0.3703, 0.5775],
          [0.4603, 0.5251, 0.5435,  ..., 0.4283, 0.5608, 0.4805],
          [0.6221, 0.4466, 0.3284,  ..., 0.5353, 0.4036, 0.4345],
          [0.5728, 0.5444, 0.5273,  ..., 0.4226, 0.6443, 0.5166]],

         [[0.5576, 0.5081, 0.3626,  ..., 0.4855, 0.4278, 0.5079],
          [0.4097, 0.5990, 0.4447,  ..., 0.3184, 0.5766, 0.4662],
          [0.6012, 0.5589, 0.4180,  ..., 0.4488, 0.5216, 0.7070],
          [0.5307, 0.5999, 0.5189,  ..., 0.4450, 0.4855, 0.4407]],

         ...,

         [[0.5010, 0.5030, 0.5050,  ..., 0.5110, 0.5010, 0.5110],
          [0.5010, 0.5030, 0.5050,  ..., 0.5110, 0.5010, 0.5110],
          [0.5010, 0.5030, 0.5050,  ..., 0.5110, 0.5010, 0.5110],
          [0.5010, 0.5030, 0.5050,  ..., 0.5110, 0.5010, 0.5110]],

         [[0.5010, 0.5030, 0.5050,  ..., 0.5110, 0.5010, 0.5110],
          [0.5010, 0.5030, 0.5050,  ..., 0.5110, 0.5010, 0.5110],
          [0.5010, 0.5030, 0.5050,  ..., 0.5110, 0.5010, 0.5110],
          [0.5010, 0.5030, 0.5050,  ..., 0.5110, 0.5010, 0.5110]],

         [[0.5010, 0.5030, 0.5050,  ..., 0.5110, 0.5010, 0.5110],
          [0.5010, 0.5030, 0.5050,  ..., 0.5110, 0.5010, 0.5110],
          [0.5010, 0.5030, 0.5050,  ..., 0.5110, 0.5010, 0.5110],
          [0.5010, 0.5030, 0.5050,  ..., 0.5110, 0.5010, 0.5110]]],


        [[[0.5468, 0.5072, 0.4318,  ..., 0.5170, 0.5737, 0.4727],
          [0.4499, 0.3949, 0.6117,  ..., 0.6228, 0.4581, 0.4865],
          [0.5547, 0.4303, 0.4636,  ..., 0.5179, 0.4876, 0.6079],
          [0.5699, 0.5003, 0.4841,  ..., 0.5316, 0.5412, 0.3830]],

         [[0.5918, 0.5657, 0.5420,  ..., 0.3579, 0.3461, 0.3695],
          [0.4407, 0.4341, 0.6429,  ..., 0.5961, 0.4173, 0.3794],
          [0.5451, 0.3642, 0.4653,  ..., 0.4865, 0.5059, 0.5606],
          [0.4688, 0.4989, 0.4549,  ..., 0.4681, 0.7108, 0.4568]],

         [[0.6235, 0.6004, 0.5126,  ..., 0.2784, 0.5317, 0.6868],
          [0.5359, 0.4609, 0.5863,  ..., 0.5106, 0.4230, 0.4698],
          [0.5393, 0.5224, 0.4612,  ..., 0.4235, 0.6515, 0.5381],
          [0.5318, 0.5270, 0.3848,  ..., 0.3456, 0.5226, 0.4354]],

         ...,

         [[0.5010, 0.5030, 0.5050,  ..., 0.5110, 0.5010, 0.5110],
          [0.5010, 0.5030, 0.5050,  ..., 0.5110, 0.5010, 0.5110],
          [0.5010, 0.5030, 0.5050,  ..., 0.5110, 0.5010, 0.5110],
          [0.5010, 0.5030, 0.5050,  ..., 0.5110, 0.5010, 0.5110]],

         [[0.5010, 0.5030, 0.5050,  ..., 0.5110, 0.5010, 0.5110],
          [0.5010, 0.5030, 0.5050,  ..., 0.5110, 0.5010, 0.5110],
          [0.5010, 0.5030, 0.5050,  ..., 0.5110, 0.5010, 0.5110],
          [0.5010, 0.5030, 0.5050,  ..., 0.5110, 0.5010, 0.5110]],

         [[0.5010, 0.5030, 0.5050,  ..., 0.5110, 0.5010, 0.5110],
          [0.5010, 0.5030, 0.5050,  ..., 0.5110, 0.5010, 0.5110],
          [0.5010, 0.5030, 0.5050,  ..., 0.5110, 0.5010, 0.5110],
          [0.5010, 0.5030, 0.5050,  ..., 0.5110, 0.5010, 0.5110]]]],
       device='cuda:0')/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([[[[0.4886, 0.5299, 0.4732,  ..., 0.4201, 0.5392, 0.4651],
          [0.5506, 0.4571, 0.6165,  ..., 0.6016, 0.5084, 0.4819],
          [0.4487, 0.3268, 0.3928,  ..., 0.4102, 0.5270, 0.6433],
          [0.5144, 0.5554, 0.5171,  ..., 0.4895, 0.5294, 0.3914]],

         [[0.4965, 0.5631, 0.5183,  ..., 0.4111, 0.3693, 0.5665],
          [0.4593, 0.5221, 0.5385,  ..., 0.4173, 0.5598, 0.4695],
          [0.6211, 0.4436, 0.3234,  ..., 0.5243, 0.4026, 0.4235],
          [0.5718, 0.5414, 0.5223,  ..., 0.4116, 0.6433, 0.5056]],

         [[0.5566, 0.5051, 0.3576,  ..., 0.4745, 0.4268, 0.4969],
          [0.4087, 0.5960, 0.4397,  ..., 0.3074, 0.5756, 0.4552],
          [0.6002, 0.5559, 0.4130,  ..., 0.4378, 0.5206, 0.6960],
          [0.5297, 0.5969, 0.5139,  ..., 0.4340, 0.4845, 0.4297]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5458, 0.5042, 0.4268,  ..., 0.5060, 0.5727, 0.4617],
          [0.4489, 0.3919, 0.6067,  ..., 0.6118, 0.4571, 0.4755],
          [0.5537, 0.4273, 0.4586,  ..., 0.5069, 0.4866, 0.5969],
          [0.5689, 0.4973, 0.4791,  ..., 0.5206, 0.5402, 0.3720]],

         [[0.5908, 0.5627, 0.5370,  ..., 0.3469, 0.3451, 0.3585],
          [0.4397, 0.4311, 0.6379,  ..., 0.5851, 0.4163, 0.3684],
          [0.5441, 0.3612, 0.4603,  ..., 0.4755, 0.5049, 0.5496],
          [0.4678, 0.4959, 0.4499,  ..., 0.4571, 0.7098, 0.4458]],

         [[0.6225, 0.5974, 0.5076,  ..., 0.2674, 0.5307, 0.6758],
          [0.5349, 0.4579, 0.5813,  ..., 0.4996, 0.4220, 0.4588],
          [0.5383, 0.5194, 0.4562,  ..., 0.4125, 0.6505, 0.5271],
          [0.5308, 0.5240, 0.3798,  ..., 0.3346, 0.5216, 0.4244]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0010,  0.0030,  0.0050,  0.0090, -0.0110,  0.0110,  0.0090,  0.0110,
         0.0010,  0.0110], device='cuda:0')
selected experts tensor([1493, 1847, 1553, 1738, 1700, 1518, 1491, 1772, 1778, 1494],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1617., 1654., 1641., 1640., 1498., 1661., 1697., 1771., 1660., 1545.],
        [1493., 1847., 1553., 1738., 1700., 1518., 1491., 1772., 1778., 1494.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4170, 0.5236, 0.5455,  ..., 0.4405, 0.5642, 0.5576],
          [0.5333, 0.5928, 0.4636,  ..., 0.4615, 0.4284, 0.6077],
          [0.5754, 0.5534, 0.6345,  ..., 0.4985, 0.4099, 0.6295],
          [0.4749, 0.5374, 0.5965,  ..., 0.6689, 0.3633, 0.3800]],

         [[0.4789, 0.4203, 0.3849,  ..., 0.5346, 0.4546, 0.4040],
          [0.3218, 0.5709, 0.5288,  ..., 0.4886, 0.5276, 0.5648],
          [0.4803, 0.3893, 0.5657,  ..., 0.5155, 0.5134, 0.4357],
          [0.5538, 0.4856, 0.5281,  ..., 0.5487, 0.4838, 0.6359]],

         [[0.4492, 0.6282, 0.5089,  ..., 0.4877, 0.4231, 0.3135],
          [0.3331, 0.5800, 0.3784,  ..., 0.4219, 0.3272, 0.5009],
          [0.5533, 0.5601, 0.5122,  ..., 0.6244, 0.5867, 0.3278],
          [0.5118, 0.5296, 0.4606,  ..., 0.4905, 0.5767, 0.4707]],

         ...,

         [[0.4818, 0.5681, 0.4540,  ..., 0.4704, 0.5529, 0.4219],
          [0.4863, 0.4793, 0.4937,  ..., 0.4680, 0.5507, 0.4685],
          [0.2925, 0.3778, 0.3994,  ..., 0.5648, 0.6675, 0.5039],
          [0.5078, 0.5226, 0.3994,  ..., 0.4195, 0.5461, 0.5453]],

         [[0.4490, 0.6074, 0.3751,  ..., 0.5581, 0.5200, 0.3194],
          [0.4347, 0.3930, 0.4803,  ..., 0.4324, 0.5033, 0.6239],
          [0.6356, 0.5366, 0.5286,  ..., 0.4853, 0.3838, 0.4281],
          [0.3934, 0.5162, 0.4965,  ..., 0.5258, 0.4782, 0.4683]],

         [[0.4699, 0.3760, 0.5932,  ..., 0.3754, 0.5876, 0.4247],
          [0.3864, 0.4430, 0.5274,  ..., 0.6386, 0.5719, 0.5586],
          [0.5952, 0.5848, 0.5786,  ..., 0.5465, 0.4444, 0.5893],
          [0.5971, 0.4862, 0.6381,  ..., 0.4352, 0.6310, 0.5244]]],


        [[[0.3967, 0.3196, 0.5537,  ..., 0.6689, 0.5097, 0.5764],
          [0.5900, 0.5196, 0.4921,  ..., 0.4769, 0.3714, 0.5816],
          [0.5286, 0.5340, 0.5833,  ..., 0.4583, 0.4810, 0.4176],
          [0.5863, 0.5309, 0.4712,  ..., 0.4591, 0.4963, 0.5169]],

         [[0.6604, 0.4601, 0.4994,  ..., 0.6039, 0.5069, 0.4518],
          [0.3643, 0.5226, 0.5624,  ..., 0.4129, 0.5781, 0.4262],
          [0.4374, 0.5618, 0.4337,  ..., 0.4792, 0.4799, 0.4847],
          [0.6078, 0.4877, 0.3215,  ..., 0.5231, 0.4212, 0.4205]],

         [[0.4208, 0.5197, 0.4787,  ..., 0.4101, 0.4771, 0.4443],
          [0.4280, 0.4734, 0.5852,  ..., 0.4525, 0.2908, 0.5624],
          [0.5403, 0.4797, 0.4463,  ..., 0.6601, 0.5276, 0.3984],
          [0.4609, 0.4360, 0.5528,  ..., 0.5706, 0.5159, 0.4286]],

         ...,

         [[0.5188, 0.5021, 0.5111,  ..., 0.4035, 0.4483, 0.4362],
          [0.4500, 0.4750, 0.3551,  ..., 0.5048, 0.6227, 0.5689],
          [0.4723, 0.4136, 0.4088,  ..., 0.3952, 0.4937, 0.6431],
          [0.4531, 0.5635, 0.5552,  ..., 0.4110, 0.3995, 0.3800]],

         [[0.4042, 0.5189, 0.5899,  ..., 0.3947, 0.4538, 0.5174],
          [0.4621, 0.5135, 0.3966,  ..., 0.4617, 0.4938, 0.4893],
          [0.5320, 0.3723, 0.3569,  ..., 0.4969, 0.5296, 0.6044],
          [0.6031, 0.5829, 0.5008,  ..., 0.4887, 0.5604, 0.5595]],

         [[0.5301, 0.4577, 0.5322,  ..., 0.6030, 0.6856, 0.5030],
          [0.3781, 0.3179, 0.4613,  ..., 0.4276, 0.5053, 0.3236],
          [0.3580, 0.5767, 0.4227,  ..., 0.4930, 0.4930, 0.5026],
          [0.6202, 0.4923, 0.6300,  ..., 0.5010, 0.6032, 0.5821]]]],
       device='cuda:0')
tensor([[[[0.4220, 0.5206, 0.5525,  ..., 0.4335, 0.5612, 0.5506],
          [0.5383, 0.5898, 0.4706,  ..., 0.4545, 0.4254, 0.6007],
          [0.5804, 0.5504, 0.6415,  ..., 0.4915, 0.4069, 0.6225],
          [0.4799, 0.5344, 0.6035,  ..., 0.6619, 0.3603, 0.3730]],

         [[0.4839, 0.4173, 0.3919,  ..., 0.5276, 0.4516, 0.3970],
          [0.3268, 0.5679, 0.5358,  ..., 0.4816, 0.5246, 0.5578],
          [0.4853, 0.3863, 0.5727,  ..., 0.5085, 0.5104, 0.4287],
          [0.5588, 0.4826, 0.5351,  ..., 0.5417, 0.4808, 0.6289]],

         [[0.4542, 0.6252, 0.5159,  ..., 0.4807, 0.4201, 0.3065],
          [0.3381, 0.5770, 0.3854,  ..., 0.4149, 0.3242, 0.4939],
          [0.5583, 0.5571, 0.5192,  ..., 0.6174, 0.5837, 0.3208],
          [0.5168, 0.5266, 0.4676,  ..., 0.4835, 0.5737, 0.4637]],

         ...,

         [[0.4868, 0.5651, 0.4610,  ..., 0.4634, 0.5499, 0.4149],
          [0.4913, 0.4763, 0.5007,  ..., 0.4610, 0.5477, 0.4615],
          [0.2975, 0.3748, 0.4064,  ..., 0.5578, 0.6645, 0.4969],
          [0.5128, 0.5196, 0.4064,  ..., 0.4125, 0.5431, 0.5383]],

         [[0.4540, 0.6044, 0.3821,  ..., 0.5511, 0.5170, 0.3124],
          [0.4397, 0.3900, 0.4873,  ..., 0.4254, 0.5003, 0.6169],
          [0.6406, 0.5336, 0.5356,  ..., 0.4783, 0.3808, 0.4211],
          [0.3984, 0.5132, 0.5035,  ..., 0.5188, 0.4752, 0.4613]],

         [[0.4749, 0.3730, 0.6002,  ..., 0.3684, 0.5846, 0.4177],
          [0.3914, 0.4400, 0.5344,  ..., 0.6316, 0.5689, 0.5516],
          [0.6002, 0.5818, 0.5856,  ..., 0.5395, 0.4414, 0.5823],
          [0.6021, 0.4832, 0.6451,  ..., 0.4282, 0.6280, 0.5174]]],


        [[[0.4017, 0.3166, 0.5607,  ..., 0.6619, 0.5067, 0.5694],
          [0.5950, 0.5166, 0.4991,  ..., 0.4699, 0.3684, 0.5746],
          [0.5336, 0.5310, 0.5903,  ..., 0.4513, 0.4780, 0.4106],
          [0.5913, 0.5279, 0.4782,  ..., 0.4521, 0.4933, 0.5099]],

         [[0.6654, 0.4571, 0.5064,  ..., 0.5969, 0.5039, 0.4448],
          [0.3693, 0.5196, 0.5694,  ..., 0.4059, 0.5751, 0.4192],
          [0.4424, 0.5588, 0.4407,  ..., 0.4722, 0.4769, 0.4777],
          [0.6128, 0.4847, 0.3285,  ..., 0.5161, 0.4182, 0.4135]],

         [[0.4258, 0.5167, 0.4857,  ..., 0.4031, 0.4741, 0.4373],
          [0.4330, 0.4704, 0.5922,  ..., 0.4455, 0.2878, 0.5554],
          [0.5453, 0.4767, 0.4533,  ..., 0.6531, 0.5246, 0.3914],
          [0.4659, 0.4330, 0.5598,  ..., 0.5636, 0.5129, 0.4216]],

         ...,

         [[0.5238, 0.4991, 0.5181,  ..., 0.3965, 0.4453, 0.4292],
          [0.4550, 0.4720, 0.3621,  ..., 0.4978, 0.6197, 0.5619],
          [0.4773, 0.4106, 0.4158,  ..., 0.3882, 0.4907, 0.6361],
          [0.4581, 0.5605, 0.5622,  ..., 0.4040, 0.3965, 0.3730]],

         [[0.4092, 0.5159, 0.5969,  ..., 0.3877, 0.4508, 0.5104],
          [0.4671, 0.5105, 0.4036,  ..., 0.4547, 0.4908, 0.4823],
          [0.5370, 0.3693, 0.3639,  ..., 0.4899, 0.5266, 0.5974],
          [0.6081, 0.5799, 0.5078,  ..., 0.4817, 0.5574, 0.5525]],

         [[0.5351, 0.4547, 0.5392,  ..., 0.5960, 0.6826, 0.4960],
          [0.3831, 0.3149, 0.4683,  ..., 0.4206, 0.5023, 0.3166],
          [0.3630, 0.5737, 0.4297,  ..., 0.4860, 0.4900, 0.4956],
          [0.6252, 0.4893, 0.6370,  ..., 0.4940, 0.6002, 0.5751]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/dyna/callbacks/activation_monitor.py:211: UserWarning: Value passed to ActivationMonitor is not a torch.Tensor, skipping.
  warnings.warn(

tensor([-0.0050,  0.0030, -0.0070,  0.0010, -0.0050, -0.0030,  0.0090,  0.0070,
         0.0030,  0.0070], device='cuda:0')
selected experts tensor([1491, 1726, 1639, 1609, 1528, 1614, 1700, 1707, 1663, 1707],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4684, 0.4614, 0.5331,  ..., 0.4427, 0.4974, 0.3730],
          [0.2588, 0.4785, 0.5016,  ..., 0.4808, 0.6282, 0.4758],
          [0.4712, 0.5243, 0.5990,  ..., 0.4987, 0.4733, 0.4391],
          [0.5717, 0.5812, 0.5352,  ..., 0.5277, 0.4988, 0.5608]],

         [[0.4634, 0.7100, 0.4862,  ..., 0.5142, 0.6830, 0.5203],
          [0.4385, 0.4034, 0.6008,  ..., 0.4791, 0.4389, 0.3640],
          [0.5508, 0.4796, 0.4652,  ..., 0.4783, 0.3411, 0.4838],
          [0.4919, 0.3921, 0.3642,  ..., 0.6293, 0.4652, 0.5875]],

         [[0.3593, 0.5336, 0.5396,  ..., 0.5886, 0.4854, 0.5057],
          [0.5959, 0.4147, 0.4113,  ..., 0.4772, 0.5168, 0.5889],
          [0.3423, 0.5793, 0.5130,  ..., 0.3562, 0.5671, 0.5543],
          [0.5324, 0.4895, 0.3437,  ..., 0.4478, 0.5213, 0.5103]],

         ...,

         [[0.3848, 0.5238, 0.5049,  ..., 0.5350, 0.3723, 0.3776],
          [0.4455, 0.6222, 0.2884,  ..., 0.5938, 0.5069, 0.7019],
          [0.4663, 0.4703, 0.4951,  ..., 0.5217, 0.4746, 0.5086],
          [0.4287, 0.4590, 0.4432,  ..., 0.6193, 0.4990, 0.5035]],

         [[0.4540, 0.5033, 0.4830,  ..., 0.5342, 0.4796, 0.5060],
          [0.5322, 0.4783, 0.3884,  ..., 0.5170, 0.5272, 0.5814],
          [0.5133, 0.5143, 0.4725,  ..., 0.4066, 0.3687, 0.4756],
          [0.3548, 0.5645, 0.6074,  ..., 0.4918, 0.5928, 0.4321]],

         [[0.5501, 0.5264, 0.4478,  ..., 0.4042, 0.5733, 0.5468],
          [0.4980, 0.4310, 0.5994,  ..., 0.4033, 0.4084, 0.5584],
          [0.6900, 0.3537, 0.4731,  ..., 0.4146, 0.5148, 0.5620],
          [0.5260, 0.4890, 0.4960,  ..., 0.4924, 0.4151, 0.4710]]],


        [[[0.5554, 0.4157, 0.5281,  ..., 0.3925, 0.5971, 0.5074],
          [0.5054, 0.4789, 0.3902,  ..., 0.4146, 0.5791, 0.5359],
          [0.5246, 0.4267, 0.5201,  ..., 0.6630, 0.4427, 0.4326],
          [0.4646, 0.5449, 0.2727,  ..., 0.4575, 0.3796, 0.4892]],

         [[0.5218, 0.6250, 0.3324,  ..., 0.4213, 0.4236, 0.6280],
          [0.4544, 0.3448, 0.4895,  ..., 0.5109, 0.4505, 0.4931],
          [0.5888, 0.4076, 0.4000,  ..., 0.4473, 0.5012, 0.4292],
          [0.3593, 0.6624, 0.3939,  ..., 0.3934, 0.6107, 0.5346]],

         [[0.5206, 0.6607, 0.5534,  ..., 0.6801, 0.5473, 0.4405],
          [0.5423, 0.3573, 0.4019,  ..., 0.4425, 0.4881, 0.4737],
          [0.5392, 0.5457, 0.5262,  ..., 0.6257, 0.6762, 0.6031],
          [0.5684, 0.4305, 0.5905,  ..., 0.5533, 0.6102, 0.4948]],

         ...,

         [[0.5552, 0.5372, 0.6273,  ..., 0.3869, 0.4746, 0.5718],
          [0.3881, 0.6598, 0.5142,  ..., 0.3873, 0.2727, 0.5581],
          [0.4963, 0.4604, 0.4546,  ..., 0.4891, 0.6018, 0.4807],
          [0.4373, 0.5058, 0.3651,  ..., 0.4833, 0.5705, 0.5306]],

         [[0.5556, 0.4730, 0.5565,  ..., 0.6202, 0.4804, 0.4297],
          [0.3738, 0.4977, 0.3376,  ..., 0.4080, 0.4179, 0.4760],
          [0.5765, 0.3618, 0.4736,  ..., 0.5410, 0.5616, 0.5128],
          [0.5498, 0.4072, 0.5150,  ..., 0.4935, 0.4341, 0.4374]],

         [[0.3937, 0.4300, 0.5335,  ..., 0.6383, 0.4650, 0.3749],
          [0.5039, 0.5878, 0.4589,  ..., 0.4930, 0.6055, 0.5329],
          [0.4897, 0.5093, 0.5700,  ..., 0.4801, 0.4203, 0.3559],
          [0.5595, 0.5416, 0.4923,  ..., 0.6096, 0.4355, 0.4463]]]],
       device='cuda:0')
tensor([[[[0.4694, 0.4644, 0.5301,  ..., 0.4477, 0.4944, 0.3720],
          [0.2598, 0.4815, 0.4986,  ..., 0.4858, 0.6252, 0.4748],
          [0.4722, 0.5273, 0.5960,  ..., 0.5037, 0.4703, 0.4381],
          [0.5727, 0.5842, 0.5322,  ..., 0.5327, 0.4958, 0.5598]],

         [[0.4644, 0.7130, 0.4832,  ..., 0.5192, 0.6800, 0.5193],
          [0.4395, 0.4064, 0.5978,  ..., 0.4841, 0.4359, 0.3630],
          [0.5518, 0.4826, 0.4622,  ..., 0.4833, 0.3381, 0.4828],
          [0.4929, 0.3951, 0.3612,  ..., 0.6343, 0.4622, 0.5865]],

         [[0.3603, 0.5366, 0.5366,  ..., 0.5936, 0.4824, 0.5047],
          [0.5969, 0.4177, 0.4083,  ..., 0.4822, 0.5138, 0.5879],
          [0.3433, 0.5823, 0.5100,  ..., 0.3612, 0.5641, 0.5533],
          [0.5334, 0.4925, 0.3407,  ..., 0.4528, 0.5183, 0.5093]],

         ...,

         [[0.3858, 0.5268, 0.5019,  ..., 0.5400, 0.3693, 0.3766],
          [0.4465, 0.6252, 0.2854,  ..., 0.5988, 0.5039, 0.7009],
          [0.4673, 0.4733, 0.4921,  ..., 0.5267, 0.4716, 0.5076],
          [0.4297, 0.4620, 0.4402,  ..., 0.6243, 0.4960, 0.5025]],

         [[0.4550, 0.5063, 0.4800,  ..., 0.5392, 0.4766, 0.5050],
          [0.5332, 0.4813, 0.3854,  ..., 0.5220, 0.5242, 0.5804],
          [0.5143, 0.5173, 0.4695,  ..., 0.4116, 0.3657, 0.4746],
          [0.3558, 0.5675, 0.6044,  ..., 0.4968, 0.5898, 0.4311]],

         [[0.5511, 0.5294, 0.4448,  ..., 0.4092, 0.5703, 0.5458],
          [0.4990, 0.4340, 0.5964,  ..., 0.4083, 0.4054, 0.5574],
          [0.6910, 0.3567, 0.4701,  ..., 0.4196, 0.5118, 0.5610],
          [0.5270, 0.4920, 0.4930,  ..., 0.4974, 0.4121, 0.4700]]],


        [[[0.5564, 0.4187, 0.5251,  ..., 0.3975, 0.5941, 0.5064],
          [0.5064, 0.4819, 0.3872,  ..., 0.4196, 0.5761, 0.5349],
          [0.5256, 0.4297, 0.5171,  ..., 0.6680, 0.4397, 0.4316],
          [0.4656, 0.5479, 0.2697,  ..., 0.4625, 0.3766, 0.4882]],

         [[0.5228, 0.6280, 0.3294,  ..., 0.4263, 0.4206, 0.6270],
          [0.4554, 0.3478, 0.4865,  ..., 0.5159, 0.4475, 0.4921],
          [0.5898, 0.4106, 0.3970,  ..., 0.4523, 0.4982, 0.4282],
          [0.3603, 0.6654, 0.3909,  ..., 0.3984, 0.6077, 0.5336]],

         [[0.5216, 0.6637, 0.5504,  ..., 0.6851, 0.5443, 0.4395],
          [0.5433, 0.3603, 0.3989,  ..., 0.4475, 0.4851, 0.4727],
          [0.5402, 0.5487, 0.5232,  ..., 0.6307, 0.6732, 0.6021],
          [0.5694, 0.4335, 0.5875,  ..., 0.5583, 0.6072, 0.4938]],

         ...,

         [[0.5562, 0.5402, 0.6243,  ..., 0.3919, 0.4716, 0.5708],
          [0.3891, 0.6628, 0.5112,  ..., 0.3923, 0.2697, 0.5571],
          [0.4973, 0.4634, 0.4516,  ..., 0.4941, 0.5988, 0.4797],
          [0.4383, 0.5088, 0.3621,  ..., 0.4883, 0.5675, 0.5296]],

         [[0.5566, 0.4760, 0.5535,  ..., 0.6252, 0.4774, 0.4287],
          [0.3748, 0.5007, 0.3346,  ..., 0.4130, 0.4149, 0.4750],
          [0.5775, 0.3648, 0.4706,  ..., 0.5460, 0.5586, 0.5118],
          [0.5508, 0.4102, 0.5120,  ..., 0.4985, 0.4311, 0.4364]],

         [[0.3947, 0.4330, 0.5305,  ..., 0.6433, 0.4620, 0.3739],
          [0.5049, 0.5908, 0.4559,  ..., 0.4980, 0.6025, 0.5319],
          [0.4907, 0.5123, 0.5670,  ..., 0.4851, 0.4173, 0.3549],
          [0.5605, 0.5446, 0.4893,  ..., 0.6146, 0.4325, 0.4453]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0010, -0.0030,  0.0030,  0.0090, -0.0010,  0.0070, -0.0130, -0.0050,
         0.0030,  0.0010], device='cuda:0')
selected experts tensor([1756, 1691, 1587, 1656, 1731, 1661, 1557, 1601, 1574, 1570],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5348, 0.4973, 0.4773,  ..., 0.4481, 0.5249, 0.6204],
          [0.4624, 0.6427, 0.4941,  ..., 0.5654, 0.6309, 0.5196],
          [0.5476, 0.4245, 0.5415,  ..., 0.4333, 0.5270, 0.4471],
          [0.4871, 0.5002, 0.3860,  ..., 0.4931, 0.5394, 0.4000]],

         [[0.5106, 0.6158, 0.4047,  ..., 0.4014, 0.6131, 0.7393],
          [0.3800, 0.4301, 0.6330,  ..., 0.6509, 0.5884, 0.4097],
          [0.5665, 0.5448, 0.4887,  ..., 0.5134, 0.3994, 0.4314],
          [0.5324, 0.4857, 0.5080,  ..., 0.5326, 0.4740, 0.5954]],

         [[0.5292, 0.5885, 0.5225,  ..., 0.5473, 0.4769, 0.6190],
          [0.5193, 0.6158, 0.4377,  ..., 0.5464, 0.5394, 0.5394],
          [0.3959, 0.5501, 0.4926,  ..., 0.5333, 0.3357, 0.4300],
          [0.5202, 0.5142, 0.5306,  ..., 0.5131, 0.4458, 0.4833]],

         ...,

         [[0.3744, 0.4675, 0.6032,  ..., 0.4798, 0.6534, 0.5397],
          [0.3992, 0.4863, 0.5176,  ..., 0.5976, 0.6415, 0.5579],
          [0.4559, 0.4499, 0.4726,  ..., 0.4726, 0.6032, 0.4004],
          [0.4394, 0.4941, 0.5070,  ..., 0.4667, 0.5556, 0.6016]],

         [[0.6942, 0.6121, 0.4295,  ..., 0.3653, 0.5650, 0.5286],
          [0.4834, 0.5320, 0.4548,  ..., 0.5323, 0.7274, 0.5423],
          [0.4408, 0.4371, 0.4285,  ..., 0.5138, 0.5234, 0.4758],
          [0.3246, 0.3094, 0.4624,  ..., 0.3772, 0.4156, 0.2684]],

         [[0.3912, 0.4279, 0.6129,  ..., 0.3897, 0.3902, 0.7061],
          [0.3503, 0.5230, 0.6605,  ..., 0.4329, 0.4648, 0.5202],
          [0.4794, 0.4648, 0.5563,  ..., 0.5328, 0.6543, 0.4635],
          [0.4254, 0.4684, 0.4937,  ..., 0.4204, 0.4068, 0.4318]]],


        [[[0.4865, 0.6786, 0.4700,  ..., 0.3464, 0.5704, 0.5431],
          [0.4077, 0.5296, 0.4432,  ..., 0.7002, 0.5113, 0.3871],
          [0.5483, 0.4286, 0.4800,  ..., 0.3581, 0.5442, 0.5132],
          [0.5723, 0.3971, 0.5078,  ..., 0.5018, 0.5046, 0.4681]],

         [[0.4520, 0.6669, 0.4403,  ..., 0.3608, 0.5865, 0.5586],
          [0.4613, 0.5072, 0.6366,  ..., 0.6631, 0.4578, 0.4485],
          [0.5249, 0.6185, 0.5459,  ..., 0.4109, 0.3686, 0.5118],
          [0.5199, 0.3938, 0.3740,  ..., 0.4989, 0.4889, 0.5176]],

         [[0.4585, 0.4769, 0.4619,  ..., 0.5204, 0.5129, 0.6316],
          [0.4978, 0.5544, 0.5127,  ..., 0.6180, 0.6534, 0.3880],
          [0.4568, 0.4475, 0.4958,  ..., 0.4452, 0.4133, 0.4909],
          [0.4818, 0.6080, 0.5172,  ..., 0.4831, 0.4373, 0.5533]],

         ...,

         [[0.4861, 0.5563, 0.5060,  ..., 0.4795, 0.5364, 0.5844],
          [0.5505, 0.4402, 0.5420,  ..., 0.5891, 0.6188, 0.5615],
          [0.6180, 0.4560, 0.4871,  ..., 0.4500, 0.5086, 0.5688],
          [0.5300, 0.4776, 0.4599,  ..., 0.4703, 0.5063, 0.5138]],

         [[0.5907, 0.4976, 0.4541,  ..., 0.4333, 0.5624, 0.4172],
          [0.5024, 0.3267, 0.7122,  ..., 0.6267, 0.6169, 0.4102],
          [0.5346, 0.5414, 0.4276,  ..., 0.4884, 0.5489, 0.5066],
          [0.4982, 0.4470, 0.3446,  ..., 0.4762, 0.4179, 0.5209]],

         [[0.3503, 0.4633, 0.5816,  ..., 0.5289, 0.4825, 0.5530],
          [0.4149, 0.3858, 0.4845,  ..., 0.5292, 0.3546, 0.4916],
          [0.5120, 0.2975, 0.6303,  ..., 0.5250, 0.6660, 0.4688],
          [0.5199, 0.5112, 0.5577,  ..., 0.5008, 0.4999, 0.5258]]]],
       device='cuda:0')
tensor([[[[0.5448, 0.5113, 0.4813,  ..., 0.4521, 0.5049, 0.6044],
          [0.4724, 0.6567, 0.4981,  ..., 0.5694, 0.6109, 0.5036],
          [0.5576, 0.4385, 0.5455,  ..., 0.4373, 0.5070, 0.4311],
          [0.4971, 0.5142, 0.3900,  ..., 0.4971, 0.5194, 0.3840]],

         [[0.5206, 0.6298, 0.4087,  ..., 0.4054, 0.5931, 0.7233],
          [0.3900, 0.4441, 0.6370,  ..., 0.6549, 0.5684, 0.3937],
          [0.5765, 0.5588, 0.4927,  ..., 0.5174, 0.3794, 0.4154],
          [0.5424, 0.4997, 0.5120,  ..., 0.5366, 0.4540, 0.5794]],

         [[0.5392, 0.6025, 0.5265,  ..., 0.5513, 0.4569, 0.6030],
          [0.5293, 0.6298, 0.4417,  ..., 0.5504, 0.5194, 0.5234],
          [0.4059, 0.5641, 0.4966,  ..., 0.5373, 0.3157, 0.4140],
          [0.5302, 0.5282, 0.5346,  ..., 0.5171, 0.4258, 0.4673]],

         ...,

         [[0.3844, 0.4815, 0.6072,  ..., 0.4838, 0.6334, 0.5237],
          [0.4092, 0.5003, 0.5216,  ..., 0.6016, 0.6215, 0.5419],
          [0.4659, 0.4639, 0.4766,  ..., 0.4766, 0.5832, 0.3844],
          [0.4494, 0.5081, 0.5110,  ..., 0.4707, 0.5356, 0.5856]],

         [[0.7042, 0.6261, 0.4335,  ..., 0.3693, 0.5450, 0.5126],
          [0.4934, 0.5460, 0.4588,  ..., 0.5363, 0.7074, 0.5263],
          [0.4508, 0.4511, 0.4325,  ..., 0.5178, 0.5034, 0.4598],
          [0.3346, 0.3234, 0.4664,  ..., 0.3812, 0.3956, 0.2524]],

         [[0.4012, 0.4419, 0.6169,  ..., 0.3937, 0.3702, 0.6901],
          [0.3603, 0.5370, 0.6645,  ..., 0.4369, 0.4448, 0.5042],
          [0.4894, 0.4788, 0.5603,  ..., 0.5368, 0.6343, 0.4475],
          [0.4354, 0.4824, 0.4977,  ..., 0.4244, 0.3868, 0.4158]]],


        [[[0.4965, 0.6926, 0.4740,  ..., 0.3504, 0.5504, 0.5271],
          [0.4177, 0.5436, 0.4472,  ..., 0.7042, 0.4913, 0.3711],
          [0.5583, 0.4426, 0.4840,  ..., 0.3621, 0.5242, 0.4972],
          [0.5823, 0.4111, 0.5118,  ..., 0.5058, 0.4846, 0.4521]],

         [[0.4620, 0.6809, 0.4443,  ..., 0.3648, 0.5665, 0.5426],
          [0.4713, 0.5212, 0.6406,  ..., 0.6671, 0.4378, 0.4325],
          [0.5349, 0.6325, 0.5499,  ..., 0.4149, 0.3486, 0.4958],
          [0.5299, 0.4078, 0.3780,  ..., 0.5029, 0.4689, 0.5016]],

         [[0.4685, 0.4909, 0.4659,  ..., 0.5244, 0.4929, 0.6156],
          [0.5078, 0.5684, 0.5167,  ..., 0.6220, 0.6334, 0.3720],
          [0.4668, 0.4615, 0.4998,  ..., 0.4492, 0.3933, 0.4749],
          [0.4918, 0.6220, 0.5212,  ..., 0.4871, 0.4173, 0.5373]],

         ...,

         [[0.4961, 0.5703, 0.5100,  ..., 0.4835, 0.5164, 0.5684],
          [0.5605, 0.4542, 0.5460,  ..., 0.5931, 0.5988, 0.5455],
          [0.6280, 0.4700, 0.4911,  ..., 0.4540, 0.4886, 0.5528],
          [0.5400, 0.4916, 0.4639,  ..., 0.4743, 0.4863, 0.4978]],

         [[0.6007, 0.5116, 0.4581,  ..., 0.4373, 0.5424, 0.4012],
          [0.5124, 0.3407, 0.7162,  ..., 0.6307, 0.5969, 0.3942],
          [0.5446, 0.5554, 0.4316,  ..., 0.4924, 0.5289, 0.4906],
          [0.5082, 0.4610, 0.3486,  ..., 0.4802, 0.3979, 0.5049]],

         [[0.3603, 0.4773, 0.5856,  ..., 0.5329, 0.4625, 0.5370],
          [0.4249, 0.3998, 0.4885,  ..., 0.5332, 0.3346, 0.4756],
          [0.5220, 0.3115, 0.6343,  ..., 0.5290, 0.6460, 0.4528],
          [0.5299, 0.5252, 0.5617,  ..., 0.5048, 0.4799, 0.5098]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0100, -0.0140, -0.0040, -0.0040, -0.0160,  0.0220, -0.0240, -0.0040,
         0.0200,  0.0160], device='cuda:0')
selected experts tensor([1218, 1436, 1319, 2006, 1508, 1528, 2156, 1516, 1882, 1815],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.3262, 0.4737, 0.4663,  ..., 0.4425, 0.3942, 0.5737],
          [0.4930, 0.4051, 0.6412,  ..., 0.6195, 0.6146, 0.5430],
          [0.5093, 0.4630, 0.4651,  ..., 0.4956, 0.6663, 0.4742],
          [0.5632, 0.5937, 0.4448,  ..., 0.4865, 0.4849, 0.4583]],

         [[0.5325, 0.5656, 0.4871,  ..., 0.4344, 0.3942, 0.2794],
          [0.3814, 0.5685, 0.6618,  ..., 0.5541, 0.5068, 0.4771],
          [0.5209, 0.4701, 0.4086,  ..., 0.4846, 0.5756, 0.6739],
          [0.5364, 0.5675, 0.4860,  ..., 0.5157, 0.5879, 0.4132]],

         [[0.6290, 0.4312, 0.3895,  ..., 0.4421, 0.5047, 0.4808],
          [0.5165, 0.5866, 0.4658,  ..., 0.6288, 0.5441, 0.4364],
          [0.6648, 0.4545, 0.4133,  ..., 0.4850, 0.4696, 0.5537],
          [0.5000, 0.4415, 0.5682,  ..., 0.4928, 0.5227, 0.3440]],

         ...,

         [[0.5020, 0.5020, 0.5060,  ..., 0.5100, 0.5000, 0.5120],
          [0.5020, 0.5020, 0.5060,  ..., 0.5100, 0.5000, 0.5120],
          [0.5020, 0.5020, 0.5060,  ..., 0.5100, 0.5000, 0.5120],
          [0.5020, 0.5020, 0.5060,  ..., 0.5100, 0.5000, 0.5120]],

         [[0.5020, 0.5020, 0.5060,  ..., 0.5100, 0.5000, 0.5120],
          [0.5020, 0.5020, 0.5060,  ..., 0.5100, 0.5000, 0.5120],
          [0.5020, 0.5020, 0.5060,  ..., 0.5100, 0.5000, 0.5120],
          [0.5020, 0.5020, 0.5060,  ..., 0.5100, 0.5000, 0.5120]],

         [[0.5020, 0.5020, 0.5060,  ..., 0.5100, 0.5000, 0.5120],
          [0.5020, 0.5020, 0.5060,  ..., 0.5100, 0.5000, 0.5120],
          [0.5020, 0.5020, 0.5060,  ..., 0.5100, 0.5000, 0.5120],
          [0.5020, 0.5020, 0.5060,  ..., 0.5100, 0.5000, 0.5120]]],


        [[[0.5315, 0.6125, 0.5234,  ..., 0.3954, 0.5501, 0.6127],
          [0.4432, 0.5439, 0.5321,  ..., 0.5047, 0.6680, 0.5006],
          [0.5589, 0.5553, 0.5660,  ..., 0.6200, 0.5385, 0.5737],
          [0.5113, 0.6281, 0.4619,  ..., 0.5703, 0.5646, 0.4378]],

         [[0.4932, 0.5094, 0.5208,  ..., 0.5846, 0.4443, 0.4326],
          [0.4415, 0.5512, 0.5973,  ..., 0.5027, 0.7233, 0.6150],
          [0.5695, 0.4909, 0.5008,  ..., 0.6186, 0.6469, 0.5933],
          [0.5975, 0.4861, 0.5110,  ..., 0.4693, 0.5655, 0.4733]],

         [[0.6078, 0.5113, 0.4939,  ..., 0.4868, 0.4388, 0.5172],
          [0.3841, 0.4755, 0.6484,  ..., 0.6074, 0.5136, 0.4527],
          [0.4326, 0.4174, 0.4204,  ..., 0.5003, 0.5042, 0.5962],
          [0.5728, 0.4389, 0.5260,  ..., 0.5321, 0.6270, 0.4118]],

         ...,

         [[0.5020, 0.5020, 0.5060,  ..., 0.5100, 0.5000, 0.5120],
          [0.5020, 0.5020, 0.5060,  ..., 0.5100, 0.5000, 0.5120],
          [0.5020, 0.5020, 0.5060,  ..., 0.5100, 0.5000, 0.5120],
          [0.5020, 0.5020, 0.5060,  ..., 0.5100, 0.5000, 0.5120]],

         [[0.5020, 0.5020, 0.5060,  ..., 0.5100, 0.5000, 0.5120],
          [0.5020, 0.5020, 0.5060,  ..., 0.5100, 0.5000, 0.5120],
          [0.5020, 0.5020, 0.5060,  ..., 0.5100, 0.5000, 0.5120],
          [0.5020, 0.5020, 0.5060,  ..., 0.5100, 0.5000, 0.5120]],

         [[0.5020, 0.5020, 0.5060,  ..., 0.5100, 0.5000, 0.5120],
          [0.5020, 0.5020, 0.5060,  ..., 0.5100, 0.5000, 0.5120],
          [0.5020, 0.5020, 0.5060,  ..., 0.5100, 0.5000, 0.5120],
          [0.5020, 0.5020, 0.5060,  ..., 0.5100, 0.5000, 0.5120]]]],
       device='cuda:0')
tensor([[[[0.3242, 0.4717, 0.4603,  ..., 0.4325, 0.3942, 0.5617],
          [0.4910, 0.4031, 0.6352,  ..., 0.6095, 0.6146, 0.5310],
          [0.5073, 0.4610, 0.4591,  ..., 0.4856, 0.6663, 0.4622],
          [0.5612, 0.5917, 0.4388,  ..., 0.4765, 0.4849, 0.4463]],

         [[0.5305, 0.5636, 0.4811,  ..., 0.4244, 0.3942, 0.2674],
          [0.3794, 0.5665, 0.6558,  ..., 0.5441, 0.5068, 0.4651],
          [0.5189, 0.4681, 0.4026,  ..., 0.4746, 0.5756, 0.6619],
          [0.5344, 0.5655, 0.4800,  ..., 0.5057, 0.5879, 0.4012]],

         [[0.6270, 0.4292, 0.3835,  ..., 0.4321, 0.5047, 0.4688],
          [0.5145, 0.5846, 0.4598,  ..., 0.6188, 0.5441, 0.4244],
          [0.6628, 0.4525, 0.4073,  ..., 0.4750, 0.4696, 0.5417],
          [0.4980, 0.4395, 0.5622,  ..., 0.4828, 0.5227, 0.3320]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5295, 0.6105, 0.5174,  ..., 0.3854, 0.5501, 0.6007],
          [0.4412, 0.5419, 0.5261,  ..., 0.4947, 0.6680, 0.4886],
          [0.5569, 0.5533, 0.5600,  ..., 0.6100, 0.5385, 0.5617],
          [0.5093, 0.6261, 0.4559,  ..., 0.5603, 0.5646, 0.4258]],

         [[0.4912, 0.5074, 0.5148,  ..., 0.5746, 0.4443, 0.4206],
          [0.4395, 0.5492, 0.5913,  ..., 0.4927, 0.7233, 0.6030],
          [0.5675, 0.4889, 0.4948,  ..., 0.6086, 0.6469, 0.5813],
          [0.5955, 0.4841, 0.5050,  ..., 0.4593, 0.5655, 0.4613]],

         [[0.6058, 0.5093, 0.4879,  ..., 0.4768, 0.4388, 0.5052],
          [0.3821, 0.4735, 0.6424,  ..., 0.5974, 0.5136, 0.4407],
          [0.4306, 0.4154, 0.4144,  ..., 0.4903, 0.5042, 0.5842],
          [0.5708, 0.4369, 0.5200,  ..., 0.5221, 0.6270, 0.3998]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([ 2.0000e-03,  2.0000e-03,  6.0000e-03,  8.0000e-03, -1.2000e-02,
         1.2000e-02,  1.0000e-02,  1.0000e-02, -6.9849e-10,  1.2000e-02],
       device='cuda:0')
selected experts tensor([1635, 1821, 1765, 1435, 1568, 1613, 1506,  729, 2065, 2247],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1756., 1691., 1587., 1656., 1731., 1661., 1557., 1601., 1574., 1570.],
        [1635., 1821., 1765., 1435., 1568., 1613., 1506.,  729., 2065., 2247.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5027, 0.5068, 0.3214,  ..., 0.3529, 0.6097, 0.4947],
          [0.4611, 0.3777, 0.4404,  ..., 0.4299, 0.6203, 0.5691],
          [0.5253, 0.4832, 0.4559,  ..., 0.3294, 0.4705, 0.7238],
          [0.5181, 0.5417, 0.5445,  ..., 0.4583, 0.4403, 0.4853]],

         [[0.5432, 0.4117, 0.5450,  ..., 0.5384, 0.5586, 0.5316],
          [0.4928, 0.4686, 0.4491,  ..., 0.4656, 0.6281, 0.5744],
          [0.5548, 0.5376, 0.4126,  ..., 0.6688, 0.6189, 0.6090],
          [0.5301, 0.5961, 0.4467,  ..., 0.6403, 0.4193, 0.3735]],

         [[0.4868, 0.3668, 0.5690,  ..., 0.4660, 0.4988, 0.5854],
          [0.6509, 0.4645, 0.3788,  ..., 0.4030, 0.6471, 0.4523],
          [0.5108, 0.6125, 0.5503,  ..., 0.5225, 0.4487, 0.4110],
          [0.5087, 0.4582, 0.4925,  ..., 0.5411, 0.5804, 0.5171]],

         ...,

         [[0.6111, 0.4957, 0.6048,  ..., 0.3502, 0.4293, 0.5406],
          [0.4252, 0.4906, 0.5580,  ..., 0.4890, 0.4393, 0.4357],
          [0.4095, 0.3305, 0.4250,  ..., 0.4479, 0.5319, 0.4883],
          [0.5186, 0.4944, 0.3802,  ..., 0.4124, 0.4458, 0.4827]],

         [[0.5084, 0.5733, 0.5551,  ..., 0.3744, 0.4696, 0.4549],
          [0.5582, 0.4350, 0.3913,  ..., 0.4323, 0.3349, 0.5382],
          [0.4209, 0.5199, 0.3541,  ..., 0.4342, 0.4836, 0.6358],
          [0.5830, 0.4288, 0.4487,  ..., 0.5371, 0.4654, 0.4494]],

         [[0.4360, 0.4917, 0.4574,  ..., 0.4842, 0.6471, 0.4171],
          [0.4410, 0.5102, 0.4775,  ..., 0.4682, 0.4032, 0.4138],
          [0.5432, 0.5393, 0.6190,  ..., 0.5011, 0.3650, 0.4675],
          [0.4529, 0.4964, 0.3746,  ..., 0.5825, 0.5247, 0.4818]]],


        [[[0.5023, 0.5400, 0.6609,  ..., 0.5030, 0.4512, 0.3084],
          [0.5377, 0.5227, 0.6245,  ..., 0.3564, 0.5335, 0.6466],
          [0.5080, 0.5601, 0.4798,  ..., 0.5282, 0.6622, 0.4214],
          [0.5010, 0.4996, 0.3746,  ..., 0.5493, 0.5914, 0.5982]],

         [[0.5379, 0.5572, 0.3834,  ..., 0.4395, 0.4921, 0.5082],
          [0.4432, 0.3489, 0.3918,  ..., 0.4506, 0.3990, 0.5304],
          [0.5768, 0.5325, 0.6362,  ..., 0.5129, 0.5890, 0.5987],
          [0.5018, 0.3704, 0.4313,  ..., 0.4323, 0.5942, 0.5045]],

         [[0.3500, 0.4434, 0.3760,  ..., 0.4314, 0.5062, 0.6723],
          [0.5064, 0.5214, 0.3732,  ..., 0.5123, 0.3985, 0.4561],
          [0.4345, 0.4973, 0.5590,  ..., 0.5464, 0.6045, 0.6600],
          [0.4759, 0.5502, 0.2944,  ..., 0.4956, 0.4553, 0.5597]],

         ...,

         [[0.5084, 0.4570, 0.5539,  ..., 0.5356, 0.4628, 0.5021],
          [0.4636, 0.4861, 0.6281,  ..., 0.4204, 0.4596, 0.4489],
          [0.4476, 0.4236, 0.5590,  ..., 0.5660, 0.5180, 0.5286],
          [0.5260, 0.5048, 0.4078,  ..., 0.6155, 0.4449, 0.5076]],

         [[0.4510, 0.4674, 0.5298,  ..., 0.6029, 0.6417, 0.6095],
          [0.3953, 0.5785, 0.4975,  ..., 0.5438, 0.5781, 0.6511],
          [0.4801, 0.4878, 0.6600,  ..., 0.4986, 0.4221, 0.4845],
          [0.4715, 0.5194, 0.3904,  ..., 0.5315, 0.5152, 0.5506]],

         [[0.5023, 0.5400, 0.6609,  ..., 0.5030, 0.4512, 0.3084],
          [0.5377, 0.5227, 0.6245,  ..., 0.3564, 0.5335, 0.6466],
          [0.5080, 0.5601, 0.4798,  ..., 0.5282, 0.6622, 0.4214],
          [0.5010, 0.4996, 0.3746,  ..., 0.5493, 0.5914, 0.5982]]]],
       device='cuda:0')
tensor([[[[0.5067, 0.5048, 0.3294,  ..., 0.3469, 0.6077, 0.4887],
          [0.4651, 0.3757, 0.4484,  ..., 0.4239, 0.6183, 0.5631],
          [0.5293, 0.4812, 0.4639,  ..., 0.3234, 0.4685, 0.7178],
          [0.5221, 0.5397, 0.5525,  ..., 0.4523, 0.4383, 0.4793]],

         [[0.5472, 0.4097, 0.5530,  ..., 0.5324, 0.5566, 0.5256],
          [0.4968, 0.4666, 0.4571,  ..., 0.4596, 0.6261, 0.5684],
          [0.5588, 0.5356, 0.4206,  ..., 0.6628, 0.6169, 0.6030],
          [0.5341, 0.5941, 0.4547,  ..., 0.6343, 0.4173, 0.3675]],

         [[0.4908, 0.3648, 0.5770,  ..., 0.4600, 0.4968, 0.5794],
          [0.6549, 0.4625, 0.3868,  ..., 0.3970, 0.6451, 0.4463],
          [0.5148, 0.6105, 0.5583,  ..., 0.5165, 0.4467, 0.4050],
          [0.5127, 0.4562, 0.5005,  ..., 0.5351, 0.5784, 0.5111]],

         ...,

         [[0.6151, 0.4937, 0.6128,  ..., 0.3442, 0.4273, 0.5346],
          [0.4292, 0.4886, 0.5660,  ..., 0.4830, 0.4373, 0.4297],
          [0.4135, 0.3285, 0.4330,  ..., 0.4419, 0.5299, 0.4823],
          [0.5226, 0.4924, 0.3882,  ..., 0.4064, 0.4438, 0.4767]],

         [[0.5124, 0.5713, 0.5631,  ..., 0.3684, 0.4676, 0.4489],
          [0.5622, 0.4330, 0.3993,  ..., 0.4263, 0.3329, 0.5322],
          [0.4249, 0.5179, 0.3621,  ..., 0.4282, 0.4816, 0.6298],
          [0.5870, 0.4268, 0.4567,  ..., 0.5311, 0.4634, 0.4434]],

         [[0.4400, 0.4897, 0.4654,  ..., 0.4782, 0.6451, 0.4111],
          [0.4450, 0.5082, 0.4855,  ..., 0.4622, 0.4012, 0.4078],
          [0.5472, 0.5373, 0.6270,  ..., 0.4951, 0.3630, 0.4615],
          [0.4569, 0.4944, 0.3826,  ..., 0.5765, 0.5227, 0.4758]]],


        [[[0.5063, 0.5380, 0.6689,  ..., 0.4970, 0.4492, 0.3024],
          [0.5417, 0.5207, 0.6325,  ..., 0.3504, 0.5315, 0.6406],
          [0.5120, 0.5581, 0.4878,  ..., 0.5222, 0.6602, 0.4154],
          [0.5050, 0.4976, 0.3826,  ..., 0.5433, 0.5894, 0.5922]],

         [[0.5419, 0.5552, 0.3914,  ..., 0.4335, 0.4901, 0.5022],
          [0.4472, 0.3469, 0.3998,  ..., 0.4446, 0.3970, 0.5244],
          [0.5808, 0.5305, 0.6442,  ..., 0.5069, 0.5870, 0.5927],
          [0.5058, 0.3684, 0.4393,  ..., 0.4263, 0.5922, 0.4985]],

         [[0.3540, 0.4414, 0.3840,  ..., 0.4254, 0.5042, 0.6663],
          [0.5104, 0.5194, 0.3812,  ..., 0.5063, 0.3965, 0.4501],
          [0.4385, 0.4953, 0.5670,  ..., 0.5404, 0.6025, 0.6540],
          [0.4799, 0.5482, 0.3024,  ..., 0.4896, 0.4533, 0.5537]],

         ...,

         [[0.5124, 0.4550, 0.5619,  ..., 0.5296, 0.4608, 0.4961],
          [0.4676, 0.4841, 0.6361,  ..., 0.4144, 0.4576, 0.4429],
          [0.4516, 0.4216, 0.5670,  ..., 0.5600, 0.5160, 0.5226],
          [0.5300, 0.5028, 0.4158,  ..., 0.6095, 0.4429, 0.5016]],

         [[0.4550, 0.4654, 0.5378,  ..., 0.5969, 0.6397, 0.6035],
          [0.3993, 0.5765, 0.5055,  ..., 0.5378, 0.5761, 0.6451],
          [0.4841, 0.4858, 0.6680,  ..., 0.4926, 0.4201, 0.4785],
          [0.4755, 0.5174, 0.3984,  ..., 0.5255, 0.5132, 0.5446]],

         [[0.5063, 0.5380, 0.6689,  ..., 0.4970, 0.4492, 0.3024],
          [0.5417, 0.5207, 0.6325,  ..., 0.3504, 0.5315, 0.6406],
          [0.5120, 0.5581, 0.4878,  ..., 0.5222, 0.6602, 0.4154],
          [0.5050, 0.4976, 0.3826,  ..., 0.5433, 0.5894, 0.5922]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0040,  0.0020, -0.0080,  0.0020, -0.0040, -0.0020,  0.0080,  0.0060,
         0.0020,  0.0060], device='cuda:0')
selected experts tensor([1566, 1538, 1685, 1632, 1591, 1817, 1663, 1629, 1606, 1657],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.2906, 0.5461, 0.5090,  ..., 0.4190, 0.6219, 0.4988],
          [0.4044, 0.4382, 0.6347,  ..., 0.6134, 0.5305, 0.5989],
          [0.5013, 0.4624, 0.6338,  ..., 0.5309, 0.5796, 0.4664],
          [0.4937, 0.5413, 0.6428,  ..., 0.4114, 0.4670, 0.4587]],

         [[0.3983, 0.4708, 0.4454,  ..., 0.5063, 0.4298, 0.4439],
          [0.4286, 0.3972, 0.5758,  ..., 0.4171, 0.5328, 0.5553],
          [0.4978, 0.3491, 0.3968,  ..., 0.4300, 0.5114, 0.4307],
          [0.5307, 0.5011, 0.6536,  ..., 0.4573, 0.7442, 0.5335]],

         [[0.6195, 0.5835, 0.3526,  ..., 0.4940, 0.4522, 0.4582],
          [0.6529, 0.6027, 0.5515,  ..., 0.6185, 0.6000, 0.5499],
          [0.6131, 0.6055, 0.3880,  ..., 0.5114, 0.4800, 0.4288],
          [0.3466, 0.5000, 0.3724,  ..., 0.3446, 0.4071, 0.3578]],

         ...,

         [[0.5698, 0.5343, 0.5227,  ..., 0.5519, 0.3991, 0.5632],
          [0.4781, 0.6285, 0.5047,  ..., 0.4645, 0.5976, 0.4681],
          [0.3610, 0.5754, 0.4911,  ..., 0.5296, 0.5447, 0.5359],
          [0.5239, 0.6027, 0.5938,  ..., 0.4636, 0.4749, 0.4538]],

         [[0.5702, 0.5398, 0.4813,  ..., 0.5716, 0.4611, 0.4905],
          [0.4457, 0.4745, 0.5225,  ..., 0.3393, 0.5844, 0.5101],
          [0.4843, 0.4698, 0.4256,  ..., 0.5546, 0.4781, 0.6138],
          [0.5025, 0.4348, 0.6112,  ..., 0.4266, 0.4858, 0.6318]],

         [[0.4784, 0.3907, 0.4816,  ..., 0.6078, 0.4232, 0.5567],
          [0.5174, 0.5032, 0.4118,  ..., 0.6527, 0.5311, 0.4967],
          [0.4747, 0.4721, 0.3922,  ..., 0.4607, 0.5207, 0.5133],
          [0.2359, 0.4886, 0.5602,  ..., 0.4490, 0.5471, 0.5177]]],


        [[[0.4866, 0.5194, 0.5327,  ..., 0.5725, 0.4937, 0.5499],
          [0.4186, 0.6185, 0.6135,  ..., 0.5452, 0.4517, 0.3911],
          [0.6323, 0.5138, 0.4194,  ..., 0.5797, 0.4746, 0.5451],
          [0.4101, 0.4370, 0.5324,  ..., 0.5126, 0.4442, 0.6208]],

         [[0.4924, 0.6013, 0.4665,  ..., 0.5360, 0.4725, 0.6138],
          [0.3820, 0.5668, 0.5062,  ..., 0.3289, 0.4127, 0.3489],
          [0.3511, 0.4266, 0.4687,  ..., 0.5261, 0.4265, 0.4009],
          [0.4831, 0.6078, 0.5815,  ..., 0.5740, 0.6737, 0.3925]],

         [[0.4663, 0.4587, 0.5069,  ..., 0.6384, 0.4869, 0.5259],
          [0.4325, 0.5943, 0.4913,  ..., 0.6051, 0.4104, 0.4531],
          [0.4561, 0.4345, 0.5437,  ..., 0.5678, 0.4614, 0.5046],
          [0.5409, 0.6692, 0.5312,  ..., 0.6420, 0.6685, 0.4302]],

         ...,

         [[0.5580, 0.6203, 0.3386,  ..., 0.6139, 0.4498, 0.4060],
          [0.4990, 0.6456, 0.5150,  ..., 0.5256, 0.4236, 0.4288],
          [0.4196, 0.5985, 0.5777,  ..., 0.5006, 0.4892, 0.4560],
          [0.5874, 0.4951, 0.4743,  ..., 0.5194, 0.5175, 0.5270]],

         [[0.3239, 0.6570, 0.4906,  ..., 0.4541, 0.5483, 0.7467],
          [0.5343, 0.4190, 0.3829,  ..., 0.5873, 0.6061, 0.5227],
          [0.5124, 0.5659, 0.5295,  ..., 0.4786, 0.4694, 0.5610],
          [0.4624, 0.4848, 0.5568,  ..., 0.6032, 0.4682, 0.4940]],

         [[0.4866, 0.5194, 0.5327,  ..., 0.5725, 0.4937, 0.5499],
          [0.4186, 0.6185, 0.6135,  ..., 0.5452, 0.4517, 0.3911],
          [0.6323, 0.5138, 0.4194,  ..., 0.5797, 0.4746, 0.5451],
          [0.4101, 0.4370, 0.5324,  ..., 0.5126, 0.4442, 0.6208]]]],
       device='cuda:0')
tensor([[[[0.2926, 0.5501, 0.5050,  ..., 0.4230, 0.6179, 0.4968],
          [0.4064, 0.4422, 0.6307,  ..., 0.6174, 0.5265, 0.5969],
          [0.5033, 0.4664, 0.6298,  ..., 0.5349, 0.5756, 0.4644],
          [0.4957, 0.5453, 0.6388,  ..., 0.4154, 0.4630, 0.4567]],

         [[0.4003, 0.4748, 0.4414,  ..., 0.5103, 0.4258, 0.4419],
          [0.4306, 0.4012, 0.5718,  ..., 0.4211, 0.5288, 0.5533],
          [0.4998, 0.3531, 0.3928,  ..., 0.4340, 0.5074, 0.4287],
          [0.5327, 0.5051, 0.6496,  ..., 0.4613, 0.7402, 0.5315]],

         [[0.6215, 0.5875, 0.3486,  ..., 0.4980, 0.4482, 0.4562],
          [0.6549, 0.6067, 0.5475,  ..., 0.6225, 0.5960, 0.5479],
          [0.6151, 0.6095, 0.3840,  ..., 0.5154, 0.4760, 0.4268],
          [0.3486, 0.5040, 0.3684,  ..., 0.3486, 0.4031, 0.3558]],

         ...,

         [[0.5718, 0.5383, 0.5187,  ..., 0.5559, 0.3951, 0.5612],
          [0.4801, 0.6325, 0.5007,  ..., 0.4685, 0.5936, 0.4661],
          [0.3630, 0.5794, 0.4871,  ..., 0.5336, 0.5407, 0.5339],
          [0.5259, 0.6067, 0.5898,  ..., 0.4676, 0.4709, 0.4518]],

         [[0.5722, 0.5438, 0.4773,  ..., 0.5756, 0.4571, 0.4885],
          [0.4477, 0.4785, 0.5185,  ..., 0.3433, 0.5804, 0.5081],
          [0.4863, 0.4738, 0.4216,  ..., 0.5586, 0.4741, 0.6118],
          [0.5045, 0.4388, 0.6072,  ..., 0.4306, 0.4818, 0.6298]],

         [[0.4804, 0.3947, 0.4776,  ..., 0.6118, 0.4192, 0.5547],
          [0.5194, 0.5072, 0.4078,  ..., 0.6567, 0.5271, 0.4947],
          [0.4767, 0.4761, 0.3882,  ..., 0.4647, 0.5167, 0.5113],
          [0.2379, 0.4926, 0.5562,  ..., 0.4530, 0.5431, 0.5157]]],


        [[[0.4886, 0.5234, 0.5287,  ..., 0.5765, 0.4897, 0.5479],
          [0.4206, 0.6225, 0.6095,  ..., 0.5492, 0.4477, 0.3891],
          [0.6343, 0.5178, 0.4154,  ..., 0.5837, 0.4706, 0.5431],
          [0.4121, 0.4410, 0.5284,  ..., 0.5166, 0.4402, 0.6188]],

         [[0.4944, 0.6053, 0.4625,  ..., 0.5400, 0.4685, 0.6118],
          [0.3840, 0.5708, 0.5022,  ..., 0.3329, 0.4087, 0.3469],
          [0.3531, 0.4306, 0.4647,  ..., 0.5301, 0.4225, 0.3989],
          [0.4851, 0.6118, 0.5775,  ..., 0.5780, 0.6697, 0.3905]],

         [[0.4683, 0.4627, 0.5029,  ..., 0.6424, 0.4829, 0.5239],
          [0.4345, 0.5983, 0.4873,  ..., 0.6091, 0.4064, 0.4511],
          [0.4581, 0.4385, 0.5397,  ..., 0.5718, 0.4574, 0.5026],
          [0.5429, 0.6732, 0.5272,  ..., 0.6460, 0.6645, 0.4282]],

         ...,

         [[0.5600, 0.6243, 0.3346,  ..., 0.6179, 0.4458, 0.4040],
          [0.5010, 0.6496, 0.5110,  ..., 0.5296, 0.4196, 0.4268],
          [0.4216, 0.6025, 0.5737,  ..., 0.5046, 0.4852, 0.4540],
          [0.5894, 0.4991, 0.4703,  ..., 0.5234, 0.5135, 0.5250]],

         [[0.3259, 0.6610, 0.4866,  ..., 0.4581, 0.5443, 0.7447],
          [0.5363, 0.4230, 0.3789,  ..., 0.5913, 0.6021, 0.5207],
          [0.5144, 0.5699, 0.5255,  ..., 0.4826, 0.4654, 0.5590],
          [0.4644, 0.4888, 0.5528,  ..., 0.6072, 0.4642, 0.4920]],

         [[0.4886, 0.5234, 0.5287,  ..., 0.5765, 0.4897, 0.5479],
          [0.4206, 0.6225, 0.6095,  ..., 0.5492, 0.4477, 0.3891],
          [0.6343, 0.5178, 0.4154,  ..., 0.5837, 0.4706, 0.5431],
          [0.4121, 0.4410, 0.5284,  ..., 0.5166, 0.4402, 0.6188]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0020, -0.0040,  0.0040,  0.0080, -0.0020,  0.0060, -0.0120, -0.0040,
         0.0040,  0.0020], device='cuda:0')
selected experts tensor([1630, 1671, 1684, 1536, 1571, 1567, 1691, 1745, 1597, 1692],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4887, 0.6131, 0.4418,  ..., 0.4267, 0.4798, 0.5882],
          [0.4083, 0.4848, 0.5008,  ..., 0.6070, 0.6036, 0.4690],
          [0.4949, 0.4842, 0.5202,  ..., 0.4171, 0.5264, 0.4223],
          [0.7474, 0.4289, 0.4972,  ..., 0.5457, 0.4039, 0.5095]],

         [[0.5406, 0.4568, 0.3973,  ..., 0.4267, 0.5223, 0.6213],
          [0.4307, 0.6294, 0.3421,  ..., 0.6545, 0.6765, 0.5920],
          [0.4796, 0.5202, 0.6510,  ..., 0.4563, 0.2773, 0.4656],
          [0.6719, 0.4124, 0.4830,  ..., 0.5077, 0.4174, 0.4504]],

         [[0.4370, 0.5473, 0.4081,  ..., 0.5060, 0.4781, 0.6502],
          [0.4389, 0.4681, 0.5136,  ..., 0.5250, 0.6748, 0.4736],
          [0.5888, 0.3061, 0.5241,  ..., 0.4711, 0.6027, 0.5123],
          [0.7104, 0.4739, 0.4580,  ..., 0.5017, 0.4849, 0.4658]],

         ...,

         [[0.5874, 0.4538, 0.4889,  ..., 0.4507, 0.5116, 0.4547],
          [0.4370, 0.5754, 0.6231,  ..., 0.5802, 0.4698, 0.4944],
          [0.4925, 0.5187, 0.3819,  ..., 0.5302, 0.5768, 0.4806],
          [0.4135, 0.4814, 0.4339,  ..., 0.6331, 0.4766, 0.5939]],

         [[0.5045, 0.6384, 0.5745,  ..., 0.6231, 0.4720, 0.5465],
          [0.3983, 0.5161, 0.6693,  ..., 0.4688, 0.4861, 0.5243],
          [0.4830, 0.4517, 0.3681,  ..., 0.4771, 0.4554, 0.4308],
          [0.3495, 0.5255, 0.3940,  ..., 0.5134, 0.5599, 0.5033]],

         [[0.5314, 0.5492, 0.4715,  ..., 0.5132, 0.4444, 0.5229],
          [0.4561, 0.5347, 0.4754,  ..., 0.4585, 0.4737, 0.5765],
          [0.5690, 0.4047, 0.5778,  ..., 0.5160, 0.5228, 0.5136],
          [0.4952, 0.3286, 0.4934,  ..., 0.4305, 0.4320, 0.4323]]],


        [[[0.4566, 0.5872, 0.3945,  ..., 0.5304, 0.4800, 0.5968],
          [0.3603, 0.5277, 0.4738,  ..., 0.6545, 0.5865, 0.4308],
          [0.4894, 0.4551, 0.4916,  ..., 0.3386, 0.5682, 0.4721],
          [0.5652, 0.3967, 0.4794,  ..., 0.4739, 0.5464, 0.4366]],

         [[0.4718, 0.5988, 0.4850,  ..., 0.3546, 0.4482, 0.5576],
          [0.4363, 0.5778, 0.4614,  ..., 0.5721, 0.4183, 0.5201],
          [0.6289, 0.6195, 0.6088,  ..., 0.5068, 0.4684, 0.4471],
          [0.6061, 0.3374, 0.5101,  ..., 0.4592, 0.4406, 0.4695]],

         [[0.5431, 0.5049, 0.4553,  ..., 0.4575, 0.5091, 0.4878],
          [0.3649, 0.4968, 0.3412,  ..., 0.6545, 0.6276, 0.4687],
          [0.5017, 0.4234, 0.5428,  ..., 0.4875, 0.3398, 0.5346],
          [0.5346, 0.5631, 0.5350,  ..., 0.4995, 0.5177, 0.4877]],

         ...,

         [[0.4520, 0.5064, 0.4224,  ..., 0.5345, 0.4595, 0.5094],
          [0.5353, 0.5456, 0.5510,  ..., 0.6084, 0.6686, 0.4634],
          [0.4283, 0.3600, 0.4781,  ..., 0.5314, 0.3668, 0.6025],
          [0.4604, 0.4669, 0.3778,  ..., 0.5182, 0.4554, 0.5332]],

         [[0.3932, 0.4534, 0.3412,  ..., 0.3430, 0.5080, 0.4876],
          [0.5637, 0.5979, 0.5821,  ..., 0.5503, 0.5631, 0.4219],
          [0.4540, 0.4802, 0.3847,  ..., 0.5621, 0.3441, 0.4361],
          [0.5038, 0.5456, 0.5186,  ..., 0.3281, 0.4701, 0.4485]],

         [[0.5126, 0.5013, 0.3778,  ..., 0.4634, 0.4682, 0.6583],
          [0.3708, 0.5759, 0.6492,  ..., 0.5442, 0.5383, 0.5105],
          [0.4734, 0.4280, 0.6000,  ..., 0.4286, 0.5035, 0.6034],
          [0.3621, 0.4483, 0.2977,  ..., 0.4527, 0.4934, 0.3157]]]],
       device='cuda:0')
tensor([[[[0.4977, 0.6261, 0.4448,  ..., 0.4297, 0.4608, 0.5732],
          [0.4173, 0.4978, 0.5038,  ..., 0.6100, 0.5846, 0.4540],
          [0.5039, 0.4972, 0.5232,  ..., 0.4201, 0.5074, 0.4073],
          [0.7564, 0.4419, 0.5002,  ..., 0.5487, 0.3849, 0.4945]],

         [[0.5496, 0.4698, 0.4003,  ..., 0.4297, 0.5033, 0.6063],
          [0.4397, 0.6424, 0.3451,  ..., 0.6575, 0.6575, 0.5770],
          [0.4886, 0.5332, 0.6540,  ..., 0.4593, 0.2583, 0.4506],
          [0.6809, 0.4254, 0.4860,  ..., 0.5107, 0.3984, 0.4354]],

         [[0.4460, 0.5603, 0.4111,  ..., 0.5090, 0.4591, 0.6352],
          [0.4479, 0.4811, 0.5166,  ..., 0.5280, 0.6558, 0.4586],
          [0.5978, 0.3191, 0.5271,  ..., 0.4741, 0.5837, 0.4973],
          [0.7194, 0.4869, 0.4610,  ..., 0.5047, 0.4659, 0.4508]],

         ...,

         [[0.5964, 0.4668, 0.4919,  ..., 0.4537, 0.4926, 0.4397],
          [0.4460, 0.5884, 0.6261,  ..., 0.5832, 0.4508, 0.4794],
          [0.5015, 0.5317, 0.3849,  ..., 0.5332, 0.5578, 0.4656],
          [0.4225, 0.4944, 0.4369,  ..., 0.6361, 0.4576, 0.5789]],

         [[0.5135, 0.6514, 0.5775,  ..., 0.6261, 0.4530, 0.5315],
          [0.4073, 0.5291, 0.6723,  ..., 0.4718, 0.4671, 0.5093],
          [0.4920, 0.4647, 0.3711,  ..., 0.4801, 0.4364, 0.4158],
          [0.3585, 0.5385, 0.3970,  ..., 0.5164, 0.5409, 0.4883]],

         [[0.5404, 0.5622, 0.4745,  ..., 0.5162, 0.4254, 0.5079],
          [0.4651, 0.5477, 0.4784,  ..., 0.4615, 0.4547, 0.5615],
          [0.5780, 0.4177, 0.5808,  ..., 0.5190, 0.5038, 0.4986],
          [0.5042, 0.3416, 0.4964,  ..., 0.4335, 0.4130, 0.4173]]],


        [[[0.4656, 0.6002, 0.3975,  ..., 0.5334, 0.4610, 0.5818],
          [0.3693, 0.5407, 0.4768,  ..., 0.6575, 0.5675, 0.4158],
          [0.4984, 0.4681, 0.4946,  ..., 0.3416, 0.5492, 0.4571],
          [0.5742, 0.4097, 0.4824,  ..., 0.4769, 0.5274, 0.4216]],

         [[0.4808, 0.6118, 0.4880,  ..., 0.3576, 0.4292, 0.5426],
          [0.4453, 0.5908, 0.4644,  ..., 0.5751, 0.3993, 0.5051],
          [0.6379, 0.6325, 0.6118,  ..., 0.5098, 0.4494, 0.4321],
          [0.6151, 0.3504, 0.5131,  ..., 0.4622, 0.4216, 0.4545]],

         [[0.5521, 0.5179, 0.4583,  ..., 0.4605, 0.4901, 0.4728],
          [0.3739, 0.5098, 0.3442,  ..., 0.6575, 0.6086, 0.4537],
          [0.5107, 0.4364, 0.5458,  ..., 0.4905, 0.3208, 0.5196],
          [0.5436, 0.5761, 0.5380,  ..., 0.5025, 0.4987, 0.4727]],

         ...,

         [[0.4610, 0.5194, 0.4254,  ..., 0.5375, 0.4405, 0.4944],
          [0.5443, 0.5586, 0.5540,  ..., 0.6114, 0.6496, 0.4484],
          [0.4373, 0.3730, 0.4811,  ..., 0.5344, 0.3478, 0.5875],
          [0.4694, 0.4799, 0.3808,  ..., 0.5212, 0.4364, 0.5182]],

         [[0.4022, 0.4664, 0.3442,  ..., 0.3460, 0.4890, 0.4726],
          [0.5727, 0.6109, 0.5851,  ..., 0.5533, 0.5441, 0.4069],
          [0.4630, 0.4932, 0.3877,  ..., 0.5651, 0.3251, 0.4211],
          [0.5128, 0.5586, 0.5216,  ..., 0.3311, 0.4511, 0.4335]],

         [[0.5216, 0.5143, 0.3808,  ..., 0.4664, 0.4492, 0.6433],
          [0.3798, 0.5889, 0.6522,  ..., 0.5472, 0.5193, 0.4955],
          [0.4824, 0.4410, 0.6030,  ..., 0.4316, 0.4845, 0.5884],
          [0.3711, 0.4613, 0.3007,  ..., 0.4557, 0.4744, 0.3007]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([-0.0090, -0.0130, -0.0030, -0.0050, -0.0150,  0.0230, -0.0250, -0.0030,
         0.0190,  0.0150], device='cuda:0')
selected experts tensor([1478, 1346, 1308, 1732, 1518, 1859, 2292, 1418, 1833, 1600],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5719, 0.5685, 0.4665,  ..., 0.3899, 0.4944, 0.5172],
          [0.5990, 0.4858, 0.5753,  ..., 0.6088, 0.5722, 0.4652],
          [0.5524, 0.3887, 0.4629,  ..., 0.4359, 0.5428, 0.6107],
          [0.5325, 0.5979, 0.6099,  ..., 0.5188, 0.4658, 0.5004]],

         [[0.6167, 0.4890, 0.5498,  ..., 0.3310, 0.4378, 0.5417],
          [0.5415, 0.3818, 0.5619,  ..., 0.5189, 0.4282, 0.4713],
          [0.4829, 0.3712, 0.4020,  ..., 0.5350, 0.4547, 0.5725],
          [0.5253, 0.5225, 0.4733,  ..., 0.4759, 0.5445, 0.4197]],

         [[0.4694, 0.4960, 0.3536,  ..., 0.3830, 0.4617, 0.3561],
          [0.4298, 0.4806, 0.6061,  ..., 0.4402, 0.3979, 0.4455],
          [0.4836, 0.5010, 0.4142,  ..., 0.4691, 0.6891, 0.5095],
          [0.4650, 0.6110, 0.5418,  ..., 0.4212, 0.5626, 0.3794]],

         ...,

         [[0.5030, 0.5010, 0.5050,  ..., 0.5110, 0.4990, 0.5110],
          [0.5030, 0.5010, 0.5050,  ..., 0.5110, 0.4990, 0.5110],
          [0.5030, 0.5010, 0.5050,  ..., 0.5110, 0.4990, 0.5110],
          [0.5030, 0.5010, 0.5050,  ..., 0.5110, 0.4990, 0.5110]],

         [[0.5030, 0.5010, 0.5050,  ..., 0.5110, 0.4990, 0.5110],
          [0.5030, 0.5010, 0.5050,  ..., 0.5110, 0.4990, 0.5110],
          [0.5030, 0.5010, 0.5050,  ..., 0.5110, 0.4990, 0.5110],
          [0.5030, 0.5010, 0.5050,  ..., 0.5110, 0.4990, 0.5110]],

         [[0.5030, 0.5010, 0.5050,  ..., 0.5110, 0.4990, 0.5110],
          [0.5030, 0.5010, 0.5050,  ..., 0.5110, 0.4990, 0.5110],
          [0.5030, 0.5010, 0.5050,  ..., 0.5110, 0.4990, 0.5110],
          [0.5030, 0.5010, 0.5050,  ..., 0.5110, 0.4990, 0.5110]]],


        [[[0.5139, 0.5016, 0.4886,  ..., 0.3579, 0.5808, 0.5231],
          [0.4442, 0.4345, 0.5060,  ..., 0.4965, 0.5343, 0.5429],
          [0.5243, 0.4659, 0.4076,  ..., 0.5004, 0.5798, 0.4580],
          [0.5320, 0.4273, 0.5515,  ..., 0.5140, 0.4696, 0.4221]],

         [[0.4840, 0.4221, 0.4665,  ..., 0.4539, 0.4474, 0.5556],
          [0.5225, 0.4335, 0.5083,  ..., 0.4976, 0.6405, 0.4889],
          [0.6013, 0.4331, 0.4718,  ..., 0.4532, 0.5397, 0.5548],
          [0.5933, 0.5651, 0.5374,  ..., 0.4354, 0.5242, 0.3830]],

         [[0.5724, 0.4630, 0.5498,  ..., 0.3659, 0.4311, 0.4999],
          [0.5284, 0.6759, 0.5496,  ..., 0.4592, 0.5498, 0.5425],
          [0.5329, 0.3966, 0.4505,  ..., 0.4936, 0.4812, 0.5970],
          [0.5531, 0.5979, 0.5179,  ..., 0.4364, 0.5075, 0.3885]],

         ...,

         [[0.5030, 0.5010, 0.5050,  ..., 0.5110, 0.4990, 0.5110],
          [0.5030, 0.5010, 0.5050,  ..., 0.5110, 0.4990, 0.5110],
          [0.5030, 0.5010, 0.5050,  ..., 0.5110, 0.4990, 0.5110],
          [0.5030, 0.5010, 0.5050,  ..., 0.5110, 0.4990, 0.5110]],

         [[0.5030, 0.5010, 0.5050,  ..., 0.5110, 0.4990, 0.5110],
          [0.5030, 0.5010, 0.5050,  ..., 0.5110, 0.4990, 0.5110],
          [0.5030, 0.5010, 0.5050,  ..., 0.5110, 0.4990, 0.5110],
          [0.5030, 0.5010, 0.5050,  ..., 0.5110, 0.4990, 0.5110]],

         [[0.5030, 0.5010, 0.5050,  ..., 0.5110, 0.4990, 0.5110],
          [0.5030, 0.5010, 0.5050,  ..., 0.5110, 0.4990, 0.5110],
          [0.5030, 0.5010, 0.5050,  ..., 0.5110, 0.4990, 0.5110],
          [0.5030, 0.5010, 0.5050,  ..., 0.5110, 0.4990, 0.5110]]]],
       device='cuda:0')
tensor([[[[0.5689, 0.5675, 0.4615,  ..., 0.3789, 0.4954, 0.5062],
          [0.5960, 0.4848, 0.5703,  ..., 0.5978, 0.5732, 0.4542],
          [0.5494, 0.3877, 0.4579,  ..., 0.4249, 0.5438, 0.5997],
          [0.5295, 0.5969, 0.6049,  ..., 0.5078, 0.4668, 0.4894]],

         [[0.6137, 0.4880, 0.5448,  ..., 0.3200, 0.4388, 0.5307],
          [0.5385, 0.3808, 0.5569,  ..., 0.5079, 0.4292, 0.4603],
          [0.4799, 0.3702, 0.3970,  ..., 0.5240, 0.4557, 0.5615],
          [0.5223, 0.5215, 0.4683,  ..., 0.4649, 0.5455, 0.4087]],

         [[0.4664, 0.4950, 0.3486,  ..., 0.3720, 0.4627, 0.3451],
          [0.4268, 0.4796, 0.6011,  ..., 0.4292, 0.3989, 0.4345],
          [0.4806, 0.5000, 0.4092,  ..., 0.4581, 0.6901, 0.4985],
          [0.4620, 0.6100, 0.5368,  ..., 0.4102, 0.5636, 0.3684]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5109, 0.5006, 0.4836,  ..., 0.3469, 0.5818, 0.5121],
          [0.4412, 0.4335, 0.5010,  ..., 0.4855, 0.5353, 0.5319],
          [0.5213, 0.4649, 0.4026,  ..., 0.4894, 0.5808, 0.4470],
          [0.5290, 0.4263, 0.5465,  ..., 0.5030, 0.4706, 0.4111]],

         [[0.4810, 0.4211, 0.4615,  ..., 0.4429, 0.4484, 0.5446],
          [0.5195, 0.4325, 0.5033,  ..., 0.4866, 0.6415, 0.4779],
          [0.5983, 0.4321, 0.4668,  ..., 0.4422, 0.5407, 0.5438],
          [0.5903, 0.5641, 0.5324,  ..., 0.4244, 0.5252, 0.3720]],

         [[0.5694, 0.4620, 0.5448,  ..., 0.3549, 0.4321, 0.4889],
          [0.5254, 0.6749, 0.5446,  ..., 0.4482, 0.5508, 0.5315],
          [0.5299, 0.3956, 0.4455,  ..., 0.4826, 0.4822, 0.5860],
          [0.5501, 0.5969, 0.5129,  ..., 0.4254, 0.5085, 0.3775]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0030,  0.0010,  0.0050,  0.0090, -0.0110,  0.0130,  0.0110,  0.0110,
        -0.0010,  0.0110], device='cuda:0')
selected experts tensor([1745, 2109, 2001, 1553, 1676, 1281, 2021,  658, 1967, 1373],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1630., 1671., 1684., 1536., 1571., 1567., 1691., 1745., 1597., 1692.],
        [1745., 2109., 2001., 1553., 1676., 1281., 2021.,  658., 1967., 1373.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5859, 0.4650, 0.4865,  ..., 0.5624, 0.5534, 0.5925],
          [0.5731, 0.4497, 0.3540,  ..., 0.5603, 0.5776, 0.5114],
          [0.4524, 0.5118, 0.5718,  ..., 0.5353, 0.4217, 0.5064],
          [0.4091, 0.4737, 0.6441,  ..., 0.4228, 0.5451, 0.4532]],

         [[0.5033, 0.5417, 0.6616,  ..., 0.5042, 0.4538, 0.3074],
          [0.5387, 0.5237, 0.6235,  ..., 0.3574, 0.5347, 0.6456],
          [0.5090, 0.5611, 0.4788,  ..., 0.5293, 0.6632, 0.4208],
          [0.5021, 0.5007, 0.3736,  ..., 0.5501, 0.5924, 0.5972]],

         [[0.4763, 0.4019, 0.4211,  ..., 0.5508, 0.6291, 0.4289],
          [0.3801, 0.4860, 0.5503,  ..., 0.4949, 0.5729, 0.6247],
          [0.4143, 0.5724, 0.3810,  ..., 0.4578, 0.5307, 0.4902],
          [0.4384, 0.5218, 0.4671,  ..., 0.5173, 0.3525, 0.4534]],

         ...,

         [[0.5014, 0.5594, 0.4121,  ..., 0.6086, 0.5144, 0.3492],
          [0.4638, 0.4389, 0.5537,  ..., 0.5562, 0.6093, 0.4626],
          [0.4020, 0.4170, 0.5195,  ..., 0.4880, 0.4820, 0.4948],
          [0.5064, 0.4375, 0.4240,  ..., 0.6123, 0.4524, 0.5796]],

         [[0.3136, 0.4802, 0.5244,  ..., 0.6404, 0.4630, 0.4385],
          [0.4899, 0.5124, 0.4447,  ..., 0.5792, 0.4179, 0.5834],
          [0.3935, 0.5509, 0.4583,  ..., 0.3152, 0.6864, 0.4733],
          [0.3931, 0.5442, 0.6102,  ..., 0.6512, 0.5062, 0.4943]],

         [[0.5597, 0.5117, 0.5813,  ..., 0.4900, 0.4108, 0.4941],
          [0.5478, 0.5451, 0.5537,  ..., 0.4324, 0.3230, 0.5320],
          [0.4879, 0.5924, 0.5199,  ..., 0.3873, 0.4447, 0.5915],
          [0.4937, 0.4089, 0.6870,  ..., 0.3565, 0.6097, 0.6201]]],


        [[[0.4095, 0.3070, 0.4197,  ..., 0.5275, 0.6605, 0.4090],
          [0.5654, 0.4739, 0.5057,  ..., 0.5916, 0.5326, 0.3918],
          [0.7092, 0.5324, 0.6485,  ..., 0.4157, 0.5192, 0.4580],
          [0.5764, 0.3642, 0.5194,  ..., 0.5460, 0.4633, 0.6028]],

         [[0.4186, 0.5241, 0.5445,  ..., 0.4410, 0.5642, 0.5546],
          [0.5350, 0.5928, 0.4615,  ..., 0.4620, 0.4279, 0.6057],
          [0.5774, 0.5534, 0.6325,  ..., 0.4983, 0.4094, 0.6275],
          [0.4767, 0.5374, 0.5949,  ..., 0.6689, 0.3633, 0.3789]],

         [[0.5281, 0.5282, 0.3282,  ..., 0.4658, 0.7031, 0.2633],
          [0.4476, 0.4698, 0.4598,  ..., 0.3984, 0.4790, 0.4423],
          [0.4747, 0.5430, 0.4035,  ..., 0.5198, 0.5872, 0.3554],
          [0.6005, 0.4645, 0.3899,  ..., 0.4007, 0.5890, 0.5268]],

         ...,

         [[0.6633, 0.4589, 0.4977,  ..., 0.6044, 0.5070, 0.4500],
          [0.3663, 0.5225, 0.5604,  ..., 0.4129, 0.5781, 0.4242],
          [0.4394, 0.5618, 0.4320,  ..., 0.4797, 0.4798, 0.4823],
          [0.6098, 0.4877, 0.3195,  ..., 0.5231, 0.4217, 0.4185]],

         [[0.5120, 0.5733, 0.5613,  ..., 0.5047, 0.4969, 0.4840],
          [0.4604, 0.6102, 0.4438,  ..., 0.4026, 0.4854, 0.4342],
          [0.4462, 0.5062, 0.4102,  ..., 0.4455, 0.5924, 0.4960],
          [0.4377, 0.4103, 0.5295,  ..., 0.5687, 0.6300, 0.3987]],

         [[0.4888, 0.6337, 0.4626,  ..., 0.6020, 0.4652, 0.3927],
          [0.5769, 0.3879, 0.4221,  ..., 0.6503, 0.4965, 0.5972],
          [0.5321, 0.4806, 0.4207,  ..., 0.4386, 0.5257, 0.4896],
          [0.3537, 0.5197, 0.5190,  ..., 0.4376, 0.5153, 0.5085]]]],
       device='cuda:0')
tensor([[[[0.5889, 0.4620, 0.4955,  ..., 0.5554, 0.5504, 0.5875],
          [0.5761, 0.4467, 0.3630,  ..., 0.5533, 0.5746, 0.5064],
          [0.4554, 0.5088, 0.5808,  ..., 0.5283, 0.4187, 0.5014],
          [0.4121, 0.4707, 0.6531,  ..., 0.4158, 0.5421, 0.4482]],

         [[0.5063, 0.5387, 0.6706,  ..., 0.4972, 0.4508, 0.3024],
          [0.5417, 0.5207, 0.6325,  ..., 0.3504, 0.5317, 0.6406],
          [0.5120, 0.5581, 0.4878,  ..., 0.5223, 0.6602, 0.4158],
          [0.5051, 0.4977, 0.3826,  ..., 0.5431, 0.5894, 0.5922]],

         [[0.4793, 0.3989, 0.4301,  ..., 0.5438, 0.6261, 0.4239],
          [0.3831, 0.4830, 0.5593,  ..., 0.4879, 0.5699, 0.6197],
          [0.4173, 0.5694, 0.3900,  ..., 0.4508, 0.5277, 0.4852],
          [0.4414, 0.5188, 0.4761,  ..., 0.5103, 0.3495, 0.4484]],

         ...,

         [[0.5044, 0.5564, 0.4211,  ..., 0.6016, 0.5114, 0.3442],
          [0.4668, 0.4359, 0.5627,  ..., 0.5492, 0.6063, 0.4576],
          [0.4050, 0.4140, 0.5285,  ..., 0.4810, 0.4790, 0.4898],
          [0.5094, 0.4345, 0.4330,  ..., 0.6053, 0.4494, 0.5746]],

         [[0.3166, 0.4772, 0.5334,  ..., 0.6334, 0.4600, 0.4335],
          [0.4929, 0.5094, 0.4537,  ..., 0.5722, 0.4149, 0.5784],
          [0.3965, 0.5479, 0.4673,  ..., 0.3082, 0.6834, 0.4683],
          [0.3961, 0.5412, 0.6192,  ..., 0.6442, 0.5032, 0.4893]],

         [[0.5627, 0.5087, 0.5903,  ..., 0.4830, 0.4078, 0.4891],
          [0.5508, 0.5421, 0.5627,  ..., 0.4254, 0.3200, 0.5270],
          [0.4909, 0.5894, 0.5289,  ..., 0.3803, 0.4417, 0.5865],
          [0.4967, 0.4059, 0.6960,  ..., 0.3495, 0.6067, 0.6151]]],


        [[[0.4125, 0.3040, 0.4287,  ..., 0.5205, 0.6575, 0.4040],
          [0.5684, 0.4709, 0.5147,  ..., 0.5846, 0.5296, 0.3868],
          [0.7122, 0.5294, 0.6575,  ..., 0.4087, 0.5162, 0.4530],
          [0.5794, 0.3612, 0.5284,  ..., 0.5390, 0.4603, 0.5978]],

         [[0.4216, 0.5211, 0.5535,  ..., 0.4340, 0.5612, 0.5496],
          [0.5380, 0.5898, 0.4705,  ..., 0.4550, 0.4249, 0.6007],
          [0.5804, 0.5504, 0.6415,  ..., 0.4913, 0.4064, 0.6225],
          [0.4797, 0.5344, 0.6039,  ..., 0.6619, 0.3603, 0.3739]],

         [[0.5311, 0.5252, 0.3372,  ..., 0.4588, 0.7001, 0.2583],
          [0.4506, 0.4668, 0.4688,  ..., 0.3914, 0.4760, 0.4373],
          [0.4777, 0.5400, 0.4125,  ..., 0.5128, 0.5842, 0.3504],
          [0.6035, 0.4615, 0.3989,  ..., 0.3937, 0.5860, 0.5218]],

         ...,

         [[0.6663, 0.4559, 0.5067,  ..., 0.5974, 0.5040, 0.4450],
          [0.3693, 0.5195, 0.5694,  ..., 0.4059, 0.5751, 0.4192],
          [0.4424, 0.5588, 0.4410,  ..., 0.4727, 0.4768, 0.4773],
          [0.6128, 0.4847, 0.3285,  ..., 0.5161, 0.4187, 0.4135]],

         [[0.5150, 0.5703, 0.5703,  ..., 0.4977, 0.4939, 0.4790],
          [0.4634, 0.6072, 0.4528,  ..., 0.3956, 0.4824, 0.4292],
          [0.4492, 0.5032, 0.4192,  ..., 0.4385, 0.5894, 0.4910],
          [0.4407, 0.4073, 0.5385,  ..., 0.5617, 0.6270, 0.3937]],

         [[0.4918, 0.6307, 0.4716,  ..., 0.5950, 0.4622, 0.3877],
          [0.5799, 0.3849, 0.4311,  ..., 0.6433, 0.4935, 0.5922],
          [0.5351, 0.4776, 0.4297,  ..., 0.4316, 0.5227, 0.4846],
          [0.3567, 0.5167, 0.5280,  ..., 0.4306, 0.5123, 0.5035]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0030,  0.0030, -0.0090,  0.0030, -0.0030, -0.0030,  0.0070,  0.0070,
         0.0030,  0.0050], device='cuda:0')
selected experts tensor([1603, 1640, 1503, 1661, 1723, 1779, 1548, 1607, 1648, 1672],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.3485, 0.5507, 0.3874,  ..., 0.5881, 0.4749, 0.4973],
          [0.6053, 0.5792, 0.4882,  ..., 0.6013, 0.3370, 0.5637],
          [0.6132, 0.4415, 0.5553,  ..., 0.4810, 0.4754, 0.5407],
          [0.4569, 0.5801, 0.5364,  ..., 0.6275, 0.5575, 0.5088]],

         [[0.4872, 0.5171, 0.5329,  ..., 0.5720, 0.4947, 0.5492],
          [0.4201, 0.6175, 0.6121,  ..., 0.5444, 0.4527, 0.3901],
          [0.6333, 0.5129, 0.4184,  ..., 0.5787, 0.4755, 0.5441],
          [0.4111, 0.4362, 0.5315,  ..., 0.5115, 0.4452, 0.6198]],

         [[0.5341, 0.5379, 0.4279,  ..., 0.6784, 0.6634, 0.4535],
          [0.6432, 0.5308, 0.6898,  ..., 0.3580, 0.4539, 0.4975],
          [0.4229, 0.4271, 0.5466,  ..., 0.4514, 0.5401, 0.4441],
          [0.4917, 0.7080, 0.5548,  ..., 0.5408, 0.4062, 0.4499]],

         ...,

         [[0.3647, 0.5029, 0.4435,  ..., 0.5023, 0.4888, 0.3167],
          [0.4983, 0.5167, 0.5212,  ..., 0.4771, 0.3115, 0.3613],
          [0.4922, 0.5545, 0.4630,  ..., 0.6257, 0.3049, 0.4606],
          [0.5266, 0.6239, 0.4652,  ..., 0.5126, 0.5313, 0.6920]],

         [[0.4106, 0.3535, 0.4643,  ..., 0.5858, 0.5953, 0.5651],
          [0.5921, 0.4653, 0.5522,  ..., 0.5254, 0.6918, 0.5889],
          [0.5419, 0.3598, 0.4681,  ..., 0.5277, 0.6339, 0.5468],
          [0.4746, 0.5362, 0.4056,  ..., 0.4594, 0.5882, 0.5951]],

         [[0.5253, 0.4738, 0.4485,  ..., 0.4439, 0.5459, 0.3339],
          [0.4515, 0.5417, 0.5183,  ..., 0.4621, 0.4728, 0.5304],
          [0.4306, 0.4689, 0.5357,  ..., 0.5145, 0.4517, 0.5861],
          [0.5665, 0.6064, 0.4952,  ..., 0.4911, 0.4081, 0.6031]]],


        [[[0.4887, 0.5220, 0.6055,  ..., 0.5005, 0.5991, 0.3749],
          [0.5324, 0.5110, 0.4478,  ..., 0.5123, 0.4986, 0.4957],
          [0.4954, 0.3607, 0.4524,  ..., 0.3375, 0.4692, 0.4154],
          [0.5813, 0.3508, 0.4560,  ..., 0.6893, 0.5391, 0.5804]],

         [[0.4680, 0.4597, 0.5326,  ..., 0.4432, 0.4999, 0.3730],
          [0.2603, 0.4766, 0.5017,  ..., 0.4810, 0.6302, 0.4758],
          [0.4713, 0.5223, 0.5990,  ..., 0.4988, 0.4753, 0.4393],
          [0.5717, 0.5787, 0.5349,  ..., 0.5277, 0.5006, 0.5608]],

         [[0.5822, 0.5350, 0.4355,  ..., 0.5107, 0.6206, 0.4355],
          [0.5712, 0.4052, 0.5800,  ..., 0.5002, 0.5346, 0.5231],
          [0.4344, 0.5267, 0.5724,  ..., 0.7040, 0.5972, 0.4928],
          [0.4021, 0.4973, 0.5371,  ..., 0.4000, 0.4001, 0.5610]],

         ...,

         [[0.5202, 0.6239, 0.3324,  ..., 0.4204, 0.4232, 0.6290],
          [0.4544, 0.3436, 0.4894,  ..., 0.5110, 0.4525, 0.4931],
          [0.5893, 0.4061, 0.3995,  ..., 0.4473, 0.5030, 0.4292],
          [0.3593, 0.6604, 0.3935,  ..., 0.3929, 0.6127, 0.5346]],

         [[0.4768, 0.4983, 0.4885,  ..., 0.5318, 0.5178, 0.5543],
          [0.5093, 0.6142, 0.4250,  ..., 0.6068, 0.4390, 0.6326],
          [0.5297, 0.4280, 0.5890,  ..., 0.5269, 0.5332, 0.2840],
          [0.4742, 0.7040, 0.5340,  ..., 0.5900, 0.4961, 0.4074]],

         [[0.4666, 0.5567, 0.3463,  ..., 0.4156, 0.6071, 0.4911],
          [0.4248, 0.4526, 0.6623,  ..., 0.4758, 0.5221, 0.6878],
          [0.4068, 0.5094, 0.3298,  ..., 0.5060, 0.3752, 0.5252],
          [0.5836, 0.5605, 0.3805,  ..., 0.5415, 0.4979, 0.5104]]]],
       device='cuda:0')
tensor([[[[0.3495, 0.5557, 0.3844,  ..., 0.5931, 0.4699, 0.4963],
          [0.6063, 0.5842, 0.4852,  ..., 0.6063, 0.3320, 0.5627],
          [0.6142, 0.4465, 0.5523,  ..., 0.4860, 0.4704, 0.5397],
          [0.4579, 0.5851, 0.5334,  ..., 0.6325, 0.5525, 0.5078]],

         [[0.4882, 0.5221, 0.5299,  ..., 0.5770, 0.4897, 0.5482],
          [0.4211, 0.6225, 0.6091,  ..., 0.5494, 0.4477, 0.3891],
          [0.6343, 0.5179, 0.4154,  ..., 0.5837, 0.4705, 0.5431],
          [0.4121, 0.4412, 0.5285,  ..., 0.5165, 0.4402, 0.6188]],

         [[0.5351, 0.5429, 0.4249,  ..., 0.6834, 0.6584, 0.4525],
          [0.6442, 0.5358, 0.6868,  ..., 0.3630, 0.4489, 0.4965],
          [0.4239, 0.4321, 0.5436,  ..., 0.4564, 0.5351, 0.4431],
          [0.4927, 0.7130, 0.5518,  ..., 0.5458, 0.4012, 0.4489]],

         ...,

         [[0.3657, 0.5079, 0.4405,  ..., 0.5073, 0.4838, 0.3157],
          [0.4993, 0.5217, 0.5182,  ..., 0.4821, 0.3065, 0.3603],
          [0.4932, 0.5595, 0.4600,  ..., 0.6307, 0.2999, 0.4596],
          [0.5276, 0.6289, 0.4622,  ..., 0.5176, 0.5263, 0.6910]],

         [[0.4116, 0.3585, 0.4613,  ..., 0.5908, 0.5903, 0.5641],
          [0.5931, 0.4703, 0.5492,  ..., 0.5304, 0.6868, 0.5879],
          [0.5429, 0.3648, 0.4651,  ..., 0.5327, 0.6289, 0.5458],
          [0.4756, 0.5412, 0.4026,  ..., 0.4644, 0.5832, 0.5941]],

         [[0.5263, 0.4788, 0.4455,  ..., 0.4489, 0.5409, 0.3329],
          [0.4525, 0.5467, 0.5153,  ..., 0.4671, 0.4678, 0.5294],
          [0.4316, 0.4739, 0.5327,  ..., 0.5195, 0.4467, 0.5851],
          [0.5675, 0.6114, 0.4922,  ..., 0.4961, 0.4031, 0.6021]]],


        [[[0.4897, 0.5270, 0.6025,  ..., 0.5055, 0.5941, 0.3739],
          [0.5334, 0.5160, 0.4448,  ..., 0.5173, 0.4936, 0.4947],
          [0.4964, 0.3657, 0.4494,  ..., 0.3425, 0.4642, 0.4144],
          [0.5823, 0.3558, 0.4530,  ..., 0.6943, 0.5341, 0.5794]],

         [[0.4690, 0.4647, 0.5296,  ..., 0.4482, 0.4949, 0.3720],
          [0.2613, 0.4816, 0.4987,  ..., 0.4860, 0.6252, 0.4748],
          [0.4723, 0.5273, 0.5960,  ..., 0.5038, 0.4703, 0.4383],
          [0.5727, 0.5837, 0.5319,  ..., 0.5327, 0.4956, 0.5598]],

         [[0.5832, 0.5400, 0.4325,  ..., 0.5157, 0.6156, 0.4345],
          [0.5722, 0.4102, 0.5770,  ..., 0.5052, 0.5296, 0.5221],
          [0.4354, 0.5317, 0.5694,  ..., 0.7090, 0.5922, 0.4918],
          [0.4031, 0.5023, 0.5341,  ..., 0.4050, 0.3951, 0.5600]],

         ...,

         [[0.5212, 0.6289, 0.3294,  ..., 0.4254, 0.4182, 0.6280],
          [0.4554, 0.3486, 0.4864,  ..., 0.5160, 0.4475, 0.4921],
          [0.5903, 0.4111, 0.3965,  ..., 0.4523, 0.4980, 0.4282],
          [0.3603, 0.6654, 0.3905,  ..., 0.3979, 0.6077, 0.5336]],

         [[0.4778, 0.5033, 0.4855,  ..., 0.5368, 0.5128, 0.5533],
          [0.5103, 0.6192, 0.4220,  ..., 0.6118, 0.4340, 0.6316],
          [0.5307, 0.4330, 0.5860,  ..., 0.5319, 0.5282, 0.2830],
          [0.4752, 0.7090, 0.5310,  ..., 0.5950, 0.4911, 0.4064]],

         [[0.4676, 0.5617, 0.3433,  ..., 0.4206, 0.6021, 0.4901],
          [0.4258, 0.4576, 0.6593,  ..., 0.4808, 0.5171, 0.6868],
          [0.4078, 0.5144, 0.3268,  ..., 0.5110, 0.3702, 0.5242],
          [0.5846, 0.5655, 0.3775,  ..., 0.5465, 0.4929, 0.5094]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0010, -0.0050,  0.0030,  0.0090, -0.0010,  0.0070, -0.0130, -0.0050,
         0.0050,  0.0010], device='cuda:0')
selected experts tensor([1744, 1665, 1562, 1620, 1754, 1478, 1708, 1540, 1754, 1559],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4523, 0.4883, 0.4747,  ..., 0.4641, 0.5429, 0.6190],
          [0.4574, 0.3752, 0.5626,  ..., 0.5175, 0.6929, 0.5178],
          [0.3257, 0.6008, 0.6359,  ..., 0.5151, 0.3405, 0.4452],
          [0.6565, 0.6385, 0.5640,  ..., 0.4998, 0.4573, 0.5030]],

         [[0.4102, 0.5386, 0.4675,  ..., 0.6350, 0.4376, 0.5770],
          [0.4178, 0.5473, 0.4843,  ..., 0.6413, 0.6069, 0.4023],
          [0.5149, 0.4415, 0.4937,  ..., 0.3719, 0.5144, 0.4009],
          [0.5123, 0.4273, 0.4754,  ..., 0.4765, 0.5093, 0.3986]],

         [[0.5818, 0.5774, 0.5075,  ..., 0.5360, 0.4744, 0.5878],
          [0.4750, 0.5844, 0.4020,  ..., 0.6323, 0.5302, 0.4625],
          [0.4250, 0.4969, 0.5207,  ..., 0.4296, 0.3992, 0.4200],
          [0.4720, 0.5152, 0.4971,  ..., 0.5382, 0.4001, 0.5233]],

         ...,

         [[0.5312, 0.3402, 0.4438,  ..., 0.3973, 0.5426, 0.5902],
          [0.4516, 0.5679, 0.5421,  ..., 0.4636, 0.5509, 0.5399],
          [0.5566, 0.5143, 0.4385,  ..., 0.3673, 0.5488, 0.4664],
          [0.4366, 0.4718, 0.5133,  ..., 0.5631, 0.4951, 0.5268]],

         [[0.5438, 0.5229, 0.5331,  ..., 0.5679, 0.5310, 0.5987],
          [0.4397, 0.5287, 0.6019,  ..., 0.5551, 0.4577, 0.5301],
          [0.4501, 0.3674, 0.6485,  ..., 0.3682, 0.5657, 0.4376],
          [0.5671, 0.4461, 0.4402,  ..., 0.6260, 0.5451, 0.5409]],

         [[0.5776, 0.5157, 0.4661,  ..., 0.5845, 0.5807, 0.5321],
          [0.5133, 0.4196, 0.5178,  ..., 0.5173, 0.6505, 0.5777],
          [0.6299, 0.4105, 0.5358,  ..., 0.4530, 0.5797, 0.3718],
          [0.4839, 0.4115, 0.6200,  ..., 0.5798, 0.5138, 0.4485]]],


        [[[0.4858, 0.3519, 0.4493,  ..., 0.3806, 0.5237, 0.5337],
          [0.5227, 0.4076, 0.5836,  ..., 0.4931, 0.4462, 0.4838],
          [0.6043, 0.3724, 0.6029,  ..., 0.5201, 0.4272, 0.4557],
          [0.5828, 0.5170, 0.5831,  ..., 0.5265, 0.5347, 0.5069]],

         [[0.6117, 0.5040, 0.5028,  ..., 0.4935, 0.5497, 0.5806],
          [0.4788, 0.5919, 0.5472,  ..., 0.5826, 0.5705, 0.5215],
          [0.5510, 0.4364, 0.5554,  ..., 0.5025, 0.5582, 0.4995],
          [0.4012, 0.4810, 0.4368,  ..., 0.4938, 0.4650, 0.4014]],

         [[0.6103, 0.5487, 0.6005,  ..., 0.6182, 0.4496, 0.6209],
          [0.3862, 0.6214, 0.5387,  ..., 0.4683, 0.5446, 0.4545],
          [0.5295, 0.5152, 0.5292,  ..., 0.5353, 0.4315, 0.3972],
          [0.5346, 0.5186, 0.6112,  ..., 0.6154, 0.6003, 0.4982]],

         ...,

         [[0.5084, 0.5202, 0.5242,  ..., 0.6386, 0.5917, 0.4819],
          [0.3686, 0.5093, 0.4588,  ..., 0.5688, 0.5512, 0.5269],
          [0.6154, 0.4091, 0.4382,  ..., 0.6413, 0.5635, 0.5317],
          [0.4808, 0.5915, 0.5930,  ..., 0.5688, 0.3684, 0.4833]],

         [[0.5409, 0.4321, 0.5479,  ..., 0.3922, 0.5686, 0.4700],
          [0.2919, 0.4570, 0.6948,  ..., 0.5864, 0.5778, 0.5410],
          [0.5530, 0.4280, 0.4679,  ..., 0.5172, 0.6349, 0.4905],
          [0.5196, 0.5093, 0.6094,  ..., 0.5085, 0.4640, 0.6157]],

         [[0.5256, 0.4454, 0.4344,  ..., 0.4928, 0.6289, 0.7306],
          [0.4864, 0.6022, 0.4924,  ..., 0.4196, 0.6177, 0.4753],
          [0.4198, 0.4081, 0.4784,  ..., 0.4353, 0.4338, 0.5217],
          [0.3881, 0.4425, 0.4668,  ..., 0.5859, 0.3147, 0.4610]]]],
       device='cuda:0')
tensor([[[[0.4603, 0.5003, 0.4767,  ..., 0.4661, 0.5249, 0.6030],
          [0.4654, 0.3872, 0.5646,  ..., 0.5195, 0.6749, 0.5018],
          [0.3337, 0.6128, 0.6379,  ..., 0.5171, 0.3225, 0.4292],
          [0.6645, 0.6505, 0.5660,  ..., 0.5018, 0.4393, 0.4870]],

         [[0.4182, 0.5506, 0.4695,  ..., 0.6370, 0.4196, 0.5610],
          [0.4258, 0.5593, 0.4863,  ..., 0.6433, 0.5889, 0.3863],
          [0.5229, 0.4535, 0.4957,  ..., 0.3739, 0.4964, 0.3849],
          [0.5203, 0.4393, 0.4774,  ..., 0.4785, 0.4913, 0.3826]],

         [[0.5898, 0.5894, 0.5095,  ..., 0.5380, 0.4564, 0.5718],
          [0.4830, 0.5964, 0.4040,  ..., 0.6343, 0.5122, 0.4465],
          [0.4330, 0.5089, 0.5227,  ..., 0.4316, 0.3812, 0.4040],
          [0.4800, 0.5272, 0.4991,  ..., 0.5402, 0.3821, 0.5073]],

         ...,

         [[0.5392, 0.3522, 0.4458,  ..., 0.3993, 0.5246, 0.5742],
          [0.4596, 0.5799, 0.5441,  ..., 0.4656, 0.5329, 0.5239],
          [0.5646, 0.5263, 0.4405,  ..., 0.3693, 0.5308, 0.4504],
          [0.4446, 0.4838, 0.5153,  ..., 0.5651, 0.4771, 0.5108]],

         [[0.5518, 0.5349, 0.5351,  ..., 0.5699, 0.5130, 0.5827],
          [0.4477, 0.5407, 0.6039,  ..., 0.5571, 0.4397, 0.5141],
          [0.4581, 0.3794, 0.6505,  ..., 0.3702, 0.5477, 0.4216],
          [0.5751, 0.4581, 0.4422,  ..., 0.6280, 0.5271, 0.5249]],

         [[0.5856, 0.5277, 0.4681,  ..., 0.5865, 0.5627, 0.5161],
          [0.5213, 0.4316, 0.5198,  ..., 0.5193, 0.6325, 0.5617],
          [0.6379, 0.4225, 0.5378,  ..., 0.4550, 0.5617, 0.3558],
          [0.4919, 0.4235, 0.6220,  ..., 0.5818, 0.4958, 0.4325]]],


        [[[0.4938, 0.3639, 0.4513,  ..., 0.3826, 0.5057, 0.5177],
          [0.5307, 0.4196, 0.5856,  ..., 0.4951, 0.4282, 0.4678],
          [0.6123, 0.3844, 0.6049,  ..., 0.5221, 0.4092, 0.4397],
          [0.5908, 0.5290, 0.5851,  ..., 0.5285, 0.5167, 0.4909]],

         [[0.6197, 0.5160, 0.5048,  ..., 0.4955, 0.5317, 0.5646],
          [0.4868, 0.6039, 0.5492,  ..., 0.5846, 0.5525, 0.5055],
          [0.5590, 0.4484, 0.5574,  ..., 0.5045, 0.5402, 0.4835],
          [0.4092, 0.4930, 0.4388,  ..., 0.4958, 0.4470, 0.3854]],

         [[0.6183, 0.5607, 0.6025,  ..., 0.6202, 0.4316, 0.6049],
          [0.3942, 0.6334, 0.5407,  ..., 0.4703, 0.5266, 0.4385],
          [0.5375, 0.5272, 0.5312,  ..., 0.5373, 0.4135, 0.3812],
          [0.5426, 0.5306, 0.6132,  ..., 0.6174, 0.5823, 0.4822]],

         ...,

         [[0.5164, 0.5322, 0.5262,  ..., 0.6406, 0.5737, 0.4659],
          [0.3766, 0.5213, 0.4608,  ..., 0.5708, 0.5332, 0.5109],
          [0.6234, 0.4211, 0.4402,  ..., 0.6433, 0.5455, 0.5157],
          [0.4888, 0.6035, 0.5950,  ..., 0.5708, 0.3504, 0.4673]],

         [[0.5489, 0.4441, 0.5499,  ..., 0.3942, 0.5506, 0.4540],
          [0.2999, 0.4690, 0.6968,  ..., 0.5884, 0.5598, 0.5250],
          [0.5610, 0.4400, 0.4699,  ..., 0.5192, 0.6169, 0.4745],
          [0.5276, 0.5213, 0.6114,  ..., 0.5105, 0.4460, 0.5997]],

         [[0.5336, 0.4574, 0.4364,  ..., 0.4948, 0.6109, 0.7146],
          [0.4944, 0.6142, 0.4944,  ..., 0.4216, 0.5997, 0.4593],
          [0.4278, 0.4201, 0.4804,  ..., 0.4373, 0.4158, 0.5057],
          [0.3961, 0.4545, 0.4688,  ..., 0.5879, 0.2967, 0.4450]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0080, -0.0120, -0.0020, -0.0060, -0.0140,  0.0220, -0.0260, -0.0020,
         0.0180,  0.0160], device='cuda:0')
selected experts tensor([1450, 1674, 1542, 1940, 1398, 1952, 1853, 1570, 1428, 1577],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5087, 0.4465, 0.5095,  ..., 0.3483, 0.3222, 0.4165],
          [0.3144, 0.6030, 0.5539,  ..., 0.5180, 0.4277, 0.4260],
          [0.6069, 0.4235, 0.4038,  ..., 0.5418, 0.4361, 0.6345],
          [0.5180, 0.4676, 0.4887,  ..., 0.4297, 0.5977, 0.4350]],

         [[0.4978, 0.4879, 0.4471,  ..., 0.3955, 0.6075, 0.5607],
          [0.5480, 0.4054, 0.4823,  ..., 0.4887, 0.5101, 0.5062],
          [0.5519, 0.4739, 0.4322,  ..., 0.5177, 0.5447, 0.5006],
          [0.5485, 0.4802, 0.5232,  ..., 0.4781, 0.4876, 0.3877]],

         [[0.5723, 0.5586, 0.4507,  ..., 0.3527, 0.4762, 0.4039],
          [0.5572, 0.5670, 0.4650,  ..., 0.6312, 0.6098, 0.4525],
          [0.5862, 0.4448, 0.4341,  ..., 0.5377, 0.5055, 0.5904],
          [0.5055, 0.6451, 0.4577,  ..., 0.5292, 0.5175, 0.4331]],

         ...,

         [[0.5020, 0.5000, 0.5040,  ..., 0.5120, 0.4980, 0.5120],
          [0.5020, 0.5000, 0.5040,  ..., 0.5120, 0.4980, 0.5120],
          [0.5020, 0.5000, 0.5040,  ..., 0.5120, 0.4980, 0.5120],
          [0.5020, 0.5000, 0.5040,  ..., 0.5120, 0.4980, 0.5120]],

         [[0.5020, 0.5000, 0.5040,  ..., 0.5120, 0.4980, 0.5120],
          [0.5020, 0.5000, 0.5040,  ..., 0.5120, 0.4980, 0.5120],
          [0.5020, 0.5000, 0.5040,  ..., 0.5120, 0.4980, 0.5120],
          [0.5020, 0.5000, 0.5040,  ..., 0.5120, 0.4980, 0.5120]],

         [[0.5020, 0.5000, 0.5040,  ..., 0.5120, 0.4980, 0.5120],
          [0.5020, 0.5000, 0.5040,  ..., 0.5120, 0.4980, 0.5120],
          [0.5020, 0.5000, 0.5040,  ..., 0.5120, 0.4980, 0.5120],
          [0.5020, 0.5000, 0.5040,  ..., 0.5120, 0.4980, 0.5120]]],


        [[[0.4341, 0.6077, 0.5616,  ..., 0.2202, 0.3396, 0.6051],
          [0.3943, 0.4345, 0.5616,  ..., 0.5130, 0.4639, 0.4170],
          [0.5422, 0.5417, 0.4956,  ..., 0.4674, 0.5346, 0.5602],
          [0.4813, 0.5665, 0.3843,  ..., 0.4619, 0.5062, 0.4236]],

         [[0.3722, 0.4282, 0.4180,  ..., 0.4250, 0.3903, 0.4966],
          [0.4757, 0.4383, 0.6347,  ..., 0.5857, 0.5457, 0.5028],
          [0.5432, 0.4749, 0.5031,  ..., 0.4798, 0.6103, 0.5515],
          [0.5661, 0.5605, 0.5003,  ..., 0.5018, 0.5223, 0.4503]],

         [[0.5364, 0.4966, 0.4170,  ..., 0.3054, 0.5001, 0.4706],
          [0.5838, 0.4864, 0.5208,  ..., 0.6372, 0.5201, 0.4619],
          [0.5550, 0.5913, 0.4911,  ..., 0.4905, 0.5426, 0.5466],
          [0.4782, 0.6697, 0.3625,  ..., 0.4109, 0.6643, 0.3615]],

         ...,

         [[0.5020, 0.5000, 0.5040,  ..., 0.5120, 0.4980, 0.5120],
          [0.5020, 0.5000, 0.5040,  ..., 0.5120, 0.4980, 0.5120],
          [0.5020, 0.5000, 0.5040,  ..., 0.5120, 0.4980, 0.5120],
          [0.5020, 0.5000, 0.5040,  ..., 0.5120, 0.4980, 0.5120]],

         [[0.5020, 0.5000, 0.5040,  ..., 0.5120, 0.4980, 0.5120],
          [0.5020, 0.5000, 0.5040,  ..., 0.5120, 0.4980, 0.5120],
          [0.5020, 0.5000, 0.5040,  ..., 0.5120, 0.4980, 0.5120],
          [0.5020, 0.5000, 0.5040,  ..., 0.5120, 0.4980, 0.5120]],

         [[0.5020, 0.5000, 0.5040,  ..., 0.5120, 0.4980, 0.5120],
          [0.5020, 0.5000, 0.5040,  ..., 0.5120, 0.4980, 0.5120],
          [0.5020, 0.5000, 0.5040,  ..., 0.5120, 0.4980, 0.5120],
          [0.5020, 0.5000, 0.5040,  ..., 0.5120, 0.4980, 0.5120]]]],
       device='cuda:0')/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([[[[0.5067, 0.4465, 0.5055,  ..., 0.3363, 0.3242, 0.4045],
          [0.3124, 0.6030, 0.5499,  ..., 0.5060, 0.4297, 0.4140],
          [0.6049, 0.4235, 0.3998,  ..., 0.5297, 0.4381, 0.6225],
          [0.5160, 0.4676, 0.4847,  ..., 0.4177, 0.5997, 0.4230]],

         [[0.4958, 0.4879, 0.4431,  ..., 0.3835, 0.6095, 0.5487],
          [0.5460, 0.4054, 0.4783,  ..., 0.4767, 0.5121, 0.4942],
          [0.5499, 0.4739, 0.4282,  ..., 0.5057, 0.5467, 0.4886],
          [0.5465, 0.4802, 0.5192,  ..., 0.4661, 0.4896, 0.3757]],

         [[0.5703, 0.5586, 0.4467,  ..., 0.3407, 0.4782, 0.3919],
          [0.5552, 0.5670, 0.4610,  ..., 0.6192, 0.6118, 0.4405],
          [0.5842, 0.4448, 0.4301,  ..., 0.5257, 0.5075, 0.5784],
          [0.5035, 0.6451, 0.4537,  ..., 0.5172, 0.5195, 0.4211]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.4321, 0.6077, 0.5576,  ..., 0.2082, 0.3416, 0.5931],
          [0.3923, 0.4345, 0.5576,  ..., 0.5010, 0.4659, 0.4050],
          [0.5402, 0.5417, 0.4916,  ..., 0.4554, 0.5366, 0.5482],
          [0.4793, 0.5665, 0.3803,  ..., 0.4499, 0.5082, 0.4116]],

         [[0.3702, 0.4282, 0.4140,  ..., 0.4130, 0.3923, 0.4846],
          [0.4737, 0.4383, 0.6307,  ..., 0.5737, 0.5477, 0.4908],
          [0.5412, 0.4749, 0.4991,  ..., 0.4678, 0.6123, 0.5395],
          [0.5641, 0.5605, 0.4963,  ..., 0.4898, 0.5243, 0.4383]],

         [[0.5344, 0.4966, 0.4130,  ..., 0.2934, 0.5021, 0.4586],
          [0.5818, 0.4864, 0.5168,  ..., 0.6252, 0.5221, 0.4499],
          [0.5530, 0.5913, 0.4871,  ..., 0.4785, 0.5446, 0.5346],
          [0.4762, 0.6697, 0.3585,  ..., 0.3989, 0.6663, 0.3495]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([ 2.0000e-03, -6.9849e-10,  4.0000e-03,  1.0000e-02, -1.2000e-02,
         1.4000e-02,  1.0000e-02,  1.2000e-02, -2.0000e-03,  1.2000e-02],
       device='cuda:0')
selected experts tensor([2128, 2166, 1706, 1972, 1548,  811, 1651, 1026, 2002, 1374],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1744., 1665., 1562., 1620., 1754., 1478., 1708., 1540., 1754., 1559.],
        [2128., 2166., 1706., 1972., 1548.,  811., 1651., 1026., 2002., 1374.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4430, 0.4541, 0.3829,  ..., 0.3719, 0.5415, 0.4380],
          [0.5309, 0.4548, 0.5023,  ..., 0.4045, 0.4776, 0.5264],
          [0.3664, 0.4326, 0.4193,  ..., 0.5215, 0.6417, 0.6214],
          [0.4583, 0.5080, 0.5247,  ..., 0.4296, 0.4750, 0.5194]],

         [[0.4153, 0.4360, 0.6591,  ..., 0.3566, 0.5325, 0.5223],
          [0.6163, 0.5025, 0.6015,  ..., 0.5424, 0.6134, 0.5408],
          [0.6182, 0.4427, 0.4351,  ..., 0.5617, 0.4274, 0.4604],
          [0.6163, 0.4183, 0.5785,  ..., 0.4841, 0.5045, 0.4972]],

         [[0.4745, 0.5126, 0.4746,  ..., 0.4425, 0.6498, 0.6703],
          [0.5907, 0.6551, 0.4562,  ..., 0.5211, 0.5274, 0.6037],
          [0.4885, 0.5262, 0.3568,  ..., 0.4741, 0.4908, 0.3652],
          [0.6117, 0.5984, 0.5709,  ..., 0.5429, 0.4051, 0.5168]],

         ...,

         [[0.4272, 0.5733, 0.6937,  ..., 0.5012, 0.5000, 0.3625],
          [0.5921, 0.3695, 0.4511,  ..., 0.4693, 0.5238, 0.4757],
          [0.5406, 0.5361, 0.5215,  ..., 0.5256, 0.4864, 0.5065],
          [0.3710, 0.3851, 0.4809,  ..., 0.5903, 0.4835, 0.4222]],

         [[0.5046, 0.4970, 0.5595,  ..., 0.5455, 0.6444, 0.3834],
          [0.4267, 0.4567, 0.6353,  ..., 0.4593, 0.5048, 0.5359],
          [0.4086, 0.5984, 0.4083,  ..., 0.5131, 0.4150, 0.6950],
          [0.6296, 0.4591, 0.5040,  ..., 0.6504, 0.3686, 0.4365]],

         [[0.5045, 0.5415, 0.6643,  ..., 0.5049, 0.4553, 0.3056],
          [0.5399, 0.5227, 0.6245,  ..., 0.3584, 0.5337, 0.6437],
          [0.5098, 0.5598, 0.4799,  ..., 0.5302, 0.6622, 0.4198],
          [0.5030, 0.4996, 0.3746,  ..., 0.5511, 0.5914, 0.5962]]],


        [[[0.6269, 0.6525, 0.5088,  ..., 0.5740, 0.5287, 0.4222],
          [0.5760, 0.4413, 0.6209,  ..., 0.4324, 0.5143, 0.5229],
          [0.5058, 0.6045, 0.4155,  ..., 0.5489, 0.4819, 0.6392],
          [0.5423, 0.4042, 0.5154,  ..., 0.4753, 0.6551, 0.5002]],

         [[0.5128, 0.5728, 0.5623,  ..., 0.5058, 0.4965, 0.4831],
          [0.4614, 0.6092, 0.4450,  ..., 0.4036, 0.4844, 0.4332],
          [0.4472, 0.5052, 0.4112,  ..., 0.4465, 0.5914, 0.4952],
          [0.4387, 0.4093, 0.5307,  ..., 0.5697, 0.6290, 0.3977]],

         [[0.5264, 0.6879, 0.6451,  ..., 0.4854, 0.5509, 0.6580],
          [0.4835, 0.4197, 0.4831,  ..., 0.4116, 0.5497, 0.4794],
          [0.4034, 0.5322, 0.3613,  ..., 0.5988, 0.4312, 0.6428],
          [0.4438, 0.5290, 0.4462,  ..., 0.4381, 0.4188, 0.4539]],

         ...,

         [[0.6608, 0.4032, 0.4576,  ..., 0.4191, 0.4681, 0.4227],
          [0.5050, 0.4454, 0.5898,  ..., 0.3755, 0.5383, 0.5075],
          [0.6314, 0.3349, 0.3622,  ..., 0.3611, 0.5272, 0.5331],
          [0.3020, 0.4307, 0.5642,  ..., 0.4851, 0.4942, 0.4997]],

         [[0.6502, 0.6525, 0.6181,  ..., 0.5332, 0.3515, 0.3788],
          [0.4138, 0.4302, 0.5799,  ..., 0.5959, 0.4763, 0.4033],
          [0.4855, 0.5098, 0.5633,  ..., 0.5343, 0.4712, 0.4565],
          [0.4530, 0.5337, 0.4472,  ..., 0.5261, 0.4846, 0.5710]],

         [[0.5045, 0.5415, 0.6643,  ..., 0.5049, 0.4553, 0.3056],
          [0.5399, 0.5227, 0.6245,  ..., 0.3584, 0.5337, 0.6437],
          [0.5098, 0.5598, 0.4799,  ..., 0.5302, 0.6622, 0.4198],
          [0.5030, 0.4996, 0.3746,  ..., 0.5511, 0.5914, 0.5962]]]],
       device='cuda:0')
tensor([[[[0.4450, 0.4521, 0.3909,  ..., 0.3639, 0.5395, 0.4340],
          [0.5329, 0.4528, 0.5103,  ..., 0.3965, 0.4756, 0.5224],
          [0.3684, 0.4306, 0.4273,  ..., 0.5135, 0.6397, 0.6174],
          [0.4603, 0.5060, 0.5327,  ..., 0.4216, 0.4730, 0.5154]],

         [[0.4173, 0.4340, 0.6671,  ..., 0.3486, 0.5305, 0.5183],
          [0.6183, 0.5005, 0.6095,  ..., 0.5344, 0.6114, 0.5368],
          [0.6202, 0.4407, 0.4431,  ..., 0.5537, 0.4254, 0.4564],
          [0.6183, 0.4163, 0.5865,  ..., 0.4761, 0.5025, 0.4932]],

         [[0.4765, 0.5106, 0.4826,  ..., 0.4345, 0.6478, 0.6663],
          [0.5927, 0.6531, 0.4642,  ..., 0.5131, 0.5254, 0.5997],
          [0.4905, 0.5242, 0.3648,  ..., 0.4661, 0.4888, 0.3612],
          [0.6137, 0.5964, 0.5789,  ..., 0.5349, 0.4031, 0.5128]],

         ...,

         [[0.4292, 0.5713, 0.7017,  ..., 0.4932, 0.4980, 0.3585],
          [0.5941, 0.3675, 0.4591,  ..., 0.4613, 0.5218, 0.4717],
          [0.5426, 0.5341, 0.5295,  ..., 0.5176, 0.4844, 0.5025],
          [0.3730, 0.3831, 0.4889,  ..., 0.5823, 0.4815, 0.4182]],

         [[0.5066, 0.4950, 0.5675,  ..., 0.5375, 0.6424, 0.3794],
          [0.4287, 0.4547, 0.6433,  ..., 0.4513, 0.5028, 0.5319],
          [0.4106, 0.5964, 0.4163,  ..., 0.5051, 0.4130, 0.6910],
          [0.6316, 0.4571, 0.5120,  ..., 0.6424, 0.3666, 0.4325]],

         [[0.5065, 0.5395, 0.6723,  ..., 0.4969, 0.4533, 0.3016],
          [0.5419, 0.5207, 0.6325,  ..., 0.3504, 0.5317, 0.6397],
          [0.5118, 0.5578, 0.4879,  ..., 0.5222, 0.6602, 0.4158],
          [0.5050, 0.4976, 0.3826,  ..., 0.5431, 0.5894, 0.5922]]],


        [[[0.6289, 0.6505, 0.5168,  ..., 0.5660, 0.5267, 0.4182],
          [0.5780, 0.4393, 0.6289,  ..., 0.4244, 0.5123, 0.5189],
          [0.5078, 0.6025, 0.4235,  ..., 0.5409, 0.4799, 0.6352],
          [0.5443, 0.4022, 0.5234,  ..., 0.4673, 0.6531, 0.4962]],

         [[0.5148, 0.5708, 0.5703,  ..., 0.4978, 0.4945, 0.4791],
          [0.4634, 0.6072, 0.4530,  ..., 0.3956, 0.4824, 0.4292],
          [0.4492, 0.5032, 0.4192,  ..., 0.4385, 0.5894, 0.4912],
          [0.4407, 0.4073, 0.5387,  ..., 0.5617, 0.6270, 0.3937]],

         [[0.5284, 0.6859, 0.6531,  ..., 0.4774, 0.5489, 0.6540],
          [0.4855, 0.4177, 0.4911,  ..., 0.4036, 0.5477, 0.4754],
          [0.4054, 0.5302, 0.3693,  ..., 0.5908, 0.4292, 0.6388],
          [0.4458, 0.5270, 0.4542,  ..., 0.4301, 0.4168, 0.4499]],

         ...,

         [[0.6628, 0.4012, 0.4656,  ..., 0.4111, 0.4661, 0.4187],
          [0.5070, 0.4434, 0.5978,  ..., 0.3675, 0.5363, 0.5035],
          [0.6334, 0.3329, 0.3702,  ..., 0.3531, 0.5252, 0.5291],
          [0.3040, 0.4287, 0.5722,  ..., 0.4771, 0.4922, 0.4957]],

         [[0.6522, 0.6505, 0.6261,  ..., 0.5252, 0.3495, 0.3748],
          [0.4158, 0.4282, 0.5879,  ..., 0.5879, 0.4743, 0.3993],
          [0.4875, 0.5078, 0.5713,  ..., 0.5263, 0.4692, 0.4525],
          [0.4550, 0.5317, 0.4552,  ..., 0.5181, 0.4826, 0.5670]],

         [[0.5065, 0.5395, 0.6723,  ..., 0.4969, 0.4533, 0.3016],
          [0.5419, 0.5207, 0.6325,  ..., 0.3504, 0.5317, 0.6397],
          [0.5118, 0.5578, 0.4879,  ..., 0.5222, 0.6602, 0.4158],
          [0.5050, 0.4976, 0.3826,  ..., 0.5431, 0.5894, 0.5922]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0020,  0.0020, -0.0080,  0.0020, -0.0040, -0.0040,  0.0080,  0.0080,
         0.0020,  0.0040], device='cuda:0')
selected experts tensor([1594, 1678, 1588, 1591, 1610, 1700, 1693, 1667, 1622, 1641],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5760, 0.4241, 0.5095,  ..., 0.6185, 0.5524, 0.3177],
          [0.3834, 0.4108, 0.5471,  ..., 0.4406, 0.4999, 0.4731],
          [0.4215, 0.6732, 0.6089,  ..., 0.5063, 0.3742, 0.3297],
          [0.4629, 0.5805, 0.6694,  ..., 0.3367, 0.3894, 0.5398]],

         [[0.5241, 0.3697, 0.5967,  ..., 0.5929, 0.4123, 0.4557],
          [0.5120, 0.4516, 0.5449,  ..., 0.4872, 0.5230, 0.4326],
          [0.2938, 0.4762, 0.5093,  ..., 0.3527, 0.6182, 0.5028],
          [0.4006, 0.3761, 0.5556,  ..., 0.6375, 0.5486, 0.4920]],

         [[0.5141, 0.5192, 0.4682,  ..., 0.3972, 0.3935, 0.5039],
          [0.3978, 0.5180, 0.5767,  ..., 0.5208, 0.4515, 0.5054],
          [0.5018, 0.5629, 0.5234,  ..., 0.4558, 0.3360, 0.5224],
          [0.5496, 0.4070, 0.4633,  ..., 0.5546, 0.3299, 0.4671]],

         ...,

         [[0.5736, 0.3114, 0.6347,  ..., 0.4490, 0.3438, 0.4771],
          [0.5779, 0.3064, 0.5454,  ..., 0.3680, 0.4855, 0.5193],
          [0.6520, 0.4589, 0.4758,  ..., 0.4047, 0.6650, 0.4640],
          [0.6395, 0.4966, 0.4772,  ..., 0.4931, 0.6079, 0.5966]],

         [[0.4807, 0.4289, 0.5325,  ..., 0.5759, 0.4721, 0.5512],
          [0.5360, 0.5576, 0.5222,  ..., 0.4010, 0.5342, 0.6921],
          [0.6529, 0.4748, 0.6182,  ..., 0.4281, 0.4294, 0.5790],
          [0.5640, 0.5662, 0.4033,  ..., 0.5721, 0.3760, 0.5975]],

         [[0.4864, 0.5146, 0.5351,  ..., 0.5725, 0.4939, 0.5502],
          [0.4191, 0.6165, 0.6131,  ..., 0.5452, 0.4519, 0.3911],
          [0.6323, 0.5118, 0.4194,  ..., 0.5802, 0.4744, 0.5453],
          [0.4101, 0.4350, 0.5325,  ..., 0.5126, 0.4442, 0.6208]]],


        [[[0.5926, 0.4122, 0.6908,  ..., 0.4717, 0.4361, 0.5332],
          [0.5175, 0.5003, 0.4303,  ..., 0.4452, 0.3580, 0.6829],
          [0.5401, 0.4141, 0.5344,  ..., 0.4519, 0.4887, 0.5369],
          [0.5435, 0.5533, 0.5466,  ..., 0.5249, 0.4679, 0.5637]],

         [[0.4760, 0.4977, 0.4905,  ..., 0.5335, 0.5166, 0.5550],
          [0.5083, 0.6132, 0.4265,  ..., 0.6078, 0.4380, 0.6345],
          [0.5287, 0.4270, 0.5900,  ..., 0.5279, 0.5324, 0.2850],
          [0.4731, 0.7022, 0.5351,  ..., 0.5910, 0.4952, 0.4084]],

         [[0.4617, 0.3868, 0.4270,  ..., 0.5131, 0.4933, 0.4616],
          [0.5769, 0.4684, 0.4303,  ..., 0.4810, 0.5103, 0.6435],
          [0.4742, 0.4849, 0.6079,  ..., 0.6777, 0.4428, 0.5548],
          [0.4377, 0.3789, 0.6051,  ..., 0.5877, 0.5128, 0.5714]],

         ...,

         [[0.3719, 0.4954, 0.3625,  ..., 0.4033, 0.5233, 0.5562],
          [0.4924, 0.5862, 0.4628,  ..., 0.3263, 0.4071, 0.5550],
          [0.5377, 0.4222, 0.4442,  ..., 0.5418, 0.4979, 0.5087],
          [0.5991, 0.4608, 0.5423,  ..., 0.4204, 0.3797, 0.4565]],

         [[0.3610, 0.5168, 0.4699,  ..., 0.5420, 0.5662, 0.4422],
          [0.5542, 0.4256, 0.5444,  ..., 0.4709, 0.4213, 0.5400],
          [0.6098, 0.5183, 0.4370,  ..., 0.3749, 0.5556, 0.5499],
          [0.4668, 0.5545, 0.6265,  ..., 0.4570, 0.4104, 0.5386]],

         [[0.4864, 0.5146, 0.5351,  ..., 0.5725, 0.4939, 0.5502],
          [0.4191, 0.6165, 0.6131,  ..., 0.5452, 0.4519, 0.3911],
          [0.6323, 0.5118, 0.4194,  ..., 0.5802, 0.4744, 0.5453],
          [0.4101, 0.4350, 0.5325,  ..., 0.5126, 0.4442, 0.6208]]]],
       device='cuda:0')
tensor([[[[0.5780, 0.4301, 0.5055,  ..., 0.6225, 0.5484, 0.3157],
          [0.3854, 0.4168, 0.5431,  ..., 0.4446, 0.4959, 0.4711],
          [0.4235, 0.6792, 0.6049,  ..., 0.5103, 0.3702, 0.3277],
          [0.4649, 0.5865, 0.6654,  ..., 0.3407, 0.3854, 0.5378]],

         [[0.5261, 0.3757, 0.5927,  ..., 0.5969, 0.4083, 0.4537],
          [0.5140, 0.4576, 0.5409,  ..., 0.4912, 0.5190, 0.4306],
          [0.2958, 0.4822, 0.5053,  ..., 0.3567, 0.6142, 0.5008],
          [0.4026, 0.3821, 0.5516,  ..., 0.6415, 0.5446, 0.4900]],

         [[0.5161, 0.5252, 0.4642,  ..., 0.4012, 0.3895, 0.5019],
          [0.3998, 0.5240, 0.5727,  ..., 0.5248, 0.4475, 0.5034],
          [0.5038, 0.5689, 0.5194,  ..., 0.4598, 0.3320, 0.5204],
          [0.5516, 0.4130, 0.4593,  ..., 0.5586, 0.3259, 0.4651]],

         ...,

         [[0.5756, 0.3174, 0.6307,  ..., 0.4530, 0.3398, 0.4751],
          [0.5799, 0.3124, 0.5414,  ..., 0.3720, 0.4815, 0.5173],
          [0.6540, 0.4649, 0.4718,  ..., 0.4087, 0.6610, 0.4620],
          [0.6415, 0.5026, 0.4732,  ..., 0.4971, 0.6039, 0.5946]],

         [[0.4827, 0.4349, 0.5285,  ..., 0.5799, 0.4681, 0.5492],
          [0.5380, 0.5636, 0.5182,  ..., 0.4050, 0.5302, 0.6901],
          [0.6549, 0.4808, 0.6142,  ..., 0.4321, 0.4254, 0.5770],
          [0.5660, 0.5722, 0.3993,  ..., 0.5761, 0.3720, 0.5955]],

         [[0.4884, 0.5206, 0.5311,  ..., 0.5765, 0.4899, 0.5482],
          [0.4211, 0.6225, 0.6091,  ..., 0.5492, 0.4479, 0.3891],
          [0.6343, 0.5178, 0.4154,  ..., 0.5842, 0.4704, 0.5433],
          [0.4121, 0.4410, 0.5285,  ..., 0.5166, 0.4402, 0.6188]]],


        [[[0.5946, 0.4182, 0.6868,  ..., 0.4757, 0.4321, 0.5312],
          [0.5195, 0.5063, 0.4263,  ..., 0.4492, 0.3540, 0.6809],
          [0.5421, 0.4201, 0.5304,  ..., 0.4559, 0.4847, 0.5349],
          [0.5455, 0.5593, 0.5426,  ..., 0.5289, 0.4639, 0.5617]],

         [[0.4780, 0.5037, 0.4865,  ..., 0.5375, 0.5126, 0.5530],
          [0.5103, 0.6192, 0.4225,  ..., 0.6118, 0.4340, 0.6325],
          [0.5307, 0.4330, 0.5860,  ..., 0.5319, 0.5284, 0.2830],
          [0.4751, 0.7082, 0.5311,  ..., 0.5950, 0.4912, 0.4064]],

         [[0.4637, 0.3928, 0.4230,  ..., 0.5171, 0.4893, 0.4596],
          [0.5789, 0.4744, 0.4263,  ..., 0.4850, 0.5063, 0.6415],
          [0.4762, 0.4909, 0.6039,  ..., 0.6817, 0.4388, 0.5528],
          [0.4397, 0.3849, 0.6011,  ..., 0.5917, 0.5088, 0.5694]],

         ...,

         [[0.3739, 0.5014, 0.3585,  ..., 0.4073, 0.5193, 0.5542],
          [0.4944, 0.5922, 0.4588,  ..., 0.3303, 0.4031, 0.5530],
          [0.5397, 0.4282, 0.4402,  ..., 0.5458, 0.4939, 0.5067],
          [0.6011, 0.4668, 0.5383,  ..., 0.4244, 0.3757, 0.4545]],

         [[0.3630, 0.5228, 0.4659,  ..., 0.5460, 0.5622, 0.4402],
          [0.5562, 0.4316, 0.5404,  ..., 0.4749, 0.4173, 0.5380],
          [0.6118, 0.5243, 0.4330,  ..., 0.3789, 0.5516, 0.5479],
          [0.4688, 0.5605, 0.6225,  ..., 0.4610, 0.4064, 0.5366]],

         [[0.4884, 0.5206, 0.5311,  ..., 0.5765, 0.4899, 0.5482],
          [0.4211, 0.6225, 0.6091,  ..., 0.5492, 0.4479, 0.3891],
          [0.6343, 0.5178, 0.4154,  ..., 0.5842, 0.4704, 0.5433],
          [0.4121, 0.4410, 0.5285,  ..., 0.5166, 0.4402, 0.6188]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0020, -0.0060,  0.0040,  0.0100, -0.0020,  0.0080, -0.0140, -0.0040,
         0.0040,  0.0020], device='cuda:0')
selected experts tensor([1546, 1708, 1684, 1627, 1634, 1650, 1556, 1685, 1629, 1665],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5146, 0.5516, 0.4007,  ..., 0.5903, 0.4906, 0.6265],
          [0.4284, 0.5928, 0.5580,  ..., 0.5997, 0.6965, 0.5274],
          [0.4484, 0.4996, 0.4878,  ..., 0.4605, 0.3407, 0.4924],
          [0.5169, 0.4090, 0.5392,  ..., 0.4716, 0.4884, 0.5964]],

         [[0.4547, 0.4710, 0.5428,  ..., 0.3319, 0.4991, 0.6850],
          [0.4169, 0.5573, 0.5334,  ..., 0.5385, 0.6405, 0.5878],
          [0.4107, 0.4899, 0.5954,  ..., 0.5373, 0.3901, 0.4722],
          [0.6095, 0.3972, 0.5789,  ..., 0.5312, 0.3739, 0.4623]],

         [[0.3970, 0.4441, 0.3993,  ..., 0.3941, 0.5580, 0.5873],
          [0.5161, 0.4928, 0.5893,  ..., 0.5983, 0.5903, 0.4395],
          [0.5652, 0.4536, 0.5009,  ..., 0.5207, 0.4372, 0.4529],
          [0.4299, 0.4439, 0.5387,  ..., 0.4196, 0.5025, 0.5116]],

         ...,

         [[0.5209, 0.4480, 0.4177,  ..., 0.4330, 0.4477, 0.6083],
          [0.4504, 0.5223, 0.5860,  ..., 0.4723, 0.6704, 0.5485],
          [0.5547, 0.3410, 0.6824,  ..., 0.5236, 0.4118, 0.4608],
          [0.5600, 0.5240, 0.3756,  ..., 0.4311, 0.4587, 0.3630]],

         [[0.5108, 0.4564, 0.4380,  ..., 0.3701, 0.4968, 0.5591],
          [0.4905, 0.5403, 0.5375,  ..., 0.4805, 0.4573, 0.4727],
          [0.3497, 0.3095, 0.5945,  ..., 0.4163, 0.5079, 0.4079],
          [0.4768, 0.5659, 0.5179,  ..., 0.4588, 0.5519, 0.4693]],

         [[0.5084, 0.5021, 0.3756,  ..., 0.4744, 0.4682, 0.6621],
          [0.3747, 0.5759, 0.6539,  ..., 0.5440, 0.5458, 0.5198],
          [0.4797, 0.4282, 0.5978,  ..., 0.4311, 0.5006, 0.6049],
          [0.3632, 0.4478, 0.3006,  ..., 0.4498, 0.4929, 0.3169]]],


        [[[0.5923, 0.5783, 0.4996,  ..., 0.4182, 0.5091, 0.7021],
          [0.3835, 0.5136, 0.5227,  ..., 0.5244, 0.6140, 0.5041],
          [0.5838, 0.4143, 0.5722,  ..., 0.5041, 0.4744, 0.4381],
          [0.5463, 0.5470, 0.4296,  ..., 0.5353, 0.4868, 0.4182]],

         [[0.4564, 0.5407, 0.5508,  ..., 0.3951, 0.4315, 0.5292],
          [0.5453, 0.5357, 0.6085,  ..., 0.5597, 0.3811, 0.5047],
          [0.3705, 0.5257, 0.6025,  ..., 0.4409, 0.5974, 0.4505],
          [0.6025, 0.4047, 0.4125,  ..., 0.4932, 0.4434, 0.5004]],

         [[0.4854, 0.4497, 0.5022,  ..., 0.4915, 0.4737, 0.5318],
          [0.4929, 0.5171, 0.4253,  ..., 0.4296, 0.6257, 0.4343],
          [0.4494, 0.5090, 0.6539,  ..., 0.4627, 0.3910, 0.4486],
          [0.6906, 0.4932, 0.6108,  ..., 0.4703, 0.4906, 0.4873]],

         ...,

         [[0.4731, 0.6204, 0.4035,  ..., 0.3450, 0.4858, 0.4686],
          [0.5508, 0.6321, 0.5921,  ..., 0.4172, 0.5325, 0.3630],
          [0.3937, 0.4133, 0.4287,  ..., 0.3993, 0.5754, 0.3656],
          [0.6865, 0.4364, 0.4632,  ..., 0.4049, 0.5836, 0.5000]],

         [[0.4699, 0.4594, 0.3848,  ..., 0.4774, 0.6605, 0.7154],
          [0.3425, 0.5597, 0.6405,  ..., 0.5669, 0.6304, 0.5469],
          [0.4912, 0.3826, 0.5409,  ..., 0.5689, 0.4539, 0.4698],
          [0.6523, 0.3887, 0.4751,  ..., 0.4385, 0.5941, 0.4357]],

         [[0.5084, 0.5021, 0.3756,  ..., 0.4744, 0.4682, 0.6621],
          [0.3747, 0.5759, 0.6539,  ..., 0.5440, 0.5458, 0.5198],
          [0.4797, 0.4282, 0.5978,  ..., 0.4311, 0.5006, 0.6049],
          [0.3632, 0.4478, 0.3006,  ..., 0.4498, 0.4929, 0.3169]]]],
       device='cuda:0')
tensor([[[[0.5216, 0.5646, 0.4017,  ..., 0.5913, 0.4716, 0.6095],
          [0.4354, 0.6058, 0.5590,  ..., 0.6007, 0.6775, 0.5104],
          [0.4554, 0.5126, 0.4888,  ..., 0.4615, 0.3217, 0.4754],
          [0.5239, 0.4220, 0.5402,  ..., 0.4726, 0.4694, 0.5794]],

         [[0.4617, 0.4840, 0.5438,  ..., 0.3329, 0.4801, 0.6680],
          [0.4239, 0.5703, 0.5344,  ..., 0.5395, 0.6215, 0.5708],
          [0.4177, 0.5029, 0.5964,  ..., 0.5383, 0.3711, 0.4552],
          [0.6165, 0.4102, 0.5799,  ..., 0.5322, 0.3549, 0.4453]],

         [[0.4040, 0.4571, 0.4003,  ..., 0.3951, 0.5390, 0.5703],
          [0.5231, 0.5058, 0.5903,  ..., 0.5993, 0.5713, 0.4225],
          [0.5722, 0.4666, 0.5019,  ..., 0.5217, 0.4182, 0.4359],
          [0.4369, 0.4569, 0.5397,  ..., 0.4206, 0.4835, 0.4946]],

         ...,

         [[0.5279, 0.4610, 0.4187,  ..., 0.4340, 0.4287, 0.5913],
          [0.4574, 0.5353, 0.5870,  ..., 0.4733, 0.6514, 0.5315],
          [0.5617, 0.3540, 0.6834,  ..., 0.5246, 0.3928, 0.4438],
          [0.5670, 0.5370, 0.3766,  ..., 0.4321, 0.4397, 0.3460]],

         [[0.5178, 0.4694, 0.4390,  ..., 0.3711, 0.4778, 0.5421],
          [0.4975, 0.5533, 0.5385,  ..., 0.4815, 0.4383, 0.4557],
          [0.3567, 0.3225, 0.5955,  ..., 0.4173, 0.4889, 0.3909],
          [0.4838, 0.5789, 0.5189,  ..., 0.4598, 0.5329, 0.4523]],

         [[0.5154, 0.5151, 0.3766,  ..., 0.4754, 0.4492, 0.6451],
          [0.3817, 0.5889, 0.6549,  ..., 0.5450, 0.5268, 0.5028],
          [0.4867, 0.4412, 0.5988,  ..., 0.4321, 0.4816, 0.5879],
          [0.3702, 0.4608, 0.3016,  ..., 0.4508, 0.4739, 0.2999]]],


        [[[0.5993, 0.5913, 0.5006,  ..., 0.4192, 0.4901, 0.6851],
          [0.3905, 0.5266, 0.5237,  ..., 0.5254, 0.5950, 0.4871],
          [0.5908, 0.4273, 0.5732,  ..., 0.5051, 0.4554, 0.4211],
          [0.5533, 0.5600, 0.4306,  ..., 0.5363, 0.4678, 0.4012]],

         [[0.4634, 0.5537, 0.5518,  ..., 0.3961, 0.4125, 0.5122],
          [0.5523, 0.5487, 0.6095,  ..., 0.5607, 0.3621, 0.4877],
          [0.3775, 0.5387, 0.6035,  ..., 0.4419, 0.5784, 0.4335],
          [0.6095, 0.4177, 0.4135,  ..., 0.4942, 0.4244, 0.4834]],

         [[0.4924, 0.4627, 0.5032,  ..., 0.4925, 0.4547, 0.5148],
          [0.4999, 0.5301, 0.4263,  ..., 0.4306, 0.6067, 0.4173],
          [0.4564, 0.5220, 0.6549,  ..., 0.4637, 0.3720, 0.4316],
          [0.6976, 0.5062, 0.6118,  ..., 0.4713, 0.4716, 0.4703]],

         ...,

         [[0.4801, 0.6334, 0.4045,  ..., 0.3460, 0.4668, 0.4516],
          [0.5578, 0.6451, 0.5931,  ..., 0.4182, 0.5135, 0.3460],
          [0.4007, 0.4263, 0.4297,  ..., 0.4003, 0.5564, 0.3486],
          [0.6935, 0.4494, 0.4642,  ..., 0.4059, 0.5646, 0.4830]],

         [[0.4769, 0.4724, 0.3858,  ..., 0.4784, 0.6415, 0.6984],
          [0.3495, 0.5727, 0.6415,  ..., 0.5679, 0.6114, 0.5299],
          [0.4982, 0.3956, 0.5419,  ..., 0.5699, 0.4349, 0.4528],
          [0.6593, 0.4017, 0.4761,  ..., 0.4395, 0.5751, 0.4187]],

         [[0.5154, 0.5151, 0.3766,  ..., 0.4754, 0.4492, 0.6451],
          [0.3817, 0.5889, 0.6549,  ..., 0.5450, 0.5268, 0.5028],
          [0.4867, 0.4412, 0.5988,  ..., 0.4321, 0.4816, 0.5879],
          [0.3702, 0.4608, 0.3016,  ..., 0.4508, 0.4739, 0.2999]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0070, -0.0130, -0.0010, -0.0070, -0.0130,  0.0210, -0.0270, -0.0010,
         0.0190,  0.0170], device='cuda:0')
selected experts tensor([1352, 1100, 1528, 2019, 1457, 2241, 1843, 1341, 1763, 1740],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5646, 0.4842, 0.4812,  ..., 0.4682, 0.4957, 0.4542],
          [0.4659, 0.4620, 0.5685,  ..., 0.6230, 0.4858, 0.4820],
          [0.6362, 0.3720, 0.3995,  ..., 0.5340, 0.5247, 0.6767],
          [0.6128, 0.4678, 0.4686,  ..., 0.3338, 0.5140, 0.4270]],

         [[0.4688, 0.5255, 0.4274,  ..., 0.4284, 0.3755, 0.4180],
          [0.5247, 0.3765, 0.7371,  ..., 0.5235, 0.5076, 0.4564],
          [0.5402, 0.3965, 0.4028,  ..., 0.4885, 0.5835, 0.5948],
          [0.5064, 0.6270, 0.5805,  ..., 0.4317, 0.5058, 0.3706]],

         [[0.5670, 0.5122, 0.5170,  ..., 0.3424, 0.4382, 0.4527],
          [0.4278, 0.5537, 0.5976,  ..., 0.5006, 0.4631, 0.6391],
          [0.5627, 0.4359, 0.4037,  ..., 0.4890, 0.5575, 0.5876],
          [0.5965, 0.5669, 0.5497,  ..., 0.3398, 0.4787, 0.4007]],

         ...,

         [[0.5010, 0.4990, 0.5030,  ..., 0.5130, 0.4970, 0.5130],
          [0.5010, 0.4990, 0.5030,  ..., 0.5130, 0.4970, 0.5130],
          [0.5010, 0.4990, 0.5030,  ..., 0.5130, 0.4970, 0.5130],
          [0.5010, 0.4990, 0.5030,  ..., 0.5130, 0.4970, 0.5130]],

         [[0.5010, 0.4990, 0.5030,  ..., 0.5130, 0.4970, 0.5130],
          [0.5010, 0.4990, 0.5030,  ..., 0.5130, 0.4970, 0.5130],
          [0.5010, 0.4990, 0.5030,  ..., 0.5130, 0.4970, 0.5130],
          [0.5010, 0.4990, 0.5030,  ..., 0.5130, 0.4970, 0.5130]],

         [[0.5010, 0.4990, 0.5030,  ..., 0.5130, 0.4970, 0.5130],
          [0.5010, 0.4990, 0.5030,  ..., 0.5130, 0.4970, 0.5130],
          [0.5010, 0.4990, 0.5030,  ..., 0.5130, 0.4970, 0.5130],
          [0.5010, 0.4990, 0.5030,  ..., 0.5130, 0.4970, 0.5130]]],


        [[[0.4417, 0.4753, 0.5226,  ..., 0.4653, 0.4602, 0.4166],
          [0.5325, 0.5755, 0.5623,  ..., 0.3974, 0.4471, 0.4583],
          [0.4678, 0.3720, 0.3552,  ..., 0.4968, 0.5261, 0.6043],
          [0.6045, 0.6090, 0.4284,  ..., 0.3924, 0.4939, 0.3347]],

         [[0.4661, 0.6205, 0.4746,  ..., 0.4350, 0.4954, 0.4508],
          [0.3613, 0.5936, 0.5800,  ..., 0.5248, 0.5788, 0.4867],
          [0.6021, 0.4829, 0.3005,  ..., 0.4892, 0.4568, 0.6057],
          [0.5579, 0.5794, 0.5640,  ..., 0.4350, 0.5437, 0.4460]],

         [[0.4910, 0.4049, 0.4756,  ..., 0.2827, 0.4291, 0.6286],
          [0.3631, 0.4966, 0.6093,  ..., 0.4489, 0.4205, 0.3347],
          [0.4635, 0.5798, 0.4775,  ..., 0.4750, 0.5483, 0.5718],
          [0.4168, 0.5621, 0.4336,  ..., 0.3860, 0.5357, 0.4067]],

         ...,

         [[0.5010, 0.4990, 0.5030,  ..., 0.5130, 0.4970, 0.5130],
          [0.5010, 0.4990, 0.5030,  ..., 0.5130, 0.4970, 0.5130],
          [0.5010, 0.4990, 0.5030,  ..., 0.5130, 0.4970, 0.5130],
          [0.5010, 0.4990, 0.5030,  ..., 0.5130, 0.4970, 0.5130]],

         [[0.5010, 0.4990, 0.5030,  ..., 0.5130, 0.4970, 0.5130],
          [0.5010, 0.4990, 0.5030,  ..., 0.5130, 0.4970, 0.5130],
          [0.5010, 0.4990, 0.5030,  ..., 0.5130, 0.4970, 0.5130],
          [0.5010, 0.4990, 0.5030,  ..., 0.5130, 0.4970, 0.5130]],

         [[0.5010, 0.4990, 0.5030,  ..., 0.5130, 0.4970, 0.5130],
          [0.5010, 0.4990, 0.5030,  ..., 0.5130, 0.4970, 0.5130],
          [0.5010, 0.4990, 0.5030,  ..., 0.5130, 0.4970, 0.5130],
          [0.5010, 0.4990, 0.5030,  ..., 0.5130, 0.4970, 0.5130]]]],
       device='cuda:0')
tensor([[[[0.5636, 0.4852, 0.4782,  ..., 0.4552, 0.4987, 0.4412],
          [0.4649, 0.4630, 0.5655,  ..., 0.6100, 0.4888, 0.4690],
          [0.6352, 0.3730, 0.3965,  ..., 0.5210, 0.5277, 0.6637],
          [0.6118, 0.4688, 0.4656,  ..., 0.3208, 0.5170, 0.4140]],

         [[0.4678, 0.5265, 0.4244,  ..., 0.4154, 0.3785, 0.4050],
          [0.5237, 0.3775, 0.7341,  ..., 0.5105, 0.5106, 0.4434],
          [0.5392, 0.3975, 0.3998,  ..., 0.4755, 0.5865, 0.5818],
          [0.5054, 0.6280, 0.5775,  ..., 0.4187, 0.5088, 0.3576]],

         [[0.5660, 0.5132, 0.5140,  ..., 0.3294, 0.4412, 0.4397],
          [0.4268, 0.5547, 0.5946,  ..., 0.4876, 0.4661, 0.6261],
          [0.5617, 0.4369, 0.4007,  ..., 0.4760, 0.5605, 0.5746],
          [0.5955, 0.5679, 0.5467,  ..., 0.3268, 0.4817, 0.3877]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.4407, 0.4763, 0.5196,  ..., 0.4523, 0.4632, 0.4036],
          [0.5315, 0.5765, 0.5593,  ..., 0.3844, 0.4501, 0.4453],
          [0.4668, 0.3730, 0.3522,  ..., 0.4838, 0.5291, 0.5913],
          [0.6035, 0.6100, 0.4254,  ..., 0.3794, 0.4969, 0.3217]],

         [[0.4651, 0.6215, 0.4716,  ..., 0.4220, 0.4984, 0.4378],
          [0.3603, 0.5946, 0.5770,  ..., 0.5118, 0.5818, 0.4737],
          [0.6011, 0.4839, 0.2975,  ..., 0.4762, 0.4598, 0.5927],
          [0.5569, 0.5804, 0.5610,  ..., 0.4220, 0.5467, 0.4330]],

         [[0.4900, 0.4059, 0.4726,  ..., 0.2697, 0.4321, 0.6156],
          [0.3621, 0.4976, 0.6063,  ..., 0.4359, 0.4235, 0.3217],
          [0.4625, 0.5808, 0.4745,  ..., 0.4620, 0.5513, 0.5588],
          [0.4158, 0.5631, 0.4306,  ..., 0.3730, 0.5387, 0.3937]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([ 0.0010, -0.0010,  0.0030,  0.0090, -0.0110,  0.0150,  0.0090,  0.0130,
        -0.0030,  0.0130], device='cuda:0')
selected experts tensor([1756, 1663, 1995, 1650, 1608, 1169, 1912, 1009, 1965, 1657],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1546., 1708., 1684., 1627., 1634., 1650., 1556., 1685., 1629., 1665.],
        [1756., 1663., 1995., 1650., 1608., 1169., 1912., 1009., 1965., 1657.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5722, 0.3915, 0.6076,  ..., 0.6077, 0.5752, 0.6300],
          [0.3979, 0.4950, 0.5686,  ..., 0.5443, 0.5834, 0.5011],
          [0.4576, 0.5186, 0.4193,  ..., 0.4738, 0.4977, 0.4913],
          [0.6983, 0.6271, 0.4560,  ..., 0.5518, 0.3898, 0.5733]],

         [[0.6679, 0.4567, 0.5000,  ..., 0.6044, 0.5069, 0.4480],
          [0.3683, 0.5206, 0.5619,  ..., 0.4129, 0.5781, 0.4217],
          [0.4412, 0.5598, 0.4340,  ..., 0.4797, 0.4797, 0.4803],
          [0.6113, 0.4856, 0.3215,  ..., 0.5229, 0.4217, 0.4165]],

         [[0.4738, 0.3740, 0.5927,  ..., 0.3754, 0.5876, 0.4207],
          [0.3904, 0.4415, 0.5269,  ..., 0.6377, 0.5719, 0.5546],
          [0.5992, 0.5828, 0.5786,  ..., 0.5462, 0.4444, 0.5848],
          [0.6011, 0.4846, 0.6381,  ..., 0.4348, 0.6310, 0.5204]],

         ...,

         [[0.4940, 0.4728, 0.4577,  ..., 0.4511, 0.5279, 0.5417],
          [0.5227, 0.5228, 0.4146,  ..., 0.4419, 0.5366, 0.3437],
          [0.5983, 0.5946, 0.6696,  ..., 0.5792, 0.5082, 0.5488],
          [0.5559, 0.4901, 0.5162,  ..., 0.7224, 0.4056, 0.6753]],

         [[0.3710, 0.4693, 0.5748,  ..., 0.4976, 0.5909, 0.5273],
          [0.5732, 0.5605, 0.6363,  ..., 0.3970, 0.4403, 0.5572],
          [0.3853, 0.5282, 0.3416,  ..., 0.4607, 0.5928, 0.5776],
          [0.5116, 0.4480, 0.3542,  ..., 0.3312, 0.4331, 0.4667]],

         [[0.4296, 0.6230, 0.5317,  ..., 0.5262, 0.4384, 0.5215],
          [0.5926, 0.4581, 0.3909,  ..., 0.4381, 0.4265, 0.3723],
          [0.5428, 0.5951, 0.3858,  ..., 0.5769, 0.4198, 0.5149],
          [0.3114, 0.3595, 0.5266,  ..., 0.4162, 0.5393, 0.4546]]],


        [[[0.5113, 0.4538, 0.5513,  ..., 0.5675, 0.4497, 0.2689],
          [0.4182, 0.4307, 0.5034,  ..., 0.4646, 0.4375, 0.3972],
          [0.4035, 0.6077, 0.4686,  ..., 0.4924, 0.4728, 0.3888],
          [0.4921, 0.5023, 0.4993,  ..., 0.5907, 0.5671, 0.4099]],

         [[0.4684, 0.6344, 0.4967,  ..., 0.4499, 0.5087, 0.4408],
          [0.3674, 0.5380, 0.5070,  ..., 0.4405, 0.5086, 0.3796],
          [0.3738, 0.4088, 0.6696,  ..., 0.5216, 0.4582, 0.5524],
          [0.5573, 0.5261, 0.6273,  ..., 0.4381, 0.6264, 0.4288]],

         [[0.4503, 0.4121, 0.5298,  ..., 0.4357, 0.3879, 0.4592],
          [0.4330, 0.4126, 0.5279,  ..., 0.6458, 0.4972, 0.6710],
          [0.4824, 0.6193, 0.5605,  ..., 0.5404, 0.4788, 0.5577],
          [0.5978, 0.6317, 0.4475,  ..., 0.5997, 0.5524, 0.6675]],

         ...,

         [[0.4234, 0.5114, 0.5431,  ..., 0.4059, 0.6013, 0.5319],
          [0.6583, 0.6073, 0.4472,  ..., 0.4615, 0.4080, 0.5106],
          [0.4450, 0.4863, 0.4344,  ..., 0.5404, 0.5029, 0.4912],
          [0.4820, 0.4681, 0.6200,  ..., 0.5520, 0.5093, 0.5483]],

         [[0.5053, 0.5412, 0.6671,  ..., 0.5039, 0.4580, 0.3046],
          [0.5409, 0.5217, 0.6255,  ..., 0.3574, 0.5347, 0.6427],
          [0.5108, 0.5591, 0.4807,  ..., 0.5293, 0.6632, 0.4188],
          [0.5042, 0.4985, 0.3756,  ..., 0.5503, 0.5924, 0.5947]],

         [[0.3793, 0.4895, 0.6062,  ..., 0.4764, 0.4798, 0.5502],
          [0.6807, 0.5477, 0.2856,  ..., 0.3673, 0.3642, 0.5280],
          [0.4373, 0.4307, 0.4788,  ..., 0.4963, 0.4672, 0.5206],
          [0.4306, 0.4598, 0.5061,  ..., 0.5916, 0.4449, 0.4684]]]],
       device='cuda:0')
tensor([[[[0.5732, 0.3905, 0.6146,  ..., 0.6007, 0.5722, 0.6270],
          [0.3989, 0.4940, 0.5756,  ..., 0.5373, 0.5804, 0.4981],
          [0.4586, 0.5176, 0.4263,  ..., 0.4668, 0.4947, 0.4883],
          [0.6993, 0.6261, 0.4630,  ..., 0.5448, 0.3868, 0.5703]],

         [[0.6689, 0.4557, 0.5070,  ..., 0.5974, 0.5039, 0.4450],
          [0.3693, 0.5196, 0.5689,  ..., 0.4059, 0.5751, 0.4187],
          [0.4422, 0.5588, 0.4410,  ..., 0.4727, 0.4767, 0.4773],
          [0.6123, 0.4846, 0.3285,  ..., 0.5159, 0.4187, 0.4135]],

         [[0.4748, 0.3730, 0.5997,  ..., 0.3684, 0.5846, 0.4177],
          [0.3914, 0.4405, 0.5339,  ..., 0.6307, 0.5689, 0.5516],
          [0.6002, 0.5818, 0.5856,  ..., 0.5392, 0.4414, 0.5818],
          [0.6021, 0.4836, 0.6451,  ..., 0.4278, 0.6280, 0.5174]],

         ...,

         [[0.4950, 0.4718, 0.4647,  ..., 0.4441, 0.5249, 0.5387],
          [0.5237, 0.5218, 0.4216,  ..., 0.4349, 0.5336, 0.3407],
          [0.5993, 0.5936, 0.6766,  ..., 0.5722, 0.5052, 0.5458],
          [0.5569, 0.4891, 0.5232,  ..., 0.7154, 0.4026, 0.6723]],

         [[0.3720, 0.4683, 0.5818,  ..., 0.4906, 0.5879, 0.5243],
          [0.5742, 0.5595, 0.6433,  ..., 0.3900, 0.4373, 0.5542],
          [0.3863, 0.5272, 0.3486,  ..., 0.4537, 0.5898, 0.5746],
          [0.5126, 0.4470, 0.3612,  ..., 0.3242, 0.4301, 0.4637]],

         [[0.4306, 0.6220, 0.5387,  ..., 0.5192, 0.4354, 0.5185],
          [0.5936, 0.4571, 0.3979,  ..., 0.4311, 0.4235, 0.3693],
          [0.5438, 0.5941, 0.3928,  ..., 0.5699, 0.4168, 0.5119],
          [0.3124, 0.3585, 0.5336,  ..., 0.4092, 0.5363, 0.4516]]],


        [[[0.5123, 0.4528, 0.5583,  ..., 0.5605, 0.4467, 0.2659],
          [0.4192, 0.4297, 0.5104,  ..., 0.4576, 0.4345, 0.3942],
          [0.4045, 0.6067, 0.4756,  ..., 0.4854, 0.4698, 0.3858],
          [0.4931, 0.5013, 0.5063,  ..., 0.5837, 0.5641, 0.4069]],

         [[0.4694, 0.6334, 0.5037,  ..., 0.4429, 0.5057, 0.4378],
          [0.3684, 0.5370, 0.5140,  ..., 0.4335, 0.5056, 0.3766],
          [0.3748, 0.4078, 0.6766,  ..., 0.5146, 0.4552, 0.5494],
          [0.5583, 0.5251, 0.6343,  ..., 0.4311, 0.6234, 0.4258]],

         [[0.4513, 0.4111, 0.5368,  ..., 0.4287, 0.3849, 0.4562],
          [0.4340, 0.4116, 0.5349,  ..., 0.6388, 0.4942, 0.6680],
          [0.4834, 0.6183, 0.5675,  ..., 0.5334, 0.4758, 0.5547],
          [0.5988, 0.6307, 0.4545,  ..., 0.5927, 0.5494, 0.6645]],

         ...,

         [[0.4244, 0.5104, 0.5501,  ..., 0.3989, 0.5983, 0.5289],
          [0.6593, 0.6063, 0.4542,  ..., 0.4545, 0.4050, 0.5076],
          [0.4460, 0.4853, 0.4414,  ..., 0.5334, 0.4999, 0.4882],
          [0.4830, 0.4671, 0.6270,  ..., 0.5450, 0.5063, 0.5453]],

         [[0.5063, 0.5402, 0.6741,  ..., 0.4969, 0.4550, 0.3016],
          [0.5419, 0.5207, 0.6325,  ..., 0.3504, 0.5317, 0.6397],
          [0.5118, 0.5581, 0.4877,  ..., 0.5223, 0.6602, 0.4158],
          [0.5052, 0.4975, 0.3826,  ..., 0.5433, 0.5894, 0.5917]],

         [[0.3803, 0.4885, 0.6132,  ..., 0.4694, 0.4768, 0.5472],
          [0.6817, 0.5467, 0.2926,  ..., 0.3603, 0.3612, 0.5250],
          [0.4383, 0.4297, 0.4858,  ..., 0.4893, 0.4642, 0.5176],
          [0.4316, 0.4588, 0.5131,  ..., 0.5846, 0.4419, 0.4654]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0010,  0.0010, -0.0070,  0.0030, -0.0030, -0.0050,  0.0070,  0.0070,
         0.0030,  0.0030], device='cuda:0')
selected experts tensor([1636, 1518, 1637, 1694, 1699, 1645, 1631, 1704, 1684, 1536],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5474, 0.5581, 0.4915,  ..., 0.3209, 0.5384, 0.4297],
          [0.5660, 0.5441, 0.5335,  ..., 0.4439, 0.4963, 0.5017],
          [0.6242, 0.5210, 0.4674,  ..., 0.5131, 0.6625, 0.3365],
          [0.6122, 0.5405, 0.5575,  ..., 0.4640, 0.4957, 0.3985]],

         [[0.5184, 0.6237, 0.3324,  ..., 0.4194, 0.4204, 0.6308],
          [0.4544, 0.3408, 0.4893,  ..., 0.5110, 0.4525, 0.4931],
          [0.5893, 0.4041, 0.3995,  ..., 0.4471, 0.5029, 0.4292],
          [0.3593, 0.6584, 0.3935,  ..., 0.3929, 0.6127, 0.5346]],

         [[0.5491, 0.5220, 0.4485,  ..., 0.4042, 0.5749, 0.5465],
          [0.4984, 0.4270, 0.5994,  ..., 0.4037, 0.4104, 0.5584],
          [0.6900, 0.3497, 0.4733,  ..., 0.4146, 0.5165, 0.5617],
          [0.5260, 0.4853, 0.4961,  ..., 0.4922, 0.4171, 0.4710]],

         ...,

         [[0.4840, 0.4726, 0.4389,  ..., 0.3580, 0.6402, 0.4579],
          [0.4755, 0.3923, 0.3446,  ..., 0.4295, 0.3825, 0.3235],
          [0.4969, 0.5951, 0.4538,  ..., 0.3107, 0.3780, 0.4432],
          [0.5121, 0.4788, 0.4949,  ..., 0.3032, 0.5198, 0.4934]],

         [[0.5015, 0.5407, 0.4563,  ..., 0.3463, 0.4709, 0.4465],
          [0.6224, 0.4955, 0.4989,  ..., 0.4275, 0.5249, 0.4925],
          [0.4035, 0.4996, 0.3472,  ..., 0.5124, 0.4199, 0.4924],
          [0.5751, 0.4779, 0.4061,  ..., 0.4643, 0.7437, 0.3496]],

         [[0.3969, 0.3774, 0.5410,  ..., 0.4485, 0.4672, 0.5431],
          [0.3485, 0.4342, 0.4989,  ..., 0.5615, 0.4955, 0.4691],
          [0.5484, 0.4511, 0.6499,  ..., 0.4929, 0.5338, 0.5412],
          [0.4263, 0.4817, 0.5630,  ..., 0.4237, 0.5488, 0.5904]]],


        [[[0.4552, 0.5378, 0.3805,  ..., 0.5672, 0.5255, 0.5670],
          [0.4030, 0.5262, 0.4664,  ..., 0.5252, 0.6311, 0.5646],
          [0.4861, 0.6086, 0.5695,  ..., 0.4500, 0.3581, 0.5956],
          [0.4713, 0.5520, 0.6273,  ..., 0.5318, 0.5113, 0.5283]],

         [[0.4714, 0.5762, 0.6130,  ..., 0.5301, 0.5293, 0.5482],
          [0.5879, 0.4623, 0.5079,  ..., 0.5400, 0.6252, 0.6198],
          [0.4163, 0.5609, 0.5181,  ..., 0.4446, 0.6145, 0.7442],
          [0.5486, 0.3891, 0.6107,  ..., 0.4987, 0.4915, 0.4465]],

         [[0.5508, 0.4429, 0.4274,  ..., 0.5806, 0.6859, 0.4311],
          [0.4363, 0.5356, 0.4312,  ..., 0.3716, 0.5445, 0.4126],
          [0.4915, 0.6053, 0.4874,  ..., 0.4328, 0.5351, 0.4178],
          [0.5765, 0.5210, 0.5331,  ..., 0.5161, 0.3689, 0.4806]],

         ...,

         [[0.5674, 0.4294, 0.4890,  ..., 0.5070, 0.6429, 0.4055],
          [0.5846, 0.5705, 0.5560,  ..., 0.3296, 0.5354, 0.5249],
          [0.4416, 0.5249, 0.5442,  ..., 0.4052, 0.5265, 0.4331],
          [0.4253, 0.5371, 0.4555,  ..., 0.4247, 0.3653, 0.5709]],

         [[0.4870, 0.5119, 0.5362,  ..., 0.5725, 0.4950, 0.5494],
          [0.4201, 0.6155, 0.6121,  ..., 0.5444, 0.4529, 0.3896],
          [0.6333, 0.5109, 0.4179,  ..., 0.5787, 0.4754, 0.5441],
          [0.4111, 0.4340, 0.5315,  ..., 0.5116, 0.4455, 0.6198]],

         [[0.5770, 0.5629, 0.3865,  ..., 0.5739, 0.5217, 0.6068],
          [0.6670, 0.3956, 0.3741,  ..., 0.5429, 0.6187, 0.3999],
          [0.5074, 0.4212, 0.5198,  ..., 0.5076, 0.5575, 0.4630],
          [0.5698, 0.6345, 0.4674,  ..., 0.6239, 0.4944, 0.3622]]]],
       device='cuda:0')
tensor([[[[0.5484, 0.5651, 0.4885,  ..., 0.3259, 0.5334, 0.4287],
          [0.5670, 0.5511, 0.5305,  ..., 0.4489, 0.4913, 0.5007],
          [0.6252, 0.5280, 0.4644,  ..., 0.5181, 0.6575, 0.3355],
          [0.6132, 0.5475, 0.5545,  ..., 0.4690, 0.4907, 0.3975]],

         [[0.5194, 0.6307, 0.3294,  ..., 0.4244, 0.4154, 0.6298],
          [0.4554, 0.3478, 0.4863,  ..., 0.5160, 0.4475, 0.4921],
          [0.5903, 0.4111, 0.3965,  ..., 0.4521, 0.4979, 0.4282],
          [0.3603, 0.6654, 0.3905,  ..., 0.3979, 0.6077, 0.5336]],

         [[0.5501, 0.5290, 0.4455,  ..., 0.4092, 0.5699, 0.5455],
          [0.4994, 0.4340, 0.5964,  ..., 0.4087, 0.4054, 0.5574],
          [0.6910, 0.3567, 0.4703,  ..., 0.4196, 0.5115, 0.5607],
          [0.5270, 0.4923, 0.4931,  ..., 0.4972, 0.4121, 0.4700]],

         ...,

         [[0.4850, 0.4796, 0.4359,  ..., 0.3630, 0.6352, 0.4569],
          [0.4765, 0.3993, 0.3416,  ..., 0.4345, 0.3775, 0.3225],
          [0.4979, 0.6021, 0.4508,  ..., 0.3157, 0.3730, 0.4422],
          [0.5131, 0.4858, 0.4919,  ..., 0.3082, 0.5148, 0.4924]],

         [[0.5025, 0.5477, 0.4533,  ..., 0.3513, 0.4659, 0.4455],
          [0.6234, 0.5025, 0.4959,  ..., 0.4325, 0.5199, 0.4915],
          [0.4045, 0.5066, 0.3442,  ..., 0.5174, 0.4149, 0.4914],
          [0.5761, 0.4849, 0.4031,  ..., 0.4693, 0.7387, 0.3486]],

         [[0.3979, 0.3844, 0.5380,  ..., 0.4535, 0.4622, 0.5421],
          [0.3495, 0.4412, 0.4959,  ..., 0.5665, 0.4905, 0.4681],
          [0.5494, 0.4581, 0.6469,  ..., 0.4979, 0.5288, 0.5402],
          [0.4273, 0.4887, 0.5600,  ..., 0.4287, 0.5438, 0.5894]]],


        [[[0.4562, 0.5448, 0.3775,  ..., 0.5722, 0.5205, 0.5660],
          [0.4040, 0.5332, 0.4634,  ..., 0.5302, 0.6261, 0.5636],
          [0.4871, 0.6156, 0.5665,  ..., 0.4550, 0.3531, 0.5946],
          [0.4723, 0.5590, 0.6243,  ..., 0.5368, 0.5063, 0.5273]],

         [[0.4724, 0.5832, 0.6100,  ..., 0.5351, 0.5243, 0.5472],
          [0.5889, 0.4693, 0.5049,  ..., 0.5450, 0.6202, 0.6188],
          [0.4173, 0.5679, 0.5151,  ..., 0.4496, 0.6095, 0.7432],
          [0.5496, 0.3961, 0.6077,  ..., 0.5037, 0.4865, 0.4455]],

         [[0.5518, 0.4499, 0.4244,  ..., 0.5856, 0.6809, 0.4301],
          [0.4373, 0.5426, 0.4282,  ..., 0.3766, 0.5395, 0.4116],
          [0.4925, 0.6123, 0.4844,  ..., 0.4378, 0.5301, 0.4168],
          [0.5775, 0.5280, 0.5301,  ..., 0.5211, 0.3639, 0.4796]],

         ...,

         [[0.5684, 0.4364, 0.4860,  ..., 0.5120, 0.6379, 0.4045],
          [0.5856, 0.5775, 0.5530,  ..., 0.3346, 0.5304, 0.5239],
          [0.4426, 0.5319, 0.5412,  ..., 0.4102, 0.5215, 0.4321],
          [0.4263, 0.5441, 0.4525,  ..., 0.4297, 0.3603, 0.5699]],

         [[0.4880, 0.5189, 0.5332,  ..., 0.5775, 0.4900, 0.5484],
          [0.4211, 0.6225, 0.6091,  ..., 0.5494, 0.4479, 0.3886],
          [0.6343, 0.5179, 0.4149,  ..., 0.5837, 0.4704, 0.5431],
          [0.4121, 0.4410, 0.5285,  ..., 0.5166, 0.4405, 0.6188]],

         [[0.5780, 0.5699, 0.3835,  ..., 0.5789, 0.5167, 0.6058],
          [0.6680, 0.4026, 0.3711,  ..., 0.5479, 0.6137, 0.3989],
          [0.5084, 0.4282, 0.5168,  ..., 0.5126, 0.5525, 0.4620],
          [0.5708, 0.6415, 0.4644,  ..., 0.6289, 0.4894, 0.3612]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0010, -0.0070,  0.0030,  0.0110, -0.0010,  0.0070, -0.0130, -0.0050,
         0.0050,  0.0010], device='cuda:0')
selected experts tensor([1636, 1680, 1601, 1686, 1700, 1584, 1511, 1675, 1685, 1626],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4103, 0.4100, 0.4475,  ..., 0.5487, 0.4904, 0.6185],
          [0.3882, 0.5013, 0.5458,  ..., 0.4501, 0.5250, 0.4836],
          [0.5155, 0.4998, 0.5708,  ..., 0.4395, 0.4391, 0.4476],
          [0.5729, 0.6560, 0.4163,  ..., 0.6016, 0.5430, 0.4673]],

         [[0.5141, 0.6429, 0.4683,  ..., 0.3702, 0.6050, 0.4656],
          [0.4422, 0.5068, 0.5438,  ..., 0.6316, 0.5106, 0.4471],
          [0.4941, 0.5971, 0.5823,  ..., 0.4073, 0.3666, 0.5001],
          [0.4579, 0.4311, 0.3775,  ..., 0.5615, 0.4597, 0.5173]],

         [[0.5117, 0.5858, 0.5283,  ..., 0.6243, 0.4628, 0.5131],
          [0.3803, 0.4984, 0.4475,  ..., 0.4824, 0.5504, 0.5606],
          [0.4526, 0.4818, 0.4622,  ..., 0.5941, 0.5048, 0.3736],
          [0.5586, 0.5901, 0.5969,  ..., 0.5533, 0.4515, 0.3709]],

         ...,

         [[0.3688, 0.4699, 0.4477,  ..., 0.3303, 0.4305, 0.4727],
          [0.5133, 0.6482, 0.6114,  ..., 0.4999, 0.6158, 0.5775],
          [0.5014, 0.4186, 0.4121,  ..., 0.6053, 0.4338, 0.4414],
          [0.4184, 0.4220, 0.4960,  ..., 0.4627, 0.3988, 0.5261]],

         [[0.5956, 0.3939, 0.5078,  ..., 0.3905, 0.5363, 0.5177],
          [0.4608, 0.4632, 0.5060,  ..., 0.4987, 0.6069, 0.5746],
          [0.5244, 0.3864, 0.4475,  ..., 0.5660, 0.5380, 0.5482],
          [0.4861, 0.5154, 0.4924,  ..., 0.5137, 0.4957, 0.4018]],

         [[0.5175, 0.5003, 0.5353,  ..., 0.4504, 0.5679, 0.5892],
          [0.4256, 0.6646, 0.6118,  ..., 0.5179, 0.6372, 0.5543],
          [0.4949, 0.4234, 0.4996,  ..., 0.4980, 0.4783, 0.4533],
          [0.4838, 0.5499, 0.5051,  ..., 0.5562, 0.4819, 0.3844]]],


        [[[0.5014, 0.6003, 0.4187,  ..., 0.4997, 0.4263, 0.5770],
          [0.5511, 0.5318, 0.4710,  ..., 0.6030, 0.5440, 0.4318],
          [0.4487, 0.5858, 0.5453,  ..., 0.4673, 0.4501, 0.4990],
          [0.5615, 0.3683, 0.3993,  ..., 0.5164, 0.4315, 0.5051]],

         [[0.5087, 0.4752, 0.4836,  ..., 0.4354, 0.4477, 0.6403],
          [0.3994, 0.5868, 0.5373,  ..., 0.4712, 0.5941, 0.5533],
          [0.4842, 0.4772, 0.6151,  ..., 0.5267, 0.5131, 0.4881],
          [0.5361, 0.5722, 0.4499,  ..., 0.5341, 0.4780, 0.3844]],

         [[0.5129, 0.6402, 0.4564,  ..., 0.6109, 0.5720, 0.5550],
          [0.4170, 0.6205, 0.4325,  ..., 0.5272, 0.5316, 0.4627],
          [0.4364, 0.5335, 0.4918,  ..., 0.4654, 0.3810, 0.4841],
          [0.6603, 0.5338, 0.4487,  ..., 0.5492, 0.5613, 0.3772]],

         ...,

         [[0.4570, 0.5388, 0.3794,  ..., 0.5170, 0.5754, 0.7019],
          [0.3365, 0.5391, 0.5433,  ..., 0.5610, 0.5609, 0.4855],
          [0.6049, 0.4005, 0.5159,  ..., 0.6298, 0.5538, 0.4196],
          [0.5672, 0.5664, 0.3928,  ..., 0.4335, 0.6102, 0.5949]],

         [[0.5055, 0.3794, 0.4438,  ..., 0.4111, 0.4419, 0.7306],
          [0.3174, 0.4904, 0.6091,  ..., 0.5170, 0.5229, 0.4856],
          [0.4683, 0.4495, 0.5339,  ..., 0.5651, 0.7098, 0.4452],
          [0.3771, 0.3892, 0.4859,  ..., 0.5223, 0.5314, 0.4046]],

         [[0.5022, 0.5381, 0.5727,  ..., 0.5576, 0.4155, 0.5935],
          [0.3660, 0.5226, 0.5818,  ..., 0.4861, 0.5065, 0.4423],
          [0.4184, 0.3986, 0.3780,  ..., 0.6044, 0.6349, 0.5107],
          [0.3822, 0.3384, 0.5756,  ..., 0.5033, 0.5295, 0.5299]]]],
       device='cuda:0')
tensor([[[[0.4163, 0.4220, 0.4475,  ..., 0.5487, 0.4724, 0.6025],
          [0.3942, 0.5133, 0.5458,  ..., 0.4501, 0.5070, 0.4676],
          [0.5215, 0.5118, 0.5708,  ..., 0.4395, 0.4211, 0.4316],
          [0.5789, 0.6680, 0.4163,  ..., 0.6016, 0.5250, 0.4513]],

         [[0.5201, 0.6549, 0.4683,  ..., 0.3702, 0.5870, 0.4496],
          [0.4482, 0.5188, 0.5438,  ..., 0.6316, 0.4926, 0.4311],
          [0.5001, 0.6091, 0.5823,  ..., 0.4073, 0.3486, 0.4841],
          [0.4639, 0.4431, 0.3775,  ..., 0.5615, 0.4417, 0.5013]],

         [[0.5177, 0.5978, 0.5283,  ..., 0.6243, 0.4448, 0.4971],
          [0.3863, 0.5104, 0.4475,  ..., 0.4824, 0.5324, 0.5446],
          [0.4586, 0.4938, 0.4622,  ..., 0.5941, 0.4868, 0.3576],
          [0.5646, 0.6021, 0.5969,  ..., 0.5533, 0.4335, 0.3549]],

         ...,

         [[0.3748, 0.4819, 0.4477,  ..., 0.3303, 0.4125, 0.4567],
          [0.5193, 0.6602, 0.6114,  ..., 0.4999, 0.5978, 0.5615],
          [0.5074, 0.4306, 0.4121,  ..., 0.6053, 0.4158, 0.4254],
          [0.4244, 0.4340, 0.4960,  ..., 0.4627, 0.3808, 0.5101]],

         [[0.6016, 0.4059, 0.5078,  ..., 0.3905, 0.5183, 0.5017],
          [0.4668, 0.4752, 0.5060,  ..., 0.4987, 0.5889, 0.5586],
          [0.5304, 0.3984, 0.4475,  ..., 0.5660, 0.5200, 0.5322],
          [0.4921, 0.5274, 0.4924,  ..., 0.5137, 0.4777, 0.3858]],

         [[0.5235, 0.5123, 0.5353,  ..., 0.4504, 0.5499, 0.5732],
          [0.4316, 0.6766, 0.6118,  ..., 0.5179, 0.6192, 0.5383],
          [0.5009, 0.4354, 0.4996,  ..., 0.4980, 0.4603, 0.4373],
          [0.4898, 0.5619, 0.5051,  ..., 0.5562, 0.4639, 0.3684]]],


        [[[0.5074, 0.6123, 0.4187,  ..., 0.4997, 0.4083, 0.5610],
          [0.5571, 0.5438, 0.4710,  ..., 0.6030, 0.5260, 0.4158],
          [0.4547, 0.5978, 0.5453,  ..., 0.4673, 0.4321, 0.4830],
          [0.5675, 0.3803, 0.3993,  ..., 0.5164, 0.4135, 0.4891]],

         [[0.5147, 0.4872, 0.4836,  ..., 0.4354, 0.4297, 0.6243],
          [0.4054, 0.5988, 0.5373,  ..., 0.4712, 0.5761, 0.5373],
          [0.4902, 0.4892, 0.6151,  ..., 0.5267, 0.4951, 0.4721],
          [0.5421, 0.5842, 0.4499,  ..., 0.5341, 0.4600, 0.3684]],

         [[0.5189, 0.6522, 0.4564,  ..., 0.6109, 0.5540, 0.5390],
          [0.4230, 0.6325, 0.4325,  ..., 0.5272, 0.5136, 0.4467],
          [0.4424, 0.5455, 0.4918,  ..., 0.4654, 0.3630, 0.4681],
          [0.6663, 0.5458, 0.4487,  ..., 0.5492, 0.5433, 0.3612]],

         ...,

         [[0.4630, 0.5508, 0.3794,  ..., 0.5170, 0.5574, 0.6859],
          [0.3425, 0.5511, 0.5433,  ..., 0.5610, 0.5429, 0.4695],
          [0.6109, 0.4125, 0.5159,  ..., 0.6298, 0.5358, 0.4036],
          [0.5732, 0.5784, 0.3928,  ..., 0.4335, 0.5922, 0.5789]],

         [[0.5115, 0.3914, 0.4438,  ..., 0.4111, 0.4239, 0.7146],
          [0.3234, 0.5024, 0.6091,  ..., 0.5170, 0.5049, 0.4696],
          [0.4743, 0.4615, 0.5339,  ..., 0.5651, 0.6918, 0.4292],
          [0.3831, 0.4012, 0.4859,  ..., 0.5223, 0.5134, 0.3886]],

         [[0.5082, 0.5501, 0.5727,  ..., 0.5576, 0.3975, 0.5775],
          [0.3720, 0.5346, 0.5818,  ..., 0.4861, 0.4885, 0.4263],
          [0.4244, 0.4106, 0.3780,  ..., 0.6044, 0.6169, 0.4947],
          [0.3882, 0.3504, 0.5756,  ..., 0.5033, 0.5115, 0.5139]]]],
       device='cuda:0', requires_grad=True)[batch=30/40]:
	 Train time/batch: 29
	 Train time/sample: 58
	 Train time/batch_in_epoch: 29
	 Train time/sample_in_epoch: 58
	 Train time/token: 59392
	 Train time/token_in_epoch: 59392
	 Train memory/current_allocated_mem: 1.1700
	 Train memory/current_active_mem: 1.1700
	 Train memory/current_inactive_mem: 0.7447
	 Train memory/current_reserved_mem: 3.8294
	 Train memory/peak_allocated_mem: 2.8054
	 Train memory/peak_active_mem: 2.8054
	 Train memory/peak_inactive_mem: 0.8642
	 Train memory/peak_reserved_mem: 3.8294
	 Train memory/alloc_retries: 0
	 Train trainer/device_train_microbatch_size: 10
	 Train loss/train/total: 0.0047
	 Train metrics/train/LanguageCrossEntropy: 9.7241
	 Train metrics/train/LanguagePerplexity: 16715.6738
	 Train metrics/train/TokenAccuracy: 0.1211
	 Train throughput/batches_per_sec: 0.4216
	 Train throughput/samples_per_sec: 0.8432
	 Train throughput/device/batches_per_sec: 0.4216
	 Train throughput/device/samples_per_sec: 0.8432
	 Train throughput/tokens_per_sec: 863.3920
	 Train throughput/device/tokens_per_sec: 863.3920
	 Train time/train: 0.0277
	 Train time/val: 0.0000
	 Train time/total: 0.0277
	 Train lr-DecoupledAdamW/group0: 0.0000
	 Train time/remaining_estimate: 0.0074
	 Train metrics/shannon_entropy: 10.6107
	 Train metrics/batch_shannon_entropy: <wandb.sdk.data_types.table.Table object at 0x77b707caac90>
	 Train metrics/seq_shannon_entropy: CustomChart(table=<wandb.sdk.data_types.table.Table object at 0x77b9a191b500>, spec=CustomChartSpec(spec_name='wandb/lineseries/v0', fields={'step': 'step', 'lineKey': 'lineKey', 'lineVal': 'lineVal'}, string_fields={'title': 'Per-Layer Shannon Entropy', 'xname': 'Block Index'}, key='', panel_type='Vega2', split_table=False))
	 Train metrics/exit_entropy: 0.6641
	 Train metrics/batch_exit_entropy: <wandb.sdk.data_types.table.Table object at 0x77b9a1b2f8f0>
	 Train metrics/seq_exit_entropy: CustomChart(table=<wandb.sdk.data_types.table.Table object at 0x77b99b11a8a0>, spec=CustomChartSpec(spec_name='wandb/lineseries/v0', fields={'step': 'step', 'lineKey': 'lineKey', 'lineVal': 'lineVal'}, string_fields={'title': 'Per-Layer Exit Entropy', 'xname': 'Block Index'}, key='', panel_type='Vega2', split_table=False))
	 Train expert_selection/ffn_layer: <wandb.sdk.data_types.image.Image object at 0x77b99f977440>
	 Train expert_selection/attn_o_layer: <wandb.sdk.data_types.image.Image object at 0x77b99f974170>
	 Train expert_selection/attn_v_layer: <wandb.sdk.data_types.image.Image object at 0x77b99f8d5d00>
	 Train l2_norm/moment/model.transformer.router: 0.0000
	 Train l2_norm/param/model.transformer.router: 0.3616
	 Train l2_norm/update/model.transformer.router: 0.0007
	 Train l2_norm/grad/model.transformer.router: 0.0001
	 Train l2_norm/moment/model.transformer.tau: 0.0000
	 Train l2_norm/param/model.transformer.tau: 1.0005
	 Train l2_norm/update/model.transformer.tau: 0.0000
	 Train l2_norm/grad/model.transformer.tau: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.attention.v: 0.0003
	 Train l2_norm/param/model.transformer.layers.0.attention.v: 20.4800
	 Train l2_norm/update/model.transformer.layers.0.attention.v: 0.0148
	 Train l2_norm/grad/model.transformer.layers.0.attention.v: 0.0004
	 Train l2_norm/moment/model.transformer.layers.0.attention.o: 0.0004
	 Train l2_norm/param/model.transformer.layers.0.attention.o: 22.6877
	 Train l2_norm/update/model.transformer.layers.0.attention.o: 0.0184
	 Train l2_norm/grad/model.transformer.layers.0.attention.o: 0.0004
	 Train l2_norm/moment/model.transformer.layers.0.attention.sel_v: 0.0001
	 Train l2_norm/param/model.transformer.layers.0.attention.sel_v: 2.2599
	 Train l2_norm/update/model.transformer.layers.0.attention.sel_v: 0.0011
	 Train l2_norm/grad/model.transformer.layers.0.attention.sel_v: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.attention.sel_o: 0.0001
	 Train l2_norm/param/model.transformer.layers.0.attention.sel_o: 2.2418
	 Train l2_norm/update/model.transformer.layers.0.attention.sel_o: 0.0013
	 Train l2_norm/grad/model.transformer.layers.0.attention.sel_o: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.attention.q.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attention.q.weight: 6.4841
	 Train l2_norm/update/model.transformer.layers.0.attention.q.weight: 0.0059
	 Train l2_norm/grad/model.transformer.layers.0.attention.q.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.attention.k.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attention.k.weight: 6.4746
	 Train l2_norm/update/model.transformer.layers.0.attention.k.weight: 0.0059
	 Train l2_norm/grad/model.transformer.layers.0.attention.k.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.ffn.keys: 0.0003
	 Train l2_norm/param/model.transformer.layers.0.ffn.keys: 14.9995
	 Train l2_norm/update/model.transformer.layers.0.ffn.keys: 0.0145
	 Train l2_norm/grad/model.transformer.layers.0.ffn.keys: 0.0003
	 Train l2_norm/moment/model.transformer.layers.0.ffn.values: 0.0006
	 Train l2_norm/param/model.transformer.layers.0.ffn.values: 7.1799
	 Train l2_norm/update/model.transformer.layers.0.ffn.values: 0.0138
	 Train l2_norm/grad/model.transformer.layers.0.ffn.values: 0.0008
	 Train l2_norm/moment/model.transformer.layers.0.ffn.expert_sel: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.ffn.expert_sel: 4.7499
	 Train l2_norm/update/model.transformer.layers.0.ffn.expert_sel: 0.0047
	 Train l2_norm/grad/model.transformer.layers.0.ffn.expert_sel: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.attn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attn_pre.weight: 20.2976
	 Train l2_norm/update/model.transformer.layers.0.attn_pre.weight: 0.0003
	 Train l2_norm/grad/model.transformer.layers.0.attn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.attn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attn_post.weight: 20.2975
	 Train l2_norm/update/model.transformer.layers.0.attn_post.weight: 0.0004
	 Train l2_norm/grad/model.transformer.layers.0.attn_post.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.ffn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.ffn_pre.weight: 20.2976
	 Train l2_norm/update/model.transformer.layers.0.ffn_pre.weight: 0.0003
	 Train l2_norm/grad/model.transformer.layers.0.ffn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.ffn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.ffn_post.weight: 20.2994
	 Train l2_norm/update/model.transformer.layers.0.ffn_post.weight: 0.0004
	 Train l2_norm/grad/model.transformer.layers.0.ffn_post.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.v: 0.0002
	 Train l2_norm/param/model.transformer.layers.1.attention.v: 20.4919
	 Train l2_norm/update/model.transformer.layers.1.attention.v: 0.0151
	 Train l2_norm/grad/model.transformer.layers.1.attention.v: 0.0003
	 Train l2_norm/moment/model.transformer.layers.1.attention.o: 0.0002
	 Train l2_norm/param/model.transformer.layers.1.attention.o: 22.6909
	 Train l2_norm/update/model.transformer.layers.1.attention.o: 0.0167
	 Train l2_norm/grad/model.transformer.layers.1.attention.o: 0.0003
	 Train l2_norm/moment/model.transformer.layers.1.attention.sel_v: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.sel_v: 2.2543
	 Train l2_norm/update/model.transformer.layers.1.attention.sel_v: 0.0010
	 Train l2_norm/grad/model.transformer.layers.1.attention.sel_v: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.sel_o: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.sel_o: 2.2406
	 Train l2_norm/update/model.transformer.layers.1.attention.sel_o: 0.0010
	 Train l2_norm/grad/model.transformer.layers.1.attention.sel_o: 0.0001
	 Train l2_norm/moment/model.transformer.layers.1.attention.q.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.q.weight: 6.4906
	 Train l2_norm/update/model.transformer.layers.1.attention.q.weight: 0.0030
	 Train l2_norm/grad/model.transformer.layers.1.attention.q.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.k.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.k.weight: 6.4854
	 Train l2_norm/update/model.transformer.layers.1.attention.k.weight: 0.0030
	 Train l2_norm/grad/model.transformer.layers.1.attention.k.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn.keys: 0.0002
	 Train l2_norm/param/model.transformer.layers.1.ffn.keys: 15.0044
	 Train l2_norm/update/model.transformer.layers.1.ffn.keys: 0.0135
	 Train l2_norm/grad/model.transformer.layers.1.ffn.keys: 0.0003
	 Train l2_norm/moment/model.transformer.layers.1.ffn.values: 0.0006
	 Train l2_norm/param/model.transformer.layers.1.ffn.values: 7.1798
	 Train l2_norm/update/model.transformer.layers.1.ffn.values: 0.0158
	 Train l2_norm/grad/model.transformer.layers.1.ffn.values: 0.0006
	 Train l2_norm/moment/model.transformer.layers.1.ffn.expert_sel: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.ffn.expert_sel: 4.7525
	 Train l2_norm/update/model.transformer.layers.1.ffn.expert_sel: 0.0042
	 Train l2_norm/grad/model.transformer.layers.1.ffn.expert_sel: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attn_pre.weight: 20.2976
	 Train l2_norm/update/model.transformer.layers.1.attn_pre.weight: 0.0003
	 Train l2_norm/grad/model.transformer.layers.1.attn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attn_post.weight: 20.2956
	 Train l2_norm/update/model.transformer.layers.1.attn_post.weight: 0.0004
	 Train l2_norm/grad/model.transformer.layers.1.attn_post.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.ffn_pre.weight: 20.2974
	 Train l2_norm/update/model.transformer.layers.1.ffn_pre.weight: 0.0003
	 Train l2_norm/grad/model.transformer.layers.1.ffn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.ffn_post.weight: 20.2977
	 Train l2_norm/update/model.transformer.layers.1.ffn_post.weight: 0.0004
	 Train l2_norm/grad/model.transformer.layers.1.ffn_post.weight: 0.0000
	 Train l2_norm/moment/model.embedding.weight: 0.0001
	 Train l2_norm/param/model.embedding.weight: 221.7073
	 Train l2_norm/update/model.embedding.weight: 0.0112
	 Train l2_norm/grad/model.embedding.weight: 0.0002
	 Train l2_norm/moment/model.lm_head.weight: 0.0006
	 Train l2_norm/param/model.lm_head.weight: 127.9729
	 Train l2_norm/update/model.lm_head.weight: 0.0504
	 Train l2_norm/grad/model.lm_head.weight: 0.0008
	 Train l2_norm/moment/model.lm_head.bias: 0.0000
	 Train l2_norm/param/model.lm_head.bias: 6.3162
	 Train l2_norm/update/model.lm_head.bias: 0.0037
	 Train l2_norm/grad/model.lm_head.bias: 0.0000
	 Train l2_norm/moment/model.out_norm.weight: 0.0000
	 Train l2_norm/param/model.out_norm.weight: 20.2990
	 Train l2_norm/update/model.out_norm.weight: 0.0005
	 Train l2_norm/grad/model.out_norm.weight: 0.0000
	 Train l2_norm/grad/global: 0.0015
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([-6.0000e-03, -1.2000e-02,  6.9849e-10, -8.0000e-03, -1.2000e-02,
         2.0000e-02, -2.8000e-02,  2.3283e-10,  1.8000e-02,  1.6000e-02],
       device='cuda:0')
selected experts tensor([1429, 1361, 1402, 2323, 1183, 2145, 1690, 1701, 1555, 1595],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4961, 0.4927, 0.4955,  ..., 0.3066, 0.4062, 0.4474],
          [0.3951, 0.4736, 0.6778,  ..., 0.5373, 0.4529, 0.3194],
          [0.5178, 0.4344, 0.5332,  ..., 0.3698, 0.6175, 0.4626],
          [0.4969, 0.4559, 0.5880,  ..., 0.4289, 0.6013, 0.4510]],

         [[0.4642, 0.4983, 0.4705,  ..., 0.5451, 0.4214, 0.4057],
          [0.4790, 0.5996, 0.5843,  ..., 0.5098, 0.6911, 0.5943],
          [0.6081, 0.3903, 0.4659,  ..., 0.6429, 0.6895, 0.6206],
          [0.6388, 0.5237, 0.5085,  ..., 0.4313, 0.5029, 0.3868]],

         [[0.4784, 0.5755, 0.4938,  ..., 0.3281, 0.4653, 0.5230],
          [0.3923, 0.5236, 0.5627,  ..., 0.3451, 0.4252, 0.3589],
          [0.5518, 0.4617, 0.4792,  ..., 0.4622, 0.4218, 0.5995],
          [0.5392, 0.6314, 0.5330,  ..., 0.4736, 0.5030, 0.4350]],

         ...,

         [[0.5000, 0.4980, 0.5020,  ..., 0.5140, 0.4960, 0.5120],
          [0.5000, 0.4980, 0.5020,  ..., 0.5140, 0.4960, 0.5120],
          [0.5000, 0.4980, 0.5020,  ..., 0.5140, 0.4960, 0.5120],
          [0.5000, 0.4980, 0.5020,  ..., 0.5140, 0.4960, 0.5120]],

         [[0.5000, 0.4980, 0.5020,  ..., 0.5140, 0.4960, 0.5120],
          [0.5000, 0.4980, 0.5020,  ..., 0.5140, 0.4960, 0.5120],
          [0.5000, 0.4980, 0.5020,  ..., 0.5140, 0.4960, 0.5120],
          [0.5000, 0.4980, 0.5020,  ..., 0.5140, 0.4960, 0.5120]],

         [[0.5000, 0.4980, 0.5020,  ..., 0.5140, 0.4960, 0.5120],
          [0.5000, 0.4980, 0.5020,  ..., 0.5140, 0.4960, 0.5120],
          [0.5000, 0.4980, 0.5020,  ..., 0.5140, 0.4960, 0.5120],
          [0.5000, 0.4980, 0.5020,  ..., 0.5140, 0.4960, 0.5120]]],


        [[[0.4763, 0.5125, 0.3915,  ..., 0.4804, 0.5299, 0.4578],
          [0.5699, 0.5144, 0.5429,  ..., 0.6024, 0.7154, 0.5197],
          [0.6091, 0.3231, 0.3462,  ..., 0.6546, 0.5744, 0.6372],
          [0.6270, 0.5462, 0.4688,  ..., 0.4989, 0.5744, 0.4917]],

         [[0.4201, 0.5365, 0.4669,  ..., 0.4530, 0.4672, 0.5170],
          [0.4796, 0.4522, 0.6803,  ..., 0.4379, 0.5318, 0.3598],
          [0.4639, 0.3806, 0.3902,  ..., 0.3306, 0.5473, 0.5409],
          [0.5402, 0.5707, 0.5671,  ..., 0.3788, 0.5420, 0.4127]],

         [[0.5818, 0.5736, 0.4207,  ..., 0.3961, 0.3581, 0.5202],
          [0.4910, 0.4272, 0.5866,  ..., 0.5771, 0.6051, 0.3786],
          [0.5894, 0.3664, 0.5053,  ..., 0.4765, 0.3953, 0.5766],
          [0.5851, 0.5133, 0.4757,  ..., 0.5795, 0.5396, 0.5580]],

         ...,

         [[0.5000, 0.4980, 0.5020,  ..., 0.5140, 0.4960, 0.5120],
          [0.5000, 0.4980, 0.5020,  ..., 0.5140, 0.4960, 0.5120],
          [0.5000, 0.4980, 0.5020,  ..., 0.5140, 0.4960, 0.5120],
          [0.5000, 0.4980, 0.5020,  ..., 0.5140, 0.4960, 0.5120]],

         [[0.5000, 0.4980, 0.5020,  ..., 0.5140, 0.4960, 0.5120],
          [0.5000, 0.4980, 0.5020,  ..., 0.5140, 0.4960, 0.5120],
          [0.5000, 0.4980, 0.5020,  ..., 0.5140, 0.4960, 0.5120],
          [0.5000, 0.4980, 0.5020,  ..., 0.5140, 0.4960, 0.5120]],

         [[0.5000, 0.4980, 0.5020,  ..., 0.5140, 0.4960, 0.5120],
          [0.5000, 0.4980, 0.5020,  ..., 0.5140, 0.4960, 0.5120],
          [0.5000, 0.4980, 0.5020,  ..., 0.5140, 0.4960, 0.5120],
          [0.5000, 0.4980, 0.5020,  ..., 0.5140, 0.4960, 0.5120]]]],
       device='cuda:0')
tensor([[[[0.4961, 0.4947, 0.4935,  ..., 0.2926, 0.4102, 0.4354],
          [0.3951, 0.4756, 0.6758,  ..., 0.5233, 0.4569, 0.3074],
          [0.5178, 0.4364, 0.5312,  ..., 0.3558, 0.6215, 0.4506],
          [0.4969, 0.4579, 0.5860,  ..., 0.4149, 0.6053, 0.4390]],

         [[0.4642, 0.5003, 0.4685,  ..., 0.5311, 0.4254, 0.3937],
          [0.4790, 0.6016, 0.5823,  ..., 0.4958, 0.6951, 0.5823],
          [0.6081, 0.3923, 0.4639,  ..., 0.6289, 0.6935, 0.6086],
          [0.6388, 0.5257, 0.5065,  ..., 0.4173, 0.5069, 0.3748]],

         [[0.4784, 0.5775, 0.4918,  ..., 0.3141, 0.4693, 0.5110],
          [0.3923, 0.5256, 0.5607,  ..., 0.3311, 0.4292, 0.3469],
          [0.5518, 0.4637, 0.4772,  ..., 0.4482, 0.4258, 0.5875],
          [0.5392, 0.6334, 0.5310,  ..., 0.4596, 0.5070, 0.4230]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.4763, 0.5145, 0.3895,  ..., 0.4664, 0.5339, 0.4458],
          [0.5699, 0.5164, 0.5409,  ..., 0.5884, 0.7194, 0.5077],
          [0.6091, 0.3251, 0.3442,  ..., 0.6406, 0.5784, 0.6252],
          [0.6270, 0.5482, 0.4668,  ..., 0.4849, 0.5784, 0.4797]],

         [[0.4201, 0.5385, 0.4649,  ..., 0.4390, 0.4712, 0.5050],
          [0.4796, 0.4542, 0.6783,  ..., 0.4239, 0.5358, 0.3478],
          [0.4639, 0.3826, 0.3882,  ..., 0.3166, 0.5513, 0.5289],
          [0.5402, 0.5727, 0.5651,  ..., 0.3648, 0.5460, 0.4007]],

         [[0.5818, 0.5756, 0.4187,  ..., 0.3821, 0.3621, 0.5082],
          [0.4910, 0.4292, 0.5846,  ..., 0.5631, 0.6091, 0.3666],
          [0.5894, 0.3684, 0.5033,  ..., 0.4625, 0.3993, 0.5646],
          [0.5851, 0.5153, 0.4737,  ..., 0.5655, 0.5436, 0.5460]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([-2.3283e-10, -2.0000e-03,  2.0000e-03,  8.0000e-03, -1.0000e-02,
         1.6000e-02,  8.0000e-03,  1.4000e-02, -4.0000e-03,  1.2000e-02],
       device='cuda:0')
selected experts tensor([1922, 1858, 2061, 1355, 1740, 1233, 1454, 1217, 2022, 1522],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1636., 1680., 1601., 1686., 1700., 1584., 1511., 1675., 1685., 1626.],
        [1922., 1858., 2061., 1355., 1740., 1233., 1454., 1217., 2022., 1522.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4007, 0.5456, 0.2479,  ..., 0.4931, 0.4826, 0.3181],
          [0.5955, 0.4910, 0.6941,  ..., 0.5515, 0.4449, 0.4926],
          [0.4547, 0.5584, 0.5291,  ..., 0.4176, 0.4628, 0.4633],
          [0.4064, 0.4278, 0.4523,  ..., 0.5413, 0.5589, 0.4867]],

         [[0.3780, 0.3366, 0.5857,  ..., 0.4694, 0.5666, 0.4934],
          [0.3933, 0.4278, 0.6012,  ..., 0.5730, 0.4880, 0.4222],
          [0.4897, 0.4705, 0.4696,  ..., 0.7044, 0.5766, 0.4423],
          [0.5727, 0.5383, 0.4299,  ..., 0.2628, 0.5656, 0.4837]],

         [[0.5458, 0.3605, 0.5533,  ..., 0.4376, 0.5998, 0.5227],
          [0.5257, 0.5434, 0.4860,  ..., 0.5083, 0.4570, 0.3806],
          [0.5842, 0.5723, 0.4822,  ..., 0.4513, 0.4350, 0.4808],
          [0.4225, 0.4401, 0.4668,  ..., 0.5638, 0.3943, 0.4967]],

         ...,

         [[0.6072, 0.4835, 0.3803,  ..., 0.5212, 0.6097, 0.4922],
          [0.5969, 0.4654, 0.4781,  ..., 0.4909, 0.5685, 0.6237],
          [0.6549, 0.4103, 0.5044,  ..., 0.4276, 0.4084, 0.4279],
          [0.5004, 0.4572, 0.5796,  ..., 0.4366, 0.4780, 0.4633]],

         [[0.4149, 0.4526, 0.3426,  ..., 0.5581, 0.6683, 0.4010],
          [0.4569, 0.6578, 0.6109,  ..., 0.5261, 0.4984, 0.6746],
          [0.5641, 0.3759, 0.4492,  ..., 0.5768, 0.4403, 0.4992],
          [0.5607, 0.4197, 0.5212,  ..., 0.4200, 0.5463, 0.5628]],

         [[0.3882, 0.5814, 0.5369,  ..., 0.4963, 0.6674, 0.4879],
          [0.5559, 0.7190, 0.4729,  ..., 0.3886, 0.4212, 0.5069],
          [0.6211, 0.5742, 0.7212,  ..., 0.6043, 0.6938, 0.5170],
          [0.5955, 0.6630, 0.5989,  ..., 0.4656, 0.5606, 0.4303]]],


        [[[0.5404, 0.5186, 0.4371,  ..., 0.5326, 0.3768, 0.5211],
          [0.3914, 0.5824, 0.4584,  ..., 0.4114, 0.4550, 0.6607],
          [0.3217, 0.6199, 0.4179,  ..., 0.4641, 0.4415, 0.5748],
          [0.5581, 0.5143, 0.2874,  ..., 0.5991, 0.5116, 0.6172]],

         [[0.4900, 0.3668, 0.5705,  ..., 0.4660, 0.4992, 0.5829],
          [0.6549, 0.4647, 0.3808,  ..., 0.4030, 0.6471, 0.4505],
          [0.5150, 0.6125, 0.5523,  ..., 0.5227, 0.4490, 0.4090],
          [0.5128, 0.4582, 0.4945,  ..., 0.5411, 0.5804, 0.5152]],

         [[0.4743, 0.4625, 0.6091,  ..., 0.4745, 0.4393, 0.5213],
          [0.5703, 0.4389, 0.4189,  ..., 0.4271, 0.4891, 0.6925],
          [0.6628, 0.6471, 0.5591,  ..., 0.5095, 0.4905, 0.6685],
          [0.4603, 0.5852, 0.4485,  ..., 0.4832, 0.3800, 0.6491]],

         ...,

         [[0.4369, 0.6507, 0.4641,  ..., 0.6903, 0.5311, 0.4631],
          [0.4287, 0.4662, 0.3435,  ..., 0.5406, 0.4074, 0.4951],
          [0.4848, 0.4962, 0.6382,  ..., 0.4933, 0.5728, 0.4711],
          [0.5823, 0.5197, 0.5791,  ..., 0.5304, 0.3357, 0.4955]],

         [[0.3657, 0.5354, 0.4232,  ..., 0.6067, 0.6327, 0.5611],
          [0.4996, 0.6444, 0.4528,  ..., 0.4741, 0.3560, 0.5662],
          [0.4477, 0.6254, 0.4299,  ..., 0.4072, 0.3929, 0.5758],
          [0.4017, 0.5265, 0.3312,  ..., 0.6043, 0.3929, 0.5495]],

         [[0.4434, 0.3331, 0.4735,  ..., 0.5763, 0.5642, 0.4852],
          [0.5997, 0.5342, 0.6274,  ..., 0.5691, 0.4873, 0.4619],
          [0.5619, 0.4365, 0.4997,  ..., 0.5392, 0.4701, 0.5186],
          [0.4583, 0.5196, 0.4342,  ..., 0.5744, 0.5005, 0.5655]]]],
       device='cuda:0')
tensor([[[[0.4007, 0.5436, 0.2539,  ..., 0.4871, 0.4806, 0.3141],
          [0.5955, 0.4890, 0.7001,  ..., 0.5455, 0.4429, 0.4886],
          [0.4547, 0.5564, 0.5351,  ..., 0.4116, 0.4608, 0.4593],
          [0.4064, 0.4258, 0.4583,  ..., 0.5353, 0.5569, 0.4827]],

         [[0.3780, 0.3346, 0.5917,  ..., 0.4634, 0.5646, 0.4894],
          [0.3933, 0.4258, 0.6072,  ..., 0.5670, 0.4860, 0.4182],
          [0.4897, 0.4685, 0.4756,  ..., 0.6984, 0.5746, 0.4383],
          [0.5727, 0.5363, 0.4359,  ..., 0.2568, 0.5636, 0.4797]],

         [[0.5458, 0.3585, 0.5593,  ..., 0.4316, 0.5978, 0.5187],
          [0.5257, 0.5414, 0.4920,  ..., 0.5023, 0.4550, 0.3766],
          [0.5842, 0.5703, 0.4882,  ..., 0.4453, 0.4330, 0.4768],
          [0.4225, 0.4381, 0.4728,  ..., 0.5578, 0.3923, 0.4927]],

         ...,

         [[0.6072, 0.4815, 0.3863,  ..., 0.5152, 0.6077, 0.4882],
          [0.5969, 0.4634, 0.4841,  ..., 0.4849, 0.5665, 0.6197],
          [0.6549, 0.4083, 0.5104,  ..., 0.4216, 0.4064, 0.4239],
          [0.5004, 0.4552, 0.5856,  ..., 0.4306, 0.4760, 0.4593]],

         [[0.4149, 0.4506, 0.3486,  ..., 0.5521, 0.6663, 0.3970],
          [0.4569, 0.6558, 0.6169,  ..., 0.5201, 0.4964, 0.6706],
          [0.5641, 0.3739, 0.4552,  ..., 0.5708, 0.4383, 0.4952],
          [0.5607, 0.4177, 0.5272,  ..., 0.4140, 0.5443, 0.5588]],

         [[0.3882, 0.5794, 0.5429,  ..., 0.4903, 0.6654, 0.4839],
          [0.5559, 0.7170, 0.4789,  ..., 0.3826, 0.4192, 0.5029],
          [0.6211, 0.5722, 0.7272,  ..., 0.5983, 0.6918, 0.5130],
          [0.5955, 0.6610, 0.6049,  ..., 0.4596, 0.5586, 0.4263]]],


        [[[0.5404, 0.5166, 0.4431,  ..., 0.5266, 0.3748, 0.5171],
          [0.3914, 0.5804, 0.4644,  ..., 0.4054, 0.4530, 0.6567],
          [0.3217, 0.6179, 0.4239,  ..., 0.4581, 0.4395, 0.5708],
          [0.5581, 0.5123, 0.2934,  ..., 0.5931, 0.5096, 0.6132]],

         [[0.4900, 0.3648, 0.5765,  ..., 0.4600, 0.4972, 0.5789],
          [0.6549, 0.4627, 0.3868,  ..., 0.3970, 0.6451, 0.4465],
          [0.5150, 0.6105, 0.5583,  ..., 0.5167, 0.4470, 0.4050],
          [0.5128, 0.4562, 0.5005,  ..., 0.5351, 0.5784, 0.5112]],

         [[0.4743, 0.4605, 0.6151,  ..., 0.4685, 0.4373, 0.5173],
          [0.5703, 0.4369, 0.4249,  ..., 0.4211, 0.4871, 0.6885],
          [0.6628, 0.6451, 0.5651,  ..., 0.5035, 0.4885, 0.6645],
          [0.4603, 0.5832, 0.4545,  ..., 0.4772, 0.3780, 0.6451]],

         ...,

         [[0.4369, 0.6487, 0.4701,  ..., 0.6843, 0.5291, 0.4591],
          [0.4287, 0.4642, 0.3495,  ..., 0.5346, 0.4054, 0.4911],
          [0.4848, 0.4942, 0.6442,  ..., 0.4873, 0.5708, 0.4671],
          [0.5823, 0.5177, 0.5851,  ..., 0.5244, 0.3337, 0.4915]],

         [[0.3657, 0.5334, 0.4292,  ..., 0.6007, 0.6307, 0.5571],
          [0.4996, 0.6424, 0.4588,  ..., 0.4681, 0.3540, 0.5622],
          [0.4477, 0.6234, 0.4359,  ..., 0.4012, 0.3909, 0.5718],
          [0.4017, 0.5245, 0.3372,  ..., 0.5983, 0.3909, 0.5455]],

         [[0.4434, 0.3311, 0.4795,  ..., 0.5703, 0.5622, 0.4812],
          [0.5997, 0.5322, 0.6334,  ..., 0.5631, 0.4853, 0.4579],
          [0.5619, 0.4345, 0.5057,  ..., 0.5332, 0.4681, 0.5146],
          [0.4583, 0.5176, 0.4402,  ..., 0.5684, 0.4985, 0.5615]]]],
       device='cuda:0', requires_grad=True)
tensor([ 2.3283e-10,  2.0000e-03, -6.0000e-03,  2.0000e-03, -4.0000e-03,
        -6.0000e-03,  8.0000e-03,  6.0000e-03,  2.0000e-03,  4.0000e-03],
       device='cuda:0')
selected experts tensor([1644, 1637, 1563, 1677, 1633, 1684, 1679, 1643, 1620, 1604],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4822, 0.5200, 0.5915,  ..., 0.4894, 0.4881, 0.5441],
          [0.6901, 0.5180, 0.4657,  ..., 0.3780, 0.4773, 0.4245],
          [0.6433, 0.3885, 0.4222,  ..., 0.4070, 0.4648, 0.2618],
          [0.4723, 0.5375, 0.5476,  ..., 0.5951, 0.7442, 0.6194]],

         [[0.4930, 0.5203, 0.5782,  ..., 0.4318, 0.3697, 0.4920],
          [0.3567, 0.4260, 0.3884,  ..., 0.7227, 0.4573, 0.4805],
          [0.3849, 0.4888, 0.5230,  ..., 0.5091, 0.3935, 0.5129],
          [0.6243, 0.4474, 0.4701,  ..., 0.5643, 0.5801, 0.6022]],

         [[0.4608, 0.5861, 0.5435,  ..., 0.3014, 0.5374, 0.5205],
          [0.3303, 0.4212, 0.4551,  ..., 0.3260, 0.6121, 0.5533],
          [0.4102, 0.3862, 0.5534,  ..., 0.6165, 0.5505, 0.4860],
          [0.4771, 0.4188, 0.4975,  ..., 0.6559, 0.3291, 0.6709]],

         ...,

         [[0.4087, 0.5595, 0.4904,  ..., 0.5003, 0.6580, 0.5233],
          [0.3854, 0.5339, 0.4585,  ..., 0.5251, 0.6545, 0.3668],
          [0.4083, 0.5950, 0.6720,  ..., 0.3873, 0.3861, 0.4240],
          [0.5770, 0.4632, 0.4194,  ..., 0.6418, 0.3825, 0.3077]],

         [[0.4851, 0.5010, 0.5522,  ..., 0.4953, 0.3607, 0.4669],
          [0.6984, 0.6398, 0.5119,  ..., 0.6160, 0.4421, 0.6709],
          [0.5756, 0.4910, 0.4270,  ..., 0.6256, 0.4750, 0.6435],
          [0.5417, 0.4412, 0.5261,  ..., 0.3243, 0.4532, 0.6648]],

         [[0.5090, 0.5880, 0.5995,  ..., 0.3391, 0.5079, 0.5598],
          [0.5051, 0.6626, 0.5454,  ..., 0.5446, 0.5010, 0.5050],
          [0.5983, 0.3718, 0.5623,  ..., 0.4061, 0.5305, 0.6542],
          [0.6225, 0.6254, 0.6079,  ..., 0.3947, 0.5432, 0.4710]]],


        [[[0.4230, 0.5068, 0.6140,  ..., 0.5881, 0.5548, 0.6097],
          [0.4768, 0.4346, 0.5023,  ..., 0.6723, 0.3797, 0.4979],
          [0.4844, 0.5922, 0.3742,  ..., 0.4027, 0.4440, 0.5675],
          [0.4983, 0.3487, 0.4959,  ..., 0.5970, 0.6228, 0.5502]],

         [[0.6220, 0.5799, 0.3535,  ..., 0.4932, 0.4517, 0.4582],
          [0.6549, 0.5992, 0.5515,  ..., 0.6165, 0.6000, 0.5499],
          [0.6151, 0.6015, 0.3880,  ..., 0.5094, 0.4801, 0.4288],
          [0.3486, 0.4957, 0.3724,  ..., 0.3426, 0.4076, 0.3578]],

         [[0.3780, 0.4217, 0.5035,  ..., 0.7372, 0.5139, 0.3851],
          [0.4899, 0.4912, 0.5469,  ..., 0.5393, 0.4960, 0.4738],
          [0.5195, 0.5109, 0.6033,  ..., 0.5340, 0.4978, 0.6171],
          [0.6352, 0.4581, 0.6509,  ..., 0.4165, 0.4732, 0.5680]],

         ...,

         [[0.4883, 0.4708, 0.4880,  ..., 0.4403, 0.3922, 0.5966],
          [0.6086, 0.5373, 0.5630,  ..., 0.6086, 0.3852, 0.4405],
          [0.4177, 0.5252, 0.4015,  ..., 0.5250, 0.6255, 0.3161],
          [0.5315, 0.3881, 0.5081,  ..., 0.4330, 0.5372, 0.3874]],

         [[0.5789, 0.6062, 0.4996,  ..., 0.5567, 0.6347, 0.5533],
          [0.4981, 0.3871, 0.6428,  ..., 0.3947, 0.4351, 0.5201],
          [0.5093, 0.5113, 0.5758,  ..., 0.5424, 0.5638, 0.5857],
          [0.5206, 0.5044, 0.5971,  ..., 0.3435, 0.6329, 0.5149]],

         [[0.6105, 0.3622, 0.6455,  ..., 0.5610, 0.3553, 0.3943],
          [0.3803, 0.4499, 0.5594,  ..., 0.4898, 0.4318, 0.5152],
          [0.5052, 0.3604, 0.5767,  ..., 0.5065, 0.4734, 0.4389],
          [0.5641, 0.5315, 0.3688,  ..., 0.5461, 0.3670, 0.4531]]]],
       device='cuda:0')
tensor([[[[0.4822, 0.5280, 0.5875,  ..., 0.4954, 0.4841, 0.5421],
          [0.6901, 0.5260, 0.4617,  ..., 0.3840, 0.4733, 0.4225],
          [0.6433, 0.3965, 0.4182,  ..., 0.4130, 0.4608, 0.2598],
          [0.4723, 0.5455, 0.5436,  ..., 0.6011, 0.7402, 0.6174]],

         [[0.4930, 0.5283, 0.5742,  ..., 0.4378, 0.3657, 0.4900],
          [0.3567, 0.4340, 0.3844,  ..., 0.7287, 0.4533, 0.4785],
          [0.3849, 0.4968, 0.5190,  ..., 0.5151, 0.3895, 0.5109],
          [0.6243, 0.4554, 0.4661,  ..., 0.5703, 0.5761, 0.6002]],

         [[0.4608, 0.5941, 0.5395,  ..., 0.3074, 0.5334, 0.5185],
          [0.3303, 0.4292, 0.4511,  ..., 0.3320, 0.6081, 0.5513],
          [0.4102, 0.3942, 0.5494,  ..., 0.6225, 0.5465, 0.4840],
          [0.4771, 0.4268, 0.4935,  ..., 0.6619, 0.3251, 0.6689]],

         ...,

         [[0.4087, 0.5675, 0.4864,  ..., 0.5063, 0.6540, 0.5213],
          [0.3854, 0.5419, 0.4545,  ..., 0.5311, 0.6505, 0.3648],
          [0.4083, 0.6030, 0.6680,  ..., 0.3933, 0.3821, 0.4220],
          [0.5770, 0.4712, 0.4154,  ..., 0.6478, 0.3785, 0.3057]],

         [[0.4851, 0.5090, 0.5482,  ..., 0.5013, 0.3567, 0.4649],
          [0.6984, 0.6478, 0.5079,  ..., 0.6220, 0.4381, 0.6689],
          [0.5756, 0.4990, 0.4230,  ..., 0.6316, 0.4710, 0.6415],
          [0.5417, 0.4492, 0.5221,  ..., 0.3303, 0.4492, 0.6628]],

         [[0.5090, 0.5960, 0.5955,  ..., 0.3451, 0.5039, 0.5578],
          [0.5051, 0.6706, 0.5414,  ..., 0.5506, 0.4970, 0.5030],
          [0.5983, 0.3798, 0.5583,  ..., 0.4121, 0.5265, 0.6522],
          [0.6225, 0.6334, 0.6039,  ..., 0.4007, 0.5392, 0.4690]]],


        [[[0.4230, 0.5148, 0.6100,  ..., 0.5941, 0.5508, 0.6077],
          [0.4768, 0.4426, 0.4983,  ..., 0.6783, 0.3757, 0.4959],
          [0.4844, 0.6002, 0.3702,  ..., 0.4087, 0.4400, 0.5655],
          [0.4983, 0.3567, 0.4919,  ..., 0.6030, 0.6188, 0.5482]],

         [[0.6220, 0.5879, 0.3495,  ..., 0.4992, 0.4477, 0.4562],
          [0.6549, 0.6072, 0.5475,  ..., 0.6225, 0.5960, 0.5479],
          [0.6151, 0.6095, 0.3840,  ..., 0.5154, 0.4761, 0.4268],
          [0.3486, 0.5037, 0.3684,  ..., 0.3486, 0.4036, 0.3558]],

         [[0.3780, 0.4297, 0.4995,  ..., 0.7432, 0.5099, 0.3831],
          [0.4899, 0.4992, 0.5429,  ..., 0.5453, 0.4920, 0.4718],
          [0.5195, 0.5189, 0.5993,  ..., 0.5400, 0.4938, 0.6151],
          [0.6352, 0.4661, 0.6469,  ..., 0.4225, 0.4692, 0.5660]],

         ...,

         [[0.4883, 0.4788, 0.4840,  ..., 0.4463, 0.3882, 0.5946],
          [0.6086, 0.5453, 0.5590,  ..., 0.6146, 0.3812, 0.4385],
          [0.4177, 0.5332, 0.3975,  ..., 0.5310, 0.6215, 0.3141],
          [0.5315, 0.3961, 0.5041,  ..., 0.4390, 0.5332, 0.3854]],

         [[0.5789, 0.6142, 0.4956,  ..., 0.5627, 0.6307, 0.5513],
          [0.4981, 0.3951, 0.6388,  ..., 0.4007, 0.4311, 0.5181],
          [0.5093, 0.5193, 0.5718,  ..., 0.5484, 0.5598, 0.5837],
          [0.5206, 0.5124, 0.5931,  ..., 0.3495, 0.6289, 0.5129]],

         [[0.6105, 0.3702, 0.6415,  ..., 0.5670, 0.3513, 0.3923],
          [0.3803, 0.4579, 0.5554,  ..., 0.4958, 0.4278, 0.5132],
          [0.5052, 0.3684, 0.5727,  ..., 0.5125, 0.4694, 0.4369],
          [0.5641, 0.5395, 0.3648,  ..., 0.5521, 0.3630, 0.4511]]]],
       device='cuda:0', requires_grad=True)
tensor([ 2.3283e-10, -8.0000e-03,  4.0000e-03,  1.0000e-02, -2.0000e-03,
         8.0000e-03, -1.2000e-02, -6.0000e-03,  4.0000e-03,  2.0000e-03],
       device='cuda:0')
selected experts tensor([1648, 1552, 1696, 1667, 1702, 1581, 1665, 1523, 1657, 1693],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5244, 0.6483, 0.4870,  ..., 0.4917, 0.6112, 0.6256],
          [0.4146, 0.5565, 0.5283,  ..., 0.4206, 0.6022, 0.5487],
          [0.6392, 0.5304, 0.6407,  ..., 0.6360, 0.3264, 0.4688],
          [0.7230, 0.5708, 0.5581,  ..., 0.5822, 0.4109, 0.4476]],

         [[0.5178, 0.5224, 0.4245,  ..., 0.4134, 0.5580, 0.6237],
          [0.5316, 0.4053, 0.4793,  ..., 0.5218, 0.5505, 0.3800],
          [0.5145, 0.4483, 0.6585,  ..., 0.4428, 0.6197, 0.4971],
          [0.6293, 0.4336, 0.5713,  ..., 0.3881, 0.4230, 0.5331]],

         [[0.4900, 0.6142, 0.6012,  ..., 0.3388, 0.4216, 0.6026],
          [0.4175, 0.4483, 0.4970,  ..., 0.6159, 0.5341, 0.5236],
          [0.5581, 0.5632, 0.6077,  ..., 0.5770, 0.4434, 0.4817],
          [0.5135, 0.4369, 0.6142,  ..., 0.6233, 0.4263, 0.5797]],

         ...,

         [[0.5966, 0.6269, 0.4079,  ..., 0.3406, 0.4863, 0.5485],
          [0.3099, 0.5521, 0.5904,  ..., 0.4752, 0.6369, 0.5770],
          [0.4594, 0.5088, 0.4809,  ..., 0.4196, 0.4425, 0.3701],
          [0.5296, 0.5311, 0.5675,  ..., 0.5741, 0.3676, 0.4234]],

         [[0.5730, 0.5002, 0.4713,  ..., 0.4130, 0.4945, 0.6666],
          [0.4085, 0.6305, 0.4601,  ..., 0.4215, 0.5509, 0.6016],
          [0.5420, 0.4805, 0.5017,  ..., 0.3899, 0.5316, 0.4548],
          [0.5362, 0.3520, 0.4027,  ..., 0.5414, 0.3766, 0.4070]],

         [[0.4333, 0.6008, 0.5397,  ..., 0.6260, 0.5807, 0.5198],
          [0.4352, 0.4857, 0.5393,  ..., 0.6099, 0.6046, 0.5545],
          [0.5848, 0.5214, 0.6920,  ..., 0.5088, 0.5677, 0.5193],
          [0.4936, 0.5565, 0.4164,  ..., 0.4973, 0.4822, 0.4922]]],


        [[[0.5658, 0.4389, 0.5937,  ..., 0.3336, 0.5355, 0.6158],
          [0.4553, 0.5201, 0.5475,  ..., 0.5770, 0.6253, 0.3927],
          [0.6129, 0.4705, 0.6068,  ..., 0.5370, 0.4468, 0.4869],
          [0.4232, 0.4444, 0.5540,  ..., 0.5143, 0.4076, 0.5380]],

         [[0.4492, 0.5035, 0.4514,  ..., 0.5047, 0.4863, 0.6144],
          [0.4558, 0.4958, 0.4446,  ..., 0.5033, 0.6668, 0.4753],
          [0.5383, 0.3620, 0.5366,  ..., 0.4872, 0.5412, 0.5315],
          [0.7088, 0.4624, 0.4904,  ..., 0.4916, 0.4871, 0.4741]],

         [[0.4784, 0.5708, 0.4311,  ..., 0.5158, 0.5104, 0.4890],
          [0.5043, 0.6359, 0.4521,  ..., 0.6539, 0.6022, 0.4642],
          [0.4575, 0.5302, 0.4395,  ..., 0.3620, 0.3993, 0.4766],
          [0.5463, 0.4714, 0.4784,  ..., 0.4330, 0.4155, 0.5229]],

         ...,

         [[0.5844, 0.4125, 0.7188,  ..., 0.5789, 0.5524, 0.4902],
          [0.4924, 0.6188, 0.4922,  ..., 0.4477, 0.5898, 0.4768],
          [0.4852, 0.4575, 0.5439,  ..., 0.5353, 0.5244, 0.4792],
          [0.3883, 0.4575, 0.5847,  ..., 0.6941, 0.4980, 0.4688]],

         [[0.4655, 0.5039, 0.6119,  ..., 0.4368, 0.4866, 0.6772],
          [0.3508, 0.4442, 0.6253,  ..., 0.5636, 0.5694, 0.6275],
          [0.5605, 0.5769, 0.5641,  ..., 0.4569, 0.4684, 0.3621],
          [0.4090, 0.4870, 0.4971,  ..., 0.5884, 0.4578, 0.4395]],

         [[0.4818, 0.4486, 0.4183,  ..., 0.4686, 0.4679, 0.6567],
          [0.5093, 0.5999, 0.5946,  ..., 0.4807, 0.6470, 0.4028],
          [0.3864, 0.4773, 0.4540,  ..., 0.3793, 0.5558, 0.4182],
          [0.4000, 0.4851, 0.3505,  ..., 0.5392, 0.4141, 0.5135]]]],
       device='cuda:0')
tensor([[[[0.5294, 0.6593, 0.4860,  ..., 0.4927, 0.5922, 0.6086],
          [0.4196, 0.5675, 0.5273,  ..., 0.4216, 0.5832, 0.5317],
          [0.6442, 0.5414, 0.6397,  ..., 0.6370, 0.3074, 0.4518],
          [0.7280, 0.5818, 0.5571,  ..., 0.5832, 0.3919, 0.4306]],

         [[0.5228, 0.5334, 0.4235,  ..., 0.4144, 0.5390, 0.6067],
          [0.5366, 0.4163, 0.4783,  ..., 0.5228, 0.5315, 0.3630],
          [0.5195, 0.4593, 0.6575,  ..., 0.4438, 0.6007, 0.4801],
          [0.6343, 0.4446, 0.5703,  ..., 0.3891, 0.4040, 0.5161]],

         [[0.4950, 0.6252, 0.6002,  ..., 0.3398, 0.4026, 0.5856],
          [0.4225, 0.4593, 0.4960,  ..., 0.6169, 0.5151, 0.5066],
          [0.5631, 0.5742, 0.6067,  ..., 0.5780, 0.4244, 0.4647],
          [0.5185, 0.4479, 0.6132,  ..., 0.6243, 0.4073, 0.5627]],

         ...,

         [[0.6016, 0.6379, 0.4069,  ..., 0.3416, 0.4673, 0.5315],
          [0.3149, 0.5631, 0.5894,  ..., 0.4762, 0.6179, 0.5600],
          [0.4644, 0.5198, 0.4799,  ..., 0.4206, 0.4235, 0.3531],
          [0.5346, 0.5421, 0.5665,  ..., 0.5751, 0.3486, 0.4064]],

         [[0.5780, 0.5112, 0.4703,  ..., 0.4140, 0.4755, 0.6496],
          [0.4135, 0.6415, 0.4591,  ..., 0.4225, 0.5319, 0.5846],
          [0.5470, 0.4915, 0.5007,  ..., 0.3909, 0.5126, 0.4378],
          [0.5412, 0.3630, 0.4017,  ..., 0.5424, 0.3576, 0.3900]],

         [[0.4383, 0.6118, 0.5387,  ..., 0.6270, 0.5617, 0.5028],
          [0.4402, 0.4967, 0.5383,  ..., 0.6109, 0.5856, 0.5375],
          [0.5898, 0.5324, 0.6910,  ..., 0.5098, 0.5487, 0.5023],
          [0.4986, 0.5675, 0.4154,  ..., 0.4983, 0.4632, 0.4752]]],


        [[[0.5708, 0.4499, 0.5927,  ..., 0.3346, 0.5165, 0.5988],
          [0.4603, 0.5311, 0.5465,  ..., 0.5780, 0.6063, 0.3757],
          [0.6179, 0.4815, 0.6058,  ..., 0.5380, 0.4278, 0.4699],
          [0.4282, 0.4554, 0.5530,  ..., 0.5153, 0.3886, 0.5210]],

         [[0.4542, 0.5145, 0.4504,  ..., 0.5057, 0.4673, 0.5974],
          [0.4608, 0.5068, 0.4436,  ..., 0.5043, 0.6478, 0.4583],
          [0.5433, 0.3730, 0.5356,  ..., 0.4882, 0.5222, 0.5145],
          [0.7138, 0.4734, 0.4894,  ..., 0.4926, 0.4681, 0.4571]],

         [[0.4834, 0.5818, 0.4301,  ..., 0.5168, 0.4914, 0.4720],
          [0.5093, 0.6469, 0.4511,  ..., 0.6549, 0.5832, 0.4472],
          [0.4625, 0.5412, 0.4385,  ..., 0.3630, 0.3803, 0.4596],
          [0.5513, 0.4824, 0.4774,  ..., 0.4340, 0.3965, 0.5059]],

         ...,

         [[0.5894, 0.4235, 0.7178,  ..., 0.5799, 0.5334, 0.4732],
          [0.4974, 0.6298, 0.4912,  ..., 0.4487, 0.5708, 0.4598],
          [0.4902, 0.4685, 0.5429,  ..., 0.5363, 0.5054, 0.4622],
          [0.3933, 0.4685, 0.5837,  ..., 0.6951, 0.4790, 0.4518]],

         [[0.4705, 0.5149, 0.6109,  ..., 0.4378, 0.4676, 0.6602],
          [0.3558, 0.4552, 0.6243,  ..., 0.5646, 0.5504, 0.6105],
          [0.5655, 0.5879, 0.5631,  ..., 0.4579, 0.4494, 0.3451],
          [0.4140, 0.4980, 0.4961,  ..., 0.5894, 0.4388, 0.4225]],

         [[0.4868, 0.4596, 0.4173,  ..., 0.4696, 0.4489, 0.6397],
          [0.5143, 0.6109, 0.5936,  ..., 0.4817, 0.6280, 0.3858],
          [0.3914, 0.4883, 0.4530,  ..., 0.3803, 0.5368, 0.4012],
          [0.4050, 0.4961, 0.3495,  ..., 0.5402, 0.3951, 0.4965]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0050, -0.0110,  0.0010, -0.0090, -0.0110,  0.0190, -0.0290, -0.0010,
         0.0190,  0.0170], device='cuda:0')
selected experts tensor([1625, 1401, 1499, 1930, 1602, 2002, 2254, 1291, 1543, 1237],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5168, 0.4871, 0.4064,  ..., 0.4600, 0.3680, 0.4369],
          [0.6405, 0.4631, 0.5378,  ..., 0.4801, 0.5386, 0.3724],
          [0.5860, 0.3875, 0.4861,  ..., 0.4938, 0.6266, 0.5549],
          [0.5879, 0.6421, 0.5506,  ..., 0.4385, 0.4218, 0.3760]],

         [[0.4253, 0.5548, 0.5685,  ..., 0.4125, 0.3616, 0.3965],
          [0.4400, 0.6394, 0.5993,  ..., 0.3444, 0.5725, 0.3599],
          [0.5813, 0.3519, 0.3758,  ..., 0.4275, 0.5730, 0.6000],
          [0.5409, 0.6719, 0.4422,  ..., 0.3753, 0.5625, 0.3485]],

         [[0.5091, 0.4267, 0.4292,  ..., 0.4736, 0.4877, 0.3915],
          [0.3729, 0.6204, 0.5728,  ..., 0.4994, 0.6087, 0.5014],
          [0.4922, 0.4653, 0.3952,  ..., 0.4366, 0.5687, 0.5370],
          [0.4814, 0.6519, 0.6225,  ..., 0.4247, 0.5490, 0.4137]],

         ...,

         [[0.4990, 0.4970, 0.5010,  ..., 0.5150, 0.4950, 0.5130],
          [0.4990, 0.4970, 0.5010,  ..., 0.5150, 0.4950, 0.5130],
          [0.4990, 0.4970, 0.5010,  ..., 0.5150, 0.4950, 0.5130],
          [0.4990, 0.4970, 0.5010,  ..., 0.5150, 0.4950, 0.5130]],

         [[0.4990, 0.4970, 0.5010,  ..., 0.5150, 0.4950, 0.5130],
          [0.4990, 0.4970, 0.5010,  ..., 0.5150, 0.4950, 0.5130],
          [0.4990, 0.4970, 0.5010,  ..., 0.5150, 0.4950, 0.5130],
          [0.4990, 0.4970, 0.5010,  ..., 0.5150, 0.4950, 0.5130]],

         [[0.4990, 0.4970, 0.5010,  ..., 0.5150, 0.4950, 0.5130],
          [0.4990, 0.4970, 0.5010,  ..., 0.5150, 0.4950, 0.5130],
          [0.4990, 0.4970, 0.5010,  ..., 0.5150, 0.4950, 0.5130],
          [0.4990, 0.4970, 0.5010,  ..., 0.5150, 0.4950, 0.5130]]],


        [[[0.5257, 0.3654, 0.5629,  ..., 0.3999, 0.3244, 0.4796],
          [0.4049, 0.5379, 0.6878,  ..., 0.5091, 0.3953, 0.5258],
          [0.4690, 0.3238, 0.4202,  ..., 0.4533, 0.6165, 0.5716],
          [0.6159, 0.5577, 0.4230,  ..., 0.3985, 0.6092, 0.2968]],

         [[0.6378, 0.5702, 0.3391,  ..., 0.4223, 0.5444, 0.5781],
          [0.4920, 0.4043, 0.6768,  ..., 0.6189, 0.6202, 0.4203],
          [0.5241, 0.4143, 0.3649,  ..., 0.3166, 0.5243, 0.6113],
          [0.5443, 0.5551, 0.5494,  ..., 0.4332, 0.5429, 0.4494]],

         [[0.5626, 0.5044, 0.5528,  ..., 0.2917, 0.3707, 0.5033],
          [0.4301, 0.4696, 0.5134,  ..., 0.5651, 0.5330, 0.3502],
          [0.5397, 0.3736, 0.3712,  ..., 0.5105, 0.4818, 0.5910],
          [0.5223, 0.5649, 0.5419,  ..., 0.4148, 0.6410, 0.4180]],

         ...,

         [[0.4990, 0.4970, 0.5010,  ..., 0.5150, 0.4950, 0.5130],
          [0.4990, 0.4970, 0.5010,  ..., 0.5150, 0.4950, 0.5130],
          [0.4990, 0.4970, 0.5010,  ..., 0.5150, 0.4950, 0.5130],
          [0.4990, 0.4970, 0.5010,  ..., 0.5150, 0.4950, 0.5130]],

         [[0.4990, 0.4970, 0.5010,  ..., 0.5150, 0.4950, 0.5130],
          [0.4990, 0.4970, 0.5010,  ..., 0.5150, 0.4950, 0.5130],
          [0.4990, 0.4970, 0.5010,  ..., 0.5150, 0.4950, 0.5130],
          [0.4990, 0.4970, 0.5010,  ..., 0.5150, 0.4950, 0.5130]],

         [[0.4990, 0.4970, 0.5010,  ..., 0.5150, 0.4950, 0.5130],
          [0.4990, 0.4970, 0.5010,  ..., 0.5150, 0.4950, 0.5130],
          [0.4990, 0.4970, 0.5010,  ..., 0.5150, 0.4950, 0.5130],
          [0.4990, 0.4970, 0.5010,  ..., 0.5150, 0.4950, 0.5130]]]],
       device='cuda:0')/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([[[[0.5178, 0.4901, 0.4054,  ..., 0.4450, 0.3730, 0.4239],
          [0.6415, 0.4661, 0.5368,  ..., 0.4651, 0.5436, 0.3594],
          [0.5870, 0.3905, 0.4851,  ..., 0.4788, 0.6316, 0.5419],
          [0.5889, 0.6451, 0.5496,  ..., 0.4235, 0.4268, 0.3630]],

         [[0.4263, 0.5578, 0.5675,  ..., 0.3975, 0.3666, 0.3835],
          [0.4410, 0.6424, 0.5983,  ..., 0.3294, 0.5775, 0.3469],
          [0.5823, 0.3549, 0.3748,  ..., 0.4125, 0.5780, 0.5870],
          [0.5419, 0.6749, 0.4412,  ..., 0.3603, 0.5675, 0.3355]],

         [[0.5101, 0.4297, 0.4282,  ..., 0.4586, 0.4927, 0.3785],
          [0.3739, 0.6234, 0.5718,  ..., 0.4844, 0.6137, 0.4884],
          [0.4932, 0.4683, 0.3942,  ..., 0.4216, 0.5737, 0.5240],
          [0.4824, 0.6549, 0.6215,  ..., 0.4097, 0.5540, 0.4007]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5267, 0.3684, 0.5619,  ..., 0.3849, 0.3294, 0.4666],
          [0.4059, 0.5409, 0.6868,  ..., 0.4941, 0.4003, 0.5128],
          [0.4700, 0.3268, 0.4192,  ..., 0.4383, 0.6215, 0.5586],
          [0.6169, 0.5607, 0.4220,  ..., 0.3835, 0.6142, 0.2838]],

         [[0.6388, 0.5732, 0.3381,  ..., 0.4073, 0.5494, 0.5651],
          [0.4930, 0.4073, 0.6758,  ..., 0.6039, 0.6252, 0.4073],
          [0.5251, 0.4173, 0.3639,  ..., 0.3016, 0.5293, 0.5983],
          [0.5453, 0.5581, 0.5484,  ..., 0.4182, 0.5479, 0.4364]],

         [[0.5636, 0.5074, 0.5518,  ..., 0.2767, 0.3757, 0.4903],
          [0.4311, 0.4726, 0.5124,  ..., 0.5501, 0.5380, 0.3372],
          [0.5407, 0.3766, 0.3702,  ..., 0.4955, 0.4868, 0.5780],
          [0.5233, 0.5679, 0.5409,  ..., 0.3998, 0.6460, 0.4050]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0010, -0.0030,  0.0010,  0.0090, -0.0110,  0.0170,  0.0090,  0.0150,
        -0.0050,  0.0130], device='cuda:0')
selected experts tensor([1861, 2059, 1825, 1660, 1624,  931, 1890, 1028, 2006, 1500],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1648., 1552., 1696., 1667., 1702., 1581., 1665., 1523., 1657., 1693.],
        [1861., 2059., 1825., 1660., 1624.,  931., 1890., 1028., 2006., 1500.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6635, 0.4042, 0.4621,  ..., 0.4166, 0.4694, 0.4242],
          [0.5061, 0.4461, 0.5924,  ..., 0.3725, 0.5388, 0.5085],
          [0.6324, 0.3359, 0.3652,  ..., 0.3590, 0.5282, 0.5343],
          [0.3030, 0.4317, 0.5672,  ..., 0.4819, 0.4948, 0.5008]],

         [[0.5229, 0.5560, 0.5262,  ..., 0.4462, 0.3005, 0.3770],
          [0.4944, 0.4599, 0.4592,  ..., 0.3871, 0.5366, 0.5734],
          [0.6387, 0.4459, 0.5161,  ..., 0.5250, 0.3741, 0.6960],
          [0.5474, 0.4403, 0.4295,  ..., 0.5166, 0.5543, 0.4171]],

         [[0.4725, 0.6418, 0.4575,  ..., 0.5563, 0.5843, 0.4711],
          [0.5102, 0.5757, 0.4833,  ..., 0.4237, 0.4066, 0.5665],
          [0.4239, 0.4759, 0.4019,  ..., 0.4687, 0.6427, 0.5210],
          [0.5028, 0.5776, 0.4237,  ..., 0.5012, 0.3446, 0.4109]],

         ...,

         [[0.4436, 0.6116, 0.5221,  ..., 0.5486, 0.6158, 0.3789],
          [0.5043, 0.5592, 0.5858,  ..., 0.4563, 0.5999, 0.5657],
          [0.5014, 0.5439, 0.5458,  ..., 0.5257, 0.5647, 0.5145],
          [0.2749, 0.5618, 0.4898,  ..., 0.3309, 0.5262, 0.4735]],

         [[0.6039, 0.5236, 0.6338,  ..., 0.6103, 0.4075, 0.4841],
          [0.5525, 0.3481, 0.4890,  ..., 0.5777, 0.4696, 0.5863],
          [0.5674, 0.5191, 0.5649,  ..., 0.5048, 0.5800, 0.4223],
          [0.4755, 0.4865, 0.4826,  ..., 0.5399, 0.4478, 0.4109]],

         [[0.4096, 0.5268, 0.4285,  ..., 0.4445, 0.3925, 0.6474],
          [0.4433, 0.4729, 0.3279,  ..., 0.4571, 0.4681, 0.5289],
          [0.4624, 0.4493, 0.5858,  ..., 0.3964, 0.4089, 0.6047],
          [0.5433, 0.3912, 0.5886,  ..., 0.4445, 0.7216, 0.4714]]],


        [[[0.4306, 0.5345, 0.5233,  ..., 0.5796, 0.7707, 0.4289],
          [0.3965, 0.4066, 0.5490,  ..., 0.4361, 0.5090, 0.4020],
          [0.4287, 0.5814, 0.5006,  ..., 0.6492, 0.4715, 0.3761],
          [0.4979, 0.5606, 0.4555,  ..., 0.5217, 0.5676, 0.5335]],

         [[0.3593, 0.4995, 0.5792,  ..., 0.4684, 0.6083, 0.3475],
          [0.5064, 0.4028, 0.6446,  ..., 0.6219, 0.4403, 0.4520],
          [0.4848, 0.5604, 0.5121,  ..., 0.4242, 0.3281, 0.6293],
          [0.3807, 0.5478, 0.4071,  ..., 0.6014, 0.4288, 0.5174]],

         [[0.4239, 0.4534, 0.4533,  ..., 0.4653, 0.5512, 0.4404],
          [0.3825, 0.5719, 0.4745,  ..., 0.4919, 0.4207, 0.3734],
          [0.5244, 0.5236, 0.4422,  ..., 0.5175, 0.3137, 0.6537],
          [0.4527, 0.5354, 0.4042,  ..., 0.4994, 0.5125, 0.6150]],

         ...,

         [[0.5135, 0.5743, 0.5658,  ..., 0.5037, 0.4993, 0.4845],
          [0.4624, 0.6097, 0.4480,  ..., 0.4006, 0.4854, 0.4337],
          [0.4484, 0.5063, 0.4142,  ..., 0.4438, 0.5924, 0.4960],
          [0.4395, 0.4108, 0.5337,  ..., 0.5669, 0.6300, 0.3987]],

         [[0.4950, 0.5137, 0.4846,  ..., 0.5192, 0.3411, 0.5691],
          [0.5588, 0.4136, 0.6560,  ..., 0.5122, 0.4468, 0.4020],
          [0.4586, 0.5388, 0.5311,  ..., 0.5051, 0.5190, 0.5691],
          [0.5129, 0.4689, 0.5687,  ..., 0.4829, 0.4360, 0.5566]],

         [[0.5827, 0.5596, 0.4151,  ..., 0.4621, 0.4427, 0.4431],
          [0.4026, 0.5104, 0.4204,  ..., 0.5196, 0.4877, 0.4428],
          [0.4561, 0.4807, 0.5213,  ..., 0.4938, 0.5127, 0.5604],
          [0.6324, 0.6273, 0.5277,  ..., 0.4670, 0.5985, 0.5309]]]],
       device='cuda:0')
tensor([[[[0.6645, 0.4012, 0.4671,  ..., 0.4116, 0.4664, 0.4192],
          [0.5071, 0.4431, 0.5974,  ..., 0.3675, 0.5358, 0.5035],
          [0.6334, 0.3329, 0.3702,  ..., 0.3540, 0.5252, 0.5293],
          [0.3040, 0.4287, 0.5722,  ..., 0.4769, 0.4918, 0.4958]],

         [[0.5239, 0.5530, 0.5312,  ..., 0.4412, 0.2975, 0.3720],
          [0.4954, 0.4569, 0.4642,  ..., 0.3821, 0.5336, 0.5684],
          [0.6397, 0.4429, 0.5211,  ..., 0.5200, 0.3711, 0.6910],
          [0.5484, 0.4373, 0.4345,  ..., 0.5116, 0.5513, 0.4121]],

         [[0.4735, 0.6388, 0.4625,  ..., 0.5513, 0.5813, 0.4661],
          [0.5112, 0.5727, 0.4883,  ..., 0.4187, 0.4036, 0.5615],
          [0.4249, 0.4729, 0.4069,  ..., 0.4637, 0.6397, 0.5160],
          [0.5038, 0.5746, 0.4287,  ..., 0.4962, 0.3416, 0.4059]],

         ...,

         [[0.4446, 0.6086, 0.5271,  ..., 0.5436, 0.6128, 0.3739],
          [0.5053, 0.5562, 0.5908,  ..., 0.4513, 0.5969, 0.5607],
          [0.5024, 0.5409, 0.5508,  ..., 0.5207, 0.5617, 0.5095],
          [0.2759, 0.5588, 0.4948,  ..., 0.3259, 0.5232, 0.4685]],

         [[0.6049, 0.5206, 0.6388,  ..., 0.6053, 0.4045, 0.4791],
          [0.5535, 0.3451, 0.4940,  ..., 0.5727, 0.4666, 0.5813],
          [0.5684, 0.5161, 0.5699,  ..., 0.4998, 0.5770, 0.4173],
          [0.4765, 0.4835, 0.4876,  ..., 0.5349, 0.4448, 0.4059]],

         [[0.4106, 0.5238, 0.4335,  ..., 0.4395, 0.3895, 0.6424],
          [0.4443, 0.4699, 0.3329,  ..., 0.4521, 0.4651, 0.5239],
          [0.4634, 0.4463, 0.5908,  ..., 0.3914, 0.4059, 0.5997],
          [0.5443, 0.3882, 0.5936,  ..., 0.4395, 0.7186, 0.4664]]],


        [[[0.4316, 0.5315, 0.5283,  ..., 0.5746, 0.7677, 0.4239],
          [0.3975, 0.4036, 0.5540,  ..., 0.4311, 0.5060, 0.3970],
          [0.4297, 0.5784, 0.5056,  ..., 0.6442, 0.4685, 0.3711],
          [0.4989, 0.5576, 0.4605,  ..., 0.5167, 0.5646, 0.5285]],

         [[0.3603, 0.4965, 0.5842,  ..., 0.4634, 0.6053, 0.3425],
          [0.5074, 0.3998, 0.6496,  ..., 0.6169, 0.4373, 0.4470],
          [0.4858, 0.5574, 0.5171,  ..., 0.4192, 0.3251, 0.6243],
          [0.3817, 0.5448, 0.4121,  ..., 0.5964, 0.4258, 0.5124]],

         [[0.4249, 0.4504, 0.4583,  ..., 0.4603, 0.5482, 0.4354],
          [0.3835, 0.5689, 0.4795,  ..., 0.4869, 0.4177, 0.3684],
          [0.5254, 0.5206, 0.4472,  ..., 0.5125, 0.3107, 0.6487],
          [0.4537, 0.5324, 0.4092,  ..., 0.4944, 0.5095, 0.6100]],

         ...,

         [[0.5145, 0.5713, 0.5708,  ..., 0.4987, 0.4963, 0.4795],
          [0.4634, 0.6067, 0.4530,  ..., 0.3956, 0.4824, 0.4287],
          [0.4494, 0.5033, 0.4192,  ..., 0.4388, 0.5894, 0.4910],
          [0.4405, 0.4078, 0.5387,  ..., 0.5619, 0.6270, 0.3937]],

         [[0.4960, 0.5107, 0.4896,  ..., 0.5142, 0.3381, 0.5641],
          [0.5598, 0.4106, 0.6610,  ..., 0.5072, 0.4438, 0.3970],
          [0.4596, 0.5358, 0.5361,  ..., 0.5001, 0.5160, 0.5641],
          [0.5139, 0.4659, 0.5737,  ..., 0.4779, 0.4330, 0.5516]],

         [[0.5837, 0.5566, 0.4201,  ..., 0.4571, 0.4397, 0.4381],
          [0.4036, 0.5074, 0.4254,  ..., 0.5146, 0.4847, 0.4378],
          [0.4571, 0.4777, 0.5263,  ..., 0.4888, 0.5097, 0.5554],
          [0.6334, 0.6243, 0.5327,  ..., 0.4620, 0.5955, 0.5259]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0010,  0.0030, -0.0050,  0.0010, -0.0030, -0.0070,  0.0070,  0.0050,
         0.0030,  0.0050], device='cuda:0')
selected experts tensor([1691, 1597, 1651, 1640, 1687, 1472, 1631, 1700, 1597, 1718],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.3738, 0.4941, 0.3606,  ..., 0.4019, 0.5223, 0.5538],
          [0.4934, 0.5852, 0.4616,  ..., 0.3253, 0.4056, 0.5540],
          [0.5387, 0.4208, 0.4427,  ..., 0.5410, 0.4970, 0.5075],
          [0.6001, 0.4601, 0.5417,  ..., 0.4194, 0.3787, 0.4555]],

         [[0.4764, 0.3802, 0.5156,  ..., 0.4075, 0.5748, 0.5499],
          [0.4975, 0.5743, 0.6544,  ..., 0.4887, 0.4630, 0.5937],
          [0.4433, 0.5705, 0.5107,  ..., 0.4572, 0.4203, 0.4249],
          [0.5457, 0.4284, 0.3642,  ..., 0.7214, 0.6973, 0.3604]],

         [[0.6722, 0.4594, 0.5021,  ..., 0.4953, 0.5596, 0.5713],
          [0.4540, 0.3719, 0.3977,  ..., 0.5900, 0.4954, 0.5270],
          [0.4858, 0.5126, 0.5342,  ..., 0.5296, 0.5191, 0.4858],
          [0.5784, 0.6567, 0.4000,  ..., 0.3616, 0.5225, 0.4316]],

         ...,

         [[0.5305, 0.3742, 0.5393,  ..., 0.3571, 0.4652, 0.5306],
          [0.4115, 0.4265, 0.5454,  ..., 0.6078, 0.4935, 0.4867],
          [0.5258, 0.4550, 0.4944,  ..., 0.4740, 0.4986, 0.5211],
          [0.5600, 0.5913, 0.5110,  ..., 0.6975, 0.3732, 0.4635]],

         [[0.4701, 0.4535, 0.4560,  ..., 0.3855, 0.4346, 0.4278],
          [0.5077, 0.5638, 0.5167,  ..., 0.4711, 0.5519, 0.6244],
          [0.5571, 0.4046, 0.4893,  ..., 0.4199, 0.3842, 0.5346],
          [0.5467, 0.5819, 0.5857,  ..., 0.4104, 0.4597, 0.6105]],

         [[0.4453, 0.6164, 0.5342,  ..., 0.4953, 0.6227, 0.5913],
          [0.4913, 0.4501, 0.5454,  ..., 0.6045, 0.4047, 0.3550],
          [0.5197, 0.5264, 0.5522,  ..., 0.5077, 0.5705, 0.5339],
          [0.6224, 0.5156, 0.4765,  ..., 0.5396, 0.5442, 0.3426]]],


        [[[0.5803, 0.4017, 0.6552,  ..., 0.5257, 0.4117, 0.4369],
          [0.5612, 0.4789, 0.5505,  ..., 0.4834, 0.5434, 0.6434],
          [0.4139, 0.5011, 0.5488,  ..., 0.4104, 0.5313, 0.4776],
          [0.4891, 0.4780, 0.5620,  ..., 0.6587, 0.4151, 0.5176]],

         [[0.4559, 0.4938, 0.3633,  ..., 0.5089, 0.5287, 0.4417],
          [0.4610, 0.5083, 0.5144,  ..., 0.6311, 0.5478, 0.5885],
          [0.4167, 0.5091, 0.5158,  ..., 0.6022, 0.5269, 0.5431],
          [0.4673, 0.6164, 0.4408,  ..., 0.4487, 0.6018, 0.5646]],

         [[0.6205, 0.5376, 0.5933,  ..., 0.4524, 0.5386, 0.4953],
          [0.5736, 0.2281, 0.5226,  ..., 0.3436, 0.5220, 0.5178],
          [0.5317, 0.5036, 0.4623,  ..., 0.6750, 0.5447, 0.4961],
          [0.5021, 0.6210, 0.4430,  ..., 0.5768, 0.6195, 0.5132]],

         ...,

         [[0.4772, 0.4964, 0.4927,  ..., 0.5352, 0.5153, 0.5533],
          [0.5093, 0.6122, 0.4255,  ..., 0.6068, 0.4370, 0.6335],
          [0.5297, 0.4260, 0.5886,  ..., 0.5265, 0.5313, 0.2840],
          [0.4740, 0.7012, 0.5341,  ..., 0.5896, 0.4942, 0.4074]],

         [[0.5258, 0.3975, 0.4411,  ..., 0.5294, 0.4852, 0.3640],
          [0.4134, 0.4781, 0.5234,  ..., 0.6133, 0.5531, 0.5451],
          [0.6432, 0.3614, 0.4531,  ..., 0.4729, 0.5570, 0.4764],
          [0.5186, 0.5203, 0.5354,  ..., 0.5730, 0.4179, 0.3749]],

         [[0.4378, 0.4477, 0.5463,  ..., 0.5429, 0.5039, 0.6568],
          [0.6504, 0.4713, 0.5685,  ..., 0.4752, 0.4052, 0.5025],
          [0.4537, 0.4780, 0.4715,  ..., 0.5335, 0.5161, 0.6470],
          [0.5365, 0.6756, 0.5278,  ..., 0.4137, 0.6856, 0.5516]]]],
       device='cuda:0')
tensor([[[[0.3748, 0.5011, 0.3576,  ..., 0.4069, 0.5193, 0.5528],
          [0.4944, 0.5922, 0.4586,  ..., 0.3303, 0.4026, 0.5530],
          [0.5397, 0.4278, 0.4397,  ..., 0.5460, 0.4940, 0.5065],
          [0.6011, 0.4671, 0.5387,  ..., 0.4244, 0.3757, 0.4545]],

         [[0.4774, 0.3872, 0.5126,  ..., 0.4125, 0.5718, 0.5489],
          [0.4985, 0.5813, 0.6514,  ..., 0.4937, 0.4600, 0.5927],
          [0.4443, 0.5775, 0.5077,  ..., 0.4622, 0.4173, 0.4239],
          [0.5467, 0.4354, 0.3612,  ..., 0.7264, 0.6943, 0.3594]],

         [[0.6732, 0.4664, 0.4991,  ..., 0.5003, 0.5566, 0.5703],
          [0.4550, 0.3789, 0.3947,  ..., 0.5950, 0.4924, 0.5260],
          [0.4868, 0.5196, 0.5312,  ..., 0.5346, 0.5161, 0.4848],
          [0.5794, 0.6637, 0.3970,  ..., 0.3666, 0.5195, 0.4306]],

         ...,

         [[0.5315, 0.3812, 0.5363,  ..., 0.3621, 0.4622, 0.5296],
          [0.4125, 0.4335, 0.5424,  ..., 0.6128, 0.4905, 0.4857],
          [0.5268, 0.4620, 0.4914,  ..., 0.4790, 0.4956, 0.5201],
          [0.5610, 0.5983, 0.5080,  ..., 0.7025, 0.3702, 0.4625]],

         [[0.4711, 0.4605, 0.4530,  ..., 0.3905, 0.4316, 0.4268],
          [0.5087, 0.5708, 0.5137,  ..., 0.4761, 0.5489, 0.6234],
          [0.5581, 0.4116, 0.4863,  ..., 0.4249, 0.3812, 0.5336],
          [0.5477, 0.5889, 0.5827,  ..., 0.4154, 0.4567, 0.6095]],

         [[0.4463, 0.6234, 0.5312,  ..., 0.5003, 0.6197, 0.5903],
          [0.4923, 0.4571, 0.5424,  ..., 0.6095, 0.4017, 0.3540],
          [0.5207, 0.5334, 0.5492,  ..., 0.5127, 0.5675, 0.5329],
          [0.6234, 0.5226, 0.4735,  ..., 0.5446, 0.5412, 0.3416]]],


        [[[0.5813, 0.4087, 0.6522,  ..., 0.5307, 0.4087, 0.4359],
          [0.5622, 0.4859, 0.5475,  ..., 0.4884, 0.5404, 0.6424],
          [0.4149, 0.5081, 0.5458,  ..., 0.4154, 0.5283, 0.4766],
          [0.4901, 0.4850, 0.5590,  ..., 0.6637, 0.4121, 0.5166]],

         [[0.4569, 0.5008, 0.3603,  ..., 0.5139, 0.5257, 0.4407],
          [0.4620, 0.5153, 0.5114,  ..., 0.6361, 0.5448, 0.5875],
          [0.4177, 0.5161, 0.5128,  ..., 0.6072, 0.5239, 0.5421],
          [0.4683, 0.6234, 0.4378,  ..., 0.4537, 0.5988, 0.5636]],

         [[0.6215, 0.5446, 0.5903,  ..., 0.4574, 0.5356, 0.4943],
          [0.5746, 0.2351, 0.5196,  ..., 0.3486, 0.5190, 0.5168],
          [0.5327, 0.5106, 0.4593,  ..., 0.6800, 0.5417, 0.4951],
          [0.5031, 0.6280, 0.4400,  ..., 0.5818, 0.6165, 0.5122]],

         ...,

         [[0.4782, 0.5034, 0.4897,  ..., 0.5402, 0.5123, 0.5523],
          [0.5103, 0.6192, 0.4225,  ..., 0.6118, 0.4340, 0.6325],
          [0.5307, 0.4330, 0.5856,  ..., 0.5315, 0.5283, 0.2830],
          [0.4750, 0.7082, 0.5311,  ..., 0.5946, 0.4912, 0.4064]],

         [[0.5268, 0.4045, 0.4381,  ..., 0.5344, 0.4822, 0.3630],
          [0.4144, 0.4851, 0.5204,  ..., 0.6183, 0.5501, 0.5441],
          [0.6442, 0.3684, 0.4501,  ..., 0.4779, 0.5540, 0.4754],
          [0.5196, 0.5273, 0.5324,  ..., 0.5780, 0.4149, 0.3739]],

         [[0.4388, 0.4547, 0.5433,  ..., 0.5479, 0.5009, 0.6558],
          [0.6514, 0.4783, 0.5655,  ..., 0.4802, 0.4022, 0.5015],
          [0.4547, 0.4850, 0.4685,  ..., 0.5385, 0.5131, 0.6460],
          [0.5375, 0.6826, 0.5248,  ..., 0.4187, 0.6826, 0.5506]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0010, -0.0070,  0.0030,  0.0090, -0.0030,  0.0090, -0.0130, -0.0050,
         0.0030,  0.0010], device='cuda:0')
selected experts tensor([1882, 1556, 1742, 1700, 1517, 1540, 1614, 1581, 1619, 1633],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4631, 0.6632, 0.5292,  ..., 0.4789, 0.4945, 0.4400],
          [0.5087, 0.5507, 0.5319,  ..., 0.4586, 0.5563, 0.4898],
          [0.3972, 0.5185, 0.5548,  ..., 0.4593, 0.5290, 0.4075],
          [0.6718, 0.4011, 0.5274,  ..., 0.4989, 0.5125, 0.5420]],

         [[0.4998, 0.6360, 0.4725,  ..., 0.4912, 0.5937, 0.4481],
          [0.5606, 0.5613, 0.6045,  ..., 0.5198, 0.5011, 0.6083],
          [0.4398, 0.6458, 0.4732,  ..., 0.4750, 0.3660, 0.3640],
          [0.5386, 0.4111, 0.5432,  ..., 0.5390, 0.4420, 0.5907]],

         [[0.5214, 0.4779, 0.4808,  ..., 0.4111, 0.3731, 0.5768],
          [0.4214, 0.5111, 0.4255,  ..., 0.5879, 0.6480, 0.5469],
          [0.4773, 0.4721, 0.5723,  ..., 0.4436, 0.6146, 0.5604],
          [0.5744, 0.4011, 0.5475,  ..., 0.5789, 0.4463, 0.5141]],

         ...,

         [[0.3563, 0.5016, 0.5347,  ..., 0.5530, 0.4808, 0.5936],
          [0.4816, 0.5594, 0.5257,  ..., 0.5339, 0.5114, 0.5780],
          [0.5906, 0.4058, 0.4051,  ..., 0.4434, 0.4974, 0.5098],
          [0.5957, 0.5184, 0.3957,  ..., 0.4359, 0.4501, 0.4790]],

         [[0.4338, 0.5651, 0.4926,  ..., 0.5081, 0.4711, 0.5775],
          [0.3680, 0.6288, 0.5073,  ..., 0.4983, 0.5629, 0.4334],
          [0.5802, 0.5016, 0.5366,  ..., 0.4958, 0.6065, 0.6007],
          [0.5420, 0.6028, 0.4312,  ..., 0.4182, 0.4747, 0.5400]],

         [[0.6180, 0.4384, 0.4226,  ..., 0.4827, 0.6211, 0.5567],
          [0.4394, 0.4931, 0.5028,  ..., 0.5547, 0.5679, 0.4893],
          [0.5635, 0.4016, 0.5052,  ..., 0.4937, 0.5328, 0.5189],
          [0.4871, 0.3992, 0.4023,  ..., 0.4815, 0.4222, 0.4320]]],


        [[[0.4413, 0.5850, 0.5651,  ..., 0.5259, 0.4820, 0.6219],
          [0.4379, 0.5841, 0.4787,  ..., 0.4855, 0.8053, 0.5040],
          [0.4309, 0.4372, 0.6462,  ..., 0.5489, 0.4439, 0.5676],
          [0.4939, 0.5519, 0.5417,  ..., 0.5426, 0.4425, 0.4831]],

         [[0.4889, 0.5051, 0.4620,  ..., 0.4287, 0.4021, 0.6702],
          [0.5296, 0.6023, 0.4126,  ..., 0.6575, 0.5239, 0.6031],
          [0.5985, 0.4040, 0.6345,  ..., 0.4986, 0.3651, 0.6126],
          [0.6240, 0.4025, 0.4836,  ..., 0.4441, 0.4151, 0.5369]],

         [[0.4218, 0.5151, 0.5072,  ..., 0.5946, 0.4478, 0.6808],
          [0.4114, 0.6554, 0.4628,  ..., 0.4938, 0.7480, 0.4948],
          [0.5253, 0.4285, 0.3971,  ..., 0.4711, 0.4147, 0.5287],
          [0.5372, 0.5103, 0.4548,  ..., 0.5770, 0.4401, 0.4145]],

         ...,

         [[0.4717, 0.4643, 0.6013,  ..., 0.5458, 0.4723, 0.5439],
          [0.4252, 0.5864, 0.5843,  ..., 0.5827, 0.5636, 0.5361],
          [0.4921, 0.4225, 0.5632,  ..., 0.4898, 0.5609, 0.6055],
          [0.4391, 0.5046, 0.4899,  ..., 0.5392, 0.3537, 0.3578]],

         [[0.6046, 0.3015, 0.4245,  ..., 0.4810, 0.5155, 0.5792],
          [0.3735, 0.4771, 0.5608,  ..., 0.4847, 0.6552, 0.5420],
          [0.4816, 0.3884, 0.4952,  ..., 0.4769, 0.5918, 0.5197],
          [0.4343, 0.5280, 0.4550,  ..., 0.4402, 0.3357, 0.3792]],

         [[0.4607, 0.5737, 0.4360,  ..., 0.4111, 0.5984, 0.5308],
          [0.3837, 0.5256, 0.3925,  ..., 0.6234, 0.6342, 0.6595],
          [0.5820, 0.4738, 0.6069,  ..., 0.4154, 0.5860, 0.5584],
          [0.5114, 0.5287, 0.4509,  ..., 0.5179, 0.3651, 0.4783]]]],
       device='cuda:0')
tensor([[[[0.4671, 0.6732, 0.5272,  ..., 0.4789, 0.4745, 0.4220],
          [0.5127, 0.5607, 0.5299,  ..., 0.4586, 0.5363, 0.4718],
          [0.4012, 0.5285, 0.5528,  ..., 0.4593, 0.5090, 0.3895],
          [0.6758, 0.4111, 0.5254,  ..., 0.4989, 0.4925, 0.5240]],

         [[0.5038, 0.6460, 0.4705,  ..., 0.4912, 0.5737, 0.4301],
          [0.5646, 0.5713, 0.6025,  ..., 0.5198, 0.4811, 0.5903],
          [0.4438, 0.6558, 0.4712,  ..., 0.4750, 0.3460, 0.3460],
          [0.5426, 0.4211, 0.5412,  ..., 0.5390, 0.4220, 0.5727]],

         [[0.5254, 0.4879, 0.4788,  ..., 0.4111, 0.3531, 0.5588],
          [0.4254, 0.5211, 0.4235,  ..., 0.5879, 0.6280, 0.5289],
          [0.4813, 0.4821, 0.5703,  ..., 0.4436, 0.5946, 0.5424],
          [0.5784, 0.4111, 0.5455,  ..., 0.5789, 0.4263, 0.4961]],

         ...,

         [[0.3603, 0.5116, 0.5327,  ..., 0.5530, 0.4608, 0.5756],
          [0.4856, 0.5694, 0.5237,  ..., 0.5339, 0.4914, 0.5600],
          [0.5946, 0.4158, 0.4031,  ..., 0.4434, 0.4774, 0.4918],
          [0.5997, 0.5284, 0.3937,  ..., 0.4359, 0.4301, 0.4610]],

         [[0.4378, 0.5751, 0.4906,  ..., 0.5081, 0.4511, 0.5595],
          [0.3720, 0.6388, 0.5053,  ..., 0.4983, 0.5429, 0.4154],
          [0.5842, 0.5116, 0.5346,  ..., 0.4958, 0.5865, 0.5827],
          [0.5460, 0.6128, 0.4292,  ..., 0.4182, 0.4547, 0.5220]],

         [[0.6220, 0.4484, 0.4206,  ..., 0.4827, 0.6011, 0.5387],
          [0.4434, 0.5031, 0.5008,  ..., 0.5547, 0.5479, 0.4713],
          [0.5675, 0.4116, 0.5032,  ..., 0.4937, 0.5128, 0.5009],
          [0.4911, 0.4092, 0.4003,  ..., 0.4815, 0.4022, 0.4140]]],


        [[[0.4453, 0.5950, 0.5631,  ..., 0.5259, 0.4620, 0.6039],
          [0.4419, 0.5941, 0.4767,  ..., 0.4855, 0.7853, 0.4860],
          [0.4349, 0.4472, 0.6442,  ..., 0.5489, 0.4239, 0.5496],
          [0.4979, 0.5619, 0.5397,  ..., 0.5426, 0.4225, 0.4651]],

         [[0.4929, 0.5151, 0.4600,  ..., 0.4287, 0.3821, 0.6522],
          [0.5336, 0.6123, 0.4106,  ..., 0.6575, 0.5039, 0.5851],
          [0.6025, 0.4140, 0.6325,  ..., 0.4986, 0.3451, 0.5946],
          [0.6280, 0.4125, 0.4816,  ..., 0.4441, 0.3951, 0.5189]],

         [[0.4258, 0.5251, 0.5052,  ..., 0.5946, 0.4278, 0.6628],
          [0.4154, 0.6654, 0.4608,  ..., 0.4938, 0.7280, 0.4768],
          [0.5293, 0.4385, 0.3951,  ..., 0.4711, 0.3947, 0.5107],
          [0.5412, 0.5203, 0.4528,  ..., 0.5770, 0.4201, 0.3965]],

         ...,

         [[0.4757, 0.4743, 0.5993,  ..., 0.5458, 0.4523, 0.5259],
          [0.4292, 0.5964, 0.5823,  ..., 0.5827, 0.5436, 0.5181],
          [0.4961, 0.4325, 0.5612,  ..., 0.4898, 0.5409, 0.5875],
          [0.4431, 0.5146, 0.4879,  ..., 0.5392, 0.3337, 0.3398]],

         [[0.6086, 0.3115, 0.4225,  ..., 0.4810, 0.4955, 0.5612],
          [0.3775, 0.4871, 0.5588,  ..., 0.4847, 0.6352, 0.5240],
          [0.4856, 0.3984, 0.4932,  ..., 0.4769, 0.5718, 0.5017],
          [0.4383, 0.5380, 0.4530,  ..., 0.4402, 0.3157, 0.3612]],

         [[0.4647, 0.5837, 0.4340,  ..., 0.4111, 0.5784, 0.5128],
          [0.3877, 0.5356, 0.3905,  ..., 0.6234, 0.6142, 0.6415],
          [0.5860, 0.4838, 0.6049,  ..., 0.4154, 0.5660, 0.5404],
          [0.5154, 0.5387, 0.4489,  ..., 0.5179, 0.3451, 0.4603]]]],
       device='cuda:0', requires_grad=True)
tensor([-4.0000e-03, -1.0000e-02,  2.0000e-03, -1.0000e-02, -1.0000e-02,
         1.8000e-02, -3.0000e-02,  2.3283e-10,  2.0000e-02,  1.8000e-02],
       device='cuda:0')
selected experts tensor([1167, 1475, 1084, 2436,  872, 1764, 1898, 1498, 1911, 2279],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4899, 0.5345, 0.4844,  ..., 0.4560, 0.5220, 0.7083],
          [0.4624, 0.5783, 0.6016,  ..., 0.3700, 0.6119, 0.3680],
          [0.5907, 0.5244, 0.4426,  ..., 0.3890, 0.4371, 0.5006],
          [0.4884, 0.5534, 0.5631,  ..., 0.4555, 0.4967, 0.4600]],

         [[0.4387, 0.5654, 0.4823,  ..., 0.4314, 0.3938, 0.4753],
          [0.3556, 0.5043, 0.6645,  ..., 0.6190, 0.5058, 0.4256],
          [0.6377, 0.5587, 0.4904,  ..., 0.5111, 0.5111, 0.7075],
          [0.5902, 0.5560, 0.4700,  ..., 0.4618, 0.4470, 0.4936]],

         [[0.5043, 0.4660, 0.5138,  ..., 0.5719, 0.4198, 0.4365],
          [0.4779, 0.5716, 0.6077,  ..., 0.5383, 0.6183, 0.5795],
          [0.5864, 0.4137, 0.5118,  ..., 0.6394, 0.5904, 0.6863],
          [0.6296, 0.5962, 0.5043,  ..., 0.4191, 0.5691, 0.4351]],

         ...,

         [[0.4980, 0.4960, 0.5000,  ..., 0.5160, 0.4940, 0.5140],
          [0.4980, 0.4960, 0.5000,  ..., 0.5160, 0.4940, 0.5140],
          [0.4980, 0.4960, 0.5000,  ..., 0.5160, 0.4940, 0.5140],
          [0.4980, 0.4960, 0.5000,  ..., 0.5160, 0.4940, 0.5140]],

         [[0.4980, 0.4960, 0.5000,  ..., 0.5160, 0.4940, 0.5140],
          [0.4980, 0.4960, 0.5000,  ..., 0.5160, 0.4940, 0.5140],
          [0.4980, 0.4960, 0.5000,  ..., 0.5160, 0.4940, 0.5140],
          [0.4980, 0.4960, 0.5000,  ..., 0.5160, 0.4940, 0.5140]],

         [[0.4980, 0.4960, 0.5000,  ..., 0.5160, 0.4940, 0.5140],
          [0.4980, 0.4960, 0.5000,  ..., 0.5160, 0.4940, 0.5140],
          [0.4980, 0.4960, 0.5000,  ..., 0.5160, 0.4940, 0.5140],
          [0.4980, 0.4960, 0.5000,  ..., 0.5160, 0.4940, 0.5140]]],


        [[[0.5438, 0.5141, 0.5453,  ..., 0.4843, 0.4328, 0.5661],
          [0.3765, 0.6977, 0.5978,  ..., 0.5056, 0.4841, 0.4588],
          [0.5269, 0.4403, 0.3442,  ..., 0.4746, 0.5682, 0.6240],
          [0.6205, 0.5313, 0.6225,  ..., 0.3629, 0.5562, 0.3408]],

         [[0.5722, 0.5510, 0.4915,  ..., 0.4107, 0.3269, 0.6332],
          [0.5297, 0.4367, 0.5651,  ..., 0.5526, 0.5619, 0.4794],
          [0.4455, 0.3916, 0.3937,  ..., 0.5482, 0.5118, 0.5388],
          [0.5755, 0.6125, 0.5336,  ..., 0.4543, 0.5857, 0.4317]],

         [[0.5023, 0.4849, 0.5732,  ..., 0.4023, 0.4136, 0.7100],
          [0.4865, 0.4821, 0.7066,  ..., 0.5630, 0.5720, 0.4303],
          [0.4851, 0.3949, 0.3854,  ..., 0.5079, 0.5535, 0.5605],
          [0.6080, 0.4782, 0.4506,  ..., 0.3558, 0.6274, 0.4908]],

         ...,

         [[0.4980, 0.4960, 0.5000,  ..., 0.5160, 0.4940, 0.5140],
          [0.4980, 0.4960, 0.5000,  ..., 0.5160, 0.4940, 0.5140],
          [0.4980, 0.4960, 0.5000,  ..., 0.5160, 0.4940, 0.5140],
          [0.4980, 0.4960, 0.5000,  ..., 0.5160, 0.4940, 0.5140]],

         [[0.4980, 0.4960, 0.5000,  ..., 0.5160, 0.4940, 0.5140],
          [0.4980, 0.4960, 0.5000,  ..., 0.5160, 0.4940, 0.5140],
          [0.4980, 0.4960, 0.5000,  ..., 0.5160, 0.4940, 0.5140],
          [0.4980, 0.4960, 0.5000,  ..., 0.5160, 0.4940, 0.5140]],

         [[0.4980, 0.4960, 0.5000,  ..., 0.5160, 0.4940, 0.5140],
          [0.4980, 0.4960, 0.5000,  ..., 0.5160, 0.4940, 0.5140],
          [0.4980, 0.4960, 0.5000,  ..., 0.5160, 0.4940, 0.5140],
          [0.4980, 0.4960, 0.5000,  ..., 0.5160, 0.4940, 0.5140]]]],
       device='cuda:0')
tensor([[[[0.4919, 0.5385, 0.4844,  ..., 0.4400, 0.5280, 0.6943],
          [0.4644, 0.5823, 0.6016,  ..., 0.3540, 0.6179, 0.3540],
          [0.5927, 0.5284, 0.4426,  ..., 0.3730, 0.4431, 0.4866],
          [0.4904, 0.5574, 0.5631,  ..., 0.4395, 0.5027, 0.4460]],

         [[0.4407, 0.5694, 0.4823,  ..., 0.4154, 0.3998, 0.4613],
          [0.3576, 0.5083, 0.6645,  ..., 0.6030, 0.5118, 0.4116],
          [0.6397, 0.5627, 0.4904,  ..., 0.4951, 0.5171, 0.6935],
          [0.5922, 0.5600, 0.4700,  ..., 0.4458, 0.4530, 0.4796]],

         [[0.5063, 0.4700, 0.5138,  ..., 0.5559, 0.4258, 0.4225],
          [0.4799, 0.5756, 0.6077,  ..., 0.5223, 0.6243, 0.5655],
          [0.5884, 0.4177, 0.5118,  ..., 0.6234, 0.5964, 0.6723],
          [0.6316, 0.6002, 0.5043,  ..., 0.4031, 0.5751, 0.4211]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5458, 0.5181, 0.5453,  ..., 0.4683, 0.4388, 0.5521],
          [0.3785, 0.7017, 0.5978,  ..., 0.4896, 0.4901, 0.4448],
          [0.5289, 0.4443, 0.3442,  ..., 0.4586, 0.5742, 0.6100],
          [0.6225, 0.5353, 0.6225,  ..., 0.3469, 0.5622, 0.3268]],

         [[0.5742, 0.5550, 0.4915,  ..., 0.3947, 0.3329, 0.6192],
          [0.5317, 0.4407, 0.5651,  ..., 0.5366, 0.5679, 0.4654],
          [0.4475, 0.3956, 0.3937,  ..., 0.5322, 0.5178, 0.5248],
          [0.5775, 0.6165, 0.5336,  ..., 0.4383, 0.5917, 0.4177]],

         [[0.5043, 0.4889, 0.5732,  ..., 0.3863, 0.4196, 0.6960],
          [0.4885, 0.4861, 0.7066,  ..., 0.5470, 0.5780, 0.4163],
          [0.4871, 0.3989, 0.3854,  ..., 0.4919, 0.5595, 0.5465],
          [0.6100, 0.4822, 0.4506,  ..., 0.3398, 0.6334, 0.4768]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([-2.0000e-03, -4.0000e-03, -2.3283e-10,  8.0000e-03, -1.0000e-02,
         1.8000e-02,  8.0000e-03,  1.6000e-02, -6.0000e-03,  1.4000e-02],
       device='cuda:0')
selected experts tensor([1642, 1603, 2035, 1429, 1370,  919, 2117, 1032, 1763, 2474],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1882., 1556., 1742., 1700., 1517., 1540., 1614., 1581., 1619., 1633.],
        [1642., 1603., 2035., 1429., 1370.,  919., 2117., 1032., 1763., 2474.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4812, 0.6668, 0.3905,  ..., 0.2638, 0.5162, 0.4988],
          [0.5126, 0.6356, 0.5419,  ..., 0.4113, 0.4418, 0.4679],
          [0.5360, 0.4490, 0.3882,  ..., 0.5681, 0.5867, 0.6103],
          [0.4200, 0.3912, 0.3929,  ..., 0.4802, 0.3843, 0.4758]],

         [[0.5128, 0.5753, 0.5648,  ..., 0.5026, 0.5009, 0.4835],
          [0.4614, 0.6107, 0.4468,  ..., 0.3996, 0.4864, 0.4332],
          [0.4474, 0.5073, 0.4132,  ..., 0.4425, 0.5938, 0.4950],
          [0.4385, 0.4118, 0.5327,  ..., 0.5659, 0.6310, 0.3977]],

         [[0.5284, 0.6991, 0.5189,  ..., 0.4341, 0.3571, 0.5882],
          [0.5793, 0.3343, 0.5337,  ..., 0.5582, 0.4907, 0.6437],
          [0.5015, 0.5285, 0.3812,  ..., 0.3535, 0.4912, 0.5938],
          [0.5029, 0.4365, 0.4089,  ..., 0.4047, 0.4146, 0.4529]],

         ...,

         [[0.6377, 0.3465, 0.5163,  ..., 0.4914, 0.5801, 0.3949],
          [0.4917, 0.5796, 0.4859,  ..., 0.5449, 0.5053, 0.4991],
          [0.5252, 0.5340, 0.4621,  ..., 0.3257, 0.3715, 0.4933],
          [0.4349, 0.4633, 0.3999,  ..., 0.6763, 0.5710, 0.5630]],

         [[0.3834, 0.5186, 0.5610,  ..., 0.4713, 0.4751, 0.4194],
          [0.5850, 0.4809, 0.5337,  ..., 0.4568, 0.6500, 0.5461],
          [0.4734, 0.5772, 0.4536,  ..., 0.5403, 0.6042, 0.4891],
          [0.5566, 0.6554, 0.4794,  ..., 0.5386, 0.4728, 0.4080]],

         [[0.4929, 0.5084, 0.5073,  ..., 0.4953, 0.4222, 0.4952],
          [0.5669, 0.3257, 0.3579,  ..., 0.4867, 0.4623, 0.5016],
          [0.4488, 0.3825, 0.5819,  ..., 0.4217, 0.4356, 0.4303],
          [0.5587, 0.4869, 0.6082,  ..., 0.3214, 0.4318, 0.3806]]],


        [[[0.5041, 0.5471, 0.6732,  ..., 0.5010, 0.4648, 0.3056],
          [0.5397, 0.5250, 0.6274,  ..., 0.3544, 0.5357, 0.6446],
          [0.5101, 0.5616, 0.4817,  ..., 0.5263, 0.6642, 0.4194],
          [0.5030, 0.5018, 0.3766,  ..., 0.5476, 0.5934, 0.5953]],

         [[0.6260, 0.3386, 0.4694,  ..., 0.5493, 0.5279, 0.5297],
          [0.3815, 0.4679, 0.5824,  ..., 0.4447, 0.5401, 0.6051],
          [0.4561, 0.6392, 0.4560,  ..., 0.6729, 0.3482, 0.5464],
          [0.5089, 0.5133, 0.5228,  ..., 0.6028, 0.4718, 0.4568]],

         [[0.4496, 0.5529, 0.4689,  ..., 0.5858, 0.6172, 0.4599],
          [0.3628, 0.5206, 0.3191,  ..., 0.3926, 0.6274, 0.5558],
          [0.4520, 0.5657, 0.6210,  ..., 0.4894, 0.6347, 0.3491],
          [0.4544, 0.5403, 0.4908,  ..., 0.3838, 0.4945, 0.3959]],

         ...,

         [[0.5128, 0.5753, 0.5648,  ..., 0.5026, 0.5009, 0.4835],
          [0.4614, 0.6107, 0.4468,  ..., 0.3996, 0.4864, 0.4332],
          [0.4474, 0.5073, 0.4132,  ..., 0.4425, 0.5938, 0.4950],
          [0.4385, 0.4118, 0.5327,  ..., 0.5659, 0.6310, 0.3977]],

         [[0.5079, 0.6061, 0.5710,  ..., 0.5556, 0.3982, 0.4687],
          [0.5869, 0.5189, 0.4362,  ..., 0.4725, 0.4708, 0.5432],
          [0.5440, 0.5495, 0.2882,  ..., 0.5938, 0.5133, 0.5030],
          [0.5717, 0.4739, 0.4294,  ..., 0.4931, 0.6857, 0.4486]],

         [[0.5041, 0.5471, 0.6732,  ..., 0.5010, 0.4648, 0.3056],
          [0.5397, 0.5250, 0.6274,  ..., 0.3544, 0.5357, 0.6446],
          [0.5101, 0.5616, 0.4817,  ..., 0.5263, 0.6642, 0.4194],
          [0.5030, 0.5018, 0.3766,  ..., 0.5476, 0.5934, 0.5953]]]],
       device='cuda:0')
tensor([[[[0.4832, 0.6628, 0.3965,  ..., 0.2598, 0.5122, 0.4948],
          [0.5146, 0.6316, 0.5479,  ..., 0.4073, 0.4378, 0.4639],
          [0.5380, 0.4450, 0.3942,  ..., 0.5641, 0.5827, 0.6063],
          [0.4220, 0.3872, 0.3989,  ..., 0.4762, 0.3803, 0.4718]],

         [[0.5148, 0.5713, 0.5708,  ..., 0.4986, 0.4969, 0.4795],
          [0.4634, 0.6067, 0.4528,  ..., 0.3956, 0.4824, 0.4292],
          [0.4494, 0.5033, 0.4192,  ..., 0.4385, 0.5898, 0.4910],
          [0.4405, 0.4078, 0.5387,  ..., 0.5619, 0.6270, 0.3937]],

         [[0.5304, 0.6951, 0.5249,  ..., 0.4301, 0.3531, 0.5842],
          [0.5813, 0.3303, 0.5397,  ..., 0.5542, 0.4867, 0.6397],
          [0.5035, 0.5245, 0.3872,  ..., 0.3495, 0.4872, 0.5898],
          [0.5049, 0.4325, 0.4149,  ..., 0.4007, 0.4106, 0.4489]],

         ...,

         [[0.6397, 0.3425, 0.5223,  ..., 0.4874, 0.5761, 0.3909],
          [0.4937, 0.5756, 0.4919,  ..., 0.5409, 0.5013, 0.4951],
          [0.5272, 0.5300, 0.4681,  ..., 0.3217, 0.3675, 0.4893],
          [0.4369, 0.4593, 0.4059,  ..., 0.6723, 0.5670, 0.5590]],

         [[0.3854, 0.5146, 0.5670,  ..., 0.4673, 0.4711, 0.4154],
          [0.5870, 0.4769, 0.5397,  ..., 0.4528, 0.6460, 0.5421],
          [0.4754, 0.5732, 0.4596,  ..., 0.5363, 0.6002, 0.4851],
          [0.5586, 0.6514, 0.4854,  ..., 0.5346, 0.4688, 0.4040]],

         [[0.4949, 0.5044, 0.5133,  ..., 0.4913, 0.4182, 0.4912],
          [0.5689, 0.3217, 0.3639,  ..., 0.4827, 0.4583, 0.4976],
          [0.4508, 0.3785, 0.5879,  ..., 0.4177, 0.4316, 0.4263],
          [0.5607, 0.4829, 0.6142,  ..., 0.3174, 0.4278, 0.3766]]],


        [[[0.5061, 0.5431, 0.6792,  ..., 0.4970, 0.4608, 0.3016],
          [0.5417, 0.5210, 0.6334,  ..., 0.3504, 0.5317, 0.6406],
          [0.5121, 0.5576, 0.4877,  ..., 0.5223, 0.6602, 0.4154],
          [0.5050, 0.4978, 0.3826,  ..., 0.5436, 0.5894, 0.5913]],

         [[0.6280, 0.3346, 0.4754,  ..., 0.5453, 0.5239, 0.5257],
          [0.3835, 0.4639, 0.5884,  ..., 0.4407, 0.5361, 0.6011],
          [0.4581, 0.6352, 0.4620,  ..., 0.6689, 0.3442, 0.5424],
          [0.5109, 0.5093, 0.5288,  ..., 0.5988, 0.4678, 0.4528]],

         [[0.4516, 0.5489, 0.4749,  ..., 0.5818, 0.6132, 0.4559],
          [0.3648, 0.5166, 0.3251,  ..., 0.3886, 0.6234, 0.5518],
          [0.4540, 0.5617, 0.6270,  ..., 0.4854, 0.6307, 0.3451],
          [0.4564, 0.5363, 0.4968,  ..., 0.3798, 0.4905, 0.3919]],

         ...,

         [[0.5148, 0.5713, 0.5708,  ..., 0.4986, 0.4969, 0.4795],
          [0.4634, 0.6067, 0.4528,  ..., 0.3956, 0.4824, 0.4292],
          [0.4494, 0.5033, 0.4192,  ..., 0.4385, 0.5898, 0.4910],
          [0.4405, 0.4078, 0.5387,  ..., 0.5619, 0.6270, 0.3937]],

         [[0.5099, 0.6021, 0.5770,  ..., 0.5516, 0.3942, 0.4647],
          [0.5889, 0.5149, 0.4422,  ..., 0.4685, 0.4668, 0.5392],
          [0.5460, 0.5455, 0.2942,  ..., 0.5898, 0.5093, 0.4990],
          [0.5737, 0.4699, 0.4354,  ..., 0.4891, 0.6817, 0.4446]],

         [[0.5061, 0.5431, 0.6792,  ..., 0.4970, 0.4608, 0.3016],
          [0.5417, 0.5210, 0.6334,  ..., 0.3504, 0.5317, 0.6406],
          [0.5121, 0.5576, 0.4877,  ..., 0.5223, 0.6602, 0.4154],
          [0.5050, 0.4978, 0.3826,  ..., 0.5436, 0.5894, 0.5913]]]],
       device='cuda:0', requires_grad=True)
tensor([-2.0000e-03,  4.0000e-03, -6.0000e-03, -2.3283e-10, -4.0000e-03,
        -6.0000e-03,  8.0000e-03,  4.0000e-03,  4.0000e-03,  4.0000e-03],
       device='cuda:0')
selected experts tensor([1698, 1768, 1598, 1528, 1456, 1567, 1718, 1744, 1748, 1559],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5455, 0.5984, 0.6171,  ..., 0.5449, 0.3447, 0.5742],
          [0.5590, 0.4656, 0.5966,  ..., 0.3017, 0.4665, 0.4013],
          [0.6047, 0.6364, 0.3623,  ..., 0.4853, 0.4080, 0.6069],
          [0.5399, 0.4342, 0.3641,  ..., 0.4939, 0.5430, 0.5572]],

         [[0.4763, 0.4975, 0.4928,  ..., 0.5372, 0.5161, 0.5538],
          [0.5082, 0.6132, 0.4245,  ..., 0.6078, 0.4375, 0.6345],
          [0.5287, 0.4270, 0.5880,  ..., 0.5277, 0.5322, 0.2850],
          [0.4731, 0.7022, 0.5332,  ..., 0.5906, 0.4952, 0.4084]],

         [[0.4200, 0.5327, 0.5072,  ..., 0.4936, 0.5522, 0.5497],
          [0.4310, 0.6247, 0.4917,  ..., 0.5386, 0.4522, 0.5049],
          [0.3012, 0.5205, 0.5205,  ..., 0.3982, 0.4879, 0.5947],
          [0.4096, 0.4692, 0.4427,  ..., 0.3067, 0.4010, 0.4107]],

         ...,

         [[0.7054, 0.6035, 0.6963,  ..., 0.3662, 0.2684, 0.4878],
          [0.5016, 0.4468, 0.4749,  ..., 0.5725, 0.6103, 0.4887],
          [0.5731, 0.4514, 0.4835,  ..., 0.5107, 0.4687, 0.4226],
          [0.5921, 0.4911, 0.3855,  ..., 0.6088, 0.4751, 0.5258]],

         [[0.5726, 0.4285, 0.5606,  ..., 0.6069, 0.4716, 0.5956],
          [0.3112, 0.4113, 0.5251,  ..., 0.3967, 0.3643, 0.4974],
          [0.5750, 0.4337, 0.6152,  ..., 0.4497, 0.4950, 0.5914],
          [0.6015, 0.4697, 0.5661,  ..., 0.4616, 0.4704, 0.5603]],

         [[0.5926, 0.6982, 0.3971,  ..., 0.5644, 0.5219, 0.4601],
          [0.4612, 0.4786, 0.3795,  ..., 0.5524, 0.4165, 0.6115],
          [0.4530, 0.4412, 0.4705,  ..., 0.6339, 0.5339, 0.3985],
          [0.5472, 0.4860, 0.4775,  ..., 0.4795, 0.4404, 0.4688]]],


        [[[0.4856, 0.5078, 0.5393,  ..., 0.5754, 0.4932, 0.5509],
          [0.4191, 0.6174, 0.6111,  ..., 0.5454, 0.4517, 0.3906],
          [0.6323, 0.5122, 0.4169,  ..., 0.5797, 0.4741, 0.5451],
          [0.4101, 0.4350, 0.5303,  ..., 0.5126, 0.4447, 0.6208]],

         [[0.4627, 0.4378, 0.4454,  ..., 0.5183, 0.5791, 0.5809],
          [0.5788, 0.5022, 0.5264,  ..., 0.6384, 0.4558, 0.4541],
          [0.4501, 0.5530, 0.5567,  ..., 0.3781, 0.4802, 0.6290],
          [0.3556, 0.4748, 0.5458,  ..., 0.4329, 0.5623, 0.5002]],

         [[0.4769, 0.5060, 0.4466,  ..., 0.4703, 0.7098, 0.3533],
          [0.3913, 0.4557, 0.3750,  ..., 0.5055, 0.6840, 0.4855],
          [0.5698, 0.4468, 0.4480,  ..., 0.5863, 0.4645, 0.5495],
          [0.4143, 0.4739, 0.4379,  ..., 0.6384, 0.3875, 0.5182]],

         ...,

         [[0.4763, 0.4975, 0.4928,  ..., 0.5372, 0.5161, 0.5538],
          [0.5082, 0.6132, 0.4245,  ..., 0.6078, 0.4375, 0.6345],
          [0.5287, 0.4270, 0.5880,  ..., 0.5277, 0.5322, 0.2850],
          [0.4731, 0.7022, 0.5332,  ..., 0.5906, 0.4952, 0.4084]],

         [[0.4162, 0.5984, 0.5212,  ..., 0.5587, 0.2998, 0.5157],
          [0.5655, 0.5053, 0.4738,  ..., 0.5587, 0.5154, 0.5852],
          [0.4344, 0.6533, 0.4813,  ..., 0.3982, 0.4308, 0.4696],
          [0.6047, 0.5014, 0.6013,  ..., 0.3897, 0.3607, 0.5485]],

         [[0.4856, 0.5078, 0.5393,  ..., 0.5754, 0.4932, 0.5509],
          [0.4191, 0.6174, 0.6111,  ..., 0.5454, 0.4517, 0.3906],
          [0.6323, 0.5122, 0.4169,  ..., 0.5797, 0.4741, 0.5451],
          [0.4101, 0.4350, 0.5303,  ..., 0.5126, 0.4447, 0.6208]]]],
       device='cuda:0')
tensor([[[[0.5475, 0.6044, 0.6151,  ..., 0.5489, 0.3407, 0.5722],
          [0.5610, 0.4716, 0.5946,  ..., 0.3057, 0.4625, 0.3993],
          [0.6067, 0.6424, 0.3603,  ..., 0.4893, 0.4040, 0.6049],
          [0.5419, 0.4402, 0.3621,  ..., 0.4979, 0.5390, 0.5552]],

         [[0.4783, 0.5035, 0.4908,  ..., 0.5412, 0.5121, 0.5518],
          [0.5102, 0.6192, 0.4225,  ..., 0.6118, 0.4335, 0.6325],
          [0.5307, 0.4330, 0.5860,  ..., 0.5317, 0.5282, 0.2830],
          [0.4751, 0.7082, 0.5312,  ..., 0.5946, 0.4912, 0.4064]],

         [[0.4220, 0.5387, 0.5052,  ..., 0.4976, 0.5482, 0.5477],
          [0.4330, 0.6307, 0.4897,  ..., 0.5426, 0.4482, 0.5029],
          [0.3032, 0.5265, 0.5185,  ..., 0.4022, 0.4839, 0.5927],
          [0.4116, 0.4752, 0.4407,  ..., 0.3107, 0.3970, 0.4087]],

         ...,

         [[0.7074, 0.6095, 0.6943,  ..., 0.3702, 0.2644, 0.4858],
          [0.5036, 0.4528, 0.4729,  ..., 0.5765, 0.6063, 0.4867],
          [0.5751, 0.4574, 0.4815,  ..., 0.5147, 0.4647, 0.4206],
          [0.5941, 0.4971, 0.3835,  ..., 0.6128, 0.4711, 0.5238]],

         [[0.5746, 0.4345, 0.5586,  ..., 0.6109, 0.4676, 0.5936],
          [0.3132, 0.4173, 0.5231,  ..., 0.4007, 0.3603, 0.4954],
          [0.5770, 0.4397, 0.6132,  ..., 0.4537, 0.4910, 0.5894],
          [0.6035, 0.4757, 0.5641,  ..., 0.4656, 0.4664, 0.5583]],

         [[0.5946, 0.7042, 0.3951,  ..., 0.5684, 0.5179, 0.4581],
          [0.4632, 0.4846, 0.3775,  ..., 0.5564, 0.4125, 0.6095],
          [0.4550, 0.4472, 0.4685,  ..., 0.6379, 0.5299, 0.3965],
          [0.5492, 0.4920, 0.4755,  ..., 0.4835, 0.4364, 0.4668]]],


        [[[0.4876, 0.5138, 0.5373,  ..., 0.5794, 0.4892, 0.5489],
          [0.4211, 0.6234, 0.6091,  ..., 0.5494, 0.4477, 0.3886],
          [0.6343, 0.5182, 0.4149,  ..., 0.5837, 0.4701, 0.5431],
          [0.4121, 0.4410, 0.5283,  ..., 0.5166, 0.4407, 0.6188]],

         [[0.4647, 0.4438, 0.4434,  ..., 0.5223, 0.5751, 0.5789],
          [0.5808, 0.5082, 0.5244,  ..., 0.6424, 0.4518, 0.4521],
          [0.4521, 0.5590, 0.5547,  ..., 0.3821, 0.4762, 0.6270],
          [0.3576, 0.4808, 0.5438,  ..., 0.4369, 0.5583, 0.4982]],

         [[0.4789, 0.5120, 0.4446,  ..., 0.4743, 0.7058, 0.3513],
          [0.3933, 0.4617, 0.3730,  ..., 0.5095, 0.6800, 0.4835],
          [0.5718, 0.4528, 0.4460,  ..., 0.5903, 0.4605, 0.5475],
          [0.4163, 0.4799, 0.4359,  ..., 0.6424, 0.3835, 0.5162]],

         ...,

         [[0.4783, 0.5035, 0.4908,  ..., 0.5412, 0.5121, 0.5518],
          [0.5102, 0.6192, 0.4225,  ..., 0.6118, 0.4335, 0.6325],
          [0.5307, 0.4330, 0.5860,  ..., 0.5317, 0.5282, 0.2830],
          [0.4751, 0.7082, 0.5312,  ..., 0.5946, 0.4912, 0.4064]],

         [[0.4182, 0.6044, 0.5192,  ..., 0.5627, 0.2958, 0.5137],
          [0.5675, 0.5113, 0.4718,  ..., 0.5627, 0.5114, 0.5832],
          [0.4364, 0.6593, 0.4793,  ..., 0.4022, 0.4268, 0.4676],
          [0.6067, 0.5074, 0.5993,  ..., 0.3937, 0.3567, 0.5465]],

         [[0.4876, 0.5138, 0.5373,  ..., 0.5794, 0.4892, 0.5489],
          [0.4211, 0.6234, 0.6091,  ..., 0.5494, 0.4477, 0.3886],
          [0.6343, 0.5182, 0.4149,  ..., 0.5837, 0.4701, 0.5431],
          [0.4121, 0.4410, 0.5283,  ..., 0.5166, 0.4407, 0.6188]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0020, -0.0060,  0.0020,  0.0080, -0.0020,  0.0100, -0.0120, -0.0040,
         0.0040,  0.0020], device='cuda:0')
selected experts tensor([1687, 1592, 1703, 1457, 1596, 1735, 1586, 1890, 1555, 1583],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4091, 0.6208, 0.4776,  ..., 0.4531, 0.6569, 0.6422],
          [0.4580, 0.4897, 0.5086,  ..., 0.6166, 0.7223, 0.4666],
          [0.6598, 0.5217, 0.4473,  ..., 0.5894, 0.3197, 0.4390],
          [0.6088, 0.4404, 0.5952,  ..., 0.4240, 0.4165, 0.4965]],

         [[0.4682, 0.5694, 0.5529,  ..., 0.3799, 0.3615, 0.4802],
          [0.4688, 0.5049, 0.5434,  ..., 0.5052, 0.4849, 0.5516],
          [0.3229, 0.5215, 0.6264,  ..., 0.4703, 0.5912, 0.4047],
          [0.6079, 0.4197, 0.4899,  ..., 0.5356, 0.4477, 0.5331]],

         [[0.3672, 0.5082, 0.4483,  ..., 0.3244, 0.5061, 0.4848],
          [0.4147, 0.5632, 0.4652,  ..., 0.5242, 0.5023, 0.4572],
          [0.5306, 0.4746, 0.5127,  ..., 0.4918, 0.6488, 0.4932],
          [0.4449, 0.5025, 0.3828,  ..., 0.5363, 0.5119, 0.5487]],

         ...,

         [[0.4595, 0.5100, 0.6023,  ..., 0.5880, 0.3829, 0.5183],
          [0.4262, 0.4746, 0.6805,  ..., 0.4988, 0.5979, 0.5319],
          [0.4072, 0.3805, 0.5061,  ..., 0.5329, 0.5436, 0.5773],
          [0.5275, 0.5216, 0.5623,  ..., 0.4787, 0.5412, 0.4187]],

         [[0.4820, 0.5704, 0.5572,  ..., 0.6271, 0.6050, 0.6261],
          [0.3501, 0.3983, 0.5185,  ..., 0.4623, 0.6178, 0.6219],
          [0.3528, 0.3084, 0.4684,  ..., 0.4976, 0.3381, 0.5140],
          [0.6439, 0.6244, 0.3714,  ..., 0.5373, 0.3606, 0.5036]],

         [[0.4614, 0.4833, 0.5432,  ..., 0.4943, 0.5246, 0.6339],
          [0.4300, 0.5158, 0.5270,  ..., 0.5555, 0.6206, 0.5565],
          [0.4771, 0.4626, 0.5111,  ..., 0.4794, 0.5640, 0.3955],
          [0.5873, 0.5690, 0.5403,  ..., 0.4720, 0.5133, 0.3746]]],


        [[[0.4224, 0.5794, 0.4113,  ..., 0.5998, 0.5169, 0.5410],
          [0.4205, 0.5685, 0.5857,  ..., 0.6470, 0.5369, 0.4519],
          [0.5294, 0.4924, 0.4203,  ..., 0.4097, 0.4506, 0.4824],
          [0.4821, 0.3810, 0.4427,  ..., 0.4254, 0.5433, 0.4967]],

         [[0.3645, 0.5718, 0.4941,  ..., 0.3694, 0.4766, 0.5864],
          [0.5377, 0.4659, 0.5041,  ..., 0.5521, 0.5363, 0.4121],
          [0.5551, 0.5022, 0.6561,  ..., 0.4807, 0.5019, 0.5135],
          [0.7092, 0.4776, 0.5051,  ..., 0.4919, 0.4901, 0.5657]],

         [[0.4365, 0.5532, 0.4155,  ..., 0.3767, 0.4718, 0.5705],
          [0.4556, 0.4457, 0.5551,  ..., 0.6128, 0.4554, 0.4572],
          [0.5507, 0.5623, 0.5111,  ..., 0.4956, 0.4226, 0.4154],
          [0.5669, 0.4893, 0.5037,  ..., 0.5205, 0.5869, 0.5748]],

         ...,

         [[0.3805, 0.5327, 0.5028,  ..., 0.3980, 0.3475, 0.5869],
          [0.4651, 0.6280, 0.6004,  ..., 0.4596, 0.4703, 0.3936],
          [0.4440, 0.3871, 0.5876,  ..., 0.4470, 0.5884, 0.5630],
          [0.6492, 0.4621, 0.3687,  ..., 0.3933, 0.5505, 0.5611]],

         [[0.4915, 0.4809, 0.4609,  ..., 0.4738, 0.5016, 0.5041],
          [0.6088, 0.5433, 0.4442,  ..., 0.4950, 0.7485, 0.4121],
          [0.3264, 0.4684, 0.4937,  ..., 0.4892, 0.3865, 0.4145],
          [0.5157, 0.5647, 0.5128,  ..., 0.4359, 0.4386, 0.4553]],

         [[0.5040, 0.5089, 0.3760,  ..., 0.4960, 0.4715, 0.6657],
          [0.3847, 0.5804, 0.6640,  ..., 0.5463, 0.5585, 0.5349],
          [0.4891, 0.4310, 0.5924,  ..., 0.4345, 0.4951, 0.6040],
          [0.3654, 0.4503, 0.3046,  ..., 0.4417, 0.4948, 0.3177]]]],
       device='cuda:0')
tensor([[[[0.4121, 0.6298, 0.4746,  ..., 0.4521, 0.6379, 0.6252],
          [0.4610, 0.4987, 0.5056,  ..., 0.6156, 0.7033, 0.4496],
          [0.6628, 0.5307, 0.4443,  ..., 0.5884, 0.3007, 0.4220],
          [0.6118, 0.4494, 0.5922,  ..., 0.4230, 0.3975, 0.4795]],

         [[0.4712, 0.5784, 0.5499,  ..., 0.3789, 0.3425, 0.4632],
          [0.4718, 0.5139, 0.5404,  ..., 0.5042, 0.4659, 0.5346],
          [0.3259, 0.5305, 0.6234,  ..., 0.4693, 0.5722, 0.3877],
          [0.6109, 0.4287, 0.4869,  ..., 0.5346, 0.4287, 0.5161]],

         [[0.3702, 0.5172, 0.4453,  ..., 0.3234, 0.4871, 0.4678],
          [0.4177, 0.5722, 0.4622,  ..., 0.5232, 0.4833, 0.4402],
          [0.5336, 0.4836, 0.5097,  ..., 0.4908, 0.6298, 0.4762],
          [0.4479, 0.5115, 0.3798,  ..., 0.5353, 0.4929, 0.5317]],

         ...,

         [[0.4625, 0.5190, 0.5993,  ..., 0.5870, 0.3639, 0.5013],
          [0.4292, 0.4836, 0.6775,  ..., 0.4978, 0.5789, 0.5149],
          [0.4102, 0.3895, 0.5031,  ..., 0.5319, 0.5246, 0.5603],
          [0.5305, 0.5306, 0.5593,  ..., 0.4777, 0.5222, 0.4017]],

         [[0.4850, 0.5794, 0.5542,  ..., 0.6261, 0.5860, 0.6091],
          [0.3531, 0.4073, 0.5155,  ..., 0.4613, 0.5988, 0.6049],
          [0.3558, 0.3174, 0.4654,  ..., 0.4966, 0.3191, 0.4970],
          [0.6469, 0.6334, 0.3684,  ..., 0.5363, 0.3416, 0.4866]],

         [[0.4644, 0.4923, 0.5402,  ..., 0.4933, 0.5056, 0.6169],
          [0.4330, 0.5248, 0.5240,  ..., 0.5545, 0.6016, 0.5395],
          [0.4801, 0.4716, 0.5081,  ..., 0.4784, 0.5450, 0.3785],
          [0.5903, 0.5780, 0.5373,  ..., 0.4710, 0.4943, 0.3576]]],


        [[[0.4254, 0.5884, 0.4083,  ..., 0.5988, 0.4979, 0.5240],
          [0.4235, 0.5775, 0.5827,  ..., 0.6460, 0.5179, 0.4349],
          [0.5324, 0.5014, 0.4173,  ..., 0.4087, 0.4316, 0.4654],
          [0.4851, 0.3900, 0.4397,  ..., 0.4244, 0.5243, 0.4797]],

         [[0.3675, 0.5808, 0.4911,  ..., 0.3684, 0.4576, 0.5694],
          [0.5407, 0.4749, 0.5011,  ..., 0.5511, 0.5173, 0.3951],
          [0.5581, 0.5112, 0.6531,  ..., 0.4797, 0.4829, 0.4965],
          [0.7122, 0.4866, 0.5021,  ..., 0.4909, 0.4711, 0.5487]],

         [[0.4395, 0.5622, 0.4125,  ..., 0.3757, 0.4528, 0.5535],
          [0.4586, 0.4547, 0.5521,  ..., 0.6118, 0.4364, 0.4402],
          [0.5537, 0.5713, 0.5081,  ..., 0.4946, 0.4036, 0.3984],
          [0.5699, 0.4983, 0.5007,  ..., 0.5195, 0.5679, 0.5578]],

         ...,

         [[0.3835, 0.5417, 0.4998,  ..., 0.3970, 0.3285, 0.5699],
          [0.4681, 0.6370, 0.5974,  ..., 0.4586, 0.4513, 0.3766],
          [0.4470, 0.3961, 0.5846,  ..., 0.4460, 0.5694, 0.5460],
          [0.6522, 0.4711, 0.3657,  ..., 0.3923, 0.5315, 0.5441]],

         [[0.4945, 0.4899, 0.4579,  ..., 0.4728, 0.4826, 0.4871],
          [0.6118, 0.5523, 0.4412,  ..., 0.4940, 0.7295, 0.3951],
          [0.3294, 0.4774, 0.4907,  ..., 0.4882, 0.3675, 0.3975],
          [0.5187, 0.5737, 0.5098,  ..., 0.4349, 0.4196, 0.4383]],

         [[0.5070, 0.5179, 0.3730,  ..., 0.4950, 0.4525, 0.6487],
          [0.3877, 0.5894, 0.6610,  ..., 0.5453, 0.5395, 0.5179],
          [0.4921, 0.4400, 0.5894,  ..., 0.4335, 0.4761, 0.5870],
          [0.3684, 0.4593, 0.3016,  ..., 0.4407, 0.4758, 0.3007]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([-0.0030, -0.0090,  0.0030, -0.0110, -0.0090,  0.0170, -0.0310,  0.0010,
         0.0190,  0.0170], device='cuda:0')
selected experts tensor([1523, 1507, 1686, 1997, 1022, 1975, 1871, 1465, 1610, 1728],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4440, 0.5445, 0.3459,  ..., 0.3927, 0.3542, 0.4227],
          [0.5032, 0.5044, 0.6670,  ..., 0.5153, 0.4041, 0.5315],
          [0.5242, 0.3903, 0.4344,  ..., 0.4952, 0.7377, 0.5655],
          [0.4978, 0.5345, 0.5954,  ..., 0.3161, 0.5197, 0.4871]],

         [[0.5246, 0.5362, 0.5263,  ..., 0.4905, 0.5417, 0.5116],
          [0.3501, 0.6153, 0.6150,  ..., 0.4524, 0.6090, 0.5334],
          [0.5735, 0.4440, 0.2529,  ..., 0.5067, 0.5518, 0.6373],
          [0.5488, 0.5486, 0.5600,  ..., 0.3746, 0.5960, 0.4246]],

         [[0.4849, 0.5309, 0.5486,  ..., 0.3603, 0.5899, 0.6253],
          [0.4334, 0.4300, 0.6807,  ..., 0.4438, 0.5547, 0.3988],
          [0.5154, 0.4797, 0.4472,  ..., 0.4741, 0.5974, 0.5418],
          [0.4491, 0.5464, 0.4144,  ..., 0.4804, 0.4731, 0.4133]],

         ...,

         [[0.4970, 0.4970, 0.4990,  ..., 0.5170, 0.4930, 0.5130],
          [0.4970, 0.4970, 0.4990,  ..., 0.5170, 0.4930, 0.5130],
          [0.4970, 0.4970, 0.4990,  ..., 0.5170, 0.4930, 0.5130],
          [0.4970, 0.4970, 0.4990,  ..., 0.5170, 0.4930, 0.5130]],

         [[0.4970, 0.4970, 0.4990,  ..., 0.5170, 0.4930, 0.5130],
          [0.4970, 0.4970, 0.4990,  ..., 0.5170, 0.4930, 0.5130],
          [0.4970, 0.4970, 0.4990,  ..., 0.5170, 0.4930, 0.5130],
          [0.4970, 0.4970, 0.4990,  ..., 0.5170, 0.4930, 0.5130]],

         [[0.4970, 0.4970, 0.4990,  ..., 0.5170, 0.4930, 0.5130],
          [0.4970, 0.4970, 0.4990,  ..., 0.5170, 0.4930, 0.5130],
          [0.4970, 0.4970, 0.4990,  ..., 0.5170, 0.4930, 0.5130],
          [0.4970, 0.4970, 0.4990,  ..., 0.5170, 0.4930, 0.5130]]],


        [[[0.4641, 0.3963, 0.4220,  ..., 0.4681, 0.5332, 0.4925],
          [0.4882, 0.4367, 0.5039,  ..., 0.5006, 0.5128, 0.5409],
          [0.5418, 0.4355, 0.3459,  ..., 0.5589, 0.5710, 0.5015],
          [0.5759, 0.4868, 0.4986,  ..., 0.4338, 0.4827, 0.4095]],

         [[0.5442, 0.4401, 0.4907,  ..., 0.4613, 0.4994, 0.5653],
          [0.4157, 0.5143, 0.6251,  ..., 0.5055, 0.6497, 0.3555],
          [0.5864, 0.4133, 0.4210,  ..., 0.4691, 0.4944, 0.6758],
          [0.6510, 0.5697, 0.4134,  ..., 0.4749, 0.6435, 0.4559]],

         [[0.4363, 0.5854, 0.5503,  ..., 0.3863, 0.4112, 0.4255],
          [0.3861, 0.4856, 0.6548,  ..., 0.6016, 0.4980, 0.4663],
          [0.3636, 0.5314, 0.4363,  ..., 0.6549, 0.5081, 0.5186],
          [0.5580, 0.5109, 0.4740,  ..., 0.5128, 0.5561, 0.4403]],

         ...,

         [[0.4970, 0.4970, 0.4990,  ..., 0.5170, 0.4930, 0.5130],
          [0.4970, 0.4970, 0.4990,  ..., 0.5170, 0.4930, 0.5130],
          [0.4970, 0.4970, 0.4990,  ..., 0.5170, 0.4930, 0.5130],
          [0.4970, 0.4970, 0.4990,  ..., 0.5170, 0.4930, 0.5130]],

         [[0.4970, 0.4970, 0.4990,  ..., 0.5170, 0.4930, 0.5130],
          [0.4970, 0.4970, 0.4990,  ..., 0.5170, 0.4930, 0.5130],
          [0.4970, 0.4970, 0.4990,  ..., 0.5170, 0.4930, 0.5130],
          [0.4970, 0.4970, 0.4990,  ..., 0.5170, 0.4930, 0.5130]],

         [[0.4970, 0.4970, 0.4990,  ..., 0.5170, 0.4930, 0.5130],
          [0.4970, 0.4970, 0.4990,  ..., 0.5170, 0.4930, 0.5130],
          [0.4970, 0.4970, 0.4990,  ..., 0.5170, 0.4930, 0.5130],
          [0.4970, 0.4970, 0.4990,  ..., 0.5170, 0.4930, 0.5130]]]],
       device='cuda:0')
tensor([[[[0.4470, 0.5475, 0.3469,  ..., 0.3757, 0.3612, 0.4097],
          [0.5062, 0.5074, 0.6680,  ..., 0.4983, 0.4111, 0.5185],
          [0.5272, 0.3933, 0.4354,  ..., 0.4782, 0.7447, 0.5525],
          [0.5008, 0.5375, 0.5964,  ..., 0.2991, 0.5267, 0.4741]],

         [[0.5276, 0.5392, 0.5273,  ..., 0.4735, 0.5487, 0.4986],
          [0.3531, 0.6183, 0.6160,  ..., 0.4354, 0.6160, 0.5204],
          [0.5765, 0.4470, 0.2539,  ..., 0.4897, 0.5588, 0.6243],
          [0.5518, 0.5516, 0.5610,  ..., 0.3576, 0.6030, 0.4116]],

         [[0.4879, 0.5339, 0.5496,  ..., 0.3433, 0.5969, 0.6123],
          [0.4364, 0.4330, 0.6817,  ..., 0.4268, 0.5617, 0.3858],
          [0.5184, 0.4827, 0.4482,  ..., 0.4571, 0.6044, 0.5288],
          [0.4521, 0.5494, 0.4154,  ..., 0.4634, 0.4801, 0.4003]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.4671, 0.3993, 0.4230,  ..., 0.4511, 0.5402, 0.4795],
          [0.4912, 0.4397, 0.5049,  ..., 0.4836, 0.5198, 0.5279],
          [0.5448, 0.4385, 0.3469,  ..., 0.5419, 0.5780, 0.4885],
          [0.5789, 0.4898, 0.4996,  ..., 0.4168, 0.4897, 0.3965]],

         [[0.5472, 0.4431, 0.4917,  ..., 0.4443, 0.5064, 0.5523],
          [0.4187, 0.5173, 0.6261,  ..., 0.4885, 0.6567, 0.3425],
          [0.5894, 0.4163, 0.4220,  ..., 0.4521, 0.5014, 0.6628],
          [0.6540, 0.5727, 0.4144,  ..., 0.4579, 0.6505, 0.4429]],

         [[0.4393, 0.5884, 0.5513,  ..., 0.3693, 0.4182, 0.4125],
          [0.3891, 0.4886, 0.6558,  ..., 0.5846, 0.5050, 0.4533],
          [0.3666, 0.5344, 0.4373,  ..., 0.6379, 0.5151, 0.5056],
          [0.5610, 0.5139, 0.4750,  ..., 0.4958, 0.5631, 0.4273]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0030, -0.0030, -0.0010,  0.0090, -0.0090,  0.0190,  0.0070,  0.0170,
        -0.0070,  0.0130], device='cuda:0')
selected experts tensor([1640, 1827, 1803, 1773, 1639,  747, 2049,  859, 2093, 1954],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1687., 1592., 1703., 1457., 1596., 1735., 1586., 1890., 1555., 1583.],
        [1640., 1827., 1803., 1773., 1639.,  747., 2049.,  859., 2093., 1954.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5032, 0.5471, 0.6759,  ..., 0.4999, 0.4660, 0.3066],
          [0.5387, 0.5239, 0.6284,  ..., 0.3534, 0.5345, 0.6456],
          [0.5091, 0.5606, 0.4827,  ..., 0.5254, 0.6632, 0.4204],
          [0.5021, 0.5010, 0.3776,  ..., 0.5463, 0.5924, 0.5963]],

         [[0.5311, 0.4509, 0.4170,  ..., 0.5500, 0.4117, 0.5892],
          [0.3153, 0.4838, 0.5101,  ..., 0.5013, 0.5642, 0.4213],
          [0.3439, 0.5943, 0.5112,  ..., 0.6088, 0.4848, 0.4242],
          [0.4834, 0.4863, 0.6401,  ..., 0.4184, 0.4820, 0.5488]],

         [[0.5587, 0.3930, 0.5053,  ..., 0.4193, 0.3499, 0.6000],
          [0.4580, 0.4360, 0.5514,  ..., 0.5522, 0.5296, 0.5510],
          [0.6633, 0.5340, 0.3990,  ..., 0.4546, 0.4718, 0.3743],
          [0.4753, 0.3874, 0.4275,  ..., 0.4775, 0.5203, 0.4776]],

         ...,

         [[0.5314, 0.5133, 0.3698,  ..., 0.5155, 0.5881, 0.6695],
          [0.5147, 0.4775, 0.4737,  ..., 0.4365, 0.4132, 0.5624],
          [0.4655, 0.6597, 0.6284,  ..., 0.5242, 0.4207, 0.5551],
          [0.5793, 0.5242, 0.4299,  ..., 0.4099, 0.5526, 0.4597]],

         [[0.5032, 0.5471, 0.6759,  ..., 0.4999, 0.4660, 0.3066],
          [0.5387, 0.5239, 0.6284,  ..., 0.3534, 0.5345, 0.6456],
          [0.5091, 0.5606, 0.4827,  ..., 0.5254, 0.6632, 0.4204],
          [0.5021, 0.5010, 0.3776,  ..., 0.5463, 0.5924, 0.5963]],

         [[0.6633, 0.4042, 0.4635,  ..., 0.4146, 0.4691, 0.4237],
          [0.5040, 0.4461, 0.5924,  ..., 0.3705, 0.5391, 0.5083],
          [0.6304, 0.3359, 0.3652,  ..., 0.3570, 0.5282, 0.5345],
          [0.3010, 0.4317, 0.5677,  ..., 0.4801, 0.4949, 0.5008]]],


        [[[0.3718, 0.5719, 0.6885,  ..., 0.4284, 0.4222, 0.5191],
          [0.4725, 0.5531, 0.5289,  ..., 0.3046, 0.4203, 0.5578],
          [0.4696, 0.4548, 0.6410,  ..., 0.3865, 0.5116, 0.4921],
          [0.4814, 0.3787, 0.2990,  ..., 0.5947, 0.5829, 0.5008]],

         [[0.6728, 0.4597, 0.5023,  ..., 0.6004, 0.5070, 0.4498],
          [0.3672, 0.5222, 0.5639,  ..., 0.4089, 0.5781, 0.4237],
          [0.4396, 0.5620, 0.4362,  ..., 0.4754, 0.4796, 0.4827],
          [0.6093, 0.4880, 0.3235,  ..., 0.5191, 0.4217, 0.4180]],

         [[0.5469, 0.5608, 0.5093,  ..., 0.6337, 0.4207, 0.5566],
          [0.3170, 0.4835, 0.4042,  ..., 0.4930, 0.5007, 0.4185],
          [0.5849, 0.4322, 0.4529,  ..., 0.4948, 0.3958, 0.4806],
          [0.4941, 0.4394, 0.2386,  ..., 0.4706, 0.4070, 0.3734]],

         ...,

         [[0.4866, 0.3678, 0.5711,  ..., 0.4635, 0.5004, 0.5839],
          [0.6519, 0.4660, 0.3813,  ..., 0.4000, 0.6481, 0.4515],
          [0.5121, 0.6135, 0.5536,  ..., 0.5196, 0.4500, 0.4095],
          [0.5094, 0.4594, 0.4955,  ..., 0.5379, 0.5814, 0.5165]],

         [[0.5413, 0.6355, 0.5060,  ..., 0.5137, 0.3315, 0.4617],
          [0.5197, 0.4526, 0.3375,  ..., 0.5285, 0.3847, 0.4760],
          [0.5357, 0.4944, 0.5504,  ..., 0.5261, 0.6873, 0.4573],
          [0.4086, 0.4355, 0.5345,  ..., 0.5231, 0.5053, 0.4855]],

         [[0.6448, 0.5069, 0.4649,  ..., 0.3552, 0.5079, 0.5292],
          [0.5673, 0.4870, 0.3287,  ..., 0.4662, 0.3977, 0.6519],
          [0.4081, 0.5179, 0.3972,  ..., 0.4650, 0.3986, 0.6366],
          [0.3893, 0.4880, 0.4473,  ..., 0.4502, 0.5425, 0.3807]]]],
       device='cuda:0')
tensor([[[[0.5062, 0.5441, 0.6809,  ..., 0.4969, 0.4630, 0.3016],
          [0.5417, 0.5209, 0.6334,  ..., 0.3504, 0.5315, 0.6406],
          [0.5121, 0.5576, 0.4877,  ..., 0.5224, 0.6602, 0.4154],
          [0.5051, 0.4980, 0.3826,  ..., 0.5433, 0.5894, 0.5913]],

         [[0.5341, 0.4479, 0.4220,  ..., 0.5470, 0.4087, 0.5842],
          [0.3183, 0.4808, 0.5151,  ..., 0.4983, 0.5612, 0.4163],
          [0.3469, 0.5913, 0.5162,  ..., 0.6058, 0.4818, 0.4192],
          [0.4864, 0.4833, 0.6451,  ..., 0.4154, 0.4790, 0.5438]],

         [[0.5617, 0.3900, 0.5103,  ..., 0.4163, 0.3469, 0.5950],
          [0.4610, 0.4330, 0.5564,  ..., 0.5492, 0.5266, 0.5460],
          [0.6663, 0.5310, 0.4040,  ..., 0.4516, 0.4688, 0.3693],
          [0.4783, 0.3844, 0.4325,  ..., 0.4745, 0.5173, 0.4726]],

         ...,

         [[0.5344, 0.5103, 0.3748,  ..., 0.5125, 0.5851, 0.6645],
          [0.5177, 0.4745, 0.4787,  ..., 0.4335, 0.4102, 0.5574],
          [0.4685, 0.6567, 0.6334,  ..., 0.5212, 0.4177, 0.5501],
          [0.5823, 0.5212, 0.4349,  ..., 0.4069, 0.5496, 0.4547]],

         [[0.5062, 0.5441, 0.6809,  ..., 0.4969, 0.4630, 0.3016],
          [0.5417, 0.5209, 0.6334,  ..., 0.3504, 0.5315, 0.6406],
          [0.5121, 0.5576, 0.4877,  ..., 0.5224, 0.6602, 0.4154],
          [0.5051, 0.4980, 0.3826,  ..., 0.5433, 0.5894, 0.5913]],

         [[0.6663, 0.4012, 0.4685,  ..., 0.4116, 0.4661, 0.4187],
          [0.5070, 0.4431, 0.5974,  ..., 0.3675, 0.5361, 0.5033],
          [0.6334, 0.3329, 0.3702,  ..., 0.3540, 0.5252, 0.5295],
          [0.3040, 0.4287, 0.5727,  ..., 0.4771, 0.4919, 0.4958]]],


        [[[0.3748, 0.5689, 0.6935,  ..., 0.4254, 0.4192, 0.5141],
          [0.4755, 0.5501, 0.5339,  ..., 0.3016, 0.4173, 0.5528],
          [0.4726, 0.4518, 0.6460,  ..., 0.3835, 0.5086, 0.4871],
          [0.4844, 0.3757, 0.3040,  ..., 0.5917, 0.5799, 0.4958]],

         [[0.6758, 0.4567, 0.5073,  ..., 0.5974, 0.5040, 0.4448],
          [0.3702, 0.5192, 0.5689,  ..., 0.4059, 0.5751, 0.4187],
          [0.4426, 0.5590, 0.4412,  ..., 0.4724, 0.4766, 0.4777],
          [0.6123, 0.4850, 0.3285,  ..., 0.5161, 0.4187, 0.4130]],

         [[0.5499, 0.5578, 0.5143,  ..., 0.6307, 0.4177, 0.5516],
          [0.3200, 0.4805, 0.4092,  ..., 0.4900, 0.4977, 0.4135],
          [0.5879, 0.4292, 0.4579,  ..., 0.4918, 0.3928, 0.4756],
          [0.4971, 0.4364, 0.2436,  ..., 0.4676, 0.4040, 0.3684]],

         ...,

         [[0.4896, 0.3648, 0.5761,  ..., 0.4605, 0.4974, 0.5789],
          [0.6549, 0.4630, 0.3863,  ..., 0.3970, 0.6451, 0.4465],
          [0.5151, 0.6105, 0.5586,  ..., 0.5166, 0.4470, 0.4045],
          [0.5124, 0.4564, 0.5005,  ..., 0.5349, 0.5784, 0.5115]],

         [[0.5443, 0.6325, 0.5110,  ..., 0.5107, 0.3285, 0.4567],
          [0.5227, 0.4496, 0.3425,  ..., 0.5255, 0.3817, 0.4710],
          [0.5387, 0.4914, 0.5554,  ..., 0.5231, 0.6843, 0.4523],
          [0.4116, 0.4325, 0.5395,  ..., 0.5201, 0.5023, 0.4805]],

         [[0.6478, 0.5039, 0.4699,  ..., 0.3522, 0.5049, 0.5242],
          [0.5703, 0.4840, 0.3337,  ..., 0.4632, 0.3947, 0.6469],
          [0.4111, 0.5149, 0.4022,  ..., 0.4620, 0.3956, 0.6316],
          [0.3923, 0.4850, 0.4523,  ..., 0.4472, 0.5395, 0.3757]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0030,  0.0030, -0.0050,  0.0010, -0.0030, -0.0050,  0.0070,  0.0030,
         0.0030,  0.0050], device='cuda:0')
selected experts tensor([1622, 1681, 1717, 1606, 1575, 1672, 1613, 1659, 1644, 1595],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4842, 0.5073, 0.5402,  ..., 0.5754, 0.4943, 0.5507],
          [0.4181, 0.6184, 0.6101,  ..., 0.5446, 0.4527, 0.3916],
          [0.6313, 0.5133, 0.4159,  ..., 0.5787, 0.4751, 0.5459],
          [0.4095, 0.4357, 0.5295,  ..., 0.5118, 0.4457, 0.6218]],

         [[0.5080, 0.5749, 0.5889,  ..., 0.3375, 0.5739, 0.5776],
          [0.5654, 0.4979, 0.5689,  ..., 0.5711, 0.5284, 0.4298],
          [0.4709, 0.6517, 0.4470,  ..., 0.5303, 0.5513, 0.3760],
          [0.5428, 0.5553, 0.4150,  ..., 0.5615, 0.5174, 0.5943]],

         [[0.5495, 0.5129, 0.5728,  ..., 0.3544, 0.4663, 0.5024],
          [0.4124, 0.3322, 0.5132,  ..., 0.4304, 0.6456, 0.4925],
          [0.5192, 0.3481, 0.5327,  ..., 0.4213, 0.3554, 0.5480],
          [0.5840, 0.3689, 0.4064,  ..., 0.4517, 0.5820, 0.4423]],

         ...,

         [[0.3636, 0.5896, 0.5049,  ..., 0.5881, 0.6402, 0.6881],
          [0.5541, 0.5434, 0.5998,  ..., 0.4444, 0.6210, 0.6847],
          [0.4676, 0.6347, 0.5255,  ..., 0.3901, 0.4242, 0.4890],
          [0.5577, 0.3652, 0.5533,  ..., 0.5359, 0.4194, 0.4379]],

         [[0.4842, 0.5073, 0.5402,  ..., 0.5754, 0.4943, 0.5507],
          [0.4181, 0.6184, 0.6101,  ..., 0.5446, 0.4527, 0.3916],
          [0.6313, 0.5133, 0.4159,  ..., 0.5787, 0.4751, 0.5459],
          [0.4095, 0.4357, 0.5295,  ..., 0.5118, 0.4457, 0.6218]],

         [[0.3736, 0.4961, 0.3586,  ..., 0.4023, 0.5239, 0.5546],
          [0.4914, 0.5872, 0.4593,  ..., 0.3253, 0.4081, 0.5558],
          [0.5367, 0.4228, 0.4407,  ..., 0.5408, 0.4991, 0.5095],
          [0.5981, 0.4618, 0.5400,  ..., 0.4194, 0.3807, 0.4575]]],


        [[[0.3870, 0.5111, 0.4828,  ..., 0.2796, 0.6447, 0.4418],
          [0.4252, 0.4989, 0.5088,  ..., 0.5475, 0.5953, 0.5534],
          [0.5649, 0.5204, 0.5502,  ..., 0.5245, 0.4721, 0.5263],
          [0.5403, 0.5207, 0.5113,  ..., 0.3841, 0.4575, 0.5507]],

         [[0.5126, 0.6275, 0.3313,  ..., 0.4175, 0.4166, 0.6337],
          [0.4524, 0.3428, 0.4875,  ..., 0.5106, 0.4527, 0.4956],
          [0.5873, 0.4056, 0.3975,  ..., 0.4473, 0.5031, 0.4308],
          [0.3573, 0.6604, 0.3919,  ..., 0.3929, 0.6127, 0.5366]],

         [[0.4072, 0.6776, 0.6407,  ..., 0.5495, 0.3959, 0.6409],
          [0.5264, 0.5858, 0.4952,  ..., 0.6490, 0.4704, 0.5215],
          [0.3884, 0.4444, 0.6362,  ..., 0.2844, 0.5221, 0.4080],
          [0.5430, 0.4355, 0.4557,  ..., 0.5340, 0.5792, 0.5076]],

         ...,

         [[0.6195, 0.5834, 0.3514,  ..., 0.4948, 0.4522, 0.4597],
          [0.6510, 0.6022, 0.5482,  ..., 0.6175, 0.6010, 0.5507],
          [0.6121, 0.6045, 0.3850,  ..., 0.5105, 0.4813, 0.4298],
          [0.3456, 0.4988, 0.3694,  ..., 0.3428, 0.4086, 0.3579]],

         [[0.4875, 0.4931, 0.4784,  ..., 0.4958, 0.5401, 0.4023],
          [0.5169, 0.3689, 0.4350,  ..., 0.3804, 0.3959, 0.3995],
          [0.5435, 0.5127, 0.4335,  ..., 0.3878, 0.4375, 0.5234],
          [0.5196, 0.6525, 0.5366,  ..., 0.5347, 0.4251, 0.5952]],

         [[0.5556, 0.4033, 0.4572,  ..., 0.6059, 0.4711, 0.6135],
          [0.4360, 0.2949, 0.5572,  ..., 0.5524, 0.5768, 0.5267],
          [0.3982, 0.5303, 0.3966,  ..., 0.4185, 0.5148, 0.5819],
          [0.4778, 0.5235, 0.5593,  ..., 0.6347, 0.6047, 0.4669]]]],
       device='cuda:0')
tensor([[[[0.4872, 0.5123, 0.5392,  ..., 0.5804, 0.4893, 0.5477],
          [0.4211, 0.6234, 0.6091,  ..., 0.5496, 0.4477, 0.3886],
          [0.6343, 0.5183, 0.4149,  ..., 0.5837, 0.4701, 0.5429],
          [0.4125, 0.4407, 0.5285,  ..., 0.5168, 0.4407, 0.6188]],

         [[0.5110, 0.5799, 0.5879,  ..., 0.3425, 0.5689, 0.5746],
          [0.5684, 0.5029, 0.5679,  ..., 0.5761, 0.5234, 0.4268],
          [0.4739, 0.6567, 0.4460,  ..., 0.5353, 0.5463, 0.3730],
          [0.5458, 0.5603, 0.4140,  ..., 0.5665, 0.5124, 0.5913]],

         [[0.5525, 0.5179, 0.5718,  ..., 0.3594, 0.4613, 0.4994],
          [0.4154, 0.3372, 0.5122,  ..., 0.4354, 0.6406, 0.4895],
          [0.5222, 0.3531, 0.5317,  ..., 0.4263, 0.3504, 0.5450],
          [0.5870, 0.3739, 0.4054,  ..., 0.4567, 0.5770, 0.4393]],

         ...,

         [[0.3666, 0.5946, 0.5039,  ..., 0.5931, 0.6352, 0.6851],
          [0.5571, 0.5484, 0.5988,  ..., 0.4494, 0.6160, 0.6817],
          [0.4706, 0.6397, 0.5245,  ..., 0.3951, 0.4192, 0.4860],
          [0.5607, 0.3702, 0.5523,  ..., 0.5409, 0.4144, 0.4349]],

         [[0.4872, 0.5123, 0.5392,  ..., 0.5804, 0.4893, 0.5477],
          [0.4211, 0.6234, 0.6091,  ..., 0.5496, 0.4477, 0.3886],
          [0.6343, 0.5183, 0.4149,  ..., 0.5837, 0.4701, 0.5429],
          [0.4125, 0.4407, 0.5285,  ..., 0.5168, 0.4407, 0.6188]],

         [[0.3766, 0.5011, 0.3576,  ..., 0.4073, 0.5189, 0.5516],
          [0.4944, 0.5922, 0.4583,  ..., 0.3303, 0.4031, 0.5528],
          [0.5397, 0.4278, 0.4397,  ..., 0.5458, 0.4941, 0.5065],
          [0.6011, 0.4668, 0.5390,  ..., 0.4244, 0.3757, 0.4545]]],


        [[[0.3900, 0.5161, 0.4818,  ..., 0.2846, 0.6397, 0.4388],
          [0.4282, 0.5039, 0.5078,  ..., 0.5525, 0.5903, 0.5504],
          [0.5679, 0.5254, 0.5492,  ..., 0.5295, 0.4671, 0.5233],
          [0.5433, 0.5257, 0.5103,  ..., 0.3891, 0.4525, 0.5477]],

         [[0.5156, 0.6325, 0.3303,  ..., 0.4225, 0.4116, 0.6307],
          [0.4554, 0.3478, 0.4865,  ..., 0.5156, 0.4477, 0.4926],
          [0.5903, 0.4106, 0.3965,  ..., 0.4523, 0.4981, 0.4278],
          [0.3603, 0.6654, 0.3909,  ..., 0.3979, 0.6077, 0.5336]],

         [[0.4102, 0.6826, 0.6397,  ..., 0.5545, 0.3909, 0.6379],
          [0.5294, 0.5908, 0.4942,  ..., 0.6540, 0.4654, 0.5185],
          [0.3914, 0.4494, 0.6352,  ..., 0.2894, 0.5171, 0.4050],
          [0.5460, 0.4405, 0.4547,  ..., 0.5390, 0.5742, 0.5046]],

         ...,

         [[0.6225, 0.5884, 0.3504,  ..., 0.4998, 0.4472, 0.4567],
          [0.6540, 0.6072, 0.5472,  ..., 0.6225, 0.5960, 0.5477],
          [0.6151, 0.6095, 0.3840,  ..., 0.5155, 0.4763, 0.4268],
          [0.3486, 0.5038, 0.3684,  ..., 0.3478, 0.4036, 0.3549]],

         [[0.4905, 0.4981, 0.4774,  ..., 0.5008, 0.5351, 0.3993],
          [0.5199, 0.3739, 0.4340,  ..., 0.3854, 0.3909, 0.3965],
          [0.5465, 0.5177, 0.4325,  ..., 0.3928, 0.4325, 0.5204],
          [0.5226, 0.6575, 0.5356,  ..., 0.5397, 0.4201, 0.5922]],

         [[0.5586, 0.4083, 0.4562,  ..., 0.6109, 0.4661, 0.6105],
          [0.4390, 0.2999, 0.5562,  ..., 0.5574, 0.5718, 0.5237],
          [0.4012, 0.5353, 0.3956,  ..., 0.4235, 0.5098, 0.5789],
          [0.4808, 0.5285, 0.5583,  ..., 0.6397, 0.5997, 0.4639]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0030, -0.0050,  0.0010,  0.0090, -0.0010,  0.0090, -0.0110, -0.0050,
         0.0050,  0.0030], device='cuda:0')
selected experts tensor([1551, 1608, 1511, 1605, 1700, 1680, 1590, 1578, 1746, 1815],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.3973, 0.5527, 0.4278,  ..., 0.6471, 0.5290, 0.5526],
          [0.4479, 0.5799, 0.5618,  ..., 0.6263, 0.5803, 0.4688],
          [0.4700, 0.5220, 0.4283,  ..., 0.4183, 0.4189, 0.4328],
          [0.4946, 0.4202, 0.4278,  ..., 0.4640, 0.5092, 0.4490]],

         [[0.5251, 0.3496, 0.5632,  ..., 0.5623, 0.5643, 0.6986],
          [0.4807, 0.5346, 0.6078,  ..., 0.5747, 0.7043, 0.4922],
          [0.5755, 0.3214, 0.5685,  ..., 0.5174, 0.3803, 0.4046],
          [0.4181, 0.5033, 0.5601,  ..., 0.4950, 0.3884, 0.5155]],

         [[0.4863, 0.5719, 0.4875,  ..., 0.3999, 0.4226, 0.7070],
          [0.4238, 0.5399, 0.4968,  ..., 0.4601, 0.5524, 0.5504],
          [0.5741, 0.5375, 0.6189,  ..., 0.5519, 0.4179, 0.5777],
          [0.6341, 0.5092, 0.4321,  ..., 0.5098, 0.5033, 0.3334]],

         ...,

         [[0.4034, 0.7034, 0.4463,  ..., 0.4618, 0.4990, 0.4610],
          [0.5779, 0.6326, 0.5651,  ..., 0.5432, 0.3989, 0.5157],
          [0.4610, 0.4661, 0.3445,  ..., 0.5049, 0.4839, 0.3558],
          [0.5208, 0.4731, 0.3786,  ..., 0.4312, 0.4054, 0.5053]],

         [[0.5760, 0.5917, 0.3786,  ..., 0.4982, 0.6070, 0.5250],
          [0.3583, 0.5453, 0.4560,  ..., 0.6795, 0.5387, 0.5940],
          [0.4547, 0.5037, 0.4691,  ..., 0.4122, 0.4655, 0.5897],
          [0.4672, 0.4289, 0.5014,  ..., 0.5209, 0.4540, 0.5324]],

         [[0.6476, 0.5628, 0.5143,  ..., 0.5205, 0.6461, 0.7103],
          [0.3913, 0.6140, 0.6245,  ..., 0.6041, 0.5109, 0.5309],
          [0.4929, 0.4358, 0.5083,  ..., 0.5276, 0.4585, 0.4591],
          [0.5982, 0.5936, 0.5675,  ..., 0.4693, 0.4236, 0.5222]]],


        [[[0.5498, 0.5424, 0.4737,  ..., 0.6097, 0.5696, 0.5882],
          [0.4936, 0.5922, 0.4927,  ..., 0.6087, 0.5793, 0.4864],
          [0.4993, 0.4424, 0.4733,  ..., 0.5319, 0.4771, 0.4724],
          [0.5707, 0.4749, 0.5524,  ..., 0.4676, 0.5689, 0.3917]],

         [[0.4780, 0.6530, 0.4264,  ..., 0.3828, 0.6221, 0.4727],
          [0.4829, 0.4830, 0.5833,  ..., 0.6648, 0.5029, 0.4826],
          [0.5034, 0.5931, 0.5444,  ..., 0.4155, 0.3848, 0.4577],
          [0.4994, 0.4050, 0.3915,  ..., 0.4853, 0.4287, 0.5579]],

         [[0.4816, 0.4744, 0.4827,  ..., 0.4688, 0.6239, 0.6143],
          [0.4049, 0.5346, 0.5218,  ..., 0.5371, 0.6480, 0.4219],
          [0.6136, 0.4596, 0.5843,  ..., 0.5175, 0.4713, 0.4438],
          [0.4736, 0.4833, 0.4582,  ..., 0.4633, 0.4866, 0.5299]],

         ...,

         [[0.4862, 0.4996, 0.5762,  ..., 0.4565, 0.5570, 0.6120],
          [0.4530, 0.5346, 0.4408,  ..., 0.4703, 0.6669, 0.5223],
          [0.3637, 0.4116, 0.4944,  ..., 0.6087, 0.6286, 0.4819],
          [0.4808, 0.4694, 0.4738,  ..., 0.4507, 0.4401, 0.4693]],

         [[0.5784, 0.5823, 0.6152,  ..., 0.5937, 0.5803, 0.5935],
          [0.5153, 0.6763, 0.5680,  ..., 0.4231, 0.6678, 0.4816],
          [0.4503, 0.3760, 0.4458,  ..., 0.5589, 0.4026, 0.5227],
          [0.3908, 0.4747, 0.4451,  ..., 0.4875, 0.4156, 0.3880]],

         [[0.5003, 0.4598, 0.5695,  ..., 0.5989, 0.6388, 0.5479],
          [0.4296, 0.5875, 0.5771,  ..., 0.5078, 0.6624, 0.5791],
          [0.5440, 0.4700, 0.5073,  ..., 0.5785, 0.5417, 0.3234],
          [0.3889, 0.5762, 0.4562,  ..., 0.5586, 0.4593, 0.4032]]]],
       device='cuda:0')
tensor([[[[0.3993, 0.5607, 0.4258,  ..., 0.6451, 0.5090, 0.5366],
          [0.4499, 0.5879, 0.5598,  ..., 0.6243, 0.5603, 0.4528],
          [0.4720, 0.5300, 0.4263,  ..., 0.4163, 0.3989, 0.4168],
          [0.4966, 0.4282, 0.4258,  ..., 0.4620, 0.4892, 0.4330]],

         [[0.5271, 0.3576, 0.5612,  ..., 0.5603, 0.5443, 0.6826],
          [0.4827, 0.5426, 0.6058,  ..., 0.5727, 0.6843, 0.4762],
          [0.5775, 0.3294, 0.5665,  ..., 0.5154, 0.3603, 0.3886],
          [0.4201, 0.5113, 0.5581,  ..., 0.4930, 0.3684, 0.4995]],

         [[0.4883, 0.5799, 0.4855,  ..., 0.3979, 0.4026, 0.6910],
          [0.4258, 0.5479, 0.4948,  ..., 0.4581, 0.5324, 0.5344],
          [0.5761, 0.5455, 0.6169,  ..., 0.5499, 0.3979, 0.5617],
          [0.6361, 0.5172, 0.4301,  ..., 0.5078, 0.4833, 0.3174]],

         ...,

         [[0.4054, 0.7114, 0.4443,  ..., 0.4598, 0.4790, 0.4450],
          [0.5799, 0.6406, 0.5631,  ..., 0.5412, 0.3789, 0.4997],
          [0.4630, 0.4741, 0.3425,  ..., 0.5029, 0.4639, 0.3398],
          [0.5228, 0.4811, 0.3766,  ..., 0.4292, 0.3854, 0.4893]],

         [[0.5780, 0.5997, 0.3766,  ..., 0.4962, 0.5870, 0.5090],
          [0.3603, 0.5533, 0.4540,  ..., 0.6775, 0.5187, 0.5780],
          [0.4567, 0.5117, 0.4671,  ..., 0.4102, 0.4455, 0.5737],
          [0.4692, 0.4369, 0.4994,  ..., 0.5189, 0.4340, 0.5164]],

         [[0.6496, 0.5708, 0.5123,  ..., 0.5185, 0.6261, 0.6943],
          [0.3933, 0.6220, 0.6225,  ..., 0.6021, 0.4909, 0.5149],
          [0.4949, 0.4438, 0.5063,  ..., 0.5256, 0.4385, 0.4431],
          [0.6002, 0.6016, 0.5655,  ..., 0.4673, 0.4036, 0.5062]]],


        [[[0.5518, 0.5504, 0.4717,  ..., 0.6077, 0.5496, 0.5722],
          [0.4956, 0.6002, 0.4907,  ..., 0.6067, 0.5593, 0.4704],
          [0.5013, 0.4504, 0.4713,  ..., 0.5299, 0.4571, 0.4564],
          [0.5727, 0.4829, 0.5504,  ..., 0.4656, 0.5489, 0.3757]],

         [[0.4800, 0.6610, 0.4244,  ..., 0.3808, 0.6021, 0.4567],
          [0.4849, 0.4910, 0.5813,  ..., 0.6628, 0.4829, 0.4666],
          [0.5054, 0.6011, 0.5424,  ..., 0.4135, 0.3648, 0.4417],
          [0.5014, 0.4130, 0.3895,  ..., 0.4833, 0.4087, 0.5419]],

         [[0.4836, 0.4824, 0.4807,  ..., 0.4668, 0.6039, 0.5983],
          [0.4069, 0.5426, 0.5198,  ..., 0.5351, 0.6280, 0.4059],
          [0.6156, 0.4676, 0.5823,  ..., 0.5155, 0.4513, 0.4278],
          [0.4756, 0.4913, 0.4562,  ..., 0.4613, 0.4666, 0.5139]],

         ...,

         [[0.4882, 0.5076, 0.5742,  ..., 0.4545, 0.5370, 0.5960],
          [0.4550, 0.5426, 0.4388,  ..., 0.4683, 0.6469, 0.5063],
          [0.3657, 0.4196, 0.4924,  ..., 0.6067, 0.6086, 0.4659],
          [0.4828, 0.4774, 0.4718,  ..., 0.4487, 0.4201, 0.4533]],

         [[0.5804, 0.5903, 0.6132,  ..., 0.5917, 0.5603, 0.5775],
          [0.5173, 0.6843, 0.5660,  ..., 0.4211, 0.6478, 0.4656],
          [0.4523, 0.3840, 0.4438,  ..., 0.5569, 0.3826, 0.5067],
          [0.3928, 0.4827, 0.4431,  ..., 0.4855, 0.3956, 0.3720]],

         [[0.5023, 0.4678, 0.5675,  ..., 0.5969, 0.6188, 0.5319],
          [0.4316, 0.5955, 0.5751,  ..., 0.5058, 0.6424, 0.5631],
          [0.5460, 0.4780, 0.5053,  ..., 0.5765, 0.5217, 0.3074],
          [0.3909, 0.5842, 0.4542,  ..., 0.5566, 0.4393, 0.3872]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0020, -0.0080,  0.0020, -0.0120, -0.0080,  0.0160, -0.0320,  0.0020,
         0.0200,  0.0160], device='cuda:0')
selected experts tensor([1319, 1416, 1294, 2443, 1240, 1862, 1580, 1582, 1871, 1777],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4807, 0.4345, 0.4416,  ..., 0.4558, 0.5484, 0.5074],
          [0.5095, 0.4095, 0.4824,  ..., 0.5085, 0.5115, 0.5252],
          [0.5567, 0.4029, 0.3655,  ..., 0.5618, 0.5530, 0.5488],
          [0.5934, 0.4796, 0.5002,  ..., 0.4329, 0.4511, 0.4417]],

         [[0.4223, 0.5063, 0.4358,  ..., 0.3738, 0.2758, 0.3696],
          [0.5245, 0.5287, 0.6502,  ..., 0.4329, 0.3640, 0.5723],
          [0.4362, 0.3708, 0.3637,  ..., 0.4964, 0.5728, 0.5682],
          [0.5915, 0.5488, 0.5287,  ..., 0.3388, 0.4879, 0.2887]],

         [[0.5682, 0.4753, 0.4409,  ..., 0.4343, 0.4860, 0.6023],
          [0.5721, 0.3949, 0.6305,  ..., 0.4029, 0.4887, 0.3311],
          [0.4553, 0.3608, 0.4053,  ..., 0.4880, 0.6001, 0.5064],
          [0.5367, 0.5711, 0.5341,  ..., 0.3082, 0.5491, 0.4465]],

         ...,

         [[0.4960, 0.4960, 0.4980,  ..., 0.5180, 0.4920, 0.5120],
          [0.4960, 0.4960, 0.4980,  ..., 0.5180, 0.4920, 0.5120],
          [0.4960, 0.4960, 0.4980,  ..., 0.5180, 0.4920, 0.5120],
          [0.4960, 0.4960, 0.4980,  ..., 0.5180, 0.4920, 0.5120]],

         [[0.4960, 0.4960, 0.4980,  ..., 0.5180, 0.4920, 0.5120],
          [0.4960, 0.4960, 0.4980,  ..., 0.5180, 0.4920, 0.5120],
          [0.4960, 0.4960, 0.4980,  ..., 0.5180, 0.4920, 0.5120],
          [0.4960, 0.4960, 0.4980,  ..., 0.5180, 0.4920, 0.5120]],

         [[0.4960, 0.4960, 0.4980,  ..., 0.5180, 0.4920, 0.5120],
          [0.4960, 0.4960, 0.4980,  ..., 0.5180, 0.4920, 0.5120],
          [0.4960, 0.4960, 0.4980,  ..., 0.5180, 0.4920, 0.5120],
          [0.4960, 0.4960, 0.4980,  ..., 0.5180, 0.4920, 0.5120]]],


        [[[0.4997, 0.5194, 0.5316,  ..., 0.5761, 0.4368, 0.4672],
          [0.5007, 0.5635, 0.5377,  ..., 0.5168, 0.6896, 0.5677],
          [0.5990, 0.4556, 0.5087,  ..., 0.6667, 0.6145, 0.6080],
          [0.6120, 0.5606, 0.5234,  ..., 0.4020, 0.5387, 0.4359]],

         [[0.4305, 0.4384, 0.6108,  ..., 0.4381, 0.3371, 0.5739],
          [0.4038, 0.5461, 0.7441,  ..., 0.5780, 0.4036, 0.4493],
          [0.4711, 0.4137, 0.5014,  ..., 0.4549, 0.5875, 0.6280],
          [0.5797, 0.4739, 0.4590,  ..., 0.4187, 0.5108, 0.3723]],

         [[0.5749, 0.4152, 0.4970,  ..., 0.3675, 0.5237, 0.5662],
          [0.5749, 0.3617, 0.6200,  ..., 0.4529, 0.4540, 0.3553],
          [0.5849, 0.4570, 0.4349,  ..., 0.5440, 0.6043, 0.6642],
          [0.4848, 0.6129, 0.4053,  ..., 0.3295, 0.5922, 0.3813]],

         ...,

         [[0.4960, 0.4960, 0.4980,  ..., 0.5180, 0.4920, 0.5120],
          [0.4960, 0.4960, 0.4980,  ..., 0.5180, 0.4920, 0.5120],
          [0.4960, 0.4960, 0.4980,  ..., 0.5180, 0.4920, 0.5120],
          [0.4960, 0.4960, 0.4980,  ..., 0.5180, 0.4920, 0.5120]],

         [[0.4960, 0.4960, 0.4980,  ..., 0.5180, 0.4920, 0.5120],
          [0.4960, 0.4960, 0.4980,  ..., 0.5180, 0.4920, 0.5120],
          [0.4960, 0.4960, 0.4980,  ..., 0.5180, 0.4920, 0.5120],
          [0.4960, 0.4960, 0.4980,  ..., 0.5180, 0.4920, 0.5120]],

         [[0.4960, 0.4960, 0.4980,  ..., 0.5180, 0.4920, 0.5120],
          [0.4960, 0.4960, 0.4980,  ..., 0.5180, 0.4920, 0.5120],
          [0.4960, 0.4960, 0.4980,  ..., 0.5180, 0.4920, 0.5120],
          [0.4960, 0.4960, 0.4980,  ..., 0.5180, 0.4920, 0.5120]]]],
       device='cuda:0')/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([[[[0.4847, 0.4385, 0.4436,  ..., 0.4378, 0.5564, 0.4954],
          [0.5135, 0.4135, 0.4844,  ..., 0.4905, 0.5195, 0.5132],
          [0.5607, 0.4069, 0.3675,  ..., 0.5438, 0.5610, 0.5368],
          [0.5974, 0.4836, 0.5022,  ..., 0.4149, 0.4591, 0.4297]],

         [[0.4263, 0.5103, 0.4378,  ..., 0.3558, 0.2838, 0.3576],
          [0.5285, 0.5327, 0.6522,  ..., 0.4149, 0.3720, 0.5603],
          [0.4402, 0.3748, 0.3657,  ..., 0.4784, 0.5808, 0.5562],
          [0.5955, 0.5528, 0.5307,  ..., 0.3208, 0.4959, 0.2767]],

         [[0.5722, 0.4793, 0.4429,  ..., 0.4163, 0.4940, 0.5903],
          [0.5761, 0.3989, 0.6325,  ..., 0.3849, 0.4967, 0.3191],
          [0.4593, 0.3648, 0.4073,  ..., 0.4700, 0.6081, 0.4944],
          [0.5407, 0.5751, 0.5361,  ..., 0.2902, 0.5571, 0.4345]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5037, 0.5234, 0.5336,  ..., 0.5581, 0.4448, 0.4552],
          [0.5047, 0.5675, 0.5397,  ..., 0.4988, 0.6976, 0.5557],
          [0.6030, 0.4596, 0.5107,  ..., 0.6487, 0.6225, 0.5960],
          [0.6160, 0.5646, 0.5254,  ..., 0.3840, 0.5467, 0.4239]],

         [[0.4345, 0.4424, 0.6128,  ..., 0.4201, 0.3451, 0.5619],
          [0.4078, 0.5501, 0.7461,  ..., 0.5600, 0.4116, 0.4373],
          [0.4751, 0.4177, 0.5034,  ..., 0.4369, 0.5955, 0.6160],
          [0.5837, 0.4779, 0.4610,  ..., 0.4007, 0.5188, 0.3603]],

         [[0.5789, 0.4192, 0.4990,  ..., 0.3495, 0.5317, 0.5542],
          [0.5789, 0.3657, 0.6220,  ..., 0.4349, 0.4620, 0.3433],
          [0.5889, 0.4610, 0.4369,  ..., 0.5260, 0.6123, 0.6522],
          [0.4888, 0.6169, 0.4073,  ..., 0.3115, 0.6002, 0.3693]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0040, -0.0040, -0.0020,  0.0080, -0.0100,  0.0200,  0.0060,  0.0180,
        -0.0080,  0.0120], device='cuda:0')
selected experts tensor([1799, 1865, 2111, 1693, 1563,  984, 1781,  957, 1655, 1976],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1551., 1608., 1511., 1605., 1700., 1680., 1590., 1578., 1746., 1815.],
        [1799., 1865., 2111., 1693., 1563.,  984., 1781.,  957., 1655., 1976.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4600, 0.5354, 0.4492,  ..., 0.4709, 0.5723, 0.5493],
          [0.3682, 0.5410, 0.3887,  ..., 0.5063, 0.5565, 0.4323],
          [0.4115, 0.3878, 0.4371,  ..., 0.4009, 0.6444, 0.4593],
          [0.6502, 0.4231, 0.3453,  ..., 0.4458, 0.5956, 0.5269]],

         [[0.3857, 0.6254, 0.4707,  ..., 0.5866, 0.5680, 0.4077],
          [0.6413, 0.3704, 0.3780,  ..., 0.5331, 0.5819, 0.5041],
          [0.3765, 0.4560, 0.4990,  ..., 0.4009, 0.5344, 0.4129],
          [0.4491, 0.4846, 0.4979,  ..., 0.4650, 0.3704, 0.4237]],

         [[0.5558, 0.5000, 0.3734,  ..., 0.5087, 0.5458, 0.4665],
          [0.5817, 0.5305, 0.6749,  ..., 0.4782, 0.4112, 0.5027],
          [0.5051, 0.6254, 0.5523,  ..., 0.4288, 0.4420, 0.4959],
          [0.4980, 0.4796, 0.3391,  ..., 0.5063, 0.3981, 0.5332]],

         ...,

         [[0.4353, 0.6507, 0.4639,  ..., 0.6896, 0.5307, 0.4687],
          [0.4272, 0.4662, 0.3444,  ..., 0.5366, 0.4070, 0.4974],
          [0.4832, 0.4962, 0.6391,  ..., 0.4893, 0.5728, 0.4726],
          [0.5803, 0.5197, 0.5786,  ..., 0.5263, 0.3357, 0.4975]],

         [[0.5707, 0.3533, 0.4545,  ..., 0.4502, 0.5321, 0.5605],
          [0.3691, 0.6752, 0.4256,  ..., 0.5747, 0.4908, 0.3817],
          [0.4433, 0.5281, 0.4032,  ..., 0.3823, 0.5975, 0.6081],
          [0.6511, 0.3786, 0.5149,  ..., 0.4781, 0.5531, 0.3993]],

         [[0.4895, 0.6534, 0.5264,  ..., 0.6363, 0.4519, 0.5486],
          [0.5397, 0.4278, 0.5834,  ..., 0.5291, 0.4601, 0.4204],
          [0.6103, 0.4408, 0.5018,  ..., 0.3883, 0.4758, 0.5411],
          [0.4205, 0.4822, 0.5918,  ..., 0.5174, 0.2834, 0.4709]]],


        [[[0.4409, 0.4093, 0.6035,  ..., 0.5914, 0.4965, 0.4930],
          [0.6431, 0.6199, 0.5605,  ..., 0.5223, 0.5310, 0.5977],
          [0.5688, 0.5012, 0.5011,  ..., 0.4589, 0.5487, 0.5622],
          [0.5573, 0.5224, 0.3734,  ..., 0.4853, 0.4562, 0.5133]],

         [[0.4600, 0.4645, 0.4461,  ..., 0.6639, 0.4079, 0.5479],
          [0.5467, 0.6363, 0.4694,  ..., 0.6551, 0.3976, 0.4496],
          [0.4887, 0.3846, 0.5815,  ..., 0.5037, 0.5046, 0.4314],
          [0.5546, 0.3841, 0.4707,  ..., 0.5364, 0.6300, 0.5539]],

         [[0.4893, 0.5298, 0.4606,  ..., 0.4911, 0.5661, 0.4949],
          [0.6502, 0.4293, 0.4526,  ..., 0.6008, 0.4869, 0.5641],
          [0.4824, 0.5468, 0.5729,  ..., 0.4264, 0.5007, 0.5515],
          [0.4646, 0.4164, 0.5332,  ..., 0.5235, 0.5548, 0.3134]],

         ...,

         [[0.5659, 0.5269, 0.5369,  ..., 0.4509, 0.4761, 0.5011],
          [0.4370, 0.5004, 0.5220,  ..., 0.4417, 0.3659, 0.3027],
          [0.4770, 0.3713, 0.4441,  ..., 0.5251, 0.5594, 0.5754],
          [0.4715, 0.5937, 0.5629,  ..., 0.5625, 0.5531, 0.4527]],

         [[0.4353, 0.6507, 0.4639,  ..., 0.6896, 0.5307, 0.4687],
          [0.4272, 0.4662, 0.3444,  ..., 0.5366, 0.4070, 0.4974],
          [0.4832, 0.4962, 0.6391,  ..., 0.4893, 0.5728, 0.4726],
          [0.5803, 0.5197, 0.5786,  ..., 0.5263, 0.3357, 0.4975]],

         [[0.4646, 0.5475, 0.4461,  ..., 0.5094, 0.6569, 0.5242],
          [0.4894, 0.5594, 0.5961,  ..., 0.3874, 0.5148, 0.4338],
          [0.5911, 0.2858, 0.4482,  ..., 0.4982, 0.4458, 0.4942],
          [0.4501, 0.4966, 0.3498,  ..., 0.6829, 0.4027, 0.3904]]]],
       device='cuda:0')
tensor([[[[0.4620, 0.5334, 0.4552,  ..., 0.4689, 0.5703, 0.5433],
          [0.3702, 0.5390, 0.3947,  ..., 0.5043, 0.5545, 0.4263],
          [0.4135, 0.3858, 0.4431,  ..., 0.3989, 0.6424, 0.4533],
          [0.6522, 0.4211, 0.3513,  ..., 0.4438, 0.5936, 0.5209]],

         [[0.3877, 0.6234, 0.4767,  ..., 0.5846, 0.5660, 0.4017],
          [0.6433, 0.3684, 0.3840,  ..., 0.5311, 0.5799, 0.4981],
          [0.3785, 0.4540, 0.5050,  ..., 0.3989, 0.5324, 0.4069],
          [0.4511, 0.4826, 0.5039,  ..., 0.4630, 0.3684, 0.4177]],

         [[0.5578, 0.4980, 0.3794,  ..., 0.5067, 0.5438, 0.4605],
          [0.5837, 0.5285, 0.6809,  ..., 0.4762, 0.4092, 0.4967],
          [0.5071, 0.6234, 0.5583,  ..., 0.4268, 0.4400, 0.4899],
          [0.5000, 0.4776, 0.3451,  ..., 0.5043, 0.3961, 0.5272]],

         ...,

         [[0.4373, 0.6487, 0.4699,  ..., 0.6876, 0.5287, 0.4627],
          [0.4292, 0.4642, 0.3504,  ..., 0.5346, 0.4050, 0.4914],
          [0.4852, 0.4942, 0.6451,  ..., 0.4873, 0.5708, 0.4666],
          [0.5823, 0.5177, 0.5846,  ..., 0.5243, 0.3337, 0.4915]],

         [[0.5727, 0.3513, 0.4605,  ..., 0.4482, 0.5301, 0.5545],
          [0.3711, 0.6732, 0.4316,  ..., 0.5727, 0.4888, 0.3757],
          [0.4453, 0.5261, 0.4092,  ..., 0.3803, 0.5955, 0.6021],
          [0.6531, 0.3766, 0.5209,  ..., 0.4761, 0.5511, 0.3933]],

         [[0.4915, 0.6514, 0.5324,  ..., 0.6343, 0.4499, 0.5426],
          [0.5417, 0.4258, 0.5894,  ..., 0.5271, 0.4581, 0.4144],
          [0.6123, 0.4388, 0.5078,  ..., 0.3863, 0.4738, 0.5351],
          [0.4225, 0.4802, 0.5978,  ..., 0.5154, 0.2814, 0.4649]]],


        [[[0.4429, 0.4073, 0.6095,  ..., 0.5894, 0.4945, 0.4870],
          [0.6451, 0.6179, 0.5665,  ..., 0.5203, 0.5290, 0.5917],
          [0.5708, 0.4992, 0.5071,  ..., 0.4569, 0.5467, 0.5562],
          [0.5593, 0.5204, 0.3794,  ..., 0.4833, 0.4542, 0.5073]],

         [[0.4620, 0.4625, 0.4521,  ..., 0.6619, 0.4059, 0.5419],
          [0.5487, 0.6343, 0.4754,  ..., 0.6531, 0.3956, 0.4436],
          [0.4907, 0.3826, 0.5875,  ..., 0.5017, 0.5026, 0.4254],
          [0.5566, 0.3821, 0.4767,  ..., 0.5344, 0.6280, 0.5479]],

         [[0.4913, 0.5278, 0.4666,  ..., 0.4891, 0.5641, 0.4889],
          [0.6522, 0.4273, 0.4586,  ..., 0.5988, 0.4849, 0.5581],
          [0.4844, 0.5448, 0.5789,  ..., 0.4244, 0.4987, 0.5455],
          [0.4666, 0.4144, 0.5392,  ..., 0.5215, 0.5528, 0.3074]],

         ...,

         [[0.5679, 0.5249, 0.5429,  ..., 0.4489, 0.4741, 0.4951],
          [0.4390, 0.4984, 0.5280,  ..., 0.4397, 0.3639, 0.2967],
          [0.4790, 0.3693, 0.4501,  ..., 0.5231, 0.5574, 0.5694],
          [0.4735, 0.5917, 0.5689,  ..., 0.5605, 0.5511, 0.4467]],

         [[0.4373, 0.6487, 0.4699,  ..., 0.6876, 0.5287, 0.4627],
          [0.4292, 0.4642, 0.3504,  ..., 0.5346, 0.4050, 0.4914],
          [0.4852, 0.4942, 0.6451,  ..., 0.4873, 0.5708, 0.4666],
          [0.5823, 0.5177, 0.5846,  ..., 0.5243, 0.3337, 0.4915]],

         [[0.4666, 0.5455, 0.4521,  ..., 0.5074, 0.6549, 0.5182],
          [0.4914, 0.5574, 0.6021,  ..., 0.3854, 0.5128, 0.4278],
          [0.5931, 0.2838, 0.4542,  ..., 0.4962, 0.4438, 0.4882],
          [0.4521, 0.4946, 0.3558,  ..., 0.6809, 0.4007, 0.3844]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0020,  0.0020, -0.0060,  0.0020, -0.0020, -0.0060,  0.0080,  0.0020,
         0.0020,  0.0060], device='cuda:0')
selected experts tensor([1710, 1615, 1732, 1644, 1561, 1576, 1696, 1491, 1662, 1697],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4956, 0.4672, 0.6622,  ..., 0.5095, 0.3940, 0.5184],
          [0.4629, 0.3306, 0.3659,  ..., 0.6614, 0.6140, 0.5733],
          [0.4191, 0.5352, 0.3995,  ..., 0.4353, 0.4394, 0.4620],
          [0.5655, 0.5079, 0.3976,  ..., 0.4161, 0.5743, 0.5023]],

         [[0.4976, 0.4628, 0.5236,  ..., 0.4923, 0.3931, 0.5591],
          [0.4067, 0.5306, 0.4725,  ..., 0.5459, 0.5001, 0.5386],
          [0.5046, 0.6474, 0.6300,  ..., 0.5216, 0.6437, 0.5240],
          [0.3931, 0.5844, 0.4903,  ..., 0.4890, 0.5082, 0.5316]],

         [[0.4651, 0.5663, 0.4207,  ..., 0.4673, 0.4001, 0.5671],
          [0.4658, 0.4029, 0.5499,  ..., 0.5330, 0.5312, 0.4027],
          [0.4404, 0.3740, 0.4960,  ..., 0.3527, 0.5848, 0.4541],
          [0.3774, 0.3921, 0.4618,  ..., 0.4621, 0.6857, 0.4084]],

         ...,

         [[0.4867, 0.4751, 0.4854,  ..., 0.4418, 0.3945, 0.5928],
          [0.6066, 0.5418, 0.5615,  ..., 0.6106, 0.3852, 0.4403],
          [0.4157, 0.5287, 0.3999,  ..., 0.5271, 0.6255, 0.3152],
          [0.5288, 0.3916, 0.5061,  ..., 0.4350, 0.5369, 0.3869]],

         [[0.6038, 0.6339, 0.3036,  ..., 0.5301, 0.3760, 0.4131],
          [0.5698, 0.4609, 0.4398,  ..., 0.5577, 0.5158, 0.4164],
          [0.6898, 0.5152, 0.3828,  ..., 0.5611, 0.6033, 0.4778],
          [0.4756, 0.5083, 0.3069,  ..., 0.5668, 0.5805, 0.5120]],

         [[0.4399, 0.6438, 0.4009,  ..., 0.6004, 0.3815, 0.4403],
          [0.5151, 0.5891, 0.3659,  ..., 0.6171, 0.6000, 0.4613],
          [0.5242, 0.5061, 0.5065,  ..., 0.5294, 0.4677, 0.6569],
          [0.6001, 0.4156, 0.5354,  ..., 0.5730, 0.5427, 0.5225]]],


        [[[0.5265, 0.5107, 0.5909,  ..., 0.5787, 0.5459, 0.5876],
          [0.4998, 0.5217, 0.4715,  ..., 0.5333, 0.5208, 0.4831],
          [0.4440, 0.3939, 0.6700,  ..., 0.4005, 0.3843, 0.3740],
          [0.5220, 0.5077, 0.6055,  ..., 0.3996, 0.4123, 0.4492]],

         [[0.4971, 0.5105, 0.6674,  ..., 0.4252, 0.7033, 0.5187],
          [0.6024, 0.4238, 0.5188,  ..., 0.4437, 0.4653, 0.4878],
          [0.5067, 0.3930, 0.4953,  ..., 0.5682, 0.4935, 0.4126],
          [0.5779, 0.3911, 0.5490,  ..., 0.5216, 0.4963, 0.5485]],

         [[0.5258, 0.3907, 0.5352,  ..., 0.6303, 0.3954, 0.5146],
          [0.6163, 0.4942, 0.5733,  ..., 0.5242, 0.4454, 0.4483],
          [0.4082, 0.4290, 0.4405,  ..., 0.5868, 0.5213, 0.4193],
          [0.4435, 0.6429, 0.5179,  ..., 0.4887, 0.5563, 0.4369]],

         ...,

         [[0.5220, 0.3306, 0.3542,  ..., 0.3768, 0.3715, 0.3445],
          [0.5731, 0.6605, 0.3578,  ..., 0.3758, 0.4308, 0.3506],
          [0.6241, 0.6249, 0.4994,  ..., 0.5063, 0.5350, 0.5895],
          [0.5309, 0.3949, 0.6700,  ..., 0.4684, 0.6310, 0.4788]],

         [[0.4867, 0.4751, 0.4854,  ..., 0.4418, 0.3945, 0.5928],
          [0.6066, 0.5418, 0.5615,  ..., 0.6106, 0.3852, 0.4403],
          [0.4157, 0.5287, 0.3999,  ..., 0.5271, 0.6255, 0.3152],
          [0.5288, 0.3916, 0.5061,  ..., 0.4350, 0.5369, 0.3869]],

         [[0.4238, 0.6194, 0.5466,  ..., 0.5145, 0.6135, 0.6498],
          [0.6395, 0.4914, 0.6417,  ..., 0.3888, 0.5710, 0.7806],
          [0.3326, 0.5167, 0.5610,  ..., 0.4708, 0.5283, 0.4792],
          [0.4948, 0.4324, 0.3777,  ..., 0.4906, 0.6580, 0.4216]]]],
       device='cuda:0')
tensor([[[[0.4976, 0.4712, 0.6602,  ..., 0.5135, 0.3900, 0.5164],
          [0.4649, 0.3346, 0.3639,  ..., 0.6654, 0.6100, 0.5713],
          [0.4211, 0.5392, 0.3975,  ..., 0.4393, 0.4354, 0.4600],
          [0.5675, 0.5119, 0.3956,  ..., 0.4201, 0.5703, 0.5003]],

         [[0.4996, 0.4668, 0.5216,  ..., 0.4963, 0.3891, 0.5571],
          [0.4087, 0.5346, 0.4705,  ..., 0.5499, 0.4961, 0.5366],
          [0.5066, 0.6514, 0.6280,  ..., 0.5256, 0.6397, 0.5220],
          [0.3951, 0.5884, 0.4883,  ..., 0.4930, 0.5042, 0.5296]],

         [[0.4671, 0.5703, 0.4187,  ..., 0.4713, 0.3961, 0.5651],
          [0.4678, 0.4069, 0.5479,  ..., 0.5370, 0.5272, 0.4007],
          [0.4424, 0.3780, 0.4940,  ..., 0.3567, 0.5808, 0.4521],
          [0.3794, 0.3961, 0.4598,  ..., 0.4661, 0.6817, 0.4064]],

         ...,

         [[0.4887, 0.4791, 0.4834,  ..., 0.4458, 0.3905, 0.5908],
          [0.6086, 0.5458, 0.5595,  ..., 0.6146, 0.3812, 0.4383],
          [0.4177, 0.5327, 0.3979,  ..., 0.5311, 0.6215, 0.3132],
          [0.5308, 0.3956, 0.5041,  ..., 0.4390, 0.5329, 0.3849]],

         [[0.6058, 0.6379, 0.3016,  ..., 0.5341, 0.3720, 0.4111],
          [0.5718, 0.4649, 0.4378,  ..., 0.5617, 0.5118, 0.4144],
          [0.6918, 0.5192, 0.3808,  ..., 0.5651, 0.5993, 0.4758],
          [0.4776, 0.5123, 0.3049,  ..., 0.5708, 0.5765, 0.5100]],

         [[0.4419, 0.6478, 0.3989,  ..., 0.6044, 0.3775, 0.4383],
          [0.5171, 0.5931, 0.3639,  ..., 0.6211, 0.5960, 0.4593],
          [0.5262, 0.5101, 0.5045,  ..., 0.5334, 0.4637, 0.6549],
          [0.6021, 0.4196, 0.5334,  ..., 0.5770, 0.5387, 0.5205]]],


        [[[0.5285, 0.5147, 0.5889,  ..., 0.5827, 0.5419, 0.5856],
          [0.5018, 0.5257, 0.4695,  ..., 0.5373, 0.5168, 0.4811],
          [0.4460, 0.3979, 0.6680,  ..., 0.4045, 0.3803, 0.3720],
          [0.5240, 0.5117, 0.6035,  ..., 0.4036, 0.4083, 0.4472]],

         [[0.4991, 0.5145, 0.6654,  ..., 0.4292, 0.6993, 0.5167],
          [0.6044, 0.4278, 0.5168,  ..., 0.4477, 0.4613, 0.4858],
          [0.5087, 0.3970, 0.4933,  ..., 0.5722, 0.4895, 0.4106],
          [0.5799, 0.3951, 0.5470,  ..., 0.5256, 0.4923, 0.5465]],

         [[0.5278, 0.3947, 0.5332,  ..., 0.6343, 0.3914, 0.5126],
          [0.6183, 0.4982, 0.5713,  ..., 0.5282, 0.4414, 0.4463],
          [0.4102, 0.4330, 0.4385,  ..., 0.5908, 0.5173, 0.4173],
          [0.4455, 0.6469, 0.5159,  ..., 0.4927, 0.5523, 0.4349]],

         ...,

         [[0.5240, 0.3346, 0.3522,  ..., 0.3808, 0.3675, 0.3425],
          [0.5751, 0.6645, 0.3558,  ..., 0.3798, 0.4268, 0.3486],
          [0.6261, 0.6289, 0.4974,  ..., 0.5103, 0.5310, 0.5875],
          [0.5329, 0.3989, 0.6680,  ..., 0.4724, 0.6270, 0.4768]],

         [[0.4887, 0.4791, 0.4834,  ..., 0.4458, 0.3905, 0.5908],
          [0.6086, 0.5458, 0.5595,  ..., 0.6146, 0.3812, 0.4383],
          [0.4177, 0.5327, 0.3979,  ..., 0.5311, 0.6215, 0.3132],
          [0.5308, 0.3956, 0.5041,  ..., 0.4390, 0.5329, 0.3849]],

         [[0.4258, 0.6234, 0.5446,  ..., 0.5185, 0.6095, 0.6478],
          [0.6415, 0.4954, 0.6397,  ..., 0.3928, 0.5670, 0.7786],
          [0.3346, 0.5207, 0.5590,  ..., 0.4748, 0.5243, 0.4772],
          [0.4968, 0.4364, 0.3757,  ..., 0.4946, 0.6540, 0.4196]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0020, -0.0040,  0.0020,  0.0100, -0.0020,  0.0080, -0.0100, -0.0040,
         0.0040,  0.0020], device='cuda:0')
selected experts tensor([1592, 1588, 1665, 1697, 1510, 1655, 1674, 1845, 1541, 1617],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.3720, 0.5847, 0.6088,  ..., 0.4594, 0.4410, 0.6411],
          [0.4697, 0.5366, 0.4236,  ..., 0.5410, 0.6623, 0.4704],
          [0.6432, 0.4713, 0.5340,  ..., 0.4322, 0.6387, 0.5552],
          [0.6369, 0.5089, 0.4662,  ..., 0.5517, 0.5672, 0.3401]],

         [[0.4315, 0.5748, 0.6004,  ..., 0.3787, 0.4660, 0.5550],
          [0.4469, 0.5148, 0.3769,  ..., 0.4972, 0.5701, 0.5906],
          [0.4380, 0.5368, 0.6588,  ..., 0.5551, 0.5107, 0.3990],
          [0.4796, 0.4703, 0.4974,  ..., 0.4645, 0.4325, 0.5216]],

         [[0.4306, 0.4577, 0.4882,  ..., 0.4876, 0.4468, 0.6227],
          [0.4675, 0.4431, 0.5872,  ..., 0.6018, 0.6677, 0.5567],
          [0.5669, 0.4174, 0.5862,  ..., 0.4023, 0.5611, 0.4596],
          [0.5655, 0.5238, 0.5405,  ..., 0.5143, 0.5463, 0.5002]],

         ...,

         [[0.6062, 0.4342, 0.7341,  ..., 0.5408, 0.5125, 0.4714],
          [0.5356, 0.3956, 0.5767,  ..., 0.5485, 0.6336, 0.6306],
          [0.4900, 0.4726, 0.7120,  ..., 0.5276, 0.4604, 0.5105],
          [0.4291, 0.4699, 0.4592,  ..., 0.4399, 0.3712, 0.4833]],

         [[0.6441, 0.3399, 0.6319,  ..., 0.5318, 0.6098, 0.5892],
          [0.4714, 0.5203, 0.6116,  ..., 0.5287, 0.6027, 0.5162],
          [0.5122, 0.4528, 0.7031,  ..., 0.4217, 0.5674, 0.5996],
          [0.6260, 0.5852, 0.3687,  ..., 0.4842, 0.5580, 0.4223]],

         [[0.5314, 0.4555, 0.4893,  ..., 0.4399, 0.5713, 0.5963],
          [0.5119, 0.6627, 0.5032,  ..., 0.5106, 0.6524, 0.6058],
          [0.4812, 0.3765, 0.5301,  ..., 0.5425, 0.6739, 0.4709],
          [0.3459, 0.4484, 0.3633,  ..., 0.4413, 0.4815, 0.4658]]],


        [[[0.4201, 0.5492, 0.5463,  ..., 0.3981, 0.5776, 0.5901],
          [0.3923, 0.5017, 0.4442,  ..., 0.4958, 0.6060, 0.3566],
          [0.4882, 0.5257, 0.6805,  ..., 0.4543, 0.5165, 0.4721],
          [0.5460, 0.4169, 0.5000,  ..., 0.4672, 0.5125, 0.5825]],

         [[0.5051, 0.5585, 0.5933,  ..., 0.4779, 0.4604, 0.5726],
          [0.4371, 0.5561, 0.5781,  ..., 0.5843, 0.4396, 0.4115],
          [0.4287, 0.4146, 0.5270,  ..., 0.3769, 0.3975, 0.4004],
          [0.7112, 0.5614, 0.4760,  ..., 0.5791, 0.6187, 0.4767]],

         [[0.4747, 0.4863, 0.4684,  ..., 0.4411, 0.4530, 0.5600],
          [0.4291, 0.5409, 0.5003,  ..., 0.6300, 0.6783, 0.4741],
          [0.4291, 0.5477, 0.5345,  ..., 0.4609, 0.3856, 0.4782],
          [0.5593, 0.4790, 0.5563,  ..., 0.5153, 0.4851, 0.5227]],

         ...,

         [[0.5411, 0.4831, 0.4217,  ..., 0.4198, 0.6350, 0.6259],
          [0.4883, 0.5320, 0.5196,  ..., 0.5843, 0.6905, 0.5262],
          [0.6396, 0.4311, 0.5505,  ..., 0.4856, 0.3148, 0.4888],
          [0.3798, 0.5286, 0.5857,  ..., 0.5933, 0.5570, 0.4908]],

         [[0.3770, 0.4842, 0.5366,  ..., 0.3741, 0.5383, 0.5564],
          [0.4515, 0.5890, 0.6186,  ..., 0.5814, 0.5607, 0.4646],
          [0.3692, 0.5264, 0.4852,  ..., 0.5604, 0.4920, 0.4318],
          [0.6071, 0.5027, 0.5645,  ..., 0.5685, 0.6578, 0.5796]],

         [[0.5210, 0.5566, 0.5872,  ..., 0.4960, 0.4737, 0.5535],
          [0.2749, 0.5264, 0.4808,  ..., 0.5025, 0.7100, 0.5992],
          [0.4404, 0.4465, 0.4711,  ..., 0.4236, 0.5677, 0.5750],
          [0.2812, 0.3687, 0.4815,  ..., 0.5153, 0.4249, 0.3531]]]],
       device='cuda:0')
tensor([[[[0.3730, 0.5917, 0.6058,  ..., 0.4564, 0.4220, 0.6261],
          [0.4707, 0.5436, 0.4206,  ..., 0.5380, 0.6433, 0.4554],
          [0.6442, 0.4783, 0.5310,  ..., 0.4292, 0.6197, 0.5402],
          [0.6379, 0.5159, 0.4632,  ..., 0.5487, 0.5482, 0.3251]],

         [[0.4325, 0.5818, 0.5974,  ..., 0.3757, 0.4470, 0.5400],
          [0.4479, 0.5218, 0.3739,  ..., 0.4942, 0.5511, 0.5756],
          [0.4390, 0.5438, 0.6558,  ..., 0.5521, 0.4917, 0.3840],
          [0.4806, 0.4773, 0.4944,  ..., 0.4615, 0.4135, 0.5066]],

         [[0.4316, 0.4647, 0.4852,  ..., 0.4846, 0.4278, 0.6077],
          [0.4685, 0.4501, 0.5842,  ..., 0.5988, 0.6487, 0.5417],
          [0.5679, 0.4244, 0.5832,  ..., 0.3993, 0.5421, 0.4446],
          [0.5665, 0.5308, 0.5375,  ..., 0.5113, 0.5273, 0.4852]],

         ...,

         [[0.6072, 0.4412, 0.7311,  ..., 0.5378, 0.4935, 0.4564],
          [0.5366, 0.4026, 0.5737,  ..., 0.5455, 0.6146, 0.6156],
          [0.4910, 0.4796, 0.7090,  ..., 0.5246, 0.4414, 0.4955],
          [0.4301, 0.4769, 0.4562,  ..., 0.4369, 0.3522, 0.4683]],

         [[0.6451, 0.3469, 0.6289,  ..., 0.5288, 0.5908, 0.5742],
          [0.4724, 0.5273, 0.6086,  ..., 0.5257, 0.5837, 0.5012],
          [0.5132, 0.4598, 0.7001,  ..., 0.4187, 0.5484, 0.5846],
          [0.6270, 0.5922, 0.3657,  ..., 0.4812, 0.5390, 0.4073]],

         [[0.5324, 0.4625, 0.4863,  ..., 0.4369, 0.5523, 0.5813],
          [0.5129, 0.6697, 0.5002,  ..., 0.5076, 0.6334, 0.5908],
          [0.4822, 0.3835, 0.5271,  ..., 0.5395, 0.6549, 0.4559],
          [0.3469, 0.4554, 0.3603,  ..., 0.4383, 0.4625, 0.4508]]],


        [[[0.4211, 0.5562, 0.5433,  ..., 0.3951, 0.5586, 0.5751],
          [0.3933, 0.5087, 0.4412,  ..., 0.4928, 0.5870, 0.3416],
          [0.4892, 0.5327, 0.6775,  ..., 0.4513, 0.4975, 0.4571],
          [0.5470, 0.4239, 0.4970,  ..., 0.4642, 0.4935, 0.5675]],

         [[0.5061, 0.5655, 0.5903,  ..., 0.4749, 0.4414, 0.5576],
          [0.4381, 0.5631, 0.5751,  ..., 0.5813, 0.4206, 0.3965],
          [0.4297, 0.4216, 0.5240,  ..., 0.3739, 0.3785, 0.3854],
          [0.7122, 0.5684, 0.4730,  ..., 0.5761, 0.5997, 0.4617]],

         [[0.4757, 0.4933, 0.4654,  ..., 0.4381, 0.4340, 0.5450],
          [0.4301, 0.5479, 0.4973,  ..., 0.6270, 0.6593, 0.4591],
          [0.4301, 0.5547, 0.5315,  ..., 0.4579, 0.3666, 0.4632],
          [0.5603, 0.4860, 0.5533,  ..., 0.5123, 0.4661, 0.5077]],

         ...,

         [[0.5421, 0.4901, 0.4187,  ..., 0.4168, 0.6160, 0.6109],
          [0.4893, 0.5390, 0.5166,  ..., 0.5813, 0.6715, 0.5112],
          [0.6406, 0.4381, 0.5475,  ..., 0.4826, 0.2958, 0.4738],
          [0.3808, 0.5356, 0.5827,  ..., 0.5903, 0.5380, 0.4758]],

         [[0.3780, 0.4912, 0.5336,  ..., 0.3711, 0.5193, 0.5414],
          [0.4525, 0.5960, 0.6156,  ..., 0.5784, 0.5417, 0.4496],
          [0.3702, 0.5334, 0.4822,  ..., 0.5574, 0.4730, 0.4168],
          [0.6081, 0.5097, 0.5615,  ..., 0.5655, 0.6388, 0.5646]],

         [[0.5220, 0.5636, 0.5842,  ..., 0.4930, 0.4547, 0.5385],
          [0.2759, 0.5334, 0.4778,  ..., 0.4995, 0.6910, 0.5842],
          [0.4414, 0.4535, 0.4681,  ..., 0.4206, 0.5487, 0.5600],
          [0.2822, 0.3757, 0.4785,  ..., 0.5123, 0.4059, 0.3381]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0010, -0.0070,  0.0030, -0.0130, -0.0070,  0.0150, -0.0310,  0.0030,
         0.0190,  0.0150], device='cuda:0')
selected experts tensor([1637, 1449, 1325, 1798, 1284, 1914, 1746, 1438, 2224, 1569],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4680, 0.5877, 0.5408,  ..., 0.3527, 0.3612, 0.5476],
          [0.5161, 0.5012, 0.6349,  ..., 0.5224, 0.5348, 0.3413],
          [0.5192, 0.3508, 0.4157,  ..., 0.3802, 0.4942, 0.4815],
          [0.6266, 0.5985, 0.5797,  ..., 0.3615, 0.5087, 0.5681]],

         [[0.4514, 0.3813, 0.4358,  ..., 0.5384, 0.4477, 0.3060],
          [0.4633, 0.4290, 0.5901,  ..., 0.5390, 0.5637, 0.5300],
          [0.4483, 0.5009, 0.3645,  ..., 0.5269, 0.5180, 0.5512],
          [0.6725, 0.5763, 0.4655,  ..., 0.4216, 0.5329, 0.4840]],

         [[0.4458, 0.6472, 0.5340,  ..., 0.3510, 0.4634, 0.5030],
          [0.5403, 0.5143, 0.6501,  ..., 0.4315, 0.4861, 0.3876],
          [0.4687, 0.4161, 0.3833,  ..., 0.4511, 0.5931, 0.5785],
          [0.6630, 0.5806, 0.5707,  ..., 0.4188, 0.5406, 0.4445]],

         ...,

         [[0.4950, 0.4950, 0.4970,  ..., 0.5190, 0.4910, 0.5110],
          [0.4950, 0.4950, 0.4970,  ..., 0.5190, 0.4910, 0.5110],
          [0.4950, 0.4950, 0.4970,  ..., 0.5190, 0.4910, 0.5110],
          [0.4950, 0.4950, 0.4970,  ..., 0.5190, 0.4910, 0.5110]],

         [[0.4950, 0.4950, 0.4970,  ..., 0.5190, 0.4910, 0.5110],
          [0.4950, 0.4950, 0.4970,  ..., 0.5190, 0.4910, 0.5110],
          [0.4950, 0.4950, 0.4970,  ..., 0.5190, 0.4910, 0.5110],
          [0.4950, 0.4950, 0.4970,  ..., 0.5190, 0.4910, 0.5110]],

         [[0.4950, 0.4950, 0.4970,  ..., 0.5190, 0.4910, 0.5110],
          [0.4950, 0.4950, 0.4970,  ..., 0.5190, 0.4910, 0.5110],
          [0.4950, 0.4950, 0.4970,  ..., 0.5190, 0.4910, 0.5110],
          [0.4950, 0.4950, 0.4970,  ..., 0.5190, 0.4910, 0.5110]]],


        [[[0.4490, 0.3436, 0.5592,  ..., 0.5682, 0.2868, 0.4183],
          [0.3953, 0.4454, 0.6204,  ..., 0.5946, 0.5949, 0.4876],
          [0.4403, 0.3794, 0.4219,  ..., 0.5293, 0.5780, 0.6362],
          [0.6284, 0.6613, 0.5187,  ..., 0.4296, 0.5510, 0.4737]],

         [[0.5379, 0.6284, 0.4917,  ..., 0.2895, 0.3300, 0.6201],
          [0.4180, 0.4369, 0.5457,  ..., 0.5580, 0.6001, 0.3927],
          [0.5396, 0.4204, 0.3755,  ..., 0.4929, 0.4982, 0.5121],
          [0.4618, 0.6175, 0.4710,  ..., 0.3222, 0.5675, 0.4311]],

         [[0.5330, 0.4846, 0.4491,  ..., 0.4011, 0.3699, 0.5715],
          [0.5215, 0.5183, 0.6615,  ..., 0.5720, 0.6153, 0.4650],
          [0.5820, 0.4609, 0.3884,  ..., 0.6497, 0.5394, 0.6037],
          [0.4976, 0.5533, 0.5203,  ..., 0.3230, 0.5752, 0.3561]],

         ...,

         [[0.4950, 0.4950, 0.4970,  ..., 0.5190, 0.4910, 0.5110],
          [0.4950, 0.4950, 0.4970,  ..., 0.5190, 0.4910, 0.5110],
          [0.4950, 0.4950, 0.4970,  ..., 0.5190, 0.4910, 0.5110],
          [0.4950, 0.4950, 0.4970,  ..., 0.5190, 0.4910, 0.5110]],

         [[0.4950, 0.4950, 0.4970,  ..., 0.5190, 0.4910, 0.5110],
          [0.4950, 0.4950, 0.4970,  ..., 0.5190, 0.4910, 0.5110],
          [0.4950, 0.4950, 0.4970,  ..., 0.5190, 0.4910, 0.5110],
          [0.4950, 0.4950, 0.4970,  ..., 0.5190, 0.4910, 0.5110]],

         [[0.4950, 0.4950, 0.4970,  ..., 0.5190, 0.4910, 0.5110],
          [0.4950, 0.4950, 0.4970,  ..., 0.5190, 0.4910, 0.5110],
          [0.4950, 0.4950, 0.4970,  ..., 0.5190, 0.4910, 0.5110],
          [0.4950, 0.4950, 0.4970,  ..., 0.5190, 0.4910, 0.5110]]]],
       device='cuda:0')
tensor([[[[0.4730, 0.5927, 0.5438,  ..., 0.3337, 0.3702, 0.5366],
          [0.5211, 0.5062, 0.6379,  ..., 0.5034, 0.5438, 0.3303],
          [0.5242, 0.3558, 0.4187,  ..., 0.3612, 0.5032, 0.4705],
          [0.6316, 0.6035, 0.5827,  ..., 0.3425, 0.5177, 0.5571]],

         [[0.4564, 0.3863, 0.4388,  ..., 0.5194, 0.4567, 0.2950],
          [0.4683, 0.4340, 0.5931,  ..., 0.5200, 0.5727, 0.5190],
          [0.4533, 0.5059, 0.3675,  ..., 0.5079, 0.5270, 0.5402],
          [0.6775, 0.5813, 0.4685,  ..., 0.4026, 0.5419, 0.4730]],

         [[0.4508, 0.6522, 0.5370,  ..., 0.3320, 0.4724, 0.4920],
          [0.5453, 0.5193, 0.6531,  ..., 0.4125, 0.4951, 0.3766],
          [0.4737, 0.4211, 0.3863,  ..., 0.4321, 0.6021, 0.5675],
          [0.6680, 0.5856, 0.5737,  ..., 0.3998, 0.5496, 0.4335]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.4540, 0.3486, 0.5622,  ..., 0.5492, 0.2958, 0.4073],
          [0.4003, 0.4504, 0.6234,  ..., 0.5756, 0.6039, 0.4766],
          [0.4453, 0.3844, 0.4249,  ..., 0.5103, 0.5870, 0.6252],
          [0.6334, 0.6663, 0.5217,  ..., 0.4106, 0.5600, 0.4627]],

         [[0.5429, 0.6334, 0.4947,  ..., 0.2705, 0.3390, 0.6091],
          [0.4230, 0.4419, 0.5487,  ..., 0.5390, 0.6091, 0.3817],
          [0.5446, 0.4254, 0.3785,  ..., 0.4739, 0.5072, 0.5011],
          [0.4668, 0.6225, 0.4740,  ..., 0.3032, 0.5765, 0.4201]],

         [[0.5380, 0.4896, 0.4521,  ..., 0.3821, 0.3789, 0.5605],
          [0.5265, 0.5233, 0.6645,  ..., 0.5530, 0.6243, 0.4540],
          [0.5870, 0.4659, 0.3914,  ..., 0.6307, 0.5484, 0.5927],
          [0.5026, 0.5583, 0.5233,  ..., 0.3040, 0.5842, 0.3451]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([-0.0050, -0.0050, -0.0030,  0.0070, -0.0090,  0.0210,  0.0050,  0.0190,
        -0.0090,  0.0110], device='cuda:0')
selected experts tensor([1717, 1719, 2284, 1844, 1648, 1049, 1599,  882, 1862, 1780],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1592., 1588., 1665., 1697., 1510., 1655., 1674., 1845., 1541., 1617.],
        [1717., 1719., 2284., 1844., 1648., 1049., 1599.,  882., 1862., 1780.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6770, 0.4609, 0.5003,  ..., 0.6008, 0.5059, 0.4500],
          [0.3672, 0.5219, 0.5624,  ..., 0.4089, 0.5761, 0.4242],
          [0.4394, 0.5620, 0.4342,  ..., 0.4752, 0.4775, 0.4827],
          [0.6093, 0.4879, 0.3215,  ..., 0.5191, 0.4197, 0.4180]],

         [[0.4864, 0.6561, 0.4429,  ..., 0.5999, 0.4359, 0.7083],
          [0.6995, 0.5422, 0.3779,  ..., 0.3865, 0.4657, 0.4819],
          [0.5406, 0.4723, 0.5122,  ..., 0.4113, 0.4482, 0.6943],
          [0.5197, 0.5388, 0.4141,  ..., 0.3815, 0.5540, 0.5474]],

         [[0.4817, 0.5671, 0.4569,  ..., 0.3687, 0.4589, 0.5313],
          [0.6340, 0.4435, 0.3849,  ..., 0.6107, 0.5140, 0.4053],
          [0.4440, 0.4791, 0.5373,  ..., 0.5162, 0.6308, 0.3789],
          [0.6641, 0.5021, 0.4665,  ..., 0.4165, 0.6344, 0.6061]],

         ...,

         [[0.4488, 0.5517, 0.4668,  ..., 0.5843, 0.6133, 0.4612],
          [0.3618, 0.5195, 0.3181,  ..., 0.3916, 0.6244, 0.5568],
          [0.4505, 0.5649, 0.6200,  ..., 0.4881, 0.6317, 0.3510],
          [0.4534, 0.5393, 0.4901,  ..., 0.3828, 0.4915, 0.3964]],

         [[0.5396, 0.5522, 0.4713,  ..., 0.5461, 0.5155, 0.4946],
          [0.5486, 0.4994, 0.4497,  ..., 0.5068, 0.5771, 0.3807],
          [0.5621, 0.3769, 0.4222,  ..., 0.3884, 0.5946, 0.5123],
          [0.4105, 0.4720, 0.5946,  ..., 0.5274, 0.5475, 0.4801]],

         [[0.6367, 0.3463, 0.5164,  ..., 0.4904, 0.5766, 0.3955],
          [0.4904, 0.5786, 0.4855,  ..., 0.5437, 0.5024, 0.5000],
          [0.5241, 0.5329, 0.4606,  ..., 0.3255, 0.3694, 0.4946],
          [0.4334, 0.4621, 0.3989,  ..., 0.6753, 0.5675, 0.5640]]],


        [[[0.4897, 0.4650, 0.4538,  ..., 0.4269, 0.5225, 0.5235],
          [0.5281, 0.6027, 0.3774,  ..., 0.3534, 0.4069, 0.4423],
          [0.3690, 0.4758, 0.5376,  ..., 0.6186, 0.4744, 0.5425],
          [0.6130, 0.5138, 0.5221,  ..., 0.4222, 0.6603, 0.6643]],

         [[0.5016, 0.5478, 0.4567,  ..., 0.5738, 0.5286, 0.5288],
          [0.5076, 0.4613, 0.3488,  ..., 0.5748, 0.6077, 0.4464],
          [0.4334, 0.3833, 0.5904,  ..., 0.4672, 0.5605, 0.4285],
          [0.4195, 0.4565, 0.5600,  ..., 0.4298, 0.5210, 0.5420]],

         [[0.3801, 0.3760, 0.5257,  ..., 0.4478, 0.5756, 0.7220],
          [0.5967, 0.6051, 0.4251,  ..., 0.5086, 0.5166, 0.6501],
          [0.5830, 0.5218, 0.4930,  ..., 0.5543, 0.5704, 0.5282],
          [0.3949, 0.4408, 0.5927,  ..., 0.4188, 0.3426, 0.4928]],

         ...,

         [[0.4906, 0.4618, 0.5533,  ..., 0.3446, 0.5356, 0.6196],
          [0.4181, 0.5594, 0.6246,  ..., 0.5371, 0.4596, 0.4464],
          [0.4694, 0.4179, 0.4342,  ..., 0.5606, 0.4661, 0.6366],
          [0.5901, 0.3606, 0.4231,  ..., 0.5129, 0.6026, 0.4467]],

         [[0.4257, 0.5767, 0.6823,  ..., 0.4028, 0.3382, 0.5834],
          [0.5399, 0.4485, 0.4079,  ..., 0.3851, 0.4995, 0.4718],
          [0.5859, 0.6074, 0.5056,  ..., 0.5224, 0.6317, 0.4624],
          [0.6295, 0.4792, 0.5249,  ..., 0.6209, 0.4993, 0.5203]],

         [[0.3573, 0.5001, 0.5776,  ..., 0.4662, 0.6063, 0.3475],
          [0.5043, 0.4033, 0.6426,  ..., 0.6204, 0.4383, 0.4522],
          [0.4825, 0.5604, 0.5100,  ..., 0.4226, 0.3261, 0.6293],
          [0.3787, 0.5480, 0.4051,  ..., 0.5994, 0.4268, 0.5176]]]],
       device='cuda:0')
tensor([[[[0.6800, 0.4579, 0.5073,  ..., 0.5978, 0.5049, 0.4450],
          [0.3702, 0.5189, 0.5694,  ..., 0.4059, 0.5751, 0.4192],
          [0.4424, 0.5590, 0.4412,  ..., 0.4722, 0.4765, 0.4777],
          [0.6123, 0.4849, 0.3285,  ..., 0.5161, 0.4187, 0.4130]],

         [[0.4894, 0.6531, 0.4499,  ..., 0.5969, 0.4349, 0.7033],
          [0.7025, 0.5392, 0.3849,  ..., 0.3835, 0.4647, 0.4769],
          [0.5436, 0.4693, 0.5192,  ..., 0.4083, 0.4472, 0.6893],
          [0.5227, 0.5358, 0.4211,  ..., 0.3785, 0.5530, 0.5424]],

         [[0.4847, 0.5641, 0.4639,  ..., 0.3657, 0.4579, 0.5263],
          [0.6370, 0.4405, 0.3919,  ..., 0.6077, 0.5130, 0.4003],
          [0.4470, 0.4761, 0.5443,  ..., 0.5132, 0.6298, 0.3739],
          [0.6671, 0.4991, 0.4735,  ..., 0.4135, 0.6334, 0.6011]],

         ...,

         [[0.4518, 0.5487, 0.4738,  ..., 0.5813, 0.6123, 0.4562],
          [0.3648, 0.5165, 0.3251,  ..., 0.3886, 0.6234, 0.5518],
          [0.4535, 0.5619, 0.6270,  ..., 0.4851, 0.6307, 0.3460],
          [0.4564, 0.5363, 0.4971,  ..., 0.3798, 0.4905, 0.3914]],

         [[0.5426, 0.5492, 0.4783,  ..., 0.5431, 0.5145, 0.4896],
          [0.5516, 0.4964, 0.4567,  ..., 0.5038, 0.5761, 0.3757],
          [0.5651, 0.3739, 0.4292,  ..., 0.3854, 0.5936, 0.5073],
          [0.4135, 0.4690, 0.6016,  ..., 0.5244, 0.5465, 0.4751]],

         [[0.6397, 0.3433, 0.5234,  ..., 0.4874, 0.5756, 0.3905],
          [0.4934, 0.5756, 0.4925,  ..., 0.5407, 0.5014, 0.4950],
          [0.5271, 0.5299, 0.4676,  ..., 0.3225, 0.3684, 0.4896],
          [0.4364, 0.4591, 0.4059,  ..., 0.6723, 0.5665, 0.5590]]],


        [[[0.4927, 0.4620, 0.4608,  ..., 0.4239, 0.5215, 0.5185],
          [0.5311, 0.5997, 0.3844,  ..., 0.3504, 0.4059, 0.4373],
          [0.3720, 0.4728, 0.5446,  ..., 0.6156, 0.4734, 0.5375],
          [0.6160, 0.5108, 0.5291,  ..., 0.4192, 0.6593, 0.6593]],

         [[0.5046, 0.5448, 0.4637,  ..., 0.5708, 0.5276, 0.5238],
          [0.5106, 0.4583, 0.3558,  ..., 0.5718, 0.6067, 0.4414],
          [0.4364, 0.3803, 0.5974,  ..., 0.4642, 0.5595, 0.4235],
          [0.4225, 0.4535, 0.5670,  ..., 0.4268, 0.5200, 0.5370]],

         [[0.3831, 0.3730, 0.5327,  ..., 0.4448, 0.5746, 0.7170],
          [0.5997, 0.6021, 0.4321,  ..., 0.5056, 0.5156, 0.6451],
          [0.5860, 0.5188, 0.5000,  ..., 0.5513, 0.5694, 0.5232],
          [0.3979, 0.4378, 0.5997,  ..., 0.4158, 0.3416, 0.4878]],

         ...,

         [[0.4936, 0.4588, 0.5603,  ..., 0.3416, 0.5346, 0.6146],
          [0.4211, 0.5564, 0.6316,  ..., 0.5341, 0.4586, 0.4414],
          [0.4724, 0.4149, 0.4412,  ..., 0.5576, 0.4651, 0.6316],
          [0.5931, 0.3576, 0.4301,  ..., 0.5099, 0.6016, 0.4417]],

         [[0.4287, 0.5737, 0.6893,  ..., 0.3998, 0.3372, 0.5784],
          [0.5429, 0.4455, 0.4149,  ..., 0.3821, 0.4985, 0.4668],
          [0.5889, 0.6044, 0.5126,  ..., 0.5194, 0.6307, 0.4574],
          [0.6325, 0.4762, 0.5319,  ..., 0.6179, 0.4983, 0.5153]],

         [[0.3603, 0.4971, 0.5846,  ..., 0.4632, 0.6053, 0.3425],
          [0.5073, 0.4003, 0.6496,  ..., 0.6174, 0.4373, 0.4472],
          [0.4855, 0.5574, 0.5170,  ..., 0.4196, 0.3251, 0.6243],
          [0.3817, 0.5450, 0.4121,  ..., 0.5964, 0.4258, 0.5126]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0030,  0.0030, -0.0070,  0.0010, -0.0010, -0.0050,  0.0070,  0.0030,
         0.0010,  0.0050], device='cuda:0')
selected experts tensor([1684, 1784, 1584, 1664, 1639, 1571, 1633, 1558, 1618, 1649],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5133, 0.6322, 0.3321,  ..., 0.4166, 0.4142, 0.6346],
          [0.4547, 0.3456, 0.4875,  ..., 0.5105, 0.4525, 0.4956],
          [0.5893, 0.4076, 0.3975,  ..., 0.4473, 0.5030, 0.4308],
          [0.3593, 0.6624, 0.3924,  ..., 0.3929, 0.6127, 0.5366]],

         [[0.6001, 0.4755, 0.5349,  ..., 0.6665, 0.6429, 0.4217],
          [0.3807, 0.4813, 0.5576,  ..., 0.5858, 0.5849, 0.6517],
          [0.4465, 0.5939, 0.5456,  ..., 0.2748, 0.5631, 0.3534],
          [0.4523, 0.4553, 0.4192,  ..., 0.6003, 0.5284, 0.5095]],

         [[0.4830, 0.4396, 0.4509,  ..., 0.5577, 0.3885, 0.4987],
          [0.5077, 0.5198, 0.5941,  ..., 0.5658, 0.4970, 0.4418],
          [0.3946, 0.4334, 0.5439,  ..., 0.5118, 0.5137, 0.5637],
          [0.5448, 0.4310, 0.4661,  ..., 0.4833, 0.4978, 0.6346]],

         ...,

         [[0.4779, 0.5087, 0.4456,  ..., 0.4690, 0.7108, 0.3552],
          [0.3923, 0.4583, 0.3740,  ..., 0.5048, 0.6859, 0.4868],
          [0.5708, 0.4498, 0.4470,  ..., 0.5853, 0.4653, 0.5507],
          [0.4153, 0.4766, 0.4369,  ..., 0.6365, 0.3885, 0.5192]],

         [[0.4404, 0.4700, 0.4812,  ..., 0.6175, 0.5367, 0.6517],
          [0.4220, 0.6589, 0.5300,  ..., 0.5014, 0.4994, 0.5814],
          [0.3275, 0.3448, 0.4477,  ..., 0.5924, 0.5386, 0.3289],
          [0.5703, 0.5821, 0.3758,  ..., 0.6851, 0.5585, 0.3760]],

         [[0.7056, 0.6065, 0.6953,  ..., 0.3670, 0.2678, 0.4890],
          [0.5025, 0.4498, 0.4734,  ..., 0.5715, 0.6113, 0.4898],
          [0.5746, 0.4546, 0.4823,  ..., 0.5099, 0.4697, 0.4236],
          [0.5931, 0.4945, 0.3841,  ..., 0.6078, 0.4759, 0.5268]]],


        [[[0.6790, 0.3564, 0.5189,  ..., 0.5933, 0.5520, 0.5567],
          [0.4607, 0.5868, 0.4627,  ..., 0.4621, 0.4337, 0.3925],
          [0.4077, 0.5726, 0.3776,  ..., 0.5076, 0.4891, 0.3842],
          [0.5498, 0.3663, 0.5480,  ..., 0.4995, 0.4877, 0.4389]],

         [[0.5703, 0.5958, 0.5576,  ..., 0.5269, 0.4939, 0.4827],
          [0.5978, 0.5474, 0.6012,  ..., 0.4824, 0.4975, 0.4734],
          [0.5031, 0.6430, 0.4569,  ..., 0.4456, 0.5811, 0.4745],
          [0.4153, 0.3796, 0.2578,  ..., 0.7426, 0.5067, 0.5454]],

         [[0.4106, 0.4666, 0.4783,  ..., 0.5244, 0.5820, 0.3367],
          [0.5227, 0.4690, 0.3479,  ..., 0.4299, 0.5130, 0.2860],
          [0.4624, 0.5570, 0.4843,  ..., 0.4350, 0.5085, 0.6023],
          [0.7239, 0.5570, 0.5804,  ..., 0.4170, 0.5301, 0.5437]],

         ...,

         [[0.6011, 0.4329, 0.4965,  ..., 0.4170, 0.5454, 0.4061],
          [0.3729, 0.4281, 0.4783,  ..., 0.5848, 0.4592, 0.5142],
          [0.6053, 0.6005, 0.4074,  ..., 0.4628, 0.3422, 0.4952],
          [0.6504, 0.3931, 0.5656,  ..., 0.5644, 0.4805, 0.4618]],

         [[0.3131, 0.4772, 0.6021,  ..., 0.5112, 0.5929, 0.6018],
          [0.4758, 0.5061, 0.5756,  ..., 0.5070, 0.4313, 0.5762],
          [0.4139, 0.2568, 0.4093,  ..., 0.4000, 0.5877, 0.4538],
          [0.5341, 0.3153, 0.5579,  ..., 0.4502, 0.4585, 0.3805]],

         [[0.4554, 0.4975, 0.3613,  ..., 0.5085, 0.5301, 0.4444],
          [0.4610, 0.5123, 0.5125,  ..., 0.6311, 0.5496, 0.5909],
          [0.4167, 0.5129, 0.5136,  ..., 0.6022, 0.5292, 0.5449],
          [0.4675, 0.6204, 0.4391,  ..., 0.4487, 0.6038, 0.5666]]]],
       device='cuda:0')
tensor([[[[0.5143, 0.6352, 0.3311,  ..., 0.4216, 0.4092, 0.6316],
          [0.4557, 0.3486, 0.4865,  ..., 0.5155, 0.4475, 0.4926],
          [0.5903, 0.4106, 0.3965,  ..., 0.4523, 0.4980, 0.4278],
          [0.3603, 0.6654, 0.3914,  ..., 0.3979, 0.6077, 0.5336]],

         [[0.6011, 0.4785, 0.5339,  ..., 0.6715, 0.6379, 0.4187],
          [0.3817, 0.4843, 0.5566,  ..., 0.5908, 0.5799, 0.6487],
          [0.4475, 0.5969, 0.5446,  ..., 0.2798, 0.5581, 0.3504],
          [0.4533, 0.4583, 0.4182,  ..., 0.6053, 0.5234, 0.5065]],

         [[0.4840, 0.4426, 0.4499,  ..., 0.5627, 0.3835, 0.4957],
          [0.5087, 0.5228, 0.5931,  ..., 0.5708, 0.4920, 0.4388],
          [0.3956, 0.4364, 0.5429,  ..., 0.5168, 0.5087, 0.5607],
          [0.5458, 0.4340, 0.4651,  ..., 0.4883, 0.4928, 0.6316]],

         ...,

         [[0.4789, 0.5117, 0.4446,  ..., 0.4740, 0.7058, 0.3522],
          [0.3933, 0.4613, 0.3730,  ..., 0.5098, 0.6809, 0.4838],
          [0.5718, 0.4528, 0.4460,  ..., 0.5903, 0.4603, 0.5477],
          [0.4163, 0.4796, 0.4359,  ..., 0.6415, 0.3835, 0.5162]],

         [[0.4414, 0.4730, 0.4802,  ..., 0.6225, 0.5317, 0.6487],
          [0.4230, 0.6619, 0.5290,  ..., 0.5064, 0.4944, 0.5784],
          [0.3285, 0.3478, 0.4467,  ..., 0.5974, 0.5336, 0.3259],
          [0.5713, 0.5851, 0.3748,  ..., 0.6901, 0.5535, 0.3730]],

         [[0.7066, 0.6095, 0.6943,  ..., 0.3720, 0.2628, 0.4860],
          [0.5035, 0.4528, 0.4724,  ..., 0.5765, 0.6063, 0.4868],
          [0.5756, 0.4576, 0.4813,  ..., 0.5149, 0.4647, 0.4206],
          [0.5941, 0.4975, 0.3831,  ..., 0.6128, 0.4709, 0.5238]]],


        [[[0.6800, 0.3594, 0.5179,  ..., 0.5983, 0.5470, 0.5537],
          [0.4617, 0.5898, 0.4617,  ..., 0.4671, 0.4287, 0.3895],
          [0.4087, 0.5756, 0.3766,  ..., 0.5126, 0.4841, 0.3812],
          [0.5508, 0.3693, 0.5470,  ..., 0.5045, 0.4827, 0.4359]],

         [[0.5713, 0.5988, 0.5566,  ..., 0.5319, 0.4889, 0.4797],
          [0.5988, 0.5504, 0.6002,  ..., 0.4874, 0.4925, 0.4704],
          [0.5041, 0.6460, 0.4559,  ..., 0.4506, 0.5761, 0.4715],
          [0.4163, 0.3826, 0.2568,  ..., 0.7476, 0.5017, 0.5424]],

         [[0.4116, 0.4696, 0.4773,  ..., 0.5294, 0.5770, 0.3337],
          [0.5237, 0.4720, 0.3469,  ..., 0.4349, 0.5080, 0.2830],
          [0.4634, 0.5600, 0.4833,  ..., 0.4400, 0.5035, 0.5993],
          [0.7249, 0.5600, 0.5794,  ..., 0.4220, 0.5251, 0.5407]],

         ...,

         [[0.6021, 0.4359, 0.4955,  ..., 0.4220, 0.5404, 0.4031],
          [0.3739, 0.4311, 0.4773,  ..., 0.5898, 0.4542, 0.5112],
          [0.6063, 0.6035, 0.4064,  ..., 0.4678, 0.3372, 0.4922],
          [0.6514, 0.3961, 0.5646,  ..., 0.5694, 0.4755, 0.4588]],

         [[0.3141, 0.4802, 0.6011,  ..., 0.5162, 0.5879, 0.5988],
          [0.4768, 0.5091, 0.5746,  ..., 0.5120, 0.4263, 0.5732],
          [0.4149, 0.2598, 0.4083,  ..., 0.4050, 0.5827, 0.4508],
          [0.5351, 0.3183, 0.5569,  ..., 0.4552, 0.4535, 0.3775]],

         [[0.4564, 0.5005, 0.3603,  ..., 0.5135, 0.5251, 0.4414],
          [0.4620, 0.5153, 0.5115,  ..., 0.6361, 0.5446, 0.5879],
          [0.4177, 0.5159, 0.5126,  ..., 0.6072, 0.5242, 0.5419],
          [0.4685, 0.6234, 0.4381,  ..., 0.4537, 0.5988, 0.5636]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0010, -0.0030,  0.0010,  0.0090, -0.0010,  0.0070, -0.0110, -0.0050,
         0.0050,  0.0030], device='cuda:0')
selected experts tensor([1655, 1652, 1626, 1640, 1637, 1673, 1535, 1699, 1630, 1637],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5059, 0.6454, 0.4275,  ..., 0.3553, 0.6135, 0.4673],
          [0.5034, 0.4825, 0.5858,  ..., 0.6214, 0.5144, 0.4490],
          [0.5233, 0.5815, 0.5398,  ..., 0.4109, 0.3561, 0.4243],
          [0.4518, 0.3901, 0.4260,  ..., 0.4934, 0.4006, 0.5114]],

         [[0.5595, 0.5344, 0.4476,  ..., 0.5413, 0.5353, 0.5892],
          [0.4649, 0.5724, 0.5767,  ..., 0.5517, 0.4995, 0.5152],
          [0.4258, 0.5843, 0.4986,  ..., 0.5023, 0.3864, 0.3411],
          [0.5414, 0.4198, 0.6338,  ..., 0.4818, 0.3587, 0.5644]],

         [[0.4097, 0.4424, 0.4733,  ..., 0.5023, 0.5008, 0.6805],
          [0.4335, 0.4175, 0.4399,  ..., 0.5957, 0.6532, 0.4610],
          [0.5770, 0.4023, 0.6205,  ..., 0.4984, 0.5533, 0.5022],
          [0.5547, 0.4122, 0.4440,  ..., 0.4941, 0.4263, 0.5700]],

         ...,

         [[0.4637, 0.6105, 0.4385,  ..., 0.3866, 0.3596, 0.5405],
          [0.4937, 0.4945, 0.6047,  ..., 0.5355, 0.6843, 0.4717],
          [0.4821, 0.5434, 0.3825,  ..., 0.5534, 0.5907, 0.4295],
          [0.4971, 0.6210, 0.4890,  ..., 0.4833, 0.4827, 0.4943]],

         [[0.3891, 0.4574, 0.3861,  ..., 0.4127, 0.4848, 0.5093],
          [0.6406, 0.6301, 0.4745,  ..., 0.6365, 0.6359, 0.5736],
          [0.5074, 0.5555, 0.4380,  ..., 0.6098, 0.6400, 0.6449],
          [0.6460, 0.4117, 0.5012,  ..., 0.4404, 0.3891, 0.4836]],

         [[0.4761, 0.4973, 0.5801,  ..., 0.5442, 0.5106, 0.6002],
          [0.3639, 0.5777, 0.5089,  ..., 0.4010, 0.5095, 0.4960],
          [0.4993, 0.3579, 0.5034,  ..., 0.4831, 0.4795, 0.3835],
          [0.4007, 0.6021, 0.5076,  ..., 0.5886, 0.3147, 0.3958]]],


        [[[0.4571, 0.5477, 0.4332,  ..., 0.6916, 0.5228, 0.5538],
          [0.4554, 0.5815, 0.5063,  ..., 0.5640, 0.6280, 0.5325],
          [0.4897, 0.4972, 0.3670,  ..., 0.5507, 0.4405, 0.4910],
          [0.5718, 0.5734, 0.5406,  ..., 0.4433, 0.5243, 0.5148]],

         [[0.4688, 0.5238, 0.4425,  ..., 0.5133, 0.6322, 0.6366],
          [0.4871, 0.4873, 0.6186,  ..., 0.5260, 0.7286, 0.3945],
          [0.5146, 0.4858, 0.4706,  ..., 0.4861, 0.5326, 0.4069],
          [0.4557, 0.5242, 0.4876,  ..., 0.4198, 0.4879, 0.5158]],

         [[0.5233, 0.5643, 0.5257,  ..., 0.5194, 0.6243, 0.4942],
          [0.3355, 0.5018, 0.4883,  ..., 0.4951, 0.5979, 0.5226],
          [0.5273, 0.4929, 0.5181,  ..., 0.4505, 0.3658, 0.4661],
          [0.6206, 0.4932, 0.5863,  ..., 0.6455, 0.5106, 0.5028]],

         ...,

         [[0.4287, 0.4875, 0.4137,  ..., 0.4833, 0.4812, 0.5695],
          [0.4087, 0.4309, 0.5801,  ..., 0.6509, 0.4824, 0.5547],
          [0.5727, 0.4270, 0.5667,  ..., 0.3031, 0.4472, 0.6762],
          [0.5290, 0.5293, 0.5100,  ..., 0.4682, 0.4597, 0.5447]],

         [[0.3979, 0.5562, 0.4227,  ..., 0.4753, 0.4320, 0.6227],
          [0.4787, 0.5535, 0.5342,  ..., 0.6677, 0.5134, 0.6530],
          [0.5265, 0.4739, 0.5546,  ..., 0.6004, 0.5773, 0.4950],
          [0.5651, 0.5310, 0.4805,  ..., 0.4227, 0.6825, 0.5459]],

         [[0.6063, 0.5543, 0.4911,  ..., 0.4728, 0.5273, 0.6709],
          [0.4859, 0.5691, 0.5695,  ..., 0.6260, 0.5538, 0.5606],
          [0.5960, 0.4381, 0.4829,  ..., 0.4217, 0.5227, 0.6143],
          [0.4287, 0.3947, 0.5423,  ..., 0.4769, 0.3855, 0.4916]]]],
       device='cuda:0')
tensor([[[[0.5059, 0.6514, 0.4235,  ..., 0.3513, 0.5955, 0.4513],
          [0.5034, 0.4885, 0.5818,  ..., 0.6174, 0.4964, 0.4330],
          [0.5233, 0.5875, 0.5358,  ..., 0.4069, 0.3381, 0.4083],
          [0.4518, 0.3961, 0.4220,  ..., 0.4894, 0.3826, 0.4954]],

         [[0.5595, 0.5404, 0.4436,  ..., 0.5373, 0.5173, 0.5732],
          [0.4649, 0.5784, 0.5727,  ..., 0.5477, 0.4815, 0.4992],
          [0.4258, 0.5903, 0.4946,  ..., 0.4983, 0.3684, 0.3251],
          [0.5414, 0.4258, 0.6298,  ..., 0.4778, 0.3407, 0.5484]],

         [[0.4097, 0.4484, 0.4693,  ..., 0.4983, 0.4828, 0.6645],
          [0.4335, 0.4235, 0.4359,  ..., 0.5917, 0.6352, 0.4450],
          [0.5770, 0.4083, 0.6165,  ..., 0.4944, 0.5353, 0.4862],
          [0.5547, 0.4182, 0.4400,  ..., 0.4901, 0.4083, 0.5540]],

         ...,

         [[0.4637, 0.6165, 0.4345,  ..., 0.3826, 0.3416, 0.5245],
          [0.4937, 0.5005, 0.6007,  ..., 0.5315, 0.6663, 0.4557],
          [0.4821, 0.5494, 0.3785,  ..., 0.5494, 0.5727, 0.4135],
          [0.4971, 0.6270, 0.4850,  ..., 0.4793, 0.4647, 0.4783]],

         [[0.3891, 0.4634, 0.3821,  ..., 0.4087, 0.4668, 0.4933],
          [0.6406, 0.6361, 0.4705,  ..., 0.6325, 0.6179, 0.5576],
          [0.5074, 0.5615, 0.4340,  ..., 0.6058, 0.6220, 0.6289],
          [0.6460, 0.4177, 0.4972,  ..., 0.4364, 0.3711, 0.4676]],

         [[0.4761, 0.5033, 0.5761,  ..., 0.5402, 0.4926, 0.5842],
          [0.3639, 0.5837, 0.5049,  ..., 0.3970, 0.4915, 0.4800],
          [0.4993, 0.3639, 0.4994,  ..., 0.4791, 0.4615, 0.3675],
          [0.4007, 0.6081, 0.5036,  ..., 0.5846, 0.2967, 0.3798]]],


        [[[0.4571, 0.5537, 0.4292,  ..., 0.6876, 0.5048, 0.5378],
          [0.4554, 0.5875, 0.5023,  ..., 0.5600, 0.6100, 0.5165],
          [0.4897, 0.5032, 0.3630,  ..., 0.5467, 0.4225, 0.4750],
          [0.5718, 0.5794, 0.5366,  ..., 0.4393, 0.5063, 0.4988]],

         [[0.4688, 0.5297, 0.4385,  ..., 0.5093, 0.6142, 0.6206],
          [0.4871, 0.4933, 0.6146,  ..., 0.5220, 0.7106, 0.3785],
          [0.5146, 0.4918, 0.4666,  ..., 0.4821, 0.5146, 0.3909],
          [0.4557, 0.5302, 0.4836,  ..., 0.4158, 0.4699, 0.4998]],

         [[0.5233, 0.5703, 0.5217,  ..., 0.5154, 0.6063, 0.4782],
          [0.3355, 0.5078, 0.4843,  ..., 0.4911, 0.5799, 0.5066],
          [0.5273, 0.4989, 0.5141,  ..., 0.4465, 0.3478, 0.4501],
          [0.6206, 0.4992, 0.5823,  ..., 0.6415, 0.4926, 0.4868]],

         ...,

         [[0.4287, 0.4935, 0.4097,  ..., 0.4793, 0.4632, 0.5535],
          [0.4087, 0.4369, 0.5761,  ..., 0.6469, 0.4644, 0.5387],
          [0.5727, 0.4330, 0.5627,  ..., 0.2991, 0.4292, 0.6602],
          [0.5290, 0.5353, 0.5060,  ..., 0.4642, 0.4417, 0.5287]],

         [[0.3979, 0.5622, 0.4187,  ..., 0.4713, 0.4140, 0.6067],
          [0.4787, 0.5595, 0.5302,  ..., 0.6637, 0.4954, 0.6370],
          [0.5265, 0.4799, 0.5506,  ..., 0.5964, 0.5593, 0.4790],
          [0.5651, 0.5370, 0.4765,  ..., 0.4187, 0.6645, 0.5299]],

         [[0.6063, 0.5603, 0.4871,  ..., 0.4688, 0.5093, 0.6549],
          [0.4859, 0.5751, 0.5655,  ..., 0.6220, 0.5358, 0.5446],
          [0.5960, 0.4441, 0.4789,  ..., 0.4177, 0.5047, 0.5983],
          [0.4287, 0.4007, 0.5383,  ..., 0.4729, 0.3675, 0.4756]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([ 1.6298e-09, -6.0000e-03,  4.0000e-03, -1.4000e-02, -6.0000e-03,
         1.4000e-02, -3.2000e-02,  4.0000e-03,  1.8000e-02,  1.6000e-02],
       device='cuda:0')
selected experts tensor([1526, 1506, 1506, 1995, 1220, 1607, 1841, 1669, 1884, 1630],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4928, 0.4918, 0.5456,  ..., 0.5980, 0.4063, 0.4867],
          [0.4982, 0.5595, 0.5240,  ..., 0.5459, 0.6793, 0.5139],
          [0.6100, 0.4468, 0.5406,  ..., 0.6705, 0.6115, 0.6398],
          [0.6160, 0.5937, 0.5127,  ..., 0.4392, 0.5085, 0.4541]],

         [[0.5458, 0.5130, 0.4038,  ..., 0.4003, 0.3912, 0.3982],
          [0.4056, 0.5648, 0.6111,  ..., 0.5465, 0.3865, 0.4287],
          [0.5857, 0.3803, 0.4047,  ..., 0.5870, 0.4348, 0.6909],
          [0.5332, 0.5276, 0.5268,  ..., 0.4876, 0.4911, 0.3368]],

         [[0.5494, 0.6063, 0.4818,  ..., 0.3572, 0.4068, 0.3848],
          [0.4930, 0.5672, 0.6143,  ..., 0.5317, 0.4331, 0.4084],
          [0.4926, 0.3225, 0.3491,  ..., 0.3980, 0.4025, 0.6242],
          [0.5782, 0.5937, 0.4814,  ..., 0.3911, 0.3754, 0.3793]],

         ...,

         [[0.4940, 0.4940, 0.4960,  ..., 0.5200, 0.4900, 0.5100],
          [0.4940, 0.4940, 0.4960,  ..., 0.5200, 0.4900, 0.5100],
          [0.4940, 0.4940, 0.4960,  ..., 0.5200, 0.4900, 0.5100],
          [0.4940, 0.4940, 0.4960,  ..., 0.5200, 0.4900, 0.5100]],

         [[0.4940, 0.4940, 0.4960,  ..., 0.5200, 0.4900, 0.5100],
          [0.4940, 0.4940, 0.4960,  ..., 0.5200, 0.4900, 0.5100],
          [0.4940, 0.4940, 0.4960,  ..., 0.5200, 0.4900, 0.5100],
          [0.4940, 0.4940, 0.4960,  ..., 0.5200, 0.4900, 0.5100]],

         [[0.4940, 0.4940, 0.4960,  ..., 0.5200, 0.4900, 0.5100],
          [0.4940, 0.4940, 0.4960,  ..., 0.5200, 0.4900, 0.5100],
          [0.4940, 0.4940, 0.4960,  ..., 0.5200, 0.4900, 0.5100],
          [0.4940, 0.4940, 0.4960,  ..., 0.5200, 0.4900, 0.5100]]],


        [[[0.5672, 0.5267, 0.5304,  ..., 0.3875, 0.4945, 0.6083],
          [0.5470, 0.4764, 0.5294,  ..., 0.6155, 0.4640, 0.4079],
          [0.5528, 0.3615, 0.3581,  ..., 0.4473, 0.3861, 0.6013],
          [0.6427, 0.4625, 0.5711,  ..., 0.3785, 0.4620, 0.4311]],

         [[0.4938, 0.5156, 0.3772,  ..., 0.4638, 0.4469, 0.3958],
          [0.4294, 0.4690, 0.7098,  ..., 0.6392, 0.3912, 0.4749],
          [0.5306, 0.4826, 0.4319,  ..., 0.4837, 0.4830, 0.5942],
          [0.5900, 0.4502, 0.4765,  ..., 0.4259, 0.6143, 0.3986]],

         [[0.5571, 0.5257, 0.4962,  ..., 0.3207, 0.4895, 0.6013],
          [0.3752, 0.4103, 0.6046,  ..., 0.5612, 0.4967, 0.3622],
          [0.5993, 0.4982, 0.4626,  ..., 0.4203, 0.5921, 0.5227],
          [0.4359, 0.5284, 0.3644,  ..., 0.4086, 0.6234, 0.5023]],

         ...,

         [[0.4940, 0.4940, 0.4960,  ..., 0.5200, 0.4900, 0.5100],
          [0.4940, 0.4940, 0.4960,  ..., 0.5200, 0.4900, 0.5100],
          [0.4940, 0.4940, 0.4960,  ..., 0.5200, 0.4900, 0.5100],
          [0.4940, 0.4940, 0.4960,  ..., 0.5200, 0.4900, 0.5100]],

         [[0.4940, 0.4940, 0.4960,  ..., 0.5200, 0.4900, 0.5100],
          [0.4940, 0.4940, 0.4960,  ..., 0.5200, 0.4900, 0.5100],
          [0.4940, 0.4940, 0.4960,  ..., 0.5200, 0.4900, 0.5100],
          [0.4940, 0.4940, 0.4960,  ..., 0.5200, 0.4900, 0.5100]],

         [[0.4940, 0.4940, 0.4960,  ..., 0.5200, 0.4900, 0.5100],
          [0.4940, 0.4940, 0.4960,  ..., 0.5200, 0.4900, 0.5100],
          [0.4940, 0.4940, 0.4960,  ..., 0.5200, 0.4900, 0.5100],
          [0.4940, 0.4940, 0.4960,  ..., 0.5200, 0.4900, 0.5100]]]],
       device='cuda:0')
tensor([[[[0.4988, 0.4978, 0.5496,  ..., 0.5780, 0.4163, 0.4767],
          [0.5042, 0.5655, 0.5280,  ..., 0.5259, 0.6893, 0.5039],
          [0.6160, 0.4528, 0.5446,  ..., 0.6505, 0.6215, 0.6298],
          [0.6220, 0.5997, 0.5167,  ..., 0.4192, 0.5185, 0.4441]],

         [[0.5518, 0.5190, 0.4078,  ..., 0.3803, 0.4012, 0.3882],
          [0.4116, 0.5708, 0.6151,  ..., 0.5265, 0.3965, 0.4187],
          [0.5917, 0.3863, 0.4087,  ..., 0.5670, 0.4448, 0.6809],
          [0.5392, 0.5336, 0.5308,  ..., 0.4676, 0.5011, 0.3268]],

         [[0.5554, 0.6123, 0.4858,  ..., 0.3372, 0.4168, 0.3748],
          [0.4990, 0.5732, 0.6183,  ..., 0.5117, 0.4431, 0.3984],
          [0.4986, 0.3285, 0.3531,  ..., 0.3780, 0.4125, 0.6142],
          [0.5842, 0.5997, 0.4854,  ..., 0.3711, 0.3854, 0.3693]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5732, 0.5327, 0.5344,  ..., 0.3675, 0.5045, 0.5983],
          [0.5530, 0.4824, 0.5334,  ..., 0.5955, 0.4740, 0.3979],
          [0.5588, 0.3675, 0.3621,  ..., 0.4273, 0.3961, 0.5913],
          [0.6487, 0.4685, 0.5751,  ..., 0.3585, 0.4720, 0.4211]],

         [[0.4998, 0.5216, 0.3812,  ..., 0.4438, 0.4569, 0.3858],
          [0.4354, 0.4750, 0.7138,  ..., 0.6192, 0.4012, 0.4649],
          [0.5366, 0.4886, 0.4359,  ..., 0.4637, 0.4930, 0.5842],
          [0.5960, 0.4562, 0.4805,  ..., 0.4059, 0.6243, 0.3886]],

         [[0.5631, 0.5317, 0.5002,  ..., 0.3007, 0.4995, 0.5913],
          [0.3812, 0.4163, 0.6086,  ..., 0.5412, 0.5067, 0.3522],
          [0.6053, 0.5042, 0.4666,  ..., 0.4003, 0.6021, 0.5127],
          [0.4419, 0.5344, 0.3684,  ..., 0.3886, 0.6334, 0.4923]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0060, -0.0060, -0.0040,  0.0060, -0.0100,  0.0220,  0.0060,  0.0200,
        -0.0100,  0.0100], device='cuda:0')
selected experts tensor([2009, 1922, 1958, 1650, 1558,  726, 1766, 1091, 1706, 1998],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1655., 1652., 1626., 1640., 1637., 1673., 1535., 1699., 1630., 1637.],
        [2009., 1922., 1958., 1650., 1558.,  726., 1766., 1091., 1706., 1998.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6366, 0.3462, 0.5173,  ..., 0.4916, 0.5776, 0.3949],
          [0.4893, 0.5776, 0.4866,  ..., 0.5444, 0.5034, 0.4991],
          [0.5231, 0.5316, 0.4618,  ..., 0.3265, 0.3704, 0.4937],
          [0.4324, 0.4611, 0.3999,  ..., 0.6763, 0.5685, 0.5633]],

         [[0.5668, 0.6245, 0.4490,  ..., 0.5339, 0.4427, 0.4057],
          [0.4180, 0.4056, 0.2874,  ..., 0.4490, 0.4608, 0.5022],
          [0.3735, 0.5809, 0.6091,  ..., 0.6047, 0.5675, 0.4807],
          [0.6009, 0.4145, 0.3561,  ..., 0.5037, 0.3489, 0.6482]],

         [[0.4338, 0.4555, 0.5658,  ..., 0.3815, 0.4473, 0.3164],
          [0.5202, 0.4560, 0.3882,  ..., 0.4936, 0.4136, 0.4623],
          [0.4305, 0.5904, 0.5274,  ..., 0.5193, 0.4553, 0.4884],
          [0.5292, 0.3911, 0.4203,  ..., 0.5758, 0.4647, 0.4459]],

         ...,

         [[0.3921, 0.5733, 0.5058,  ..., 0.5449, 0.4089, 0.4505],
          [0.6312, 0.6336, 0.5318,  ..., 0.4558, 0.4935, 0.3760],
          [0.4930, 0.4042, 0.5739,  ..., 0.3181, 0.6138, 0.3861],
          [0.4883, 0.3524, 0.5876,  ..., 0.5676, 0.5838, 0.4594]],

         [[0.4233, 0.6171, 0.5800,  ..., 0.5106, 0.5225, 0.5810],
          [0.4142, 0.4684, 0.5658,  ..., 0.5464, 0.5570, 0.5191],
          [0.4956, 0.4613, 0.5550,  ..., 0.4979, 0.5521, 0.4743],
          [0.5849, 0.5862, 0.4644,  ..., 0.4855, 0.4714, 0.4716]],

         [[0.4857, 0.3677, 0.5696,  ..., 0.4643, 0.4998, 0.5820],
          [0.6509, 0.4654, 0.3803,  ..., 0.4010, 0.6471, 0.4507],
          [0.5116, 0.6125, 0.5528,  ..., 0.5206, 0.4490, 0.4085],
          [0.5085, 0.4584, 0.4950,  ..., 0.5391, 0.5800, 0.5155]]],


        [[[0.6240, 0.4596, 0.4412,  ..., 0.6624, 0.6595, 0.5655],
          [0.5175, 0.4952, 0.5782,  ..., 0.5418, 0.5043, 0.6223],
          [0.5948, 0.5776, 0.4719,  ..., 0.4413, 0.4150, 0.3553],
          [0.5391, 0.6134, 0.5600,  ..., 0.4795, 0.4449, 0.4657]],

         [[0.4408, 0.4107, 0.4661,  ..., 0.5877, 0.6613, 0.5108],
          [0.3254, 0.4131, 0.4538,  ..., 0.4994, 0.5680, 0.4409],
          [0.5839, 0.4620, 0.5758,  ..., 0.5423, 0.6471, 0.4558],
          [0.4631, 0.5420, 0.3670,  ..., 0.4090, 0.4719, 0.4404]],

         [[0.6786, 0.4606, 0.5017,  ..., 0.6018, 0.5070, 0.4490],
          [0.3662, 0.5210, 0.5634,  ..., 0.4099, 0.5776, 0.4227],
          [0.4384, 0.5613, 0.4354,  ..., 0.4762, 0.4783, 0.4818],
          [0.6083, 0.4868, 0.3225,  ..., 0.5201, 0.4207, 0.4175]],

         ...,

         [[0.4047, 0.4777, 0.4574,  ..., 0.6491, 0.4255, 0.3912],
          [0.4305, 0.4968, 0.4695,  ..., 0.7000, 0.5269, 0.5039],
          [0.4329, 0.6166, 0.6086,  ..., 0.4878, 0.4434, 0.2699],
          [0.5316, 0.4872, 0.4400,  ..., 0.5503, 0.6017, 0.4590]],

         [[0.6027, 0.4838, 0.3803,  ..., 0.5193, 0.6087, 0.4920],
          [0.5924, 0.4654, 0.4784,  ..., 0.4884, 0.5685, 0.6237],
          [0.6509, 0.4098, 0.5043,  ..., 0.4260, 0.4084, 0.4275],
          [0.4967, 0.4572, 0.5796,  ..., 0.4341, 0.4777, 0.4631]],

         [[0.6920, 0.5441, 0.3269,  ..., 0.5396, 0.5475, 0.2668],
          [0.3680, 0.4866, 0.6109,  ..., 0.4094, 0.4065, 0.4674],
          [0.4964, 0.4502, 0.5330,  ..., 0.4835, 0.2679, 0.4953],
          [0.3219, 0.5311, 0.5240,  ..., 0.3438, 0.5466, 0.5172]]]],
       device='cuda:0')
tensor([[[[0.6406, 0.3442, 0.5233,  ..., 0.4876, 0.5756, 0.3909],
          [0.4933, 0.5756, 0.4926,  ..., 0.5404, 0.5014, 0.4951],
          [0.5271, 0.5296, 0.4678,  ..., 0.3225, 0.3684, 0.4897],
          [0.4364, 0.4591, 0.4059,  ..., 0.6723, 0.5665, 0.5593]],

         [[0.5708, 0.6225, 0.4550,  ..., 0.5299, 0.4407, 0.4017],
          [0.4220, 0.4036, 0.2934,  ..., 0.4450, 0.4588, 0.4982],
          [0.3775, 0.5789, 0.6151,  ..., 0.6007, 0.5655, 0.4767],
          [0.6049, 0.4125, 0.3621,  ..., 0.4997, 0.3469, 0.6442]],

         [[0.4378, 0.4535, 0.5718,  ..., 0.3775, 0.4453, 0.3124],
          [0.5242, 0.4540, 0.3942,  ..., 0.4896, 0.4116, 0.4583],
          [0.4345, 0.5884, 0.5334,  ..., 0.5153, 0.4533, 0.4844],
          [0.5332, 0.3891, 0.4263,  ..., 0.5718, 0.4627, 0.4419]],

         ...,

         [[0.3961, 0.5713, 0.5118,  ..., 0.5409, 0.4069, 0.4465],
          [0.6352, 0.6316, 0.5378,  ..., 0.4518, 0.4915, 0.3720],
          [0.4970, 0.4022, 0.5799,  ..., 0.3141, 0.6118, 0.3821],
          [0.4923, 0.3504, 0.5936,  ..., 0.5636, 0.5818, 0.4554]],

         [[0.4273, 0.6151, 0.5860,  ..., 0.5066, 0.5205, 0.5770],
          [0.4182, 0.4664, 0.5718,  ..., 0.5424, 0.5550, 0.5151],
          [0.4996, 0.4593, 0.5610,  ..., 0.4939, 0.5501, 0.4703],
          [0.5889, 0.5842, 0.4704,  ..., 0.4815, 0.4694, 0.4676]],

         [[0.4897, 0.3657, 0.5756,  ..., 0.4603, 0.4978, 0.5780],
          [0.6549, 0.4634, 0.3863,  ..., 0.3970, 0.6451, 0.4467],
          [0.5156, 0.6105, 0.5588,  ..., 0.5166, 0.4470, 0.4045],
          [0.5125, 0.4564, 0.5010,  ..., 0.5351, 0.5780, 0.5115]]],


        [[[0.6280, 0.4576, 0.4472,  ..., 0.6584, 0.6575, 0.5615],
          [0.5215, 0.4932, 0.5842,  ..., 0.5378, 0.5023, 0.6183],
          [0.5988, 0.5756, 0.4779,  ..., 0.4373, 0.4130, 0.3513],
          [0.5431, 0.6114, 0.5660,  ..., 0.4755, 0.4429, 0.4617]],

         [[0.4448, 0.4087, 0.4721,  ..., 0.5837, 0.6593, 0.5068],
          [0.3294, 0.4111, 0.4598,  ..., 0.4954, 0.5660, 0.4369],
          [0.5879, 0.4600, 0.5818,  ..., 0.5383, 0.6451, 0.4518],
          [0.4671, 0.5400, 0.3730,  ..., 0.4050, 0.4699, 0.4364]],

         [[0.6826, 0.4586, 0.5077,  ..., 0.5978, 0.5050, 0.4450],
          [0.3702, 0.5190, 0.5694,  ..., 0.4059, 0.5756, 0.4187],
          [0.4424, 0.5593, 0.4414,  ..., 0.4722, 0.4763, 0.4778],
          [0.6123, 0.4848, 0.3285,  ..., 0.5161, 0.4187, 0.4135]],

         ...,

         [[0.4087, 0.4757, 0.4634,  ..., 0.6451, 0.4235, 0.3872],
          [0.4345, 0.4948, 0.4755,  ..., 0.6960, 0.5249, 0.4999],
          [0.4369, 0.6146, 0.6146,  ..., 0.4838, 0.4414, 0.2659],
          [0.5356, 0.4852, 0.4460,  ..., 0.5463, 0.5997, 0.4550]],

         [[0.6067, 0.4818, 0.3863,  ..., 0.5153, 0.6067, 0.4880],
          [0.5964, 0.4634, 0.4844,  ..., 0.4844, 0.5665, 0.6197],
          [0.6549, 0.4078, 0.5103,  ..., 0.4220, 0.4064, 0.4235],
          [0.5007, 0.4552, 0.5856,  ..., 0.4301, 0.4757, 0.4591]],

         [[0.6960, 0.5421, 0.3329,  ..., 0.5356, 0.5455, 0.2628],
          [0.3720, 0.4846, 0.6169,  ..., 0.4054, 0.4045, 0.4634],
          [0.5004, 0.4482, 0.5390,  ..., 0.4795, 0.2659, 0.4913],
          [0.3259, 0.5291, 0.5300,  ..., 0.3398, 0.5446, 0.5132]]]],
       device='cuda:0', requires_grad=True)
tensor([-4.0000e-03,  2.0000e-03, -6.0000e-03, -2.3283e-10, -2.0000e-03,
        -4.0000e-03,  8.0000e-03,  4.0000e-03,  2.0000e-03,  4.0000e-03],
       device='cuda:0')
selected experts tensor([1534, 1653, 1670, 1606, 1610, 1680, 1706, 1620, 1684, 1621],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.7046, 0.6055, 0.6963,  ..., 0.3670, 0.2688, 0.4900],
          [0.5017, 0.4488, 0.4747,  ..., 0.5705, 0.6123, 0.4909],
          [0.5736, 0.4536, 0.4832,  ..., 0.5088, 0.4707, 0.4246],
          [0.5921, 0.4934, 0.3851,  ..., 0.6063, 0.4769, 0.5275]],

         [[0.4006, 0.6605, 0.4255,  ..., 0.2907, 0.5253, 0.6051],
          [0.6089, 0.4992, 0.3069,  ..., 0.5205, 0.4990, 0.4867],
          [0.5982, 0.5175, 0.5202,  ..., 0.5259, 0.6020, 0.4762],
          [0.6250, 0.4379, 0.4565,  ..., 0.5653, 0.5958, 0.5403]],

         [[0.4950, 0.5744, 0.3542,  ..., 0.5352, 0.5147, 0.5294],
          [0.4748, 0.5711, 0.4640,  ..., 0.6346, 0.6178, 0.4811],
          [0.5030, 0.4920, 0.4557,  ..., 0.3720, 0.4390, 0.3825],
          [0.5304, 0.4464, 0.4164,  ..., 0.5511, 0.6330, 0.5219]],

         ...,

         [[0.4882, 0.4290, 0.4574,  ..., 0.5091, 0.4527, 0.4544],
          [0.5333, 0.2846, 0.6050,  ..., 0.5923, 0.4318, 0.5524],
          [0.4563, 0.4290, 0.5417,  ..., 0.6063, 0.4937, 0.5256],
          [0.4067, 0.5326, 0.4998,  ..., 0.4599, 0.5281, 0.5662]],

         [[0.4002, 0.5635, 0.4647,  ..., 0.5155, 0.4450, 0.4670],
          [0.5741, 0.4914, 0.5980,  ..., 0.5048, 0.5056, 0.4047],
          [0.3978, 0.4914, 0.4283,  ..., 0.4550, 0.5821, 0.6338],
          [0.4479, 0.3893, 0.4664,  ..., 0.4342, 0.5418, 0.4738]],

         [[0.6205, 0.5849, 0.3524,  ..., 0.4935, 0.4530, 0.4611],
          [0.6520, 0.6037, 0.5487,  ..., 0.6165, 0.6015, 0.5517],
          [0.6131, 0.6060, 0.3860,  ..., 0.5096, 0.4822, 0.4308],
          [0.3466, 0.5001, 0.3704,  ..., 0.3426, 0.4091, 0.3589]]],


        [[[0.5430, 0.6985, 0.4174,  ..., 0.5330, 0.4342, 0.4967],
          [0.4961, 0.4324, 0.5432,  ..., 0.4819, 0.4936, 0.4466],
          [0.6223, 0.4982, 0.5766,  ..., 0.5530, 0.4309, 0.5715],
          [0.5179, 0.4281, 0.4913,  ..., 0.3780, 0.4067, 0.5318]],

         [[0.6780, 0.4675, 0.5838,  ..., 0.3957, 0.4958, 0.3351],
          [0.4191, 0.5362, 0.3305,  ..., 0.4725, 0.5583, 0.3880],
          [0.3673, 0.5119, 0.5942,  ..., 0.4689, 0.3817, 0.6338],
          [0.4883, 0.4783, 0.5890,  ..., 0.7189, 0.6081, 0.4979]],

         [[0.5114, 0.6321, 0.3331,  ..., 0.4146, 0.4147, 0.6356],
          [0.4539, 0.3438, 0.4884,  ..., 0.5095, 0.4535, 0.4968],
          [0.5883, 0.4066, 0.3985,  ..., 0.4461, 0.5037, 0.4313],
          [0.3583, 0.6614, 0.3934,  ..., 0.3919, 0.6137, 0.5379]],

         ...,

         [[0.3834, 0.6139, 0.5291,  ..., 0.5595, 0.5194, 0.4104],
          [0.4887, 0.5471, 0.5933,  ..., 0.4032, 0.5228, 0.4918],
          [0.5561, 0.3429, 0.5135,  ..., 0.4198, 0.4462, 0.4165],
          [0.4602, 0.4764, 0.4950,  ..., 0.3971, 0.5309, 0.4885]],

         [[0.4063, 0.5639, 0.4875,  ..., 0.5010, 0.6609, 0.5247],
          [0.3834, 0.5381, 0.4560,  ..., 0.5250, 0.6574, 0.3688],
          [0.4063, 0.5990, 0.6700,  ..., 0.3873, 0.3881, 0.4260],
          [0.5750, 0.4670, 0.4174,  ..., 0.6418, 0.3845, 0.3105]],

         [[0.5367, 0.6166, 0.4009,  ..., 0.6049, 0.3654, 0.5165],
          [0.5264, 0.5934, 0.4528,  ..., 0.4729, 0.6484, 0.5575],
          [0.5750, 0.4180, 0.3569,  ..., 0.5677, 0.5493, 0.4507],
          [0.4912, 0.4209, 0.5871,  ..., 0.5330, 0.4491, 0.3977]]]],
       device='cuda:0')
tensor([[[[0.7066, 0.6095, 0.6943,  ..., 0.3730, 0.2628, 0.4860],
          [0.5037, 0.4528, 0.4727,  ..., 0.5765, 0.6063, 0.4869],
          [0.5756, 0.4576, 0.4812,  ..., 0.5148, 0.4647, 0.4206],
          [0.5941, 0.4974, 0.3831,  ..., 0.6123, 0.4709, 0.5235]],

         [[0.4026, 0.6645, 0.4235,  ..., 0.2967, 0.5193, 0.6011],
          [0.6109, 0.5032, 0.3049,  ..., 0.5265, 0.4930, 0.4827],
          [0.6002, 0.5215, 0.5182,  ..., 0.5319, 0.5960, 0.4722],
          [0.6270, 0.4419, 0.4545,  ..., 0.5713, 0.5898, 0.5363]],

         [[0.4970, 0.5784, 0.3522,  ..., 0.5412, 0.5087, 0.5254],
          [0.4768, 0.5751, 0.4620,  ..., 0.6406, 0.6118, 0.4771],
          [0.5050, 0.4960, 0.4537,  ..., 0.3780, 0.4330, 0.3785],
          [0.5324, 0.4504, 0.4144,  ..., 0.5571, 0.6270, 0.5179]],

         ...,

         [[0.4902, 0.4330, 0.4554,  ..., 0.5151, 0.4467, 0.4504],
          [0.5353, 0.2886, 0.6030,  ..., 0.5983, 0.4258, 0.5484],
          [0.4583, 0.4330, 0.5397,  ..., 0.6123, 0.4877, 0.5216],
          [0.4087, 0.5366, 0.4978,  ..., 0.4659, 0.5221, 0.5622]],

         [[0.4022, 0.5675, 0.4627,  ..., 0.5215, 0.4390, 0.4630],
          [0.5761, 0.4954, 0.5960,  ..., 0.5108, 0.4996, 0.4007],
          [0.3998, 0.4954, 0.4263,  ..., 0.4610, 0.5761, 0.6298],
          [0.4499, 0.3933, 0.4644,  ..., 0.4402, 0.5358, 0.4698]],

         [[0.6225, 0.5889, 0.3504,  ..., 0.4995, 0.4470, 0.4571],
          [0.6540, 0.6077, 0.5467,  ..., 0.6225, 0.5955, 0.5477],
          [0.6151, 0.6100, 0.3840,  ..., 0.5156, 0.4762, 0.4268],
          [0.3486, 0.5041, 0.3684,  ..., 0.3486, 0.4031, 0.3549]]],


        [[[0.5450, 0.7025, 0.4154,  ..., 0.5390, 0.4282, 0.4927],
          [0.4981, 0.4364, 0.5412,  ..., 0.4879, 0.4876, 0.4426],
          [0.6243, 0.5022, 0.5746,  ..., 0.5590, 0.4249, 0.5675],
          [0.5199, 0.4321, 0.4893,  ..., 0.3840, 0.4007, 0.5278]],

         [[0.6800, 0.4715, 0.5818,  ..., 0.4017, 0.4898, 0.3311],
          [0.4211, 0.5402, 0.3285,  ..., 0.4785, 0.5523, 0.3840],
          [0.3693, 0.5159, 0.5922,  ..., 0.4749, 0.3757, 0.6298],
          [0.4903, 0.4823, 0.5870,  ..., 0.7249, 0.6021, 0.4939]],

         [[0.5134, 0.6361, 0.3311,  ..., 0.4206, 0.4087, 0.6316],
          [0.4559, 0.3478, 0.4864,  ..., 0.5155, 0.4475, 0.4928],
          [0.5903, 0.4106, 0.3965,  ..., 0.4521, 0.4977, 0.4273],
          [0.3603, 0.6654, 0.3914,  ..., 0.3979, 0.6077, 0.5339]],

         ...,

         [[0.3854, 0.6179, 0.5271,  ..., 0.5655, 0.5134, 0.4064],
          [0.4907, 0.5511, 0.5913,  ..., 0.4092, 0.5168, 0.4878],
          [0.5581, 0.3469, 0.5115,  ..., 0.4258, 0.4402, 0.4125],
          [0.4622, 0.4804, 0.4930,  ..., 0.4031, 0.5249, 0.4845]],

         [[0.4083, 0.5679, 0.4855,  ..., 0.5070, 0.6549, 0.5207],
          [0.3854, 0.5421, 0.4540,  ..., 0.5310, 0.6514, 0.3648],
          [0.4083, 0.6030, 0.6680,  ..., 0.3933, 0.3821, 0.4220],
          [0.5770, 0.4710, 0.4154,  ..., 0.6478, 0.3785, 0.3065]],

         [[0.5387, 0.6206, 0.3989,  ..., 0.6109, 0.3594, 0.5125],
          [0.5284, 0.5974, 0.4508,  ..., 0.4789, 0.6424, 0.5535],
          [0.5770, 0.4220, 0.3549,  ..., 0.5737, 0.5433, 0.4467],
          [0.4932, 0.4249, 0.5851,  ..., 0.5390, 0.4431, 0.3937]]]],
       device='cuda:0', requires_grad=True)
tensor([-2.0000e-03, -4.0000e-03,  2.0000e-03,  8.0000e-03,  2.3283e-10,
         6.0000e-03, -1.0000e-02, -6.0000e-03,  6.0000e-03,  4.0000e-03],
       device='cuda:0')
selected experts tensor([1709, 1636, 1518, 1574, 1703, 1565, 1495, 1690, 1724, 1770],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5151, 0.5052, 0.5004,  ..., 0.5867, 0.5548, 0.6936],
          [0.3604, 0.5218, 0.5763,  ..., 0.4575, 0.6893, 0.5387],
          [0.5298, 0.4710, 0.5343,  ..., 0.5021, 0.5031, 0.5758],
          [0.5395, 0.5093, 0.4885,  ..., 0.5391, 0.5029, 0.3909]],

         [[0.3929, 0.5260, 0.4877,  ..., 0.4896, 0.4606, 0.5637],
          [0.4806, 0.5227, 0.5705,  ..., 0.5652, 0.5921, 0.5084],
          [0.5837, 0.5043, 0.5210,  ..., 0.4633, 0.5873, 0.5448],
          [0.5273, 0.4761, 0.3867,  ..., 0.4903, 0.4792, 0.3710]],

         [[0.4873, 0.5073, 0.4462,  ..., 0.4623, 0.5324, 0.6450],
          [0.5368, 0.5400, 0.5040,  ..., 0.6116, 0.5969, 0.4098],
          [0.5005, 0.5696, 0.4916,  ..., 0.4953, 0.4457, 0.5849],
          [0.5025, 0.5265, 0.5305,  ..., 0.4741, 0.4814, 0.4975]],

         ...,

         [[0.5451, 0.5456, 0.4542,  ..., 0.5289, 0.5993, 0.6261],
          [0.4088, 0.5120, 0.6438,  ..., 0.5281, 0.6172, 0.5446],
          [0.5885, 0.4071, 0.4246,  ..., 0.5369, 0.5916, 0.5241],
          [0.3514, 0.4776, 0.4190,  ..., 0.5321, 0.4210, 0.5587]],

         [[0.3985, 0.5333, 0.3554,  ..., 0.5405, 0.5143, 0.6404],
          [0.4168, 0.5234, 0.5901,  ..., 0.5379, 0.6228, 0.4551],
          [0.5785, 0.4085, 0.4665,  ..., 0.4403, 0.4841, 0.6567],
          [0.4972, 0.3218, 0.4658,  ..., 0.4384, 0.4248, 0.3782]],

         [[0.4183, 0.5706, 0.5233,  ..., 0.4667, 0.6630, 0.6585],
          [0.4424, 0.5792, 0.5192,  ..., 0.5575, 0.6223, 0.6111],
          [0.5100, 0.4369, 0.5744,  ..., 0.5177, 0.5258, 0.5156],
          [0.5412, 0.5011, 0.5299,  ..., 0.4667, 0.4149, 0.4778]]],


        [[[0.4439, 0.6147, 0.4152,  ..., 0.5180, 0.5659, 0.5835],
          [0.3740, 0.4663, 0.6501,  ..., 0.5966, 0.7113, 0.4324],
          [0.5965, 0.5473, 0.4517,  ..., 0.5352, 0.3612, 0.5188],
          [0.4601, 0.4734, 0.4791,  ..., 0.5119, 0.4428, 0.4443]],

         [[0.4913, 0.5966, 0.4227,  ..., 0.5560, 0.5385, 0.6701],
          [0.4164, 0.5000, 0.4959,  ..., 0.4824, 0.6116, 0.6030],
          [0.4528, 0.4589, 0.5084,  ..., 0.6409, 0.4224, 0.5538],
          [0.6524, 0.4945, 0.5344,  ..., 0.5463, 0.4319, 0.4400]],

         [[0.4598, 0.6392, 0.4123,  ..., 0.3750, 0.5945, 0.4778],
          [0.5119, 0.4345, 0.6061,  ..., 0.6250, 0.5274, 0.4710],
          [0.5202, 0.5886, 0.5124,  ..., 0.4184, 0.3656, 0.4262],
          [0.5180, 0.3850, 0.3689,  ..., 0.4943, 0.4103, 0.5342]],

         ...,

         [[0.5576, 0.4323, 0.4380,  ..., 0.4935, 0.3991, 0.5912],
          [0.4458, 0.4949, 0.6756,  ..., 0.3796, 0.6675, 0.5382],
          [0.6810, 0.4925, 0.6187,  ..., 0.5985, 0.5649, 0.5114],
          [0.4458, 0.5261, 0.5221,  ..., 0.5313, 0.3890, 0.4589]],

         [[0.4804, 0.5706, 0.5174,  ..., 0.4947, 0.4885, 0.5375],
          [0.4164, 0.4827, 0.6127,  ..., 0.4606, 0.7163, 0.5291],
          [0.5083, 0.4019, 0.4347,  ..., 0.4686, 0.5883, 0.5337],
          [0.6776, 0.6055, 0.5720,  ..., 0.3281, 0.3586, 0.4262]],

         [[0.4504, 0.6068, 0.4166,  ..., 0.4331, 0.5897, 0.5540],
          [0.4598, 0.4867, 0.4721,  ..., 0.5415, 0.6223, 0.4019],
          [0.6886, 0.3869, 0.4438,  ..., 0.5056, 0.4424, 0.5587],
          [0.5468, 0.5080, 0.5491,  ..., 0.4473, 0.4630, 0.4098]]]],
       device='cuda:0')
tensor([[[[0.5141, 0.5102, 0.4954,  ..., 0.5837, 0.5378, 0.6766],
          [0.3594, 0.5268, 0.5713,  ..., 0.4545, 0.6723, 0.5217],
          [0.5288, 0.4760, 0.5293,  ..., 0.4991, 0.4861, 0.5588],
          [0.5385, 0.5143, 0.4835,  ..., 0.5361, 0.4859, 0.3739]],

         [[0.3919, 0.5310, 0.4827,  ..., 0.4866, 0.4436, 0.5467],
          [0.4796, 0.5277, 0.5655,  ..., 0.5622, 0.5751, 0.4914],
          [0.5827, 0.5093, 0.5160,  ..., 0.4603, 0.5703, 0.5278],
          [0.5263, 0.4811, 0.3817,  ..., 0.4873, 0.4622, 0.3540]],

         [[0.4863, 0.5123, 0.4412,  ..., 0.4593, 0.5154, 0.6280],
          [0.5358, 0.5450, 0.4990,  ..., 0.6086, 0.5799, 0.3928],
          [0.4995, 0.5746, 0.4866,  ..., 0.4923, 0.4287, 0.5679],
          [0.5015, 0.5315, 0.5255,  ..., 0.4711, 0.4644, 0.4805]],

         ...,

         [[0.5441, 0.5506, 0.4492,  ..., 0.5259, 0.5823, 0.6091],
          [0.4078, 0.5170, 0.6388,  ..., 0.5251, 0.6002, 0.5276],
          [0.5875, 0.4121, 0.4196,  ..., 0.5339, 0.5746, 0.5071],
          [0.3504, 0.4826, 0.4140,  ..., 0.5291, 0.4040, 0.5417]],

         [[0.3975, 0.5383, 0.3504,  ..., 0.5375, 0.4973, 0.6234],
          [0.4158, 0.5284, 0.5851,  ..., 0.5349, 0.6058, 0.4381],
          [0.5775, 0.4135, 0.4615,  ..., 0.4373, 0.4671, 0.6397],
          [0.4962, 0.3268, 0.4608,  ..., 0.4354, 0.4078, 0.3612]],

         [[0.4173, 0.5756, 0.5183,  ..., 0.4637, 0.6460, 0.6415],
          [0.4414, 0.5842, 0.5142,  ..., 0.5545, 0.6053, 0.5941],
          [0.5090, 0.4419, 0.5694,  ..., 0.5147, 0.5088, 0.4986],
          [0.5402, 0.5061, 0.5249,  ..., 0.4637, 0.3979, 0.4608]]],


        [[[0.4429, 0.6197, 0.4102,  ..., 0.5150, 0.5489, 0.5665],
          [0.3730, 0.4713, 0.6451,  ..., 0.5936, 0.6943, 0.4154],
          [0.5955, 0.5523, 0.4467,  ..., 0.5322, 0.3442, 0.5018],
          [0.4591, 0.4784, 0.4741,  ..., 0.5089, 0.4258, 0.4273]],

         [[0.4903, 0.6016, 0.4177,  ..., 0.5530, 0.5215, 0.6531],
          [0.4154, 0.5050, 0.4909,  ..., 0.4794, 0.5946, 0.5860],
          [0.4518, 0.4639, 0.5034,  ..., 0.6379, 0.4054, 0.5368],
          [0.6514, 0.4995, 0.5294,  ..., 0.5433, 0.4149, 0.4230]],

         [[0.4588, 0.6442, 0.4073,  ..., 0.3720, 0.5775, 0.4608],
          [0.5109, 0.4395, 0.6011,  ..., 0.6220, 0.5104, 0.4540],
          [0.5192, 0.5936, 0.5074,  ..., 0.4154, 0.3486, 0.4092],
          [0.5170, 0.3900, 0.3639,  ..., 0.4913, 0.3933, 0.5172]],

         ...,

         [[0.5566, 0.4373, 0.4330,  ..., 0.4905, 0.3821, 0.5742],
          [0.4448, 0.4999, 0.6706,  ..., 0.3766, 0.6505, 0.5212],
          [0.6800, 0.4975, 0.6137,  ..., 0.5955, 0.5479, 0.4944],
          [0.4448, 0.5311, 0.5171,  ..., 0.5283, 0.3720, 0.4419]],

         [[0.4794, 0.5756, 0.5124,  ..., 0.4917, 0.4715, 0.5205],
          [0.4154, 0.4877, 0.6077,  ..., 0.4576, 0.6993, 0.5121],
          [0.5073, 0.4069, 0.4297,  ..., 0.4656, 0.5713, 0.5167],
          [0.6766, 0.6105, 0.5670,  ..., 0.3251, 0.3416, 0.4092]],

         [[0.4494, 0.6118, 0.4116,  ..., 0.4301, 0.5727, 0.5370],
          [0.4588, 0.4917, 0.4671,  ..., 0.5385, 0.6053, 0.3849],
          [0.6876, 0.3919, 0.4388,  ..., 0.5026, 0.4254, 0.5417],
          [0.5458, 0.5130, 0.5441,  ..., 0.4443, 0.4460, 0.3928]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0010, -0.0050,  0.0050, -0.0150, -0.0050,  0.0150, -0.0330,  0.0030,
         0.0170,  0.0170], device='cuda:0')
selected experts tensor([1548, 1495, 1357, 1913, 1463, 1929, 1659, 1561, 1920, 1539],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4079, 0.4385, 0.5577,  ..., 0.4784, 0.3725, 0.4368],
          [0.4831, 0.4746, 0.6311,  ..., 0.5636, 0.4249, 0.4177],
          [0.4932, 0.3207, 0.4563,  ..., 0.5653, 0.5029, 0.4732],
          [0.6963, 0.5378, 0.4913,  ..., 0.4008, 0.4675, 0.3738]],

         [[0.5729, 0.6309, 0.4755,  ..., 0.4959, 0.4713, 0.5330],
          [0.4660, 0.5334, 0.6365,  ..., 0.5136, 0.6639, 0.3939],
          [0.5638, 0.4664, 0.4707,  ..., 0.4454, 0.5560, 0.5121],
          [0.6164, 0.4675, 0.5507,  ..., 0.3795, 0.6170, 0.5132]],

         [[0.4820, 0.5672, 0.4526,  ..., 0.4383, 0.3466, 0.3702],
          [0.3207, 0.5772, 0.6455,  ..., 0.6430, 0.5316, 0.4577],
          [0.5159, 0.3632, 0.4275,  ..., 0.5079, 0.4672, 0.6370],
          [0.6099, 0.5207, 0.5485,  ..., 0.3750, 0.6170, 0.5400]],

         ...,

         [[0.4930, 0.4930, 0.4950,  ..., 0.5210, 0.4890, 0.5090],
          [0.4930, 0.4930, 0.4950,  ..., 0.5210, 0.4890, 0.5090],
          [0.4930, 0.4930, 0.4950,  ..., 0.5210, 0.4890, 0.5090],
          [0.4930, 0.4930, 0.4950,  ..., 0.5210, 0.4890, 0.5090]],

         [[0.4930, 0.4930, 0.4950,  ..., 0.5210, 0.4890, 0.5090],
          [0.4930, 0.4930, 0.4950,  ..., 0.5210, 0.4890, 0.5090],
          [0.4930, 0.4930, 0.4950,  ..., 0.5210, 0.4890, 0.5090],
          [0.4930, 0.4930, 0.4950,  ..., 0.5210, 0.4890, 0.5090]],

         [[0.4930, 0.4930, 0.4950,  ..., 0.5210, 0.4890, 0.5090],
          [0.4930, 0.4930, 0.4950,  ..., 0.5210, 0.4890, 0.5090],
          [0.4930, 0.4930, 0.4950,  ..., 0.5210, 0.4890, 0.5090],
          [0.4930, 0.4930, 0.4950,  ..., 0.5210, 0.4890, 0.5090]]],


        [[[0.4853, 0.4084, 0.5388,  ..., 0.4653, 0.4571, 0.5936],
          [0.5083, 0.4773, 0.6595,  ..., 0.5140, 0.5736, 0.4673],
          [0.5181, 0.4313, 0.5148,  ..., 0.5750, 0.6224, 0.5666],
          [0.5719, 0.5819, 0.4560,  ..., 0.3504, 0.5999, 0.3666]],

         [[0.5264, 0.4480, 0.5153,  ..., 0.4157, 0.5505, 0.5372],
          [0.5320, 0.4419, 0.6059,  ..., 0.4468, 0.4607, 0.3999],
          [0.5871, 0.4060, 0.3925,  ..., 0.5961, 0.5253, 0.6190],
          [0.5757, 0.5672, 0.4834,  ..., 0.3452, 0.5845, 0.4425]],

         [[0.4946, 0.5111, 0.5381,  ..., 0.5784, 0.4139, 0.4734],
          [0.4959, 0.5475, 0.5682,  ..., 0.5588, 0.7044, 0.5557],
          [0.5805, 0.4046, 0.4774,  ..., 0.6829, 0.6483, 0.6796],
          [0.5983, 0.5214, 0.5079,  ..., 0.3831, 0.5750, 0.4640]],

         ...,

         [[0.4930, 0.4930, 0.4950,  ..., 0.5210, 0.4890, 0.5090],
          [0.4930, 0.4930, 0.4950,  ..., 0.5210, 0.4890, 0.5090],
          [0.4930, 0.4930, 0.4950,  ..., 0.5210, 0.4890, 0.5090],
          [0.4930, 0.4930, 0.4950,  ..., 0.5210, 0.4890, 0.5090]],

         [[0.4930, 0.4930, 0.4950,  ..., 0.5210, 0.4890, 0.5090],
          [0.4930, 0.4930, 0.4950,  ..., 0.5210, 0.4890, 0.5090],
          [0.4930, 0.4930, 0.4950,  ..., 0.5210, 0.4890, 0.5090],
          [0.4930, 0.4930, 0.4950,  ..., 0.5210, 0.4890, 0.5090]],

         [[0.4930, 0.4930, 0.4950,  ..., 0.5210, 0.4890, 0.5090],
          [0.4930, 0.4930, 0.4950,  ..., 0.5210, 0.4890, 0.5090],
          [0.4930, 0.4930, 0.4950,  ..., 0.5210, 0.4890, 0.5090],
          [0.4930, 0.4930, 0.4950,  ..., 0.5210, 0.4890, 0.5090]]]],
       device='cuda:0')/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([[[[0.4149, 0.4455, 0.5627,  ..., 0.4574, 0.3835, 0.4278],
          [0.4901, 0.4816, 0.6361,  ..., 0.5426, 0.4359, 0.4087],
          [0.5002, 0.3277, 0.4613,  ..., 0.5443, 0.5139, 0.4642],
          [0.7033, 0.5448, 0.4963,  ..., 0.3798, 0.4785, 0.3648]],

         [[0.5799, 0.6379, 0.4805,  ..., 0.4749, 0.4823, 0.5240],
          [0.4730, 0.5404, 0.6415,  ..., 0.4926, 0.6749, 0.3849],
          [0.5708, 0.4734, 0.4757,  ..., 0.4244, 0.5670, 0.5031],
          [0.6234, 0.4745, 0.5557,  ..., 0.3585, 0.6280, 0.5042]],

         [[0.4890, 0.5742, 0.4576,  ..., 0.4173, 0.3576, 0.3612],
          [0.3277, 0.5842, 0.6505,  ..., 0.6220, 0.5426, 0.4487],
          [0.5229, 0.3702, 0.4325,  ..., 0.4869, 0.4782, 0.6280],
          [0.6169, 0.5277, 0.5535,  ..., 0.3540, 0.6280, 0.5310]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.4923, 0.4154, 0.5438,  ..., 0.4443, 0.4681, 0.5846],
          [0.5153, 0.4843, 0.6645,  ..., 0.4930, 0.5846, 0.4583],
          [0.5251, 0.4383, 0.5198,  ..., 0.5540, 0.6334, 0.5576],
          [0.5789, 0.5889, 0.4610,  ..., 0.3294, 0.6109, 0.3576]],

         [[0.5334, 0.4550, 0.5203,  ..., 0.3947, 0.5615, 0.5282],
          [0.5390, 0.4489, 0.6109,  ..., 0.4258, 0.4717, 0.3909],
          [0.5941, 0.4130, 0.3975,  ..., 0.5751, 0.5363, 0.6100],
          [0.5827, 0.5742, 0.4884,  ..., 0.3242, 0.5955, 0.4335]],

         [[0.5016, 0.5181, 0.5431,  ..., 0.5574, 0.4249, 0.4644],
          [0.5029, 0.5545, 0.5732,  ..., 0.5378, 0.7154, 0.5467],
          [0.5875, 0.4116, 0.4824,  ..., 0.6619, 0.6593, 0.6706],
          [0.6053, 0.5284, 0.5129,  ..., 0.3621, 0.5860, 0.4550]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0070, -0.0070, -0.0050,  0.0050, -0.0090,  0.0230,  0.0050,  0.0210,
        -0.0110,  0.0090], device='cuda:0')
selected experts tensor([1935, 1694, 1738, 1640, 1719,  823, 2088, 1149, 1868, 1730],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1709., 1636., 1518., 1574., 1703., 1565., 1495., 1690., 1724., 1770.],
        [1935., 1694., 1738., 1640., 1719.,  823., 2088., 1149., 1868., 1730.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.6376, 0.3452, 0.5163,  ..., 0.4921, 0.5766, 0.3955],
          [0.4901, 0.5766, 0.4856,  ..., 0.5457, 0.5025, 0.5001],
          [0.5240, 0.5306, 0.4608,  ..., 0.3275, 0.3694, 0.4946],
          [0.4334, 0.4601, 0.3989,  ..., 0.6773, 0.5675, 0.5643]],

         [[0.4200, 0.4719, 0.5633,  ..., 0.6302, 0.4268, 0.4347],
          [0.6000, 0.4915, 0.4335,  ..., 0.5065, 0.5974, 0.5382],
          [0.6693, 0.4453, 0.5951,  ..., 0.6094, 0.6620, 0.4580],
          [0.5577, 0.5093, 0.4970,  ..., 0.5365, 0.5124, 0.6210]],

         [[0.4181, 0.5243, 0.5523,  ..., 0.4404, 0.5637, 0.5539],
          [0.5353, 0.5904, 0.4636,  ..., 0.4602, 0.4254, 0.6061],
          [0.5774, 0.5511, 0.6345,  ..., 0.4962, 0.4083, 0.6284],
          [0.4767, 0.5349, 0.5965,  ..., 0.6669, 0.3613, 0.3789]],

         ...,

         [[0.4875, 0.4738, 0.3966,  ..., 0.4076, 0.6568, 0.6057],
          [0.5816, 0.6936, 0.6619,  ..., 0.5981, 0.4714, 0.6842],
          [0.5587, 0.4978, 0.4864,  ..., 0.4626, 0.6550, 0.4884],
          [0.4811, 0.6091, 0.4385,  ..., 0.6252, 0.5287, 0.4711]],

         [[0.4820, 0.5284, 0.5407,  ..., 0.4665, 0.3479, 0.5631],
          [0.5697, 0.5632, 0.5347,  ..., 0.4751, 0.5349, 0.7043],
          [0.5764, 0.6170, 0.3816,  ..., 0.5268, 0.4131, 0.6608],
          [0.5191, 0.5169, 0.3551,  ..., 0.6546, 0.4773, 0.3780]],

         [[0.4343, 0.6506, 0.4630,  ..., 0.6951, 0.5293, 0.4706],
          [0.4262, 0.4652, 0.3434,  ..., 0.5396, 0.4064, 0.4964],
          [0.4824, 0.4955, 0.6381,  ..., 0.4922, 0.5718, 0.4716],
          [0.5793, 0.5187, 0.5776,  ..., 0.5289, 0.3347, 0.4967]]],


        [[[0.4195, 0.5885, 0.4390,  ..., 0.4675, 0.5018, 0.5016],
          [0.3299, 0.4555, 0.4241,  ..., 0.5183, 0.4504, 0.5418],
          [0.5649, 0.5946, 0.5676,  ..., 0.5953, 0.5713, 0.4636],
          [0.5797, 0.5029, 0.4905,  ..., 0.2936, 0.4654, 0.6094]],

         [[0.6813, 0.4608, 0.5006,  ..., 0.6028, 0.5060, 0.4500],
          [0.3672, 0.5200, 0.5624,  ..., 0.4104, 0.5766, 0.4242],
          [0.4394, 0.5600, 0.4344,  ..., 0.4772, 0.4773, 0.4827],
          [0.6093, 0.4857, 0.3215,  ..., 0.5214, 0.4197, 0.4185]],

         [[0.5114, 0.5723, 0.5638,  ..., 0.5032, 0.5020, 0.4851],
          [0.4604, 0.6087, 0.4463,  ..., 0.4001, 0.4833, 0.4342],
          [0.4466, 0.5041, 0.4122,  ..., 0.4440, 0.5904, 0.4967],
          [0.4370, 0.4093, 0.5317,  ..., 0.5669, 0.6290, 0.3987]],

         ...,

         [[0.4319, 0.5531, 0.3250,  ..., 0.6150, 0.5427, 0.4328],
          [0.4643, 0.5208, 0.4572,  ..., 0.6893, 0.4749, 0.5259],
          [0.3764, 0.5234, 0.5317,  ..., 0.5459, 0.6559, 0.3545],
          [0.4039, 0.5429, 0.6470,  ..., 0.4592, 0.4545, 0.5744]],

         [[0.5030, 0.5497, 0.6831,  ..., 0.5011, 0.4745, 0.3082],
          [0.5389, 0.5220, 0.6264,  ..., 0.3545, 0.5321, 0.6456],
          [0.5091, 0.5586, 0.4809,  ..., 0.5274, 0.6612, 0.4204],
          [0.5019, 0.4994, 0.3756,  ..., 0.5486, 0.5904, 0.5958]],

         [[0.6659, 0.4027, 0.4658,  ..., 0.4166, 0.4669, 0.4246],
          [0.5038, 0.4444, 0.5904,  ..., 0.3725, 0.5368, 0.5085],
          [0.6304, 0.3339, 0.3632,  ..., 0.3590, 0.5267, 0.5344],
          [0.3002, 0.4297, 0.5657,  ..., 0.4817, 0.4929, 0.5007]]]],
       device='cuda:0')
tensor([[[[0.6406, 0.3442, 0.5233,  ..., 0.4871, 0.5756, 0.3905],
          [0.4931, 0.5756, 0.4926,  ..., 0.5407, 0.5015, 0.4951],
          [0.5270, 0.5296, 0.4678,  ..., 0.3225, 0.3684, 0.4896],
          [0.4364, 0.4591, 0.4059,  ..., 0.6723, 0.5665, 0.5593]],

         [[0.4230, 0.4709, 0.5703,  ..., 0.6252, 0.4258, 0.4297],
          [0.6030, 0.4905, 0.4405,  ..., 0.5015, 0.5964, 0.5332],
          [0.6723, 0.4443, 0.6021,  ..., 0.6044, 0.6610, 0.4530],
          [0.5607, 0.5083, 0.5040,  ..., 0.5315, 0.5114, 0.6160]],

         [[0.4211, 0.5233, 0.5593,  ..., 0.4354, 0.5627, 0.5489],
          [0.5383, 0.5894, 0.4706,  ..., 0.4552, 0.4244, 0.6011],
          [0.5804, 0.5501, 0.6415,  ..., 0.4912, 0.4073, 0.6234],
          [0.4797, 0.5339, 0.6035,  ..., 0.6619, 0.3603, 0.3739]],

         ...,

         [[0.4905, 0.4728, 0.4036,  ..., 0.4026, 0.6558, 0.6007],
          [0.5846, 0.6926, 0.6689,  ..., 0.5931, 0.4704, 0.6792],
          [0.5617, 0.4968, 0.4934,  ..., 0.4576, 0.6540, 0.4834],
          [0.4841, 0.6081, 0.4455,  ..., 0.6202, 0.5277, 0.4661]],

         [[0.4850, 0.5274, 0.5477,  ..., 0.4615, 0.3469, 0.5581],
          [0.5727, 0.5622, 0.5417,  ..., 0.4701, 0.5339, 0.6993],
          [0.5794, 0.6160, 0.3886,  ..., 0.5218, 0.4121, 0.6558],
          [0.5221, 0.5159, 0.3621,  ..., 0.6496, 0.4763, 0.3730]],

         [[0.4373, 0.6496, 0.4700,  ..., 0.6901, 0.5283, 0.4656],
          [0.4292, 0.4642, 0.3504,  ..., 0.5346, 0.4054, 0.4914],
          [0.4854, 0.4945, 0.6451,  ..., 0.4872, 0.5708, 0.4666],
          [0.5823, 0.5177, 0.5846,  ..., 0.5239, 0.3337, 0.4917]]],


        [[[0.4225, 0.5875, 0.4460,  ..., 0.4625, 0.5008, 0.4966],
          [0.3329, 0.4545, 0.4311,  ..., 0.5133, 0.4494, 0.5368],
          [0.5679, 0.5936, 0.5746,  ..., 0.5903, 0.5703, 0.4586],
          [0.5827, 0.5019, 0.4975,  ..., 0.2886, 0.4644, 0.6044]],

         [[0.6843, 0.4598, 0.5076,  ..., 0.5978, 0.5050, 0.4450],
          [0.3702, 0.5190, 0.5694,  ..., 0.4054, 0.5756, 0.4192],
          [0.4424, 0.5590, 0.4414,  ..., 0.4722, 0.4763, 0.4777],
          [0.6123, 0.4847, 0.3285,  ..., 0.5164, 0.4187, 0.4135]],

         [[0.5144, 0.5713, 0.5708,  ..., 0.4982, 0.5010, 0.4801],
          [0.4634, 0.6077, 0.4533,  ..., 0.3951, 0.4823, 0.4292],
          [0.4496, 0.5031, 0.4192,  ..., 0.4390, 0.5894, 0.4917],
          [0.4400, 0.4083, 0.5387,  ..., 0.5619, 0.6280, 0.3937]],

         ...,

         [[0.4349, 0.5521, 0.3320,  ..., 0.6100, 0.5417, 0.4278],
          [0.4673, 0.5198, 0.4642,  ..., 0.6843, 0.4739, 0.5209],
          [0.3794, 0.5224, 0.5387,  ..., 0.5409, 0.6549, 0.3495],
          [0.4069, 0.5419, 0.6540,  ..., 0.4542, 0.4535, 0.5694]],

         [[0.5060, 0.5487, 0.6901,  ..., 0.4961, 0.4735, 0.3032],
          [0.5419, 0.5210, 0.6334,  ..., 0.3495, 0.5311, 0.6406],
          [0.5121, 0.5576, 0.4879,  ..., 0.5224, 0.6602, 0.4154],
          [0.5049, 0.4984, 0.3826,  ..., 0.5436, 0.5894, 0.5908]],

         [[0.6689, 0.4017, 0.4728,  ..., 0.4116, 0.4659, 0.4196],
          [0.5068, 0.4434, 0.5974,  ..., 0.3675, 0.5358, 0.5035],
          [0.6334, 0.3329, 0.3702,  ..., 0.3540, 0.5257, 0.5294],
          [0.3032, 0.4287, 0.5727,  ..., 0.4767, 0.4919, 0.4957]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0030,  0.0010, -0.0070,  0.0010, -0.0010, -0.0050,  0.0070,  0.0050,
         0.0010,  0.0050], device='cuda:0')
selected experts tensor([1603, 1588, 1669, 1684, 1665, 1683, 1691, 1534, 1584, 1683],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.7036, 0.6065, 0.6973,  ..., 0.3669, 0.2678, 0.4889],
          [0.5007, 0.4498, 0.4757,  ..., 0.5695, 0.6113, 0.4901],
          [0.5726, 0.4549, 0.4841,  ..., 0.5078, 0.4697, 0.4236],
          [0.5911, 0.4944, 0.3861,  ..., 0.6053, 0.4757, 0.5265]],

         [[0.4310, 0.4965, 0.4784,  ..., 0.3542, 0.5345, 0.3385],
          [0.5958, 0.5357, 0.3145,  ..., 0.4730, 0.5067, 0.4170],
          [0.3893, 0.4024, 0.4207,  ..., 0.5561, 0.4199, 0.5938],
          [0.5235, 0.5089, 0.5980,  ..., 0.5213, 0.3466, 0.5262]],

         [[0.4655, 0.4626, 0.5297,  ..., 0.4431, 0.5051, 0.3741],
          [0.2583, 0.4796, 0.5024,  ..., 0.4796, 0.6302, 0.4774],
          [0.4699, 0.5243, 0.5990,  ..., 0.4971, 0.4754, 0.4423],
          [0.5692, 0.5812, 0.5345,  ..., 0.5257, 0.5002, 0.5623]],

         ...,

         [[0.6070, 0.5735, 0.6373,  ..., 0.4979, 0.5607, 0.5666],
          [0.6475, 0.5187, 0.6172,  ..., 0.6881, 0.5578, 0.4122],
          [0.5582, 0.4940, 0.3916,  ..., 0.4303, 0.4554, 0.5700],
          [0.5269, 0.4447, 0.6915,  ..., 0.5347, 0.4587, 0.4725]],

         [[0.4048, 0.4329, 0.6310,  ..., 0.5242, 0.7406, 0.5985],
          [0.5136, 0.3819, 0.5507,  ..., 0.3877, 0.5445, 0.5480],
          [0.4673, 0.4408, 0.6291,  ..., 0.4051, 0.5633, 0.5011],
          [0.4708, 0.5213, 0.4657,  ..., 0.5018, 0.5471, 0.5239]],

         [[0.4856, 0.4763, 0.4856,  ..., 0.4395, 0.3969, 0.5909],
          [0.6056, 0.5425, 0.5628,  ..., 0.6081, 0.3862, 0.4413],
          [0.4143, 0.5299, 0.4005,  ..., 0.5240, 0.6265, 0.3162],
          [0.5277, 0.3926, 0.5071,  ..., 0.4323, 0.5382, 0.3884]]],


        [[[0.4319, 0.5032, 0.3437,  ..., 0.5089, 0.4746, 0.5519],
          [0.6009, 0.4062, 0.6409,  ..., 0.5129, 0.5114, 0.4468],
          [0.5515, 0.3681, 0.4341,  ..., 0.5672, 0.4213, 0.5039],
          [0.5483, 0.4124, 0.4389,  ..., 0.5762, 0.4152, 0.4959]],

         [[0.5097, 0.6340, 0.3341,  ..., 0.4131, 0.4128, 0.6346],
          [0.4527, 0.3456, 0.4893,  ..., 0.5087, 0.4525, 0.4959],
          [0.5873, 0.4076, 0.3995,  ..., 0.4451, 0.5027, 0.4308],
          [0.3573, 0.6624, 0.3944,  ..., 0.3909, 0.6122, 0.5369]],

         [[0.4750, 0.5003, 0.5005,  ..., 0.5363, 0.5165, 0.5531],
          [0.5073, 0.6162, 0.4255,  ..., 0.6048, 0.4380, 0.6355],
          [0.5278, 0.4300, 0.5886,  ..., 0.5241, 0.5328, 0.2868],
          [0.4719, 0.7052, 0.5342,  ..., 0.5880, 0.4963, 0.4089]],

         ...,

         [[0.5529, 0.5340, 0.4587,  ..., 0.5561, 0.5369, 0.3238],
          [0.4914, 0.5986, 0.4726,  ..., 0.5786, 0.3959, 0.4538],
          [0.6250, 0.4742, 0.3972,  ..., 0.5455, 0.4774, 0.4885],
          [0.3430, 0.3912, 0.4014,  ..., 0.4791, 0.3955, 0.2997]],

         [[0.4827, 0.5024, 0.5497,  ..., 0.5790, 0.4935, 0.5451],
          [0.4186, 0.6204, 0.6116,  ..., 0.5422, 0.4527, 0.3916],
          [0.6322, 0.5153, 0.4179,  ..., 0.5767, 0.4750, 0.5461],
          [0.4095, 0.4380, 0.5315,  ..., 0.5095, 0.4452, 0.6218]],

         [[0.3796, 0.4998, 0.3615,  ..., 0.3994, 0.5238, 0.5524],
          [0.4912, 0.5892, 0.4613,  ..., 0.3233, 0.4081, 0.5555],
          [0.5365, 0.4243, 0.4427,  ..., 0.5385, 0.4990, 0.5094],
          [0.5986, 0.4638, 0.5425,  ..., 0.4169, 0.3816, 0.4575]]]],
       device='cuda:0')
tensor([[[[0.7066, 0.6095, 0.6943,  ..., 0.3739, 0.2628, 0.4859],
          [0.5037, 0.4528, 0.4727,  ..., 0.5765, 0.6063, 0.4871],
          [0.5756, 0.4579, 0.4811,  ..., 0.5148, 0.4647, 0.4206],
          [0.5941, 0.4974, 0.3831,  ..., 0.6123, 0.4707, 0.5235]],

         [[0.4340, 0.4995, 0.4754,  ..., 0.3612, 0.5295, 0.3355],
          [0.5988, 0.5387, 0.3115,  ..., 0.4800, 0.5017, 0.4140],
          [0.3923, 0.4054, 0.4177,  ..., 0.5631, 0.4149, 0.5908],
          [0.5265, 0.5119, 0.5950,  ..., 0.5283, 0.3416, 0.5232]],

         [[0.4685, 0.4656, 0.5267,  ..., 0.4501, 0.5001, 0.3711],
          [0.2613, 0.4826, 0.4994,  ..., 0.4866, 0.6252, 0.4744],
          [0.4729, 0.5273, 0.5960,  ..., 0.5041, 0.4704, 0.4393],
          [0.5722, 0.5842, 0.5315,  ..., 0.5327, 0.4952, 0.5593]],

         ...,

         [[0.6100, 0.5765, 0.6343,  ..., 0.5049, 0.5557, 0.5636],
          [0.6505, 0.5217, 0.6142,  ..., 0.6951, 0.5528, 0.4092],
          [0.5612, 0.4970, 0.3886,  ..., 0.4373, 0.4504, 0.5670],
          [0.5299, 0.4477, 0.6885,  ..., 0.5417, 0.4537, 0.4695]],

         [[0.4078, 0.4359, 0.6280,  ..., 0.5312, 0.7356, 0.5955],
          [0.5166, 0.3849, 0.5477,  ..., 0.3947, 0.5395, 0.5450],
          [0.4703, 0.4438, 0.6261,  ..., 0.4121, 0.5583, 0.4981],
          [0.4738, 0.5243, 0.4627,  ..., 0.5088, 0.5421, 0.5209]],

         [[0.4886, 0.4793, 0.4826,  ..., 0.4465, 0.3919, 0.5879],
          [0.6086, 0.5455, 0.5598,  ..., 0.6151, 0.3812, 0.4383],
          [0.4173, 0.5329, 0.3975,  ..., 0.5310, 0.6215, 0.3132],
          [0.5307, 0.3956, 0.5041,  ..., 0.4393, 0.5332, 0.3854]]],


        [[[0.4349, 0.5062, 0.3407,  ..., 0.5159, 0.4696, 0.5489],
          [0.6039, 0.4092, 0.6379,  ..., 0.5199, 0.5064, 0.4438],
          [0.5545, 0.3711, 0.4311,  ..., 0.5742, 0.4163, 0.5009],
          [0.5513, 0.4154, 0.4359,  ..., 0.5832, 0.4102, 0.4929]],

         [[0.5127, 0.6370, 0.3311,  ..., 0.4201, 0.4078, 0.6316],
          [0.4557, 0.3486, 0.4863,  ..., 0.5157, 0.4475, 0.4929],
          [0.5903, 0.4106, 0.3965,  ..., 0.4521, 0.4977, 0.4278],
          [0.3603, 0.6654, 0.3914,  ..., 0.3979, 0.6072, 0.5339]],

         [[0.4780, 0.5033, 0.4975,  ..., 0.5433, 0.5115, 0.5501],
          [0.5103, 0.6192, 0.4225,  ..., 0.6118, 0.4330, 0.6325],
          [0.5308, 0.4330, 0.5856,  ..., 0.5311, 0.5278, 0.2838],
          [0.4749, 0.7082, 0.5312,  ..., 0.5950, 0.4913, 0.4059]],

         ...,

         [[0.5559, 0.5370, 0.4557,  ..., 0.5631, 0.5319, 0.3208],
          [0.4944, 0.6016, 0.4696,  ..., 0.5856, 0.3909, 0.4508],
          [0.6280, 0.4772, 0.3942,  ..., 0.5525, 0.4724, 0.4855],
          [0.3460, 0.3942, 0.3984,  ..., 0.4861, 0.3905, 0.2967]],

         [[0.4857, 0.5054, 0.5467,  ..., 0.5860, 0.4885, 0.5421],
          [0.4216, 0.6234, 0.6086,  ..., 0.5492, 0.4477, 0.3886],
          [0.6352, 0.5183, 0.4149,  ..., 0.5837, 0.4700, 0.5431],
          [0.4125, 0.4410, 0.5285,  ..., 0.5165, 0.4402, 0.6188]],

         [[0.3826, 0.5028, 0.3585,  ..., 0.4064, 0.5188, 0.5494],
          [0.4942, 0.5922, 0.4583,  ..., 0.3303, 0.4031, 0.5525],
          [0.5395, 0.4273, 0.4397,  ..., 0.5455, 0.4940, 0.5064],
          [0.6016, 0.4668, 0.5395,  ..., 0.4239, 0.3766, 0.4545]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0030, -0.0030,  0.0030,  0.0090, -0.0010,  0.0070, -0.0090, -0.0070,
         0.0050,  0.0030], device='cuda:0')
selected experts tensor([1559, 1675, 1711, 1602, 1717, 1499, 1649, 1618, 1628, 1726],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4914, 0.4947, 0.5145,  ..., 0.5990, 0.5552, 0.6747],
          [0.3915, 0.5138, 0.5821,  ..., 0.4657, 0.7177, 0.5173],
          [0.4848, 0.4305, 0.5325,  ..., 0.4784, 0.4892, 0.6064],
          [0.4723, 0.5230, 0.4989,  ..., 0.5280, 0.4741, 0.3974]],

         [[0.4345, 0.5682, 0.5002,  ..., 0.5186, 0.5214, 0.5874],
          [0.4259, 0.4558, 0.4441,  ..., 0.4284, 0.5482, 0.5284],
          [0.4797, 0.5611, 0.6062,  ..., 0.5919, 0.4328, 0.4216],
          [0.7245, 0.4748, 0.5968,  ..., 0.5469, 0.4262, 0.4520]],

         [[0.5656, 0.4493, 0.5203,  ..., 0.4780, 0.6325, 0.5775],
          [0.5403, 0.6294, 0.5806,  ..., 0.6149, 0.6101, 0.5134],
          [0.5164, 0.4755, 0.5559,  ..., 0.4884, 0.5555, 0.5536],
          [0.3632, 0.4290, 0.3708,  ..., 0.5077, 0.4423, 0.3397]],

         ...,

         [[0.5606, 0.6194, 0.5944,  ..., 0.3733, 0.5141, 0.5538],
          [0.4089, 0.4464, 0.5261,  ..., 0.5364, 0.5859, 0.5458],
          [0.4772, 0.3893, 0.4438,  ..., 0.5570, 0.4872, 0.5758],
          [0.4553, 0.5221, 0.4974,  ..., 0.4544, 0.4247, 0.4057]],

         [[0.6125, 0.5502, 0.5578,  ..., 0.5695, 0.5642, 0.5029],
          [0.4998, 0.5003, 0.6502,  ..., 0.5546, 0.5623, 0.6405],
          [0.4197, 0.3842, 0.4400,  ..., 0.5473, 0.3629, 0.4720],
          [0.5555, 0.5299, 0.4680,  ..., 0.6084, 0.4748, 0.4127]],

         [[0.4696, 0.4435, 0.5261,  ..., 0.5101, 0.4836, 0.5609],
          [0.4131, 0.5999, 0.6169,  ..., 0.4922, 0.6325, 0.5621],
          [0.5444, 0.3708, 0.5392,  ..., 0.6283, 0.5338, 0.5722],
          [0.4688, 0.5145, 0.5486,  ..., 0.5558, 0.4107, 0.3919]]],


        [[[0.4098, 0.6794, 0.4928,  ..., 0.5461, 0.5302, 0.4886],
          [0.4989, 0.6175, 0.5665,  ..., 0.5165, 0.6073, 0.4903],
          [0.4463, 0.5493, 0.3845,  ..., 0.4563, 0.5543, 0.4929],
          [0.5032, 0.3763, 0.5312,  ..., 0.4597, 0.5443, 0.6007]],

         [[0.4875, 0.6456, 0.4247,  ..., 0.4327, 0.5882, 0.4258],
          [0.5149, 0.4835, 0.5368,  ..., 0.6283, 0.4936, 0.4858],
          [0.4789, 0.5891, 0.4943,  ..., 0.4308, 0.3593, 0.4773],
          [0.5031, 0.3781, 0.3672,  ..., 0.5062, 0.4009, 0.5587]],

         [[0.4317, 0.5294, 0.5643,  ..., 0.4914, 0.3817, 0.4640],
          [0.5895, 0.5227, 0.5720,  ..., 0.6338, 0.4309, 0.5869],
          [0.3453, 0.5644, 0.6146,  ..., 0.4500, 0.5077, 0.4951],
          [0.5495, 0.4081, 0.4166,  ..., 0.5269, 0.4009, 0.5864]],

         ...,

         [[0.5198, 0.3781, 0.4110,  ..., 0.4256, 0.5552, 0.6568],
          [0.5104, 0.5357, 0.4861,  ..., 0.4869, 0.6068, 0.5922],
          [0.5089, 0.5122, 0.4453,  ..., 0.4413, 0.4841, 0.4623],
          [0.4070, 0.4915, 0.4157,  ..., 0.4797, 0.5472, 0.5341]],

         [[0.5098, 0.4747, 0.4692,  ..., 0.5258, 0.5892, 0.5998],
          [0.4089, 0.5369, 0.4299,  ..., 0.6883, 0.5978, 0.5893],
          [0.4676, 0.4360, 0.4299,  ..., 0.4534, 0.5426, 0.6550],
          [0.4512, 0.4768, 0.5411,  ..., 0.5196, 0.5149, 0.4698]],

         [[0.6408, 0.5768, 0.5238,  ..., 0.5355, 0.6476, 0.7081],
          [0.4004, 0.6221, 0.6216,  ..., 0.6070, 0.5053, 0.5392],
          [0.4967, 0.4396, 0.5013,  ..., 0.5345, 0.4574, 0.4667],
          [0.6003, 0.5929, 0.5696,  ..., 0.4679, 0.4200, 0.5305]]]],
       device='cuda:0')
tensor([[[[0.4894, 0.4987, 0.5085,  ..., 0.5950, 0.5392, 0.6567],
          [0.3895, 0.5178, 0.5761,  ..., 0.4617, 0.7017, 0.4993],
          [0.4828, 0.4345, 0.5265,  ..., 0.4744, 0.4732, 0.5884],
          [0.4703, 0.5270, 0.4929,  ..., 0.5240, 0.4581, 0.3794]],

         [[0.4325, 0.5722, 0.4942,  ..., 0.5146, 0.5054, 0.5694],
          [0.4239, 0.4598, 0.4381,  ..., 0.4244, 0.5322, 0.5104],
          [0.4777, 0.5651, 0.6002,  ..., 0.5879, 0.4168, 0.4036],
          [0.7225, 0.4788, 0.5908,  ..., 0.5429, 0.4102, 0.4340]],

         [[0.5636, 0.4533, 0.5143,  ..., 0.4740, 0.6165, 0.5595],
          [0.5383, 0.6334, 0.5746,  ..., 0.6109, 0.5941, 0.4954],
          [0.5144, 0.4795, 0.5499,  ..., 0.4844, 0.5395, 0.5356],
          [0.3612, 0.4330, 0.3648,  ..., 0.5037, 0.4263, 0.3217]],

         ...,

         [[0.5586, 0.6234, 0.5884,  ..., 0.3693, 0.4981, 0.5358],
          [0.4069, 0.4504, 0.5201,  ..., 0.5324, 0.5699, 0.5278],
          [0.4752, 0.3933, 0.4378,  ..., 0.5530, 0.4712, 0.5578],
          [0.4533, 0.5261, 0.4914,  ..., 0.4504, 0.4087, 0.3877]],

         [[0.6105, 0.5542, 0.5518,  ..., 0.5655, 0.5482, 0.4849],
          [0.4978, 0.5043, 0.6442,  ..., 0.5506, 0.5463, 0.6225],
          [0.4177, 0.3882, 0.4340,  ..., 0.5433, 0.3469, 0.4540],
          [0.5535, 0.5339, 0.4620,  ..., 0.6044, 0.4588, 0.3947]],

         [[0.4676, 0.4475, 0.5201,  ..., 0.5061, 0.4676, 0.5429],
          [0.4111, 0.6039, 0.6109,  ..., 0.4882, 0.6165, 0.5441],
          [0.5424, 0.3748, 0.5332,  ..., 0.6243, 0.5178, 0.5542],
          [0.4668, 0.5185, 0.5426,  ..., 0.5518, 0.3947, 0.3739]]],


        [[[0.4078, 0.6834, 0.4868,  ..., 0.5421, 0.5142, 0.4706],
          [0.4969, 0.6215, 0.5605,  ..., 0.5125, 0.5913, 0.4723],
          [0.4443, 0.5533, 0.3785,  ..., 0.4523, 0.5383, 0.4749],
          [0.5012, 0.3803, 0.5252,  ..., 0.4557, 0.5283, 0.5827]],

         [[0.4855, 0.6496, 0.4187,  ..., 0.4287, 0.5722, 0.4078],
          [0.5129, 0.4875, 0.5308,  ..., 0.6243, 0.4776, 0.4678],
          [0.4769, 0.5931, 0.4883,  ..., 0.4268, 0.3433, 0.4593],
          [0.5011, 0.3821, 0.3612,  ..., 0.5022, 0.3849, 0.5407]],

         [[0.4297, 0.5334, 0.5583,  ..., 0.4874, 0.3657, 0.4460],
          [0.5875, 0.5267, 0.5660,  ..., 0.6298, 0.4149, 0.5689],
          [0.3433, 0.5684, 0.6086,  ..., 0.4460, 0.4917, 0.4771],
          [0.5475, 0.4121, 0.4106,  ..., 0.5229, 0.3849, 0.5684]],

         ...,

         [[0.5178, 0.3821, 0.4050,  ..., 0.4216, 0.5392, 0.6388],
          [0.5084, 0.5397, 0.4801,  ..., 0.4829, 0.5908, 0.5742],
          [0.5069, 0.5162, 0.4393,  ..., 0.4373, 0.4681, 0.4443],
          [0.4050, 0.4955, 0.4097,  ..., 0.4757, 0.5312, 0.5161]],

         [[0.5078, 0.4787, 0.4632,  ..., 0.5218, 0.5732, 0.5818],
          [0.4069, 0.5409, 0.4239,  ..., 0.6843, 0.5818, 0.5713],
          [0.4656, 0.4400, 0.4239,  ..., 0.4494, 0.5266, 0.6370],
          [0.4492, 0.4808, 0.5351,  ..., 0.5156, 0.4989, 0.4518]],

         [[0.6388, 0.5808, 0.5178,  ..., 0.5315, 0.6316, 0.6901],
          [0.3984, 0.6261, 0.6156,  ..., 0.6030, 0.4893, 0.5212],
          [0.4947, 0.4436, 0.4953,  ..., 0.5305, 0.4414, 0.4487],
          [0.5983, 0.5969, 0.5636,  ..., 0.4639, 0.4040, 0.5125]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0020, -0.0040,  0.0060, -0.0160, -0.0040,  0.0140, -0.0340,  0.0040,
         0.0160,  0.0180], device='cuda:0')
selected experts tensor([1502, 1472, 1111, 2144, 1271, 1959, 1930, 1681, 1691, 1623],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.3825, 0.4112, 0.5492,  ..., 0.4796, 0.3510, 0.4017],
          [0.4927, 0.4484, 0.6515,  ..., 0.5576, 0.3967, 0.3915],
          [0.5375, 0.3310, 0.4831,  ..., 0.5827, 0.5147, 0.5111],
          [0.6788, 0.5704, 0.4773,  ..., 0.3886, 0.4367, 0.3575]],

         [[0.5547, 0.5571, 0.5097,  ..., 0.2729, 0.5381, 0.6981],
          [0.5795, 0.4770, 0.6021,  ..., 0.4393, 0.5726, 0.3674],
          [0.5118, 0.4750, 0.4803,  ..., 0.4060, 0.4638, 0.5203],
          [0.4940, 0.5103, 0.4261,  ..., 0.3950, 0.6114, 0.4540]],

         [[0.3162, 0.4221, 0.4889,  ..., 0.5048, 0.4048, 0.5094],
          [0.4845, 0.4472, 0.6568,  ..., 0.6161, 0.5593, 0.4224],
          [0.5676, 0.4121, 0.4667,  ..., 0.5750, 0.6823, 0.6369],
          [0.6029, 0.5453, 0.4862,  ..., 0.4369, 0.4619, 0.5005]],

         ...,

         [[0.4920, 0.4920, 0.4940,  ..., 0.5220, 0.4880, 0.5080],
          [0.4920, 0.4920, 0.4940,  ..., 0.5220, 0.4880, 0.5080],
          [0.4920, 0.4920, 0.4940,  ..., 0.5220, 0.4880, 0.5080],
          [0.4920, 0.4920, 0.4940,  ..., 0.5220, 0.4880, 0.5080]],

         [[0.4920, 0.4920, 0.4940,  ..., 0.5220, 0.4880, 0.5080],
          [0.4920, 0.4920, 0.4940,  ..., 0.5220, 0.4880, 0.5080],
          [0.4920, 0.4920, 0.4940,  ..., 0.5220, 0.4880, 0.5080],
          [0.4920, 0.4920, 0.4940,  ..., 0.5220, 0.4880, 0.5080]],

         [[0.4920, 0.4920, 0.4940,  ..., 0.5220, 0.4880, 0.5080],
          [0.4920, 0.4920, 0.4940,  ..., 0.5220, 0.4880, 0.5080],
          [0.4920, 0.4920, 0.4940,  ..., 0.5220, 0.4880, 0.5080],
          [0.4920, 0.4920, 0.4940,  ..., 0.5220, 0.4880, 0.5080]]],


        [[[0.4022, 0.4791, 0.3794,  ..., 0.5011, 0.5282, 0.5121],
          [0.3974, 0.5628, 0.6155,  ..., 0.4724, 0.4273, 0.4832],
          [0.5438, 0.5884, 0.4616,  ..., 0.5256, 0.5260, 0.4927],
          [0.5575, 0.5322, 0.5521,  ..., 0.4469, 0.4938, 0.4490]],

         [[0.5082, 0.4954, 0.5219,  ..., 0.5442, 0.3715, 0.4705],
          [0.5293, 0.6145, 0.5658,  ..., 0.5607, 0.6420, 0.5269],
          [0.5983, 0.4097, 0.5139,  ..., 0.6536, 0.5807, 0.6073],
          [0.6254, 0.5149, 0.5335,  ..., 0.4181, 0.5499, 0.4482]],

         [[0.5205, 0.5494, 0.5117,  ..., 0.4745, 0.4316, 0.4182],
          [0.4305, 0.6460, 0.5610,  ..., 0.5247, 0.5664, 0.4671],
          [0.5590, 0.4250, 0.2584,  ..., 0.4963, 0.4244, 0.6629],
          [0.5373, 0.5580, 0.6672,  ..., 0.3751, 0.4709, 0.4543]],

         ...,

         [[0.4920, 0.4920, 0.4940,  ..., 0.5220, 0.4880, 0.5080],
          [0.4920, 0.4920, 0.4940,  ..., 0.5220, 0.4880, 0.5080],
          [0.4920, 0.4920, 0.4940,  ..., 0.5220, 0.4880, 0.5080],
          [0.4920, 0.4920, 0.4940,  ..., 0.5220, 0.4880, 0.5080]],

         [[0.4920, 0.4920, 0.4940,  ..., 0.5220, 0.4880, 0.5080],
          [0.4920, 0.4920, 0.4940,  ..., 0.5220, 0.4880, 0.5080],
          [0.4920, 0.4920, 0.4940,  ..., 0.5220, 0.4880, 0.5080],
          [0.4920, 0.4920, 0.4940,  ..., 0.5220, 0.4880, 0.5080]],

         [[0.4920, 0.4920, 0.4940,  ..., 0.5220, 0.4880, 0.5080],
          [0.4920, 0.4920, 0.4940,  ..., 0.5220, 0.4880, 0.5080],
          [0.4920, 0.4920, 0.4940,  ..., 0.5220, 0.4880, 0.5080],
          [0.4920, 0.4920, 0.4940,  ..., 0.5220, 0.4880, 0.5080]]]],
       device='cuda:0')
tensor([[[[0.3905, 0.4192, 0.5552,  ..., 0.4576, 0.3630, 0.3937],
          [0.5007, 0.4564, 0.6575,  ..., 0.5356, 0.4087, 0.3835],
          [0.5455, 0.3390, 0.4891,  ..., 0.5607, 0.5267, 0.5031],
          [0.6868, 0.5784, 0.4833,  ..., 0.3666, 0.4487, 0.3495]],

         [[0.5627, 0.5651, 0.5157,  ..., 0.2509, 0.5501, 0.6901],
          [0.5875, 0.4850, 0.6081,  ..., 0.4173, 0.5846, 0.3594],
          [0.5198, 0.4830, 0.4863,  ..., 0.3840, 0.4758, 0.5123],
          [0.5020, 0.5183, 0.4321,  ..., 0.3730, 0.6234, 0.4460]],

         [[0.3242, 0.4301, 0.4949,  ..., 0.4828, 0.4168, 0.5014],
          [0.4925, 0.4552, 0.6628,  ..., 0.5941, 0.5713, 0.4144],
          [0.5756, 0.4201, 0.4727,  ..., 0.5530, 0.6943, 0.6289],
          [0.6109, 0.5533, 0.4922,  ..., 0.4149, 0.4739, 0.4925]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.4102, 0.4871, 0.3854,  ..., 0.4791, 0.5402, 0.5041],
          [0.4054, 0.5708, 0.6215,  ..., 0.4504, 0.4393, 0.4752],
          [0.5518, 0.5964, 0.4676,  ..., 0.5036, 0.5380, 0.4847],
          [0.5655, 0.5402, 0.5581,  ..., 0.4249, 0.5058, 0.4410]],

         [[0.5162, 0.5034, 0.5279,  ..., 0.5222, 0.3835, 0.4625],
          [0.5373, 0.6225, 0.5718,  ..., 0.5387, 0.6540, 0.5189],
          [0.6063, 0.4177, 0.5199,  ..., 0.6316, 0.5927, 0.5993],
          [0.6334, 0.5229, 0.5395,  ..., 0.3961, 0.5619, 0.4402]],

         [[0.5285, 0.5574, 0.5177,  ..., 0.4525, 0.4436, 0.4102],
          [0.4385, 0.6540, 0.5670,  ..., 0.5027, 0.5784, 0.4591],
          [0.5670, 0.4330, 0.2644,  ..., 0.4743, 0.4364, 0.6549],
          [0.5453, 0.5660, 0.6732,  ..., 0.3531, 0.4829, 0.4463]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([-0.0080, -0.0080, -0.0060,  0.0040, -0.0100,  0.0240,  0.0040,  0.0220,
        -0.0120,  0.0080], device='cuda:0')
selected experts tensor([2009, 1998, 1995, 1840, 1775,  793, 1683,  892, 1678, 1721],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1559., 1675., 1711., 1602., 1717., 1499., 1649., 1618., 1628., 1726.],
        [2009., 1998., 1995., 1840., 1775.,  793., 1683.,  892., 1678., 1721.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4893, 0.5762, 0.4385,  ..., 0.5602, 0.6717, 0.5271],
          [0.2818, 0.5137, 0.5358,  ..., 0.5554, 0.6534, 0.6158],
          [0.3387, 0.4579, 0.5551,  ..., 0.4556, 0.5618, 0.5173],
          [0.4115, 0.4259, 0.4064,  ..., 0.4711, 0.5434, 0.5245]],

         [[0.3936, 0.6426, 0.5324,  ..., 0.5622, 0.3818, 0.4934],
          [0.5464, 0.5325, 0.4443,  ..., 0.3914, 0.4264, 0.5483],
          [0.4901, 0.5407, 0.6362,  ..., 0.6591, 0.4888, 0.4808],
          [0.4847, 0.6648, 0.4562,  ..., 0.3467, 0.4591, 0.3500]],

         [[0.4807, 0.4684, 0.3086,  ..., 0.4829, 0.4341, 0.5762],
          [0.4864, 0.5463, 0.3310,  ..., 0.3192, 0.4126, 0.5762],
          [0.4452, 0.5543, 0.5518,  ..., 0.4850, 0.3383, 0.4947],
          [0.3274, 0.4832, 0.4102,  ..., 0.5046, 0.4613, 0.2862]],

         ...,

         [[0.4394, 0.3331, 0.4721,  ..., 0.5759, 0.5642, 0.4848],
          [0.5977, 0.5342, 0.6263,  ..., 0.5691, 0.4875, 0.4619],
          [0.5602, 0.4365, 0.4976,  ..., 0.5394, 0.4701, 0.5189],
          [0.4563, 0.5196, 0.4327,  ..., 0.5744, 0.5006, 0.5655]],

         [[0.5124, 0.5728, 0.5628,  ..., 0.5044, 0.5043, 0.4839],
          [0.4614, 0.6092, 0.4453,  ..., 0.4011, 0.4843, 0.4332],
          [0.4476, 0.5053, 0.4112,  ..., 0.4450, 0.5914, 0.4956],
          [0.4382, 0.4103, 0.5307,  ..., 0.5677, 0.6300, 0.3977]],

         [[0.5326, 0.4726, 0.5595,  ..., 0.6775, 0.6254, 0.6392],
          [0.3801, 0.5747, 0.5016,  ..., 0.6740, 0.4635, 0.4481],
          [0.4756, 0.5015, 0.4017,  ..., 0.8017, 0.4107, 0.5777],
          [0.5712, 0.5304, 0.4164,  ..., 0.3260, 0.5704, 0.5929]]],


        [[[0.3565, 0.5584, 0.5200,  ..., 0.5038, 0.5003, 0.5406],
          [0.4243, 0.4331, 0.4269,  ..., 0.4619, 0.5317, 0.4594],
          [0.4234, 0.3462, 0.4107,  ..., 0.6109, 0.4468, 0.3973],
          [0.4720, 0.4954, 0.4753,  ..., 0.5032, 0.5647, 0.5700]],

         [[0.6839, 0.4630, 0.4996,  ..., 0.6043, 0.5071, 0.4488],
          [0.3682, 0.5210, 0.5614,  ..., 0.4114, 0.5776, 0.4232],
          [0.4404, 0.5610, 0.4334,  ..., 0.4783, 0.4782, 0.4817],
          [0.6103, 0.4867, 0.3205,  ..., 0.5224, 0.4207, 0.4175]],

         [[0.5484, 0.4321, 0.4007,  ..., 0.5118, 0.6354, 0.4109],
          [0.4868, 0.5975, 0.4998,  ..., 0.6520, 0.6309, 0.3697],
          [0.6814, 0.5862, 0.3792,  ..., 0.5701, 0.6735, 0.3788],
          [0.3405, 0.6426, 0.4334,  ..., 0.4472, 0.4150, 0.4553]],

         ...,

         [[0.4191, 0.5254, 0.5523,  ..., 0.4419, 0.5651, 0.5534],
          [0.5365, 0.5914, 0.4625,  ..., 0.4612, 0.4264, 0.6051],
          [0.5784, 0.5519, 0.6335,  ..., 0.4969, 0.4089, 0.6274],
          [0.4777, 0.5359, 0.5955,  ..., 0.6679, 0.3623, 0.3779]],

         [[0.3601, 0.4379, 0.4817,  ..., 0.4985, 0.5053, 0.4399],
          [0.5484, 0.5838, 0.4797,  ..., 0.6243, 0.4635, 0.4099],
          [0.6431, 0.3418, 0.5903,  ..., 0.3845, 0.4389, 0.3968],
          [0.5392, 0.3878, 0.4669,  ..., 0.7507, 0.5685, 0.4113]],

         [[0.4748, 0.4369, 0.6344,  ..., 0.5479, 0.2648, 0.5464],
          [0.4101, 0.4927, 0.5416,  ..., 0.6340, 0.4931, 0.3922],
          [0.6634, 0.5260, 0.4944,  ..., 0.4096, 0.3650, 0.4194],
          [0.5175, 0.5252, 0.5209,  ..., 0.7355, 0.5280, 0.2760]]]],
       device='cuda:0')
tensor([[[[0.4913, 0.5742, 0.4465,  ..., 0.5542, 0.6697, 0.5231],
          [0.2838, 0.5117, 0.5438,  ..., 0.5494, 0.6514, 0.6118],
          [0.3407, 0.4559, 0.5631,  ..., 0.4496, 0.5598, 0.5133],
          [0.4135, 0.4239, 0.4144,  ..., 0.4651, 0.5414, 0.5205]],

         [[0.3956, 0.6406, 0.5404,  ..., 0.5562, 0.3798, 0.4894],
          [0.5484, 0.5305, 0.4523,  ..., 0.3854, 0.4244, 0.5443],
          [0.4921, 0.5387, 0.6442,  ..., 0.6531, 0.4868, 0.4768],
          [0.4867, 0.6628, 0.4642,  ..., 0.3407, 0.4571, 0.3460]],

         [[0.4827, 0.4664, 0.3166,  ..., 0.4769, 0.4321, 0.5722],
          [0.4884, 0.5443, 0.3390,  ..., 0.3132, 0.4106, 0.5722],
          [0.4472, 0.5523, 0.5598,  ..., 0.4790, 0.3363, 0.4907],
          [0.3294, 0.4812, 0.4182,  ..., 0.4986, 0.4593, 0.2822]],

         ...,

         [[0.4414, 0.3311, 0.4801,  ..., 0.5699, 0.5622, 0.4808],
          [0.5997, 0.5322, 0.6343,  ..., 0.5631, 0.4855, 0.4579],
          [0.5622, 0.4345, 0.5056,  ..., 0.5334, 0.4681, 0.5149],
          [0.4583, 0.5176, 0.4407,  ..., 0.5684, 0.4986, 0.5615]],

         [[0.5144, 0.5708, 0.5708,  ..., 0.4984, 0.5023, 0.4799],
          [0.4634, 0.6072, 0.4533,  ..., 0.3951, 0.4823, 0.4292],
          [0.4496, 0.5033, 0.4192,  ..., 0.4390, 0.5894, 0.4916],
          [0.4402, 0.4083, 0.5387,  ..., 0.5617, 0.6280, 0.3937]],

         [[0.5346, 0.4706, 0.5675,  ..., 0.6715, 0.6234, 0.6352],
          [0.3821, 0.5727, 0.5096,  ..., 0.6680, 0.4615, 0.4441],
          [0.4776, 0.4995, 0.4097,  ..., 0.7957, 0.4087, 0.5737],
          [0.5732, 0.5284, 0.4244,  ..., 0.3200, 0.5684, 0.5889]]],


        [[[0.3585, 0.5564, 0.5280,  ..., 0.4978, 0.4983, 0.5366],
          [0.4263, 0.4311, 0.4349,  ..., 0.4559, 0.5297, 0.4554],
          [0.4254, 0.3442, 0.4187,  ..., 0.6049, 0.4448, 0.3933],
          [0.4740, 0.4934, 0.4833,  ..., 0.4972, 0.5627, 0.5660]],

         [[0.6859, 0.4610, 0.5076,  ..., 0.5983, 0.5051, 0.4448],
          [0.3702, 0.5190, 0.5694,  ..., 0.4054, 0.5756, 0.4192],
          [0.4424, 0.5590, 0.4414,  ..., 0.4723, 0.4762, 0.4777],
          [0.6123, 0.4847, 0.3285,  ..., 0.5164, 0.4187, 0.4135]],

         [[0.5504, 0.4301, 0.4087,  ..., 0.5058, 0.6334, 0.4069],
          [0.4888, 0.5955, 0.5078,  ..., 0.6460, 0.6289, 0.3657],
          [0.6834, 0.5842, 0.3872,  ..., 0.5641, 0.6715, 0.3748],
          [0.3425, 0.6406, 0.4414,  ..., 0.4412, 0.4130, 0.4513]],

         ...,

         [[0.4211, 0.5234, 0.5603,  ..., 0.4359, 0.5631, 0.5494],
          [0.5385, 0.5894, 0.4705,  ..., 0.4552, 0.4244, 0.6011],
          [0.5804, 0.5499, 0.6415,  ..., 0.4909, 0.4069, 0.6234],
          [0.4797, 0.5339, 0.6035,  ..., 0.6619, 0.3603, 0.3739]],

         [[0.3621, 0.4359, 0.4897,  ..., 0.4925, 0.5033, 0.4359],
          [0.5504, 0.5818, 0.4877,  ..., 0.6183, 0.4615, 0.4059],
          [0.6451, 0.3398, 0.5983,  ..., 0.3785, 0.4369, 0.3928],
          [0.5412, 0.3858, 0.4749,  ..., 0.7447, 0.5665, 0.4073]],

         [[0.4768, 0.4349, 0.6424,  ..., 0.5419, 0.2628, 0.5424],
          [0.4121, 0.4907, 0.5496,  ..., 0.6280, 0.4911, 0.3882],
          [0.6654, 0.5240, 0.5024,  ..., 0.4036, 0.3630, 0.4154],
          [0.5195, 0.5232, 0.5289,  ..., 0.7295, 0.5260, 0.2720]]]],
       device='cuda:0', requires_grad=True)
tensor([-2.0000e-03,  2.0000e-03, -8.0000e-03, -2.3283e-10, -2.0000e-03,
        -6.0000e-03,  6.0000e-03,  6.0000e-03,  2.0000e-03,  4.0000e-03],
       device='cuda:0')
selected experts tensor([1815, 1711, 1474, 1718, 1773, 1580, 1694, 1678, 1521, 1420],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5954, 0.4493, 0.5790,  ..., 0.4587, 0.5873, 0.5847],
          [0.4576, 0.5435, 0.5709,  ..., 0.4393, 0.4840, 0.4307],
          [0.3431, 0.4565, 0.5249,  ..., 0.5686, 0.4827, 0.5833],
          [0.4708, 0.5018, 0.4891,  ..., 0.4929, 0.5622, 0.6078]],

         [[0.4462, 0.5507, 0.4630,  ..., 0.5138, 0.6953, 0.5095],
          [0.4517, 0.5920, 0.5814,  ..., 0.4886, 0.6062, 0.6896],
          [0.4883, 0.4682, 0.4341,  ..., 0.3642, 0.4965, 0.4037],
          [0.4922, 0.5740, 0.5695,  ..., 0.4567, 0.4860, 0.5083]],

         [[0.4129, 0.3092, 0.5524,  ..., 0.3915, 0.5892, 0.4365],
          [0.5949, 0.4792, 0.5804,  ..., 0.4861, 0.5071, 0.4623],
          [0.4836, 0.4365, 0.5458,  ..., 0.3525, 0.5754, 0.5286],
          [0.4063, 0.5444, 0.4769,  ..., 0.4815, 0.6978, 0.4298]],

         ...,

         [[0.6089, 0.3662, 0.6426,  ..., 0.5605, 0.3573, 0.3939],
          [0.3783, 0.4539, 0.5574,  ..., 0.4897, 0.4338, 0.5154],
          [0.5032, 0.3644, 0.5742,  ..., 0.5066, 0.4752, 0.4384],
          [0.5621, 0.5352, 0.3668,  ..., 0.5461, 0.3681, 0.4531]],

         [[0.4758, 0.4989, 0.5011,  ..., 0.5369, 0.5174, 0.5521],
          [0.5083, 0.6152, 0.4245,  ..., 0.6063, 0.4390, 0.6345],
          [0.5288, 0.4295, 0.5876,  ..., 0.5248, 0.5338, 0.2858],
          [0.4730, 0.7042, 0.5330,  ..., 0.5890, 0.4973, 0.4084]],

         [[0.2810, 0.3949, 0.4659,  ..., 0.4685, 0.2954, 0.4541],
          [0.5616, 0.4670, 0.3480,  ..., 0.4156, 0.3476, 0.3976],
          [0.4576, 0.5418, 0.5790,  ..., 0.5155, 0.6188, 0.3102],
          [0.5183, 0.4548, 0.5393,  ..., 0.4156, 0.4357, 0.5257]]],


        [[[0.4698, 0.5035, 0.5933,  ..., 0.5648, 0.4195, 0.4060],
          [0.4344, 0.3289, 0.5139,  ..., 0.4350, 0.5868, 0.3722],
          [0.5002, 0.5466, 0.4317,  ..., 0.5332, 0.5056, 0.5249],
          [0.6467, 0.3653, 0.4023,  ..., 0.5122, 0.4764, 0.5073]],

         [[0.5098, 0.6339, 0.3331,  ..., 0.4136, 0.4129, 0.6336],
          [0.4537, 0.3438, 0.4882,  ..., 0.5095, 0.4535, 0.4950],
          [0.5883, 0.4066, 0.3985,  ..., 0.4461, 0.5036, 0.4298],
          [0.3583, 0.6614, 0.3934,  ..., 0.3919, 0.6132, 0.5359]],

         [[0.4157, 0.5744, 0.2573,  ..., 0.5135, 0.5835, 0.5134],
          [0.4404, 0.5211, 0.5704,  ..., 0.5970, 0.4171, 0.3750],
          [0.4082, 0.5716, 0.5366,  ..., 0.5102, 0.4998, 0.6613],
          [0.4530, 0.6482, 0.5208,  ..., 0.4374, 0.3854, 0.4278]],

         ...,

         [[0.4663, 0.4616, 0.5283,  ..., 0.4444, 0.5071, 0.3731],
          [0.2593, 0.4784, 0.5014,  ..., 0.4808, 0.6312, 0.4760],
          [0.4710, 0.5234, 0.5980,  ..., 0.4980, 0.4765, 0.4415],
          [0.5702, 0.5802, 0.5332,  ..., 0.5264, 0.5015, 0.5615]],

         [[0.5129, 0.5517, 0.4401,  ..., 0.4652, 0.4765, 0.4874],
          [0.3917, 0.5239, 0.4833,  ..., 0.4722, 0.6141, 0.3768],
          [0.4734, 0.2750, 0.4674,  ..., 0.3525, 0.5325, 0.3939],
          [0.4215, 0.6438, 0.2371,  ..., 0.3868, 0.4629, 0.3578]],

         [[0.5078, 0.4214, 0.5190,  ..., 0.7054, 0.5634, 0.4470],
          [0.3806, 0.6069, 0.6345,  ..., 0.5286, 0.4508, 0.4808],
          [0.4595, 0.4413, 0.4676,  ..., 0.4208, 0.6216, 0.7276],
          [0.5117, 0.5180, 0.3786,  ..., 0.3140, 0.4039, 0.5994]]]],
       device='cuda:0')
tensor([[[[0.5974, 0.4533, 0.5770,  ..., 0.4647, 0.5813, 0.5827],
          [0.4596, 0.5475, 0.5689,  ..., 0.4453, 0.4780, 0.4287],
          [0.3451, 0.4605, 0.5229,  ..., 0.5746, 0.4767, 0.5813],
          [0.4728, 0.5058, 0.4871,  ..., 0.4989, 0.5562, 0.6058]],

         [[0.4482, 0.5547, 0.4610,  ..., 0.5198, 0.6893, 0.5075],
          [0.4537, 0.5960, 0.5794,  ..., 0.4946, 0.6002, 0.6876],
          [0.4903, 0.4722, 0.4321,  ..., 0.3702, 0.4905, 0.4017],
          [0.4942, 0.5780, 0.5675,  ..., 0.4627, 0.4800, 0.5063]],

         [[0.4149, 0.3132, 0.5504,  ..., 0.3975, 0.5832, 0.4345],
          [0.5969, 0.4832, 0.5784,  ..., 0.4921, 0.5011, 0.4603],
          [0.4856, 0.4405, 0.5438,  ..., 0.3585, 0.5694, 0.5266],
          [0.4083, 0.5484, 0.4749,  ..., 0.4875, 0.6918, 0.4278]],

         ...,

         [[0.6109, 0.3702, 0.6406,  ..., 0.5665, 0.3513, 0.3919],
          [0.3803, 0.4579, 0.5554,  ..., 0.4957, 0.4278, 0.5134],
          [0.5052, 0.3684, 0.5722,  ..., 0.5126, 0.4692, 0.4364],
          [0.5641, 0.5392, 0.3648,  ..., 0.5521, 0.3621, 0.4511]],

         [[0.4778, 0.5029, 0.4991,  ..., 0.5429, 0.5114, 0.5501],
          [0.5103, 0.6192, 0.4225,  ..., 0.6123, 0.4330, 0.6325],
          [0.5308, 0.4335, 0.5856,  ..., 0.5308, 0.5278, 0.2838],
          [0.4750, 0.7082, 0.5310,  ..., 0.5950, 0.4913, 0.4064]],

         [[0.2830, 0.3989, 0.4639,  ..., 0.4745, 0.2894, 0.4521],
          [0.5636, 0.4710, 0.3460,  ..., 0.4216, 0.3416, 0.3956],
          [0.4596, 0.5458, 0.5770,  ..., 0.5215, 0.6128, 0.3082],
          [0.5203, 0.4588, 0.5373,  ..., 0.4216, 0.4297, 0.5237]]],


        [[[0.4718, 0.5075, 0.5913,  ..., 0.5708, 0.4135, 0.4040],
          [0.4364, 0.3329, 0.5119,  ..., 0.4410, 0.5808, 0.3702],
          [0.5022, 0.5506, 0.4297,  ..., 0.5392, 0.4996, 0.5229],
          [0.6487, 0.3693, 0.4003,  ..., 0.5182, 0.4704, 0.5053]],

         [[0.5118, 0.6379, 0.3311,  ..., 0.4196, 0.4069, 0.6316],
          [0.4557, 0.3478, 0.4862,  ..., 0.5155, 0.4475, 0.4930],
          [0.5903, 0.4106, 0.3965,  ..., 0.4521, 0.4976, 0.4278],
          [0.3603, 0.6654, 0.3914,  ..., 0.3979, 0.6072, 0.5339]],

         [[0.4177, 0.5784, 0.2553,  ..., 0.5195, 0.5775, 0.5114],
          [0.4424, 0.5251, 0.5684,  ..., 0.6030, 0.4111, 0.3730],
          [0.4102, 0.5756, 0.5346,  ..., 0.5162, 0.4938, 0.6593],
          [0.4550, 0.6522, 0.5188,  ..., 0.4434, 0.3794, 0.4258]],

         ...,

         [[0.4683, 0.4656, 0.5263,  ..., 0.4504, 0.5011, 0.3711],
          [0.2613, 0.4824, 0.4994,  ..., 0.4868, 0.6252, 0.4740],
          [0.4730, 0.5274, 0.5960,  ..., 0.5040, 0.4705, 0.4395],
          [0.5722, 0.5842, 0.5312,  ..., 0.5324, 0.4955, 0.5595]],

         [[0.5149, 0.5557, 0.4381,  ..., 0.4712, 0.4705, 0.4854],
          [0.3937, 0.5279, 0.4813,  ..., 0.4782, 0.6081, 0.3748],
          [0.4754, 0.2790, 0.4654,  ..., 0.3585, 0.5265, 0.3919],
          [0.4235, 0.6478, 0.2351,  ..., 0.3928, 0.4569, 0.3558]],

         [[0.5098, 0.4254, 0.5170,  ..., 0.7114, 0.5574, 0.4450],
          [0.3826, 0.6109, 0.6325,  ..., 0.5346, 0.4448, 0.4788],
          [0.4615, 0.4453, 0.4656,  ..., 0.4268, 0.6156, 0.7256],
          [0.5137, 0.5220, 0.3766,  ..., 0.3200, 0.3979, 0.5974]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0020, -0.0040,  0.0020,  0.0100, -0.0020,  0.0080, -0.0100, -0.0060,
         0.0060,  0.0020], device='cuda:0')
selected experts tensor([1602, 1483, 1618, 1676, 1561, 1753, 1476, 1718, 1871, 1626],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4995, 0.5406, 0.3878,  ..., 0.4184, 0.5062, 0.5626],
          [0.5114, 0.4315, 0.5503,  ..., 0.4360, 0.4504, 0.5473],
          [0.6409, 0.4856, 0.4646,  ..., 0.5961, 0.3672, 0.3264],
          [0.6822, 0.5097, 0.5455,  ..., 0.5834, 0.3513, 0.4660]],

         [[0.5867, 0.5939, 0.4914,  ..., 0.5757, 0.5394, 0.5994],
          [0.4879, 0.5321, 0.6239,  ..., 0.5315, 0.5174, 0.5394],
          [0.5051, 0.5367, 0.4540,  ..., 0.6172, 0.4428, 0.4501],
          [0.4327, 0.5285, 0.4559,  ..., 0.4923, 0.5224, 0.4263]],

         [[0.5700, 0.5883, 0.6016,  ..., 0.5647, 0.5588, 0.6145],
          [0.5082, 0.5991, 0.5474,  ..., 0.5857, 0.5130, 0.5631],
          [0.6167, 0.5138, 0.5053,  ..., 0.6418, 0.4552, 0.5556],
          [0.5862, 0.5712, 0.4758,  ..., 0.5529, 0.4509, 0.4903]],

         ...,

         [[0.5284, 0.4171, 0.5001,  ..., 0.5757, 0.5366, 0.6887],
          [0.4650, 0.6349, 0.5431,  ..., 0.4471, 0.5084, 0.5121],
          [0.5415, 0.5236, 0.4743,  ..., 0.4117, 0.4423, 0.4363],
          [0.5772, 0.4934, 0.3451,  ..., 0.4028, 0.4323, 0.4160]],

         [[0.5905, 0.4641, 0.4746,  ..., 0.3561, 0.5268, 0.5299],
          [0.4341, 0.5702, 0.6836,  ..., 0.4776, 0.5335, 0.5752],
          [0.5403, 0.3238, 0.6440,  ..., 0.6144, 0.5379, 0.4439],
          [0.5072, 0.5225, 0.5098,  ..., 0.4958, 0.5745, 0.4696]],

         [[0.4174, 0.5892, 0.5243,  ..., 0.5488, 0.6001, 0.5841],
          [0.4052, 0.4783, 0.6161,  ..., 0.5862, 0.5825, 0.5655],
          [0.5814, 0.3745, 0.4831,  ..., 0.5137, 0.5090, 0.3703],
          [0.5772, 0.4846, 0.5455,  ..., 0.4795, 0.4437, 0.4624]]],


        [[[0.5386, 0.6126, 0.6161,  ..., 0.5104, 0.5259, 0.5424],
          [0.4236, 0.6088, 0.4511,  ..., 0.4874, 0.5389, 0.4506],
          [0.5914, 0.5491, 0.3947,  ..., 0.4495, 0.3125, 0.4472],
          [0.5153, 0.4353, 0.4886,  ..., 0.5886, 0.3880, 0.3784]],

         [[0.4597, 0.6501, 0.3451,  ..., 0.3750, 0.5944, 0.5184],
          [0.4862, 0.4825, 0.5716,  ..., 0.6640, 0.5243, 0.3739],
          [0.4696, 0.6037, 0.4405,  ..., 0.3732, 0.4428, 0.4559],
          [0.4868, 0.3195, 0.3933,  ..., 0.4932, 0.4139, 0.5723]],

         [[0.4703, 0.4387, 0.4092,  ..., 0.3981, 0.5317, 0.5411],
          [0.4222, 0.5345, 0.4793,  ..., 0.5306, 0.5137, 0.5101],
          [0.4708, 0.3921, 0.5095,  ..., 0.4864, 0.5877, 0.4491],
          [0.4681, 0.4128, 0.3859,  ..., 0.5463, 0.5265, 0.5169]],

         ...,

         [[0.5843, 0.4825, 0.5535,  ..., 0.4684, 0.6039, 0.7681],
          [0.4303, 0.6475, 0.5821,  ..., 0.6227, 0.4608, 0.5393],
          [0.5657, 0.3796, 0.6109,  ..., 0.6463, 0.3935, 0.5874],
          [0.5427, 0.4810, 0.6020,  ..., 0.6427, 0.3870, 0.5310]],

         [[0.4798, 0.4781, 0.6175,  ..., 0.5136, 0.4586, 0.6704],
          [0.4669, 0.5977, 0.6404,  ..., 0.5762, 0.5130, 0.5711],
          [0.4938, 0.4039, 0.4348,  ..., 0.5690, 0.3780, 0.5869],
          [0.5354, 0.4648, 0.3646,  ..., 0.4480, 0.4143, 0.5291]],

         [[0.4952, 0.6176, 0.4809,  ..., 0.6236, 0.6001, 0.6410],
          [0.4979, 0.5113, 0.5046,  ..., 0.4919, 0.5437, 0.4908],
          [0.5843, 0.5835, 0.5450,  ..., 0.4303, 0.4466, 0.5157],
          [0.5206, 0.4291, 0.4734,  ..., 0.4838, 0.5728, 0.5167]]]],
       device='cuda:0')
tensor([[[[0.4965, 0.5436, 0.3808,  ..., 0.4154, 0.4912, 0.5436],
          [0.5084, 0.4345, 0.5433,  ..., 0.4330, 0.4354, 0.5283],
          [0.6379, 0.4886, 0.4576,  ..., 0.5931, 0.3522, 0.3074],
          [0.6792, 0.5127, 0.5385,  ..., 0.5804, 0.3363, 0.4470]],

         [[0.5837, 0.5969, 0.4844,  ..., 0.5727, 0.5244, 0.5804],
          [0.4849, 0.5351, 0.6169,  ..., 0.5285, 0.5024, 0.5204],
          [0.5021, 0.5397, 0.4470,  ..., 0.6142, 0.4278, 0.4311],
          [0.4297, 0.5315, 0.4489,  ..., 0.4893, 0.5074, 0.4073]],

         [[0.5670, 0.5913, 0.5946,  ..., 0.5617, 0.5438, 0.5955],
          [0.5052, 0.6021, 0.5404,  ..., 0.5827, 0.4980, 0.5441],
          [0.6137, 0.5168, 0.4983,  ..., 0.6388, 0.4402, 0.5366],
          [0.5832, 0.5742, 0.4688,  ..., 0.5499, 0.4359, 0.4713]],

         ...,

         [[0.5254, 0.4201, 0.4931,  ..., 0.5727, 0.5216, 0.6697],
          [0.4620, 0.6379, 0.5361,  ..., 0.4441, 0.4934, 0.4931],
          [0.5385, 0.5266, 0.4673,  ..., 0.4087, 0.4273, 0.4173],
          [0.5742, 0.4964, 0.3381,  ..., 0.3998, 0.4173, 0.3970]],

         [[0.5875, 0.4671, 0.4676,  ..., 0.3531, 0.5118, 0.5109],
          [0.4311, 0.5732, 0.6766,  ..., 0.4746, 0.5185, 0.5562],
          [0.5373, 0.3268, 0.6370,  ..., 0.6114, 0.5229, 0.4249],
          [0.5042, 0.5255, 0.5028,  ..., 0.4928, 0.5595, 0.4506]],

         [[0.4144, 0.5922, 0.5173,  ..., 0.5458, 0.5851, 0.5651],
          [0.4022, 0.4813, 0.6091,  ..., 0.5832, 0.5675, 0.5465],
          [0.5784, 0.3775, 0.4761,  ..., 0.5107, 0.4940, 0.3513],
          [0.5742, 0.4876, 0.5385,  ..., 0.4765, 0.4287, 0.4434]]],


        [[[0.5356, 0.6156, 0.6091,  ..., 0.5074, 0.5109, 0.5234],
          [0.4206, 0.6118, 0.4441,  ..., 0.4844, 0.5239, 0.4316],
          [0.5884, 0.5521, 0.3877,  ..., 0.4465, 0.2975, 0.4282],
          [0.5123, 0.4383, 0.4816,  ..., 0.5856, 0.3730, 0.3594]],

         [[0.4567, 0.6531, 0.3381,  ..., 0.3720, 0.5794, 0.4994],
          [0.4832, 0.4855, 0.5646,  ..., 0.6610, 0.5093, 0.3549],
          [0.4666, 0.6067, 0.4335,  ..., 0.3702, 0.4278, 0.4369],
          [0.4838, 0.3225, 0.3863,  ..., 0.4902, 0.3989, 0.5533]],

         [[0.4673, 0.4417, 0.4022,  ..., 0.3951, 0.5167, 0.5221],
          [0.4192, 0.5375, 0.4723,  ..., 0.5276, 0.4987, 0.4911],
          [0.4678, 0.3951, 0.5025,  ..., 0.4834, 0.5727, 0.4301],
          [0.4651, 0.4158, 0.3789,  ..., 0.5433, 0.5115, 0.4979]],

         ...,

         [[0.5813, 0.4855, 0.5465,  ..., 0.4654, 0.5889, 0.7491],
          [0.4273, 0.6505, 0.5751,  ..., 0.6197, 0.4458, 0.5203],
          [0.5627, 0.3826, 0.6039,  ..., 0.6433, 0.3785, 0.5684],
          [0.5397, 0.4840, 0.5950,  ..., 0.6397, 0.3720, 0.5120]],

         [[0.4768, 0.4811, 0.6105,  ..., 0.5106, 0.4436, 0.6514],
          [0.4639, 0.6007, 0.6334,  ..., 0.5732, 0.4980, 0.5521],
          [0.4908, 0.4069, 0.4278,  ..., 0.5660, 0.3630, 0.5679],
          [0.5324, 0.4678, 0.3576,  ..., 0.4450, 0.3993, 0.5101]],

         [[0.4922, 0.6206, 0.4739,  ..., 0.6206, 0.5851, 0.6220],
          [0.4949, 0.5143, 0.4976,  ..., 0.4889, 0.5287, 0.4718],
          [0.5813, 0.5865, 0.5380,  ..., 0.4273, 0.4316, 0.4967],
          [0.5176, 0.4321, 0.4664,  ..., 0.4808, 0.5578, 0.4977]]]],
       device='cuda:0', requires_grad=True)/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:40: UserWarning: Cannot split tensor of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(
/nfs-share/pa511/code_bases/dyna_project/dyna/.venv/lib/python3.12/site-packages/composer/core/data_spec.py:29: UserWarning: Cannot split list of length 2 into batches of size 10. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.
  warnings.warn(

tensor([ 0.0030, -0.0030,  0.0070, -0.0170, -0.0030,  0.0130, -0.0350,  0.0030,
         0.0150,  0.0190], device='cuda:0')
selected experts tensor([1296, 1623, 1191, 1869, 1147, 1683, 1860, 2222, 1550, 1943],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5212, 0.5565, 0.5034,  ..., 0.3221, 0.4932, 0.5549],
          [0.5156, 0.4031, 0.6549,  ..., 0.4384, 0.5465, 0.3610],
          [0.5242, 0.4002, 0.4429,  ..., 0.4436, 0.5291, 0.5668],
          [0.4054, 0.5699, 0.4974,  ..., 0.3887, 0.5352, 0.4054]],

         [[0.3932, 0.4976, 0.4501,  ..., 0.4760, 0.4407, 0.3718],
          [0.4933, 0.5604, 0.7140,  ..., 0.6072, 0.4519, 0.4925],
          [0.4571, 0.4919, 0.4645,  ..., 0.5169, 0.5669, 0.5016],
          [0.5229, 0.3768, 0.4366,  ..., 0.4950, 0.4605, 0.4499]],

         [[0.6042, 0.5032, 0.5174,  ..., 0.4019, 0.4965, 0.5533],
          [0.5652, 0.3754, 0.6081,  ..., 0.6076, 0.5886, 0.3700],
          [0.5541, 0.3603, 0.5283,  ..., 0.5538, 0.5961, 0.6503],
          [0.5789, 0.5237, 0.3029,  ..., 0.4332, 0.5177, 0.4281]],

         ...,

         [[0.4910, 0.4910, 0.4930,  ..., 0.5230, 0.4870, 0.5070],
          [0.4910, 0.4910, 0.4930,  ..., 0.5230, 0.4870, 0.5070],
          [0.4910, 0.4910, 0.4930,  ..., 0.5230, 0.4870, 0.5070],
          [0.4910, 0.4910, 0.4930,  ..., 0.5230, 0.4870, 0.5070]],

         [[0.4910, 0.4910, 0.4930,  ..., 0.5230, 0.4870, 0.5070],
          [0.4910, 0.4910, 0.4930,  ..., 0.5230, 0.4870, 0.5070],
          [0.4910, 0.4910, 0.4930,  ..., 0.5230, 0.4870, 0.5070],
          [0.4910, 0.4910, 0.4930,  ..., 0.5230, 0.4870, 0.5070]],

         [[0.4910, 0.4910, 0.4930,  ..., 0.5230, 0.4870, 0.5070],
          [0.4910, 0.4910, 0.4930,  ..., 0.5230, 0.4870, 0.5070],
          [0.4910, 0.4910, 0.4930,  ..., 0.5230, 0.4870, 0.5070],
          [0.4910, 0.4910, 0.4930,  ..., 0.5230, 0.4870, 0.5070]]],


        [[[0.5637, 0.6208, 0.4275,  ..., 0.4546, 0.4057, 0.5864],
          [0.5462, 0.5671, 0.4785,  ..., 0.4219, 0.6763, 0.3836],
          [0.6736, 0.4188, 0.4255,  ..., 0.5250, 0.4836, 0.5571],
          [0.6803, 0.5229, 0.6118,  ..., 0.4223, 0.3877, 0.3539]],

         [[0.5327, 0.4693, 0.5482,  ..., 0.6143, 0.5172, 0.4897],
          [0.4860, 0.6102, 0.6095,  ..., 0.6090, 0.7040, 0.5462],
          [0.6710, 0.3843, 0.4680,  ..., 0.6988, 0.6392, 0.6395],
          [0.6555, 0.4789, 0.4963,  ..., 0.4379, 0.5378, 0.5104]],

         [[0.4716, 0.5794, 0.5020,  ..., 0.3611, 0.4635, 0.4952],
          [0.4207, 0.4989, 0.6898,  ..., 0.6884, 0.4770, 0.5006],
          [0.4804, 0.4593, 0.3881,  ..., 0.5871, 0.5583, 0.6304],
          [0.4443, 0.5537, 0.3293,  ..., 0.4579, 0.4742, 0.5204]],

         ...,

         [[0.4910, 0.4910, 0.4930,  ..., 0.5230, 0.4870, 0.5070],
          [0.4910, 0.4910, 0.4930,  ..., 0.5230, 0.4870, 0.5070],
          [0.4910, 0.4910, 0.4930,  ..., 0.5230, 0.4870, 0.5070],
          [0.4910, 0.4910, 0.4930,  ..., 0.5230, 0.4870, 0.5070]],

         [[0.4910, 0.4910, 0.4930,  ..., 0.5230, 0.4870, 0.5070],
          [0.4910, 0.4910, 0.4930,  ..., 0.5230, 0.4870, 0.5070],
          [0.4910, 0.4910, 0.4930,  ..., 0.5230, 0.4870, 0.5070],
          [0.4910, 0.4910, 0.4930,  ..., 0.5230, 0.4870, 0.5070]],

         [[0.4910, 0.4910, 0.4930,  ..., 0.5230, 0.4870, 0.5070],
          [0.4910, 0.4910, 0.4930,  ..., 0.5230, 0.4870, 0.5070],
          [0.4910, 0.4910, 0.4930,  ..., 0.5230, 0.4870, 0.5070],
          [0.4910, 0.4910, 0.4930,  ..., 0.5230, 0.4870, 0.5070]]]],
       device='cuda:0')
tensor([[[[0.5302, 0.5655, 0.5104,  ..., 0.2991, 0.5062, 0.5479],
          [0.5246, 0.4121, 0.6619,  ..., 0.4154, 0.5595, 0.3540],
          [0.5332, 0.4092, 0.4499,  ..., 0.4206, 0.5421, 0.5598],
          [0.4144, 0.5789, 0.5044,  ..., 0.3657, 0.5482, 0.3984]],

         [[0.4022, 0.5066, 0.4571,  ..., 0.4530, 0.4537, 0.3648],
          [0.5023, 0.5694, 0.7210,  ..., 0.5842, 0.4649, 0.4855],
          [0.4661, 0.5009, 0.4715,  ..., 0.4939, 0.5799, 0.4946],
          [0.5319, 0.3858, 0.4436,  ..., 0.4720, 0.4735, 0.4429]],

         [[0.6132, 0.5122, 0.5244,  ..., 0.3789, 0.5095, 0.5463],
          [0.5742, 0.3844, 0.6151,  ..., 0.5846, 0.6016, 0.3630],
          [0.5631, 0.3693, 0.5353,  ..., 0.5308, 0.6091, 0.6433],
          [0.5879, 0.5327, 0.3099,  ..., 0.4102, 0.5307, 0.4211]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5727, 0.6298, 0.4345,  ..., 0.4316, 0.4187, 0.5794],
          [0.5552, 0.5761, 0.4855,  ..., 0.3989, 0.6893, 0.3766],
          [0.6826, 0.4278, 0.4325,  ..., 0.5020, 0.4966, 0.5501],
          [0.6893, 0.5319, 0.6188,  ..., 0.3993, 0.4007, 0.3469]],

         [[0.5417, 0.4783, 0.5552,  ..., 0.5913, 0.5302, 0.4827],
          [0.4950, 0.6192, 0.6165,  ..., 0.5860, 0.7170, 0.5392],
          [0.6800, 0.3933, 0.4750,  ..., 0.6758, 0.6522, 0.6325],
          [0.6645, 0.4879, 0.5033,  ..., 0.4149, 0.5508, 0.5034]],

         [[0.4806, 0.5884, 0.5090,  ..., 0.3381, 0.4765, 0.4882],
          [0.4297, 0.5079, 0.6968,  ..., 0.6654, 0.4900, 0.4936],
          [0.4894, 0.4683, 0.3951,  ..., 0.5641, 0.5713, 0.6234],
          [0.4533, 0.5627, 0.3363,  ..., 0.4349, 0.4872, 0.5134]],

         ...,

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0090, -0.0090, -0.0070,  0.0030, -0.0110,  0.0250,  0.0030,  0.0230,
        -0.0130,  0.0070], device='cuda:0')
selected experts tensor([2068, 1786, 2018, 1670, 1665,  853, 1774, 1258, 1475, 1817],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1602., 1483., 1618., 1676., 1561., 1753., 1476., 1718., 1871., 1626.],
        [2068., 1786., 2018., 1670., 1665.,  853., 1774., 1258., 1475., 1817.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4932, 0.5103, 0.4928,  ..., 0.4621, 0.4594, 0.4237],
          [0.4370, 0.5414, 0.5004,  ..., 0.5705, 0.4075, 0.4784],
          [0.5466, 0.3845, 0.5310,  ..., 0.3830, 0.5604, 0.5787],
          [0.4806, 0.5704, 0.4970,  ..., 0.4194, 0.5834, 0.3913]],

         [[0.3768, 0.5709, 0.5681,  ..., 0.3199, 0.5729, 0.5958],
          [0.4404, 0.4985, 0.6372,  ..., 0.4399, 0.5531, 0.5403],
          [0.5249, 0.3676, 0.5247,  ..., 0.4689, 0.5999, 0.5153],
          [0.5435, 0.5842, 0.6747,  ..., 0.4949, 0.4769, 0.3881]],

         [[0.3764, 0.4886, 0.6048,  ..., 0.4749, 0.4765, 0.5527],
          [0.6787, 0.5480, 0.2856,  ..., 0.3653, 0.3642, 0.5301],
          [0.4355, 0.4307, 0.4787,  ..., 0.4943, 0.4669, 0.5228],
          [0.4286, 0.4596, 0.5063,  ..., 0.5901, 0.4447, 0.4704]],

         ...,

         [[0.4176, 0.5244, 0.5533,  ..., 0.4409, 0.5661, 0.5551],
          [0.5355, 0.5904, 0.4635,  ..., 0.4604, 0.4274, 0.6066],
          [0.5774, 0.5506, 0.6345,  ..., 0.4958, 0.4103, 0.6284],
          [0.4767, 0.5349, 0.5965,  ..., 0.6669, 0.3633, 0.3789]],

         [[0.3360, 0.4995, 0.5291,  ..., 0.5585, 0.5383, 0.7043],
          [0.6084, 0.5245, 0.5871,  ..., 0.4844, 0.4483, 0.5111],
          [0.5774, 0.5339, 0.5700,  ..., 0.6590, 0.6856, 0.3978],
          [0.5445, 0.4674, 0.5772,  ..., 0.4498, 0.4255, 0.4744]],

         [[0.4925, 0.6152, 0.4160,  ..., 0.5887, 0.3428, 0.5479],
          [0.5425, 0.5617, 0.4426,  ..., 0.4371, 0.4669, 0.3335],
          [0.5082, 0.3971, 0.5686,  ..., 0.4443, 0.4099, 0.5067],
          [0.4619, 0.5470, 0.3812,  ..., 0.5537, 0.5315, 0.4739]]],


        [[[0.4152, 0.5414, 0.5426,  ..., 0.5179, 0.5666, 0.4104],
          [0.5553, 0.5468, 0.2454,  ..., 0.6187, 0.5112, 0.5483],
          [0.5266, 0.4460, 0.5748,  ..., 0.4756, 0.5604, 0.2618],
          [0.4619, 0.4245, 0.5266,  ..., 0.4171, 0.5417, 0.5149]],

         [[0.5142, 0.3470, 0.4670,  ..., 0.5152, 0.4645, 0.5425],
          [0.4300, 0.5951, 0.3877,  ..., 0.4824, 0.4500, 0.3448],
          [0.5367, 0.5050, 0.5506,  ..., 0.3862, 0.5061, 0.5934],
          [0.3212, 0.3721, 0.4685,  ..., 0.6136, 0.6060, 0.5573]],

         [[0.3672, 0.5219, 0.4535,  ..., 0.6159, 0.5066, 0.5260],
          [0.4339, 0.5620, 0.3825,  ..., 0.4520, 0.5268, 0.5261],
          [0.4476, 0.4886, 0.4217,  ..., 0.5686, 0.4773, 0.4304],
          [0.2937, 0.6063, 0.5540,  ..., 0.6773, 0.5661, 0.3413]],

         ...,

         [[0.5606, 0.4833, 0.4950,  ..., 0.5830, 0.4943, 0.5648],
          [0.5159, 0.5366, 0.4955,  ..., 0.6687, 0.3949, 0.5013],
          [0.4072, 0.5025, 0.4506,  ..., 0.5811, 0.4222, 0.4860],
          [0.4399, 0.5084, 0.4448,  ..., 0.5645, 0.5752, 0.4849]],

         [[0.6745, 0.6506, 0.3952,  ..., 0.4595, 0.5957, 0.4629],
          [0.4281, 0.4410, 0.4376,  ..., 0.5074, 0.5729, 0.4114],
          [0.4067, 0.5918, 0.4499,  ..., 0.3725, 0.4628, 0.5715],
          [0.4257, 0.5852, 0.4388,  ..., 0.6302, 0.4452, 0.3987]],

         [[0.3949, 0.6101, 0.5838,  ..., 0.5067, 0.4231, 0.6014],
          [0.4281, 0.4649, 0.4613,  ..., 0.5278, 0.5800, 0.5255],
          [0.4355, 0.5349, 0.4409,  ..., 0.6242, 0.5369, 0.4356],
          [0.3609, 0.3929, 0.4265,  ..., 0.2648, 0.3561, 0.5356]]]],
       device='cuda:0')
tensor([[[[0.4962, 0.5093, 0.4998,  ..., 0.4571, 0.4564, 0.4187],
          [0.4400, 0.5404, 0.5074,  ..., 0.5655, 0.4045, 0.4734],
          [0.5496, 0.3835, 0.5380,  ..., 0.3780, 0.5574, 0.5737],
          [0.4836, 0.5694, 0.5040,  ..., 0.4144, 0.5804, 0.3863]],

         [[0.3798, 0.5699, 0.5751,  ..., 0.3149, 0.5699, 0.5908],
          [0.4434, 0.4975, 0.6442,  ..., 0.4349, 0.5501, 0.5353],
          [0.5279, 0.3666, 0.5317,  ..., 0.4639, 0.5969, 0.5103],
          [0.5465, 0.5832, 0.6817,  ..., 0.4899, 0.4739, 0.3831]],

         [[0.3794, 0.4876, 0.6118,  ..., 0.4699, 0.4735, 0.5477],
          [0.6817, 0.5470, 0.2926,  ..., 0.3603, 0.3612, 0.5251],
          [0.4385, 0.4297, 0.4857,  ..., 0.4893, 0.4639, 0.5178],
          [0.4316, 0.4586, 0.5133,  ..., 0.5851, 0.4417, 0.4654]],

         ...,

         [[0.4206, 0.5234, 0.5603,  ..., 0.4359, 0.5631, 0.5501],
          [0.5385, 0.5894, 0.4705,  ..., 0.4554, 0.4244, 0.6016],
          [0.5804, 0.5496, 0.6415,  ..., 0.4908, 0.4073, 0.6234],
          [0.4797, 0.5339, 0.6035,  ..., 0.6619, 0.3603, 0.3739]],

         [[0.3390, 0.4985, 0.5361,  ..., 0.5535, 0.5353, 0.6993],
          [0.6114, 0.5235, 0.5941,  ..., 0.4794, 0.4453, 0.5061],
          [0.5804, 0.5329, 0.5770,  ..., 0.6540, 0.6826, 0.3928],
          [0.5475, 0.4664, 0.5842,  ..., 0.4448, 0.4225, 0.4694]],

         [[0.4955, 0.6142, 0.4230,  ..., 0.5837, 0.3398, 0.5429],
          [0.5455, 0.5607, 0.4496,  ..., 0.4321, 0.4639, 0.3285],
          [0.5112, 0.3961, 0.5756,  ..., 0.4393, 0.4069, 0.5017],
          [0.4649, 0.5460, 0.3882,  ..., 0.5487, 0.5285, 0.4689]]],


        [[[0.4182, 0.5404, 0.5496,  ..., 0.5129, 0.5636, 0.4054],
          [0.5583, 0.5458, 0.2524,  ..., 0.6137, 0.5082, 0.5433],
          [0.5296, 0.4450, 0.5818,  ..., 0.4706, 0.5574, 0.2568],
          [0.4649, 0.4235, 0.5336,  ..., 0.4121, 0.5387, 0.5099]],

         [[0.5172, 0.3460, 0.4740,  ..., 0.5102, 0.4615, 0.5375],
          [0.4330, 0.5941, 0.3947,  ..., 0.4774, 0.4470, 0.3398],
          [0.5397, 0.5040, 0.5576,  ..., 0.3812, 0.5031, 0.5884],
          [0.3242, 0.3711, 0.4755,  ..., 0.6086, 0.6030, 0.5523]],

         [[0.3702, 0.5209, 0.4605,  ..., 0.6109, 0.5036, 0.5210],
          [0.4369, 0.5610, 0.3895,  ..., 0.4470, 0.5238, 0.5211],
          [0.4506, 0.4876, 0.4287,  ..., 0.5636, 0.4743, 0.4254],
          [0.2967, 0.6053, 0.5610,  ..., 0.6723, 0.5631, 0.3363]],

         ...,

         [[0.5636, 0.4823, 0.5020,  ..., 0.5780, 0.4913, 0.5598],
          [0.5189, 0.5356, 0.5025,  ..., 0.6637, 0.3919, 0.4963],
          [0.4102, 0.5015, 0.4576,  ..., 0.5761, 0.4192, 0.4810],
          [0.4429, 0.5074, 0.4518,  ..., 0.5595, 0.5722, 0.4799]],

         [[0.6775, 0.6496, 0.4022,  ..., 0.4545, 0.5927, 0.4579],
          [0.4311, 0.4400, 0.4446,  ..., 0.5024, 0.5699, 0.4064],
          [0.4097, 0.5908, 0.4569,  ..., 0.3675, 0.4598, 0.5665],
          [0.4287, 0.5842, 0.4458,  ..., 0.6252, 0.4422, 0.3937]],

         [[0.3979, 0.6091, 0.5908,  ..., 0.5017, 0.4201, 0.5964],
          [0.4311, 0.4639, 0.4683,  ..., 0.5228, 0.5770, 0.5205],
          [0.4385, 0.5339, 0.4479,  ..., 0.6192, 0.5339, 0.4306],
          [0.3639, 0.3919, 0.4335,  ..., 0.2598, 0.3531, 0.5306]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0030,  0.0010, -0.0070, -0.0010, -0.0030, -0.0050,  0.0050,  0.0050,
         0.0030,  0.0050], device='cuda:0')
selected experts tensor([1685, 1578, 1690, 1539, 1733, 1583, 1684, 1708, 1611, 1573],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5770, 0.5659, 0.5280,  ..., 0.5571, 0.5442, 0.6097],
          [0.4068, 0.6047, 0.3669,  ..., 0.6136, 0.4015, 0.4269],
          [0.4936, 0.5447, 0.5637,  ..., 0.5937, 0.4833, 0.4122],
          [0.5940, 0.4219, 0.3847,  ..., 0.3937, 0.5174, 0.5263]],

         [[0.6849, 0.5864, 0.6640,  ..., 0.5549, 0.3922, 0.4461],
          [0.3131, 0.6376, 0.5895,  ..., 0.4757, 0.5901, 0.4538],
          [0.4144, 0.5778, 0.6328,  ..., 0.4737, 0.5662, 0.5364],
          [0.5296, 0.4984, 0.6032,  ..., 0.6540, 0.5633, 0.3687]],

         [[0.5770, 0.5683, 0.3847,  ..., 0.5714, 0.5238, 0.6083],
          [0.6670, 0.3996, 0.3741,  ..., 0.5412, 0.6182, 0.4019],
          [0.5072, 0.4252, 0.5196,  ..., 0.5051, 0.5575, 0.4647],
          [0.5693, 0.6385, 0.4679,  ..., 0.6210, 0.4944, 0.3642]],

         ...,

         [[0.4671, 0.4626, 0.5287,  ..., 0.4438, 0.5074, 0.3741],
          [0.2603, 0.4794, 0.5025,  ..., 0.4799, 0.6302, 0.4769],
          [0.4719, 0.5243, 0.5990,  ..., 0.4970, 0.4756, 0.4423],
          [0.5712, 0.5812, 0.5345,  ..., 0.5257, 0.5005, 0.5623]],

         [[0.5208, 0.5353, 0.4567,  ..., 0.6016, 0.4865, 0.5095],
          [0.3811, 0.4329, 0.4757,  ..., 0.5079, 0.4660, 0.4136],
          [0.4929, 0.5764, 0.4945,  ..., 0.6264, 0.3941, 0.4577],
          [0.5467, 0.3935, 0.5829,  ..., 0.5695, 0.5054, 0.3588]],

         [[0.4537, 0.5433, 0.6236,  ..., 0.5257, 0.5981, 0.6684],
          [0.4315, 0.4271, 0.3769,  ..., 0.5209, 0.3033, 0.4815],
          [0.4747, 0.4166, 0.5183,  ..., 0.4499, 0.5612, 0.3944],
          [0.3927, 0.5114, 0.3861,  ..., 0.5993, 0.5532, 0.4471]]],


        [[[0.6150, 0.4377, 0.3367,  ..., 0.3551, 0.6131, 0.4662],
          [0.5203, 0.5697, 0.3861,  ..., 0.3524, 0.4491, 0.5577],
          [0.4759, 0.5408, 0.6822,  ..., 0.6939, 0.4034, 0.4890],
          [0.4801, 0.6501, 0.4226,  ..., 0.6505, 0.4467, 0.4435]],

         [[0.4992, 0.5066, 0.4638,  ..., 0.4093, 0.4629, 0.3642],
          [0.6076, 0.6102, 0.5439,  ..., 0.3900, 0.3689, 0.5685],
          [0.5736, 0.4629, 0.4430,  ..., 0.5102, 0.4039, 0.5177],
          [0.4744, 0.5515, 0.3543,  ..., 0.4453, 0.5454, 0.5032]],

         [[0.4770, 0.4522, 0.5132,  ..., 0.5890, 0.3563, 0.3393],
          [0.5346, 0.4276, 0.4873,  ..., 0.5164, 0.5169, 0.4606],
          [0.4363, 0.4842, 0.4303,  ..., 0.3877, 0.6782, 0.5947],
          [0.3983, 0.4553, 0.4751,  ..., 0.4165, 0.5877, 0.4686]],

         ...,

         [[0.4883, 0.5009, 0.3861,  ..., 0.5140, 0.4845, 0.3741],
          [0.4803, 0.4181, 0.5139,  ..., 0.5833, 0.4672, 0.5563],
          [0.5409, 0.6167, 0.4284,  ..., 0.5472, 0.5440, 0.4638],
          [0.6178, 0.4339, 0.4070,  ..., 0.4117, 0.4697, 0.5064]],

         [[0.6396, 0.4072, 0.6093,  ..., 0.4581, 0.3258, 0.6046],
          [0.4383, 0.5616, 0.5990,  ..., 0.4947, 0.4546, 0.4940],
          [0.6696, 0.4764, 0.2980,  ..., 0.6722, 0.4251, 0.4898],
          [0.3946, 0.5219, 0.2813,  ..., 0.3533, 0.4935, 0.4468]],

         [[0.3969, 0.4770, 0.5676,  ..., 0.2528, 0.4546, 0.5334],
          [0.4957, 0.5527, 0.4355,  ..., 0.5227, 0.3927, 0.6373],
          [0.6187, 0.5389, 0.6172,  ..., 0.4764, 0.5681, 0.4664],
          [0.5193, 0.5911, 0.4529,  ..., 0.5511, 0.4034, 0.4708]]]],
       device='cuda:0')
tensor([[[[0.5780, 0.5689, 0.5250,  ..., 0.5641, 0.5392, 0.6067],
          [0.4078, 0.6077, 0.3639,  ..., 0.6206, 0.3965, 0.4239],
          [0.4946, 0.5477, 0.5607,  ..., 0.6007, 0.4783, 0.4092],
          [0.5950, 0.4249, 0.3817,  ..., 0.4007, 0.5124, 0.5233]],

         [[0.6859, 0.5894, 0.6610,  ..., 0.5619, 0.3872, 0.4431],
          [0.3141, 0.6406, 0.5865,  ..., 0.4827, 0.5851, 0.4508],
          [0.4154, 0.5808, 0.6298,  ..., 0.4807, 0.5612, 0.5334],
          [0.5306, 0.5014, 0.6002,  ..., 0.6610, 0.5583, 0.3657]],

         [[0.5780, 0.5713, 0.3817,  ..., 0.5784, 0.5188, 0.6053],
          [0.6680, 0.4026, 0.3711,  ..., 0.5482, 0.6132, 0.3989],
          [0.5082, 0.4282, 0.5166,  ..., 0.5121, 0.5525, 0.4617],
          [0.5703, 0.6415, 0.4649,  ..., 0.6280, 0.4894, 0.3612]],

         ...,

         [[0.4681, 0.4656, 0.5257,  ..., 0.4508, 0.5024, 0.3711],
          [0.2613, 0.4824, 0.4995,  ..., 0.4869, 0.6252, 0.4739],
          [0.4729, 0.5273, 0.5960,  ..., 0.5040, 0.4706, 0.4393],
          [0.5722, 0.5842, 0.5315,  ..., 0.5327, 0.4955, 0.5593]],

         [[0.5218, 0.5383, 0.4537,  ..., 0.6086, 0.4815, 0.5065],
          [0.3821, 0.4359, 0.4727,  ..., 0.5149, 0.4610, 0.4106],
          [0.4939, 0.5794, 0.4915,  ..., 0.6334, 0.3891, 0.4547],
          [0.5477, 0.3965, 0.5799,  ..., 0.5765, 0.5004, 0.3558]],

         [[0.4547, 0.5463, 0.6206,  ..., 0.5327, 0.5931, 0.6654],
          [0.4325, 0.4301, 0.3739,  ..., 0.5279, 0.2983, 0.4785],
          [0.4757, 0.4196, 0.5153,  ..., 0.4569, 0.5562, 0.3914],
          [0.3937, 0.5144, 0.3831,  ..., 0.6063, 0.5482, 0.4441]]],


        [[[0.6160, 0.4407, 0.3337,  ..., 0.3621, 0.6081, 0.4632],
          [0.5213, 0.5727, 0.3831,  ..., 0.3594, 0.4441, 0.5547],
          [0.4769, 0.5438, 0.6792,  ..., 0.7009, 0.3984, 0.4860],
          [0.4811, 0.6531, 0.4196,  ..., 0.6575, 0.4417, 0.4405]],

         [[0.5002, 0.5096, 0.4608,  ..., 0.4163, 0.4579, 0.3612],
          [0.6086, 0.6132, 0.5409,  ..., 0.3970, 0.3639, 0.5655],
          [0.5746, 0.4659, 0.4400,  ..., 0.5172, 0.3989, 0.5147],
          [0.4754, 0.5545, 0.3513,  ..., 0.4523, 0.5404, 0.5002]],

         [[0.4780, 0.4552, 0.5102,  ..., 0.5960, 0.3513, 0.3363],
          [0.5356, 0.4306, 0.4843,  ..., 0.5234, 0.5119, 0.4576],
          [0.4373, 0.4872, 0.4273,  ..., 0.3947, 0.6732, 0.5917],
          [0.3993, 0.4583, 0.4721,  ..., 0.4235, 0.5827, 0.4656]],

         ...,

         [[0.4893, 0.5039, 0.3831,  ..., 0.5210, 0.4795, 0.3711],
          [0.4813, 0.4211, 0.5109,  ..., 0.5903, 0.4622, 0.5533],
          [0.5419, 0.6197, 0.4254,  ..., 0.5542, 0.5390, 0.4608],
          [0.6188, 0.4369, 0.4040,  ..., 0.4187, 0.4647, 0.5034]],

         [[0.6406, 0.4102, 0.6063,  ..., 0.4651, 0.3208, 0.6016],
          [0.4393, 0.5646, 0.5960,  ..., 0.5017, 0.4496, 0.4910],
          [0.6706, 0.4794, 0.2950,  ..., 0.6792, 0.4201, 0.4868],
          [0.3956, 0.5249, 0.2783,  ..., 0.3603, 0.4885, 0.4438]],

         [[0.3979, 0.4800, 0.5646,  ..., 0.2598, 0.4496, 0.5304],
          [0.4967, 0.5557, 0.4325,  ..., 0.5297, 0.3877, 0.6343],
          [0.6197, 0.5419, 0.6142,  ..., 0.4834, 0.5631, 0.4634],
          [0.5203, 0.5941, 0.4499,  ..., 0.5581, 0.3984, 0.4678]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0010, -0.0030,  0.0030,  0.0090, -0.0010,  0.0070, -0.0090, -0.0070,
         0.0050,  0.0030], device='cuda:0')
selected experts tensor([1797, 1567, 1748, 1616, 1559, 1587, 1651, 1495, 1716, 1648],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.4404, 0.5580, 0.5567,  ..., 0.4616, 0.4404, 0.5486],
          [0.4973, 0.5655, 0.3800,  ..., 0.4415, 0.4702, 0.4902],
          [0.4611, 0.4935, 0.6101,  ..., 0.4962, 0.5906, 0.4797],
          [0.6598, 0.4361, 0.4492,  ..., 0.5127, 0.5383, 0.6317]],

         [[0.4265, 0.4639, 0.5470,  ..., 0.4888, 0.4414, 0.6012],
          [0.4638, 0.6047, 0.4791,  ..., 0.5371, 0.6412, 0.4814],
          [0.4585, 0.4339, 0.5262,  ..., 0.4698, 0.5722, 0.5647],
          [0.5238, 0.4421, 0.5023,  ..., 0.5584, 0.4042, 0.5116]],

         [[0.3829, 0.6214, 0.5264,  ..., 0.4969, 0.4418, 0.5563],
          [0.5169, 0.4462, 0.5009,  ..., 0.5383, 0.4756, 0.4959],
          [0.4767, 0.5285, 0.5424,  ..., 0.4708, 0.6020, 0.4962],
          [0.6033, 0.4854, 0.4559,  ..., 0.5545, 0.4461, 0.5225]],

         ...,

         [[0.3733, 0.4810, 0.4821,  ..., 0.4703, 0.5007, 0.7318],
          [0.5464, 0.6564, 0.6068,  ..., 0.6426, 0.5269, 0.6261],
          [0.5144, 0.4058, 0.5836,  ..., 0.3962, 0.5264, 0.4178],
          [0.4839, 0.6797, 0.4497,  ..., 0.4743, 0.4404, 0.4024]],

         [[0.4370, 0.5693, 0.4530,  ..., 0.5400, 0.4399, 0.6382],
          [0.4755, 0.5745, 0.5379,  ..., 0.4963, 0.4936, 0.5705],
          [0.5021, 0.5265, 0.3112,  ..., 0.5899, 0.4509, 0.4773],
          [0.4019, 0.4390, 0.5695,  ..., 0.4853, 0.5165, 0.4761]],

         [[0.4322, 0.4851, 0.5359,  ..., 0.4401, 0.5248, 0.5922],
          [0.3996, 0.4544, 0.6504,  ..., 0.5127, 0.6403, 0.5664],
          [0.5147, 0.4414, 0.5158,  ..., 0.5347, 0.5164, 0.5206],
          [0.4628, 0.4756, 0.4097,  ..., 0.5033, 0.3576, 0.5063]]],


        [[[0.5440, 0.6182, 0.5318,  ..., 0.5521, 0.4158, 0.5488],
          [0.4730, 0.6103, 0.5438,  ..., 0.5795, 0.5521, 0.5722],
          [0.4813, 0.4205, 0.4817,  ..., 0.5507, 0.6204, 0.5397],
          [0.5910, 0.3913, 0.4329,  ..., 0.6059, 0.4252, 0.5960]],

         [[0.3343, 0.4988, 0.5237,  ..., 0.5502, 0.4342, 0.5328],
          [0.4696, 0.4983, 0.3966,  ..., 0.5407, 0.6449, 0.5156],
          [0.5040, 0.3326, 0.5518,  ..., 0.4084, 0.6138, 0.5341],
          [0.5844, 0.5018, 0.4710,  ..., 0.5747, 0.4873, 0.5342]],

         [[0.4948, 0.5252, 0.3719,  ..., 0.4499, 0.5506, 0.5703],
          [0.5104, 0.5307, 0.5584,  ..., 0.6735, 0.6185, 0.5580],
          [0.6140, 0.5551, 0.4097,  ..., 0.4730, 0.4481, 0.4732],
          [0.5359, 0.4310, 0.6068,  ..., 0.4659, 0.4069, 0.5065]],

         ...,

         [[0.4553, 0.4607, 0.4671,  ..., 0.4557, 0.5372, 0.6469],
          [0.5330, 0.6772, 0.6664,  ..., 0.6980, 0.5878, 0.5402],
          [0.5212, 0.3728, 0.6138,  ..., 0.4485, 0.5314, 0.4684],
          [0.4761, 0.5207, 0.3975,  ..., 0.5709, 0.5983, 0.6622]],

         [[0.4270, 0.5585, 0.4078,  ..., 0.5814, 0.5352, 0.5696],
          [0.5029, 0.5585, 0.6769,  ..., 0.4731, 0.4656, 0.4343],
          [0.4745, 0.5252, 0.4139,  ..., 0.4202, 0.4971, 0.6055],
          [0.5577, 0.5129, 0.4860,  ..., 0.5235, 0.4555, 0.4797]],

         [[0.4498, 0.5599, 0.4813,  ..., 0.5555, 0.5873, 0.6825],
          [0.5196, 0.5222, 0.5864,  ..., 0.4713, 0.6421, 0.6224],
          [0.3861, 0.5000, 0.3611,  ..., 0.5942, 0.6096, 0.4865],
          [0.4289, 0.5717, 0.4291,  ..., 0.4797, 0.4775, 0.6432]]]],
       device='cuda:0')
tensor([[[[0.4364, 0.5600, 0.5487,  ..., 0.4596, 0.4244, 0.5306],
          [0.4933, 0.5675, 0.3720,  ..., 0.4395, 0.4542, 0.4722],
          [0.4571, 0.4955, 0.6021,  ..., 0.4942, 0.5746, 0.4617],
          [0.6558, 0.4381, 0.4412,  ..., 0.5107, 0.5223, 0.6137]],

         [[0.4225, 0.4659, 0.5390,  ..., 0.4868, 0.4254, 0.5832],
          [0.4598, 0.6067, 0.4711,  ..., 0.5351, 0.6252, 0.4634],
          [0.4545, 0.4359, 0.5182,  ..., 0.4678, 0.5562, 0.5467],
          [0.5198, 0.4441, 0.4943,  ..., 0.5564, 0.3882, 0.4936]],

         [[0.3789, 0.6234, 0.5184,  ..., 0.4949, 0.4258, 0.5383],
          [0.5129, 0.4482, 0.4929,  ..., 0.5363, 0.4596, 0.4779],
          [0.4727, 0.5305, 0.5344,  ..., 0.4688, 0.5860, 0.4782],
          [0.5993, 0.4874, 0.4479,  ..., 0.5525, 0.4301, 0.5045]],

         ...,

         [[0.3693, 0.4830, 0.4741,  ..., 0.4683, 0.4847, 0.7138],
          [0.5424, 0.6584, 0.5988,  ..., 0.6406, 0.5109, 0.6081],
          [0.5104, 0.4078, 0.5756,  ..., 0.3942, 0.5104, 0.3998],
          [0.4799, 0.6817, 0.4417,  ..., 0.4723, 0.4244, 0.3844]],

         [[0.4330, 0.5713, 0.4450,  ..., 0.5380, 0.4239, 0.6202],
          [0.4715, 0.5765, 0.5299,  ..., 0.4943, 0.4776, 0.5525],
          [0.4981, 0.5285, 0.3032,  ..., 0.5879, 0.4349, 0.4593],
          [0.3979, 0.4410, 0.5615,  ..., 0.4833, 0.5005, 0.4581]],

         [[0.4282, 0.4871, 0.5279,  ..., 0.4381, 0.5088, 0.5742],
          [0.3956, 0.4564, 0.6424,  ..., 0.5107, 0.6243, 0.5484],
          [0.5107, 0.4434, 0.5078,  ..., 0.5327, 0.5004, 0.5026],
          [0.4588, 0.4776, 0.4017,  ..., 0.5013, 0.3416, 0.4883]]],


        [[[0.5400, 0.6202, 0.5238,  ..., 0.5501, 0.3998, 0.5308],
          [0.4690, 0.6123, 0.5358,  ..., 0.5775, 0.5361, 0.5542],
          [0.4773, 0.4225, 0.4737,  ..., 0.5487, 0.6044, 0.5217],
          [0.5870, 0.3933, 0.4249,  ..., 0.6039, 0.4092, 0.5780]],

         [[0.3303, 0.5008, 0.5157,  ..., 0.5482, 0.4182, 0.5148],
          [0.4656, 0.5003, 0.3886,  ..., 0.5387, 0.6289, 0.4976],
          [0.5000, 0.3346, 0.5438,  ..., 0.4064, 0.5978, 0.5161],
          [0.5804, 0.5038, 0.4630,  ..., 0.5727, 0.4713, 0.5162]],

         [[0.4908, 0.5272, 0.3639,  ..., 0.4479, 0.5346, 0.5523],
          [0.5064, 0.5327, 0.5504,  ..., 0.6715, 0.6025, 0.5400],
          [0.6100, 0.5571, 0.4017,  ..., 0.4710, 0.4321, 0.4552],
          [0.5319, 0.4330, 0.5988,  ..., 0.4639, 0.3909, 0.4885]],

         ...,

         [[0.4513, 0.4627, 0.4591,  ..., 0.4537, 0.5212, 0.6289],
          [0.5290, 0.6792, 0.6584,  ..., 0.6960, 0.5718, 0.5222],
          [0.5172, 0.3748, 0.6058,  ..., 0.4465, 0.5154, 0.4504],
          [0.4721, 0.5227, 0.3895,  ..., 0.5689, 0.5823, 0.6442]],

         [[0.4230, 0.5605, 0.3998,  ..., 0.5794, 0.5192, 0.5516],
          [0.4989, 0.5605, 0.6689,  ..., 0.4711, 0.4496, 0.4163],
          [0.4705, 0.5272, 0.4059,  ..., 0.4182, 0.4811, 0.5875],
          [0.5537, 0.5149, 0.4780,  ..., 0.5215, 0.4395, 0.4617]],

         [[0.4458, 0.5619, 0.4733,  ..., 0.5535, 0.5713, 0.6645],
          [0.5156, 0.5242, 0.5784,  ..., 0.4693, 0.6261, 0.6044],
          [0.3821, 0.5020, 0.3531,  ..., 0.5922, 0.5936, 0.4685],
          [0.4249, 0.5737, 0.4211,  ..., 0.4777, 0.4615, 0.6252]]]],
       device='cuda:0', requires_grad=True)
tensor([ 0.0040, -0.0020,  0.0080, -0.0180, -0.0020,  0.0120, -0.0360,  0.0020,
         0.0160,  0.0180], device='cuda:0')
selected experts tensor([1403, 1374, 1143, 1801, 1389, 2132, 1735, 1546, 1670, 2191],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
Attention expert selection process
sel2__shape torch.Size([2, 1024, 4, 10])
tensor([[[[0.5312, 0.3865, 0.4121,  ..., 0.4503, 0.3537, 0.5735],
          [0.4782, 0.5474, 0.6038,  ..., 0.4177, 0.5684, 0.4171],
          [0.5067, 0.4493, 0.3353,  ..., 0.3960, 0.4715, 0.5303],
          [0.5855, 0.5803, 0.5685,  ..., 0.4029, 0.5294, 0.4605]],

         [[0.4745, 0.5370, 0.6565,  ..., 0.4911, 0.3429, 0.5735],
          [0.4776, 0.6484, 0.5642,  ..., 0.5092, 0.6141, 0.3771],
          [0.5331, 0.3931, 0.3843,  ..., 0.5105, 0.5379, 0.6818],
          [0.6826, 0.6554, 0.6416,  ..., 0.4532, 0.4110, 0.4366]],

         [[0.4011, 0.5560, 0.3774,  ..., 0.5141, 0.5130, 0.4204],
          [0.4872, 0.5836, 0.5575,  ..., 0.5303, 0.6438, 0.3699],
          [0.5846, 0.4896, 0.4164,  ..., 0.6345, 0.5545, 0.5701],
          [0.7022, 0.5343, 0.5281,  ..., 0.5192, 0.5555, 0.5460]],

         ...,

         [[0.4040, 0.5579, 0.5494,  ..., 0.2945, 0.4410, 0.5768],
          [0.4002, 0.4346, 0.4816,  ..., 0.5623, 0.5797, 0.4228],
          [0.6106, 0.4769, 0.6398,  ..., 0.4911, 0.4391, 0.5739],
          [0.4688, 0.4914, 0.5363,  ..., 0.4422, 0.4876, 0.3555]],

         [[0.5579, 0.5234, 0.4159,  ..., 0.4712, 0.3683, 0.6076],
          [0.4652, 0.5442, 0.6094,  ..., 0.5414, 0.6187, 0.4868],
          [0.4682, 0.3630, 0.5021,  ..., 0.4700, 0.4690, 0.6340],
          [0.5850, 0.6851, 0.4961,  ..., 0.5187, 0.4902, 0.4643]],

         [[0.4900, 0.4900, 0.4920,  ..., 0.5240, 0.4880, 0.5060],
          [0.4900, 0.4900, 0.4920,  ..., 0.5240, 0.4880, 0.5060],
          [0.4900, 0.4900, 0.4920,  ..., 0.5240, 0.4880, 0.5060],
          [0.4900, 0.4900, 0.4920,  ..., 0.5240, 0.4880, 0.5060]]],


        [[[0.5387, 0.6143, 0.5908,  ..., 0.3870, 0.4602, 0.5331],
          [0.4730, 0.6018, 0.5525,  ..., 0.4884, 0.5306, 0.3654],
          [0.5445, 0.3074, 0.2977,  ..., 0.6218, 0.5282, 0.7222],
          [0.6037, 0.5981, 0.5177,  ..., 0.4853, 0.4524, 0.4228]],

         [[0.5206, 0.5459, 0.4499,  ..., 0.3499, 0.4668, 0.5916],
          [0.4491, 0.5309, 0.5025,  ..., 0.4290, 0.5243, 0.3799],
          [0.5290, 0.4120, 0.3310,  ..., 0.5209, 0.6438, 0.6995],
          [0.5732, 0.5789, 0.4912,  ..., 0.4276, 0.4338, 0.5037]],

         [[0.5921, 0.5070, 0.4668,  ..., 0.4140, 0.5403, 0.5457],
          [0.5034, 0.3108, 0.5973,  ..., 0.4961, 0.5246, 0.4016],
          [0.6189, 0.3503, 0.4766,  ..., 0.5267, 0.6765, 0.6749],
          [0.5236, 0.4861, 0.4661,  ..., 0.4201, 0.5844, 0.4025]],

         ...,

         [[0.5770, 0.5708, 0.4298,  ..., 0.4865, 0.3724, 0.6688],
          [0.5779, 0.6134, 0.6863,  ..., 0.5584, 0.5270, 0.4077],
          [0.4967, 0.6009, 0.3604,  ..., 0.3431, 0.3963, 0.5939],
          [0.4995, 0.3712, 0.5861,  ..., 0.3816, 0.4887, 0.5749]],

         [[0.4900, 0.4900, 0.4920,  ..., 0.5240, 0.4880, 0.5060],
          [0.4900, 0.4900, 0.4920,  ..., 0.5240, 0.4880, 0.5060],
          [0.4900, 0.4900, 0.4920,  ..., 0.5240, 0.4880, 0.5060],
          [0.4900, 0.4900, 0.4920,  ..., 0.5240, 0.4880, 0.5060]],

         [[0.4900, 0.4900, 0.4920,  ..., 0.5240, 0.4880, 0.5060],
          [0.4900, 0.4900, 0.4920,  ..., 0.5240, 0.4880, 0.5060],
          [0.4900, 0.4900, 0.4920,  ..., 0.5240, 0.4880, 0.5060],
          [0.4900, 0.4900, 0.4920,  ..., 0.5240, 0.4880, 0.5060]]]],
       device='cuda:0')[batch=40/40]:
	 Train time/batch: 39
	 Train time/sample: 78
	 Train time/batch_in_epoch: 39
	 Train time/sample_in_epoch: 78
	 Train time/token: 79872
	 Train time/token_in_epoch: 79872
	 Train memory/current_allocated_mem: 1.1979
	 Train memory/current_active_mem: 1.1979
	 Train memory/current_inactive_mem: 0.7483
	 Train memory/current_reserved_mem: 3.8609
	 Train memory/peak_allocated_mem: 2.8091
	 Train memory/peak_active_mem: 2.8091
	 Train memory/peak_inactive_mem: 0.8676
	 Train memory/peak_reserved_mem: 3.8609
	 Train memory/alloc_retries: 0
	 Train trainer/device_train_microbatch_size: 10
	 Train loss/train/total: 0.0046
	 Train metrics/train/LanguageCrossEntropy: 9.3493
	 Train metrics/train/LanguagePerplexity: 11490.7705
	 Train metrics/train/TokenAccuracy: 0.1696
	 Train throughput/batches_per_sec: 0.4201
	 Train throughput/samples_per_sec: 0.8402
	 Train throughput/device/batches_per_sec: 0.4201
	 Train throughput/device/samples_per_sec: 0.8402
	 Train throughput/tokens_per_sec: 860.3856
	 Train throughput/device/tokens_per_sec: 860.3856
	 Train time/train: 0.0344
	 Train time/val: 0.0000
	 Train time/total: 0.0344
	 Train lr-DecoupledAdamW/group0: 0.0001
	 Train time/remaining_estimate: 0.0000
	 Train metrics/shannon_entropy: 10.5679
	 Train metrics/batch_shannon_entropy: <wandb.sdk.data_types.table.Table object at 0x77b707caac90>
	 Train metrics/seq_shannon_entropy: CustomChart(table=<wandb.sdk.data_types.table.Table object at 0x77b9a1850920>, spec=CustomChartSpec(spec_name='wandb/lineseries/v0', fields={'step': 'step', 'lineKey': 'lineKey', 'lineVal': 'lineVal'}, string_fields={'title': 'Per-Layer Shannon Entropy', 'xname': 'Block Index'}, key='', panel_type='Vega2', split_table=False))
	 Train metrics/exit_entropy: 0.6680
	 Train metrics/batch_exit_entropy: <wandb.sdk.data_types.table.Table object at 0x77b9a1b2f8f0>
	 Train metrics/seq_exit_entropy: CustomChart(table=<wandb.sdk.data_types.table.Table object at 0x77b99b36b3b0>, spec=CustomChartSpec(spec_name='wandb/lineseries/v0', fields={'step': 'step', 'lineKey': 'lineKey', 'lineVal': 'lineVal'}, string_fields={'title': 'Per-Layer Exit Entropy', 'xname': 'Block Index'}, key='', panel_type='Vega2', split_table=False))
	 Train expert_selection/ffn_layer: <wandb.sdk.data_types.image.Image object at 0x77b9a00a1640>
	 Train expert_selection/attn_o_layer: <wandb.sdk.data_types.image.Image object at 0x77b99b3ac860>
	 Train expert_selection/attn_v_layer: <wandb.sdk.data_types.image.Image object at 0x77b9a0459700>
	 Train l2_norm/moment/model.transformer.router: 0.0001
	 Train l2_norm/param/model.transformer.router: 0.3625
	 Train l2_norm/update/model.transformer.router: 0.0010
	 Train l2_norm/grad/model.transformer.router: 0.0001
	 Train l2_norm/moment/model.transformer.tau: 0.0000
	 Train l2_norm/param/model.transformer.tau: 1.0009
	 Train l2_norm/update/model.transformer.tau: 0.0000
	 Train l2_norm/grad/model.transformer.tau: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.attention.v: 0.0002
	 Train l2_norm/param/model.transformer.layers.0.attention.v: 20.4801
	 Train l2_norm/update/model.transformer.layers.0.attention.v: 0.0179
	 Train l2_norm/grad/model.transformer.layers.0.attention.v: 0.0004
	 Train l2_norm/moment/model.transformer.layers.0.attention.o: 0.0003
	 Train l2_norm/param/model.transformer.layers.0.attention.o: 22.6886
	 Train l2_norm/update/model.transformer.layers.0.attention.o: 0.0226
	 Train l2_norm/grad/model.transformer.layers.0.attention.o: 0.0004
	 Train l2_norm/moment/model.transformer.layers.0.attention.sel_v: 0.0001
	 Train l2_norm/param/model.transformer.layers.0.attention.sel_v: 2.2599
	 Train l2_norm/update/model.transformer.layers.0.attention.sel_v: 0.0017
	 Train l2_norm/grad/model.transformer.layers.0.attention.sel_v: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.attention.sel_o: 0.0001
	 Train l2_norm/param/model.transformer.layers.0.attention.sel_o: 2.2417
	 Train l2_norm/update/model.transformer.layers.0.attention.sel_o: 0.0016
	 Train l2_norm/grad/model.transformer.layers.0.attention.sel_o: 0.0001
	 Train l2_norm/moment/model.transformer.layers.0.attention.q.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attention.q.weight: 6.4854
	 Train l2_norm/update/model.transformer.layers.0.attention.q.weight: 0.0074
	 Train l2_norm/grad/model.transformer.layers.0.attention.q.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.attention.k.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attention.k.weight: 6.4759
	 Train l2_norm/update/model.transformer.layers.0.attention.k.weight: 0.0073
	 Train l2_norm/grad/model.transformer.layers.0.attention.k.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.ffn.keys: 0.0002
	 Train l2_norm/param/model.transformer.layers.0.ffn.keys: 15.0009
	 Train l2_norm/update/model.transformer.layers.0.ffn.keys: 0.0172
	 Train l2_norm/grad/model.transformer.layers.0.ffn.keys: 0.0003
	 Train l2_norm/moment/model.transformer.layers.0.ffn.values: 0.0004
	 Train l2_norm/param/model.transformer.layers.0.ffn.values: 7.1832
	 Train l2_norm/update/model.transformer.layers.0.ffn.values: 0.0157
	 Train l2_norm/grad/model.transformer.layers.0.ffn.values: 0.0007
	 Train l2_norm/moment/model.transformer.layers.0.ffn.expert_sel: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.ffn.expert_sel: 4.7507
	 Train l2_norm/update/model.transformer.layers.0.ffn.expert_sel: 0.0054
	 Train l2_norm/grad/model.transformer.layers.0.ffn.expert_sel: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.attn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attn_pre.weight: 20.2975
	 Train l2_norm/update/model.transformer.layers.0.attn_pre.weight: 0.0004
	 Train l2_norm/grad/model.transformer.layers.0.attn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.attn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.attn_post.weight: 20.2977
	 Train l2_norm/update/model.transformer.layers.0.attn_post.weight: 0.0005
	 Train l2_norm/grad/model.transformer.layers.0.attn_post.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.ffn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.ffn_pre.weight: 20.2976
	 Train l2_norm/update/model.transformer.layers.0.ffn_pre.weight: 0.0004
	 Train l2_norm/grad/model.transformer.layers.0.ffn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.0.ffn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.0.ffn_post.weight: 20.3028
	 Train l2_norm/update/model.transformer.layers.0.ffn_post.weight: 0.0006
	 Train l2_norm/grad/model.transformer.layers.0.ffn_post.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.v: 0.0001
	 Train l2_norm/param/model.transformer.layers.1.attention.v: 20.4927
	 Train l2_norm/update/model.transformer.layers.1.attention.v: 0.0161
	 Train l2_norm/grad/model.transformer.layers.1.attention.v: 0.0003
	 Train l2_norm/moment/model.transformer.layers.1.attention.o: 0.0002
	 Train l2_norm/param/model.transformer.layers.1.attention.o: 22.6914
	 Train l2_norm/update/model.transformer.layers.1.attention.o: 0.0196
	 Train l2_norm/grad/model.transformer.layers.1.attention.o: 0.0003
	 Train l2_norm/moment/model.transformer.layers.1.attention.sel_v: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.sel_v: 2.2542
	 Train l2_norm/update/model.transformer.layers.1.attention.sel_v: 0.0009
	 Train l2_norm/grad/model.transformer.layers.1.attention.sel_v: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.sel_o: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.sel_o: 2.2407
	 Train l2_norm/update/model.transformer.layers.1.attention.sel_o: 0.0014
	 Train l2_norm/grad/model.transformer.layers.1.attention.sel_o: 0.0001
	 Train l2_norm/moment/model.transformer.layers.1.attention.q.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.q.weight: 6.4908
	 Train l2_norm/update/model.transformer.layers.1.attention.q.weight: 0.0036
	 Train l2_norm/grad/model.transformer.layers.1.attention.q.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attention.k.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attention.k.weight: 6.4856
	 Train l2_norm/update/model.transformer.layers.1.attention.k.weight: 0.0035
	 Train l2_norm/grad/model.transformer.layers.1.attention.k.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn.keys: 0.0001
	 Train l2_norm/param/model.transformer.layers.1.ffn.keys: 15.0052
	 Train l2_norm/update/model.transformer.layers.1.ffn.keys: 0.0151
	 Train l2_norm/grad/model.transformer.layers.1.ffn.keys: 0.0002
	 Train l2_norm/moment/model.transformer.layers.1.ffn.values: 0.0003
	 Train l2_norm/param/model.transformer.layers.1.ffn.values: 7.1829
	 Train l2_norm/update/model.transformer.layers.1.ffn.values: 0.0156
	 Train l2_norm/grad/model.transformer.layers.1.ffn.values: 0.0005
	 Train l2_norm/moment/model.transformer.layers.1.ffn.expert_sel: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.ffn.expert_sel: 4.7524
	 Train l2_norm/update/model.transformer.layers.1.ffn.expert_sel: 0.0049
	 Train l2_norm/grad/model.transformer.layers.1.ffn.expert_sel: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attn_pre.weight: 20.2974
	 Train l2_norm/update/model.transformer.layers.1.attn_pre.weight: 0.0003
	 Train l2_norm/grad/model.transformer.layers.1.attn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.attn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.attn_post.weight: 20.2914
	 Train l2_norm/update/model.transformer.layers.1.attn_post.weight: 0.0006
	 Train l2_norm/grad/model.transformer.layers.1.attn_post.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn_pre.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.ffn_pre.weight: 20.2970
	 Train l2_norm/update/model.transformer.layers.1.ffn_pre.weight: 0.0004
	 Train l2_norm/grad/model.transformer.layers.1.ffn_pre.weight: 0.0000
	 Train l2_norm/moment/model.transformer.layers.1.ffn_post.weight: 0.0000
	 Train l2_norm/param/model.transformer.layers.1.ffn_post.weight: 20.2978
	 Train l2_norm/update/model.transformer.layers.1.ffn_post.weight: 0.0004
	 Train l2_norm/grad/model.transformer.layers.1.ffn_post.weight: 0.0000
	 Train l2_norm/moment/model.embedding.weight: 0.0001
	 Train l2_norm/param/model.embedding.weight: 221.7064
	 Train l2_norm/update/model.embedding.weight: 0.0138
	 Train l2_norm/grad/model.embedding.weight: 0.0002
	 Train l2_norm/moment/model.lm_head.weight: 0.0006
	 Train l2_norm/param/model.lm_head.weight: 127.9571
	 Train l2_norm/update/model.lm_head.weight: 0.0729
	 Train l2_norm/grad/model.lm_head.weight: 0.0008
	 Train l2_norm/moment/model.lm_head.bias: 0.0000
	 Train l2_norm/param/model.lm_head.bias: 6.3163
	 Train l2_norm/update/model.lm_head.bias: 0.0048
	 Train l2_norm/grad/model.lm_head.bias: 0.0000
	 Train l2_norm/moment/model.out_norm.weight: 0.0000
	 Train l2_norm/param/model.out_norm.weight: 20.3048
	 Train l2_norm/update/model.out_norm.weight: 0.0009
	 Train l2_norm/grad/model.out_norm.weight: 0.0000
	 Train l2_norm/grad/global: 0.0014
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                         activations/average/_output.0 ‚ñà‚ñÅ
wandb:                          activations/average/model.embedding_output.0 ‚ñÅ‚ñà
wandb:                          activations/average/model.embedding_output.1 ‚ñà‚ñÅ
wandb:                             activations/average/model.lm_head_input.0 ‚ñà‚ñÅ
wandb:                            activations/average/model.lm_head_output.0 ‚ñà‚ñÅ
wandb:                            activations/average/model.lm_head_output.1 ‚ñà‚ñÅ
wandb:                            activations/average/model.out_norm_input.0 ‚ñà‚ñÅ
wandb:                           activations/average/model.out_norm_output.0 ‚ñà‚ñÅ
wandb:                           activations/average/model.out_norm_output.1 ‚ñà‚ñÅ
wandb:    activations/average/model.transformer.layers.0.attention.k_input.0 ‚ñà‚ñÅ
wandb:   activations/average/model.transformer.layers.0.attention.k_output.0 ‚ñà‚ñÅ
wandb:   activations/average/model.transformer.layers.0.attention.k_output.1 ‚ñÅ‚ñà
wandb:   activations/average/model.transformer.layers.0.attention.pe_input.0 ‚ñÅ‚ñà
wandb:   activations/average/model.transformer.layers.0.attention.pe_input.1 ‚ñà‚ñÅ
wandb:  activations/average/model.transformer.layers.0.attention.pe_output.0 ‚ñÅ‚ñà
wandb:  activations/average/model.transformer.layers.0.attention.pe_output.1 ‚ñà‚ñÅ
wandb:    activations/average/model.transformer.layers.0.attention.q_input.0 ‚ñà‚ñÅ
wandb:   activations/average/model.transformer.layers.0.attention.q_output.0 ‚ñÅ‚ñà
wandb:   activations/average/model.transformer.layers.0.attention.q_output.1 ‚ñà‚ñÅ
wandb:      activations/average/model.transformer.layers.0.attention_input.0 ‚ñà‚ñÅ
wandb:      activations/average/model.transformer.layers.0.attention_input.1 ‚ñà‚ñÅ
wandb:      activations/average/model.transformer.layers.0.attention_input.2 ‚ñà‚ñÅ
wandb:     activations/average/model.transformer.layers.0.attention_output.0 ‚ñà‚ñÅ
wandb:      activations/average/model.transformer.layers.0.attn_post_input.0 ‚ñà‚ñÅ
wandb:     activations/average/model.transformer.layers.0.attn_post_output.0 ‚ñà‚ñÅ
wandb:     activations/average/model.transformer.layers.0.attn_post_output.1 ‚ñà‚ñÅ
wandb:       activations/average/model.transformer.layers.0.attn_pre_input.0 ‚ñà‚ñÅ
wandb:      activations/average/model.transformer.layers.0.attn_pre_output.0 ‚ñÅ‚ñà
wandb:      activations/average/model.transformer.layers.0.attn_pre_output.1 ‚ñà‚ñÅ
wandb:            activations/average/model.transformer.layers.0.ffn_input.0 ‚ñà‚ñÅ
wandb:            activations/average/model.transformer.layers.0.ffn_input.1 ‚ñà‚ñÅ
wandb:           activations/average/model.transformer.layers.0.ffn_output.0 ‚ñà‚ñÅ
wandb:       activations/average/model.transformer.layers.0.ffn_post_input.0 ‚ñà‚ñÅ
wandb:      activations/average/model.transformer.layers.0.ffn_post_output.0 ‚ñà‚ñÅ
wandb:      activations/average/model.transformer.layers.0.ffn_post_output.1 ‚ñà‚ñÅ
wandb:        activations/average/model.transformer.layers.0.ffn_pre_input.0 ‚ñà‚ñÅ
wandb:       activations/average/model.transformer.layers.0.ffn_pre_output.0 ‚ñà‚ñÅ
wandb:       activations/average/model.transformer.layers.0.ffn_pre_output.1 ‚ñà‚ñÅ
wandb:                activations/average/model.transformer.layers.0_input.0 ‚ñà‚ñÅ
wandb:                activations/average/model.transformer.layers.0_input.2 ‚ñà‚ñÅ
wandb:                activations/average/model.transformer.layers.0_input.3 ‚ñÅ‚ñà
wandb:                activations/average/model.transformer.layers.0_input.4 ‚ñà‚ñÅ
wandb:                activations/average/model.transformer.layers.0_input.5 ‚ñÅ‚ñà
wandb:               activations/average/model.transformer.layers.0_output.0 ‚ñà‚ñÅ
wandb:               activations/average/model.transformer.layers.0_output.4 ‚ñà‚ñÅ
wandb:    activations/average/model.transformer.layers.1.attention.k_input.0 ‚ñà‚ñÅ
wandb:   activations/average/model.transformer.layers.1.attention.k_output.0 ‚ñÅ‚ñà
wandb:   activations/average/model.transformer.layers.1.attention.k_output.1 ‚ñà‚ñÅ
wandb:   activations/average/model.transformer.layers.1.attention.pe_input.0 ‚ñÅ‚ñà
wandb:   activations/average/model.transformer.layers.1.attention.pe_input.1 ‚ñÅ‚ñà
wandb:  activations/average/model.transformer.layers.1.attention.pe_output.0 ‚ñÅ‚ñà
wandb:  activations/average/model.transformer.layers.1.attention.pe_output.1 ‚ñÅ‚ñà
wandb:    activations/average/model.transformer.layers.1.attention.q_input.0 ‚ñà‚ñÅ
wandb:   activations/average/model.transformer.layers.1.attention.q_output.0 ‚ñÅ‚ñà
wandb:   activations/average/model.transformer.layers.1.attention.q_output.1 ‚ñÅ‚ñà
wandb:      activations/average/model.transformer.layers.1.attention_input.0 ‚ñà‚ñÅ
wandb:      activations/average/model.transformer.layers.1.attention_input.1 ‚ñà‚ñÅ
wandb:      activations/average/model.transformer.layers.1.attention_input.2 ‚ñà‚ñÅ
wandb:     activations/average/model.transformer.layers.1.attention_output.0 ‚ñà‚ñÅ
wandb:      activations/average/model.transformer.layers.1.attn_post_input.0 ‚ñà‚ñÅ
wandb:     activations/average/model.transformer.layers.1.attn_post_output.0 ‚ñÅ‚ñà
wandb:     activations/average/model.transformer.layers.1.attn_post_output.1 ‚ñà‚ñÅ
wandb:       activations/average/model.transformer.layers.1.attn_pre_input.0 ‚ñà‚ñÅ
wandb:      activations/average/model.transformer.layers.1.attn_pre_output.0 ‚ñà‚ñÅ
wandb:      activations/average/model.transformer.layers.1.attn_pre_output.1 ‚ñà‚ñÅ
wandb:            activations/average/model.transformer.layers.1.ffn_input.0 ‚ñà‚ñÅ
wandb:            activations/average/model.transformer.layers.1.ffn_input.1 ‚ñà‚ñÅ
wandb:           activations/average/model.transformer.layers.1.ffn_output.0 ‚ñà‚ñÅ
wandb:       activations/average/model.transformer.layers.1.ffn_post_input.0 ‚ñà‚ñÅ
wandb:      activations/average/model.transformer.layers.1.ffn_post_output.0 ‚ñà‚ñÅ
wandb:      activations/average/model.transformer.layers.1.ffn_post_output.1 ‚ñà‚ñÅ
wandb:        activations/average/model.transformer.layers.1.ffn_pre_input.0 ‚ñà‚ñÅ
wandb:       activations/average/model.transformer.layers.1.ffn_pre_output.0 ‚ñà‚ñÅ
wandb:       activations/average/model.transformer.layers.1.ffn_pre_output.1 ‚ñà‚ñÅ
wandb:                activations/average/model.transformer.layers.1_input.0 ‚ñà‚ñÅ
wandb:                activations/average/model.transformer.layers.1_input.2 ‚ñà‚ñÅ
wandb:                activations/average/model.transformer.layers.1_input.3 ‚ñÅ‚ñà
wandb:                activations/average/model.transformer.layers.1_input.4 ‚ñà‚ñÅ
wandb:                activations/average/model.transformer.layers.1_input.5 ‚ñÅ‚ñà
wandb:               activations/average/model.transformer.layers.1_output.0 ‚ñà‚ñÅ
wandb:               activations/average/model.transformer.layers.1_output.4 ‚ñà‚ñÅ
wandb:                         activations/average/model.transformer_input.0 ‚ñà‚ñÅ
wandb:                         activations/average/model.transformer_input.1 ‚ñà‚ñÅ
wandb:                        activations/average/model.transformer_output.0 ‚ñà‚ñÅ
wandb:                                    activations/average/model_output.0 ‚ñà‚ñÅ
wandb:                                        activations/kurtosis/_output.0 ‚ñÅ‚ñà
wandb:                         activations/kurtosis/model.embedding_output.0 ‚ñÅ‚ñà
wandb:                         activations/kurtosis/model.embedding_output.1 ‚ñÅ‚ñà
wandb:                            activations/kurtosis/model.lm_head_input.0 ‚ñà‚ñÅ
wandb:                           activations/kurtosis/model.lm_head_output.0 ‚ñÅ‚ñà
wandb:                           activations/kurtosis/model.lm_head_output.1 ‚ñÅ‚ñà
wandb:                           activations/kurtosis/model.out_norm_input.0 ‚ñà‚ñÅ
wandb:                          activations/kurtosis/model.out_norm_output.0 ‚ñà‚ñÅ
wandb:                          activations/kurtosis/model.out_norm_output.1 ‚ñà‚ñÅ
wandb:   activations/kurtosis/model.transformer.layers.0.attention.k_input.0 ‚ñÅ‚ñà
wandb:  activations/kurtosis/model.transformer.layers.0.attention.k_output.0 ‚ñÅ‚ñà
wandb:  activations/kurtosis/model.transformer.layers.0.attention.k_output.1 ‚ñà‚ñÅ
wandb:  activations/kurtosis/model.transformer.layers.0.attention.pe_input.0 ‚ñÅ‚ñà
wandb:  activations/kurtosis/model.transformer.layers.0.attention.pe_input.1 ‚ñà‚ñÅ
wandb: activations/kurtosis/model.transformer.layers.0.attention.pe_output.0 ‚ñÅ‚ñà
wandb: activations/kurtosis/model.transformer.layers.0.attention.pe_output.1 ‚ñà‚ñÅ
wandb:   activations/kurtosis/model.transformer.layers.0.attention.q_input.0 ‚ñÅ‚ñà
wandb:  activations/kurtosis/model.transformer.layers.0.attention.q_output.0 ‚ñÅ‚ñà
wandb:  activations/kurtosis/model.transformer.layers.0.attention.q_output.1 ‚ñà‚ñÅ
wandb:     activations/kurtosis/model.transformer.layers.0.attention_input.0 ‚ñÅ‚ñà
wandb:     activations/kurtosis/model.transformer.layers.0.attention_input.1 ‚ñÅ‚ñà
wandb:     activations/kurtosis/model.transformer.layers.0.attention_input.2 ‚ñÅ‚ñà
wandb:      activations/kurtosis/model.transformer.layers.0.attn_pre_input.0 ‚ñÅ‚ñà
wandb:     activations/kurtosis/model.transformer.layers.0.attn_pre_output.0 ‚ñÅ‚ñà
wandb:     activations/kurtosis/model.transformer.layers.0.attn_pre_output.1 ‚ñÅ‚ñà
wandb:           activations/kurtosis/model.transformer.layers.0.ffn_input.0 ‚ñÅ‚ñà
wandb:           activations/kurtosis/model.transformer.layers.0.ffn_input.1 ‚ñÅ‚ñà
wandb:          activations/kurtosis/model.transformer.layers.0.ffn_output.0 ‚ñà‚ñÅ
wandb:      activations/kurtosis/model.transformer.layers.0.ffn_post_input.0 ‚ñà‚ñÅ
wandb:     activations/kurtosis/model.transformer.layers.0.ffn_post_output.0 ‚ñà‚ñÅ
wandb:     activations/kurtosis/model.transformer.layers.0.ffn_post_output.1 ‚ñà‚ñÅ
wandb:       activations/kurtosis/model.transformer.layers.0.ffn_pre_input.0 ‚ñÅ‚ñà
wandb:      activations/kurtosis/model.transformer.layers.0.ffn_pre_output.0 ‚ñÅ‚ñà
wandb:      activations/kurtosis/model.transformer.layers.0.ffn_pre_output.1 ‚ñÅ‚ñà
wandb:               activations/kurtosis/model.transformer.layers.0_input.0 ‚ñà‚ñÅ
wandb:               activations/kurtosis/model.transformer.layers.0_input.2 ‚ñÅ‚ñà
wandb:               activations/kurtosis/model.transformer.layers.0_input.3 ‚ñà‚ñÅ
wandb:               activations/kurtosis/model.transformer.layers.0_input.4 ‚ñà‚ñÅ
wandb:              activations/kurtosis/model.transformer.layers.0_output.0 ‚ñà‚ñÅ
wandb:              activations/kurtosis/model.transformer.layers.0_output.4 ‚ñà‚ñÅ
wandb:   activations/kurtosis/model.transformer.layers.1.attention.k_input.0 ‚ñà‚ñÅ
wandb:  activations/kurtosis/model.transformer.layers.1.attention.k_output.0 ‚ñà‚ñÅ
wandb:  activations/kurtosis/model.transformer.layers.1.attention.k_output.1 ‚ñà‚ñÅ
wandb:  activations/kurtosis/model.transformer.layers.1.attention.pe_input.1 ‚ñà‚ñÅ
wandb: activations/kurtosis/model.transformer.layers.1.attention.pe_output.1 ‚ñà‚ñÅ
wandb:     activations/kurtosis/model.transformer.layers.1.attention_input.1 ‚ñà‚ñÅ
wandb:     activations/kurtosis/model.transformer.layers.1.attention_input.2 ‚ñà‚ñÅ
wandb:      activations/kurtosis/model.transformer.layers.1.attn_pre_input.0 ‚ñà‚ñÅ
wandb:     activations/kurtosis/model.transformer.layers.1.attn_pre_output.0 ‚ñà‚ñÅ
wandb:     activations/kurtosis/model.transformer.layers.1.attn_pre_output.1 ‚ñà‚ñÅ
wandb:       activations/kurtosis/model.transformer.layers.1.ffn_pre_input.0 ‚ñà‚ñÅ
wandb:      activations/kurtosis/model.transformer.layers.1.ffn_pre_output.0 ‚ñà‚ñÅ
wandb:      activations/kurtosis/model.transformer.layers.1.ffn_pre_output.1 ‚ñà‚ñÅ
wandb:               activations/kurtosis/model.transformer.layers.1_input.0 ‚ñà‚ñÅ
wandb:               activations/kurtosis/model.transformer.layers.1_input.2 ‚ñÅ‚ñà
wandb:               activations/kurtosis/model.transformer.layers.1_input.3 ‚ñà‚ñÅ
wandb:               activations/kurtosis/model.transformer.layers.1_input.4 ‚ñà‚ñÅ
wandb:              activations/kurtosis/model.transformer.layers.1_output.0 ‚ñà‚ñÅ
wandb:              activations/kurtosis/model.transformer.layers.1_output.4 ‚ñà‚ñÅ
wandb:                        activations/kurtosis/model.transformer_input.0 ‚ñÅ‚ñà
wandb:                        activations/kurtosis/model.transformer_input.1 ‚ñÅ‚ñà
wandb:                       activations/kurtosis/model.transformer_output.0 ‚ñà‚ñÅ
wandb:                                   activations/kurtosis/model_output.0 ‚ñÅ‚ñà
wandb:                                         activations/l2_norm/_output.0 ‚ñà‚ñÅ
wandb:                          activations/l2_norm/model.embedding_output.0 ‚ñÅ‚ñà
wandb:                          activations/l2_norm/model.embedding_output.1 ‚ñÅ‚ñà
wandb:                             activations/l2_norm/model.lm_head_input.0 ‚ñÅ‚ñà
wandb:                            activations/l2_norm/model.lm_head_output.0 ‚ñà‚ñÅ
wandb:                            activations/l2_norm/model.lm_head_output.1 ‚ñÅ‚ñà
wandb:                            activations/l2_norm/model.out_norm_input.0 ‚ñÅ‚ñà
wandb:                           activations/l2_norm/model.out_norm_output.0 ‚ñÅ‚ñà
wandb:                           activations/l2_norm/model.out_norm_output.1 ‚ñÅ‚ñà
wandb:    activations/l2_norm/model.transformer.layers.0.attention.k_input.0 ‚ñà‚ñÅ
wandb:   activations/l2_norm/model.transformer.layers.0.attention.k_output.0 ‚ñà‚ñÅ
wandb:   activations/l2_norm/model.transformer.layers.0.attention.k_output.1 ‚ñÅ‚ñà
wandb:   activations/l2_norm/model.transformer.layers.0.attention.pe_input.0 ‚ñÅ‚ñà
wandb:   activations/l2_norm/model.transformer.layers.0.attention.pe_input.1 ‚ñà‚ñÅ
wandb:  activations/l2_norm/model.transformer.layers.0.attention.pe_output.0 ‚ñÅ‚ñà
wandb:  activations/l2_norm/model.transformer.layers.0.attention.pe_output.1 ‚ñà‚ñÅ
wandb:    activations/l2_norm/model.transformer.layers.0.attention.q_input.0 ‚ñà‚ñÅ
wandb:   activations/l2_norm/model.transformer.layers.0.attention.q_output.0 ‚ñÅ‚ñà
wandb:   activations/l2_norm/model.transformer.layers.0.attention.q_output.1 ‚ñà‚ñÅ
wandb:      activations/l2_norm/model.transformer.layers.0.attention_input.0 ‚ñà‚ñÅ
wandb:      activations/l2_norm/model.transformer.layers.0.attention_input.1 ‚ñà‚ñÅ
wandb:      activations/l2_norm/model.transformer.layers.0.attention_input.2 ‚ñà‚ñÅ
wandb:     activations/l2_norm/model.transformer.layers.0.attention_output.0 ‚ñÅ‚ñà
wandb:      activations/l2_norm/model.transformer.layers.0.attn_post_input.0 ‚ñÅ‚ñà
wandb:     activations/l2_norm/model.transformer.layers.0.attn_post_output.0 ‚ñÅ‚ñà
wandb:     activations/l2_norm/model.transformer.layers.0.attn_post_output.1 ‚ñÅ‚ñà
wandb:       activations/l2_norm/model.transformer.layers.0.attn_pre_input.0 ‚ñÅ‚ñà
wandb:      activations/l2_norm/model.transformer.layers.0.attn_pre_output.0 ‚ñÅ‚ñà
wandb:      activations/l2_norm/model.transformer.layers.0.attn_pre_output.1 ‚ñà‚ñÅ
wandb:            activations/l2_norm/model.transformer.layers.0.ffn_input.0 ‚ñÅ‚ñà
wandb:            activations/l2_norm/model.transformer.layers.0.ffn_input.1 ‚ñÅ‚ñà
wandb:           activations/l2_norm/model.transformer.layers.0.ffn_output.0 ‚ñÅ‚ñà
wandb:       activations/l2_norm/model.transformer.layers.0.ffn_post_input.0 ‚ñÅ‚ñà
wandb:      activations/l2_norm/model.transformer.layers.0.ffn_post_output.0 ‚ñÅ‚ñà
wandb:      activations/l2_norm/model.transformer.layers.0.ffn_post_output.1 ‚ñÅ‚ñà
wandb:        activations/l2_norm/model.transformer.layers.0.ffn_pre_input.0 ‚ñÅ‚ñà
wandb:       activations/l2_norm/model.transformer.layers.0.ffn_pre_output.0 ‚ñÅ‚ñà
wandb:       activations/l2_norm/model.transformer.layers.0.ffn_pre_output.1 ‚ñÅ‚ñà
wandb:                activations/l2_norm/model.transformer.layers.0_input.0 ‚ñÅ‚ñà
wandb:                activations/l2_norm/model.transformer.layers.0_input.2 ‚ñÅ‚ñà
wandb:                activations/l2_norm/model.transformer.layers.0_input.3 ‚ñÅ‚ñà
wandb:                activations/l2_norm/model.transformer.layers.0_input.4 ‚ñà‚ñÅ
wandb:                activations/l2_norm/model.transformer.layers.0_input.5 ‚ñÅ‚ñà
wandb:               activations/l2_norm/model.transformer.layers.0_output.0 ‚ñÅ‚ñà
wandb:               activations/l2_norm/model.transformer.layers.0_output.4 ‚ñà‚ñÅ
wandb:    activations/l2_norm/model.transformer.layers.1.attention.k_input.0 ‚ñÅ‚ñà
wandb:   activations/l2_norm/model.transformer.layers.1.attention.k_output.0 ‚ñà‚ñÅ
wandb:   activations/l2_norm/model.transformer.layers.1.attention.k_output.1 ‚ñà‚ñÅ
wandb:   activations/l2_norm/model.transformer.layers.1.attention.pe_input.0 ‚ñÅ‚ñà
wandb:   activations/l2_norm/model.transformer.layers.1.attention.pe_input.1 ‚ñà‚ñÅ
wandb:  activations/l2_norm/model.transformer.layers.1.attention.pe_output.0 ‚ñÅ‚ñà
wandb:  activations/l2_norm/model.transformer.layers.1.attention.pe_output.1 ‚ñà‚ñÅ
wandb:    activations/l2_norm/model.transformer.layers.1.attention.q_input.0 ‚ñÅ‚ñà
wandb:   activations/l2_norm/model.transformer.layers.1.attention.q_output.0 ‚ñÅ‚ñà
wandb:   activations/l2_norm/model.transformer.layers.1.attention.q_output.1 ‚ñÅ‚ñà
wandb:      activations/l2_norm/model.transformer.layers.1.attention_input.0 ‚ñÅ‚ñà
wandb:      activations/l2_norm/model.transformer.layers.1.attention_input.1 ‚ñÅ‚ñà
wandb:      activations/l2_norm/model.transformer.layers.1.attention_input.2 ‚ñÅ‚ñà
wandb:     activations/l2_norm/model.transformer.layers.1.attention_output.0 ‚ñÅ‚ñà
wandb:      activations/l2_norm/model.transformer.layers.1.attn_post_input.0 ‚ñÅ‚ñà
wandb:     activations/l2_norm/model.transformer.layers.1.attn_post_output.0 ‚ñÅ‚ñà
wandb:     activations/l2_norm/model.transformer.layers.1.attn_post_output.1 ‚ñÅ‚ñà
wandb:       activations/l2_norm/model.transformer.layers.1.attn_pre_input.0 ‚ñÅ‚ñà
wandb:      activations/l2_norm/model.transformer.layers.1.attn_pre_output.0 ‚ñÅ‚ñà
wandb:      activations/l2_norm/model.transformer.layers.1.attn_pre_output.1 ‚ñÅ‚ñà
wandb:            activations/l2_norm/model.transformer.layers.1.ffn_input.0 ‚ñÅ‚ñà
wandb:            activations/l2_norm/model.transformer.layers.1.ffn_input.1 ‚ñÅ‚ñà
wandb:           activations/l2_norm/model.transformer.layers.1.ffn_output.0 ‚ñÅ‚ñà
wandb:       activations/l2_norm/model.transformer.layers.1.ffn_post_input.0 ‚ñÅ‚ñà
wandb:      activations/l2_norm/model.transformer.layers.1.ffn_post_output.0 ‚ñÅ‚ñà
wandb:      activations/l2_norm/model.transformer.layers.1.ffn_post_output.1 ‚ñÅ‚ñà
wandb:        activations/l2_norm/model.transformer.layers.1.ffn_pre_input.0 ‚ñÅ‚ñà
wandb:       activations/l2_norm/model.transformer.layers.1.ffn_pre_output.0 ‚ñÅ‚ñà
wandb:       activations/l2_norm/model.transformer.layers.1.ffn_pre_output.1 ‚ñÅ‚ñà
wandb:                activations/l2_norm/model.transformer.layers.1_input.0 ‚ñÅ‚ñà
wandb:                activations/l2_norm/model.transformer.layers.1_input.2 ‚ñÅ‚ñà
wandb:                activations/l2_norm/model.transformer.layers.1_input.3 ‚ñÅ‚ñà
wandb:                activations/l2_norm/model.transformer.layers.1_input.4 ‚ñà‚ñÅ
wandb:                activations/l2_norm/model.transformer.layers.1_input.5 ‚ñÅ‚ñà
wandb:               activations/l2_norm/model.transformer.layers.1_output.0 ‚ñÅ‚ñà
wandb:               activations/l2_norm/model.transformer.layers.1_output.4 ‚ñà‚ñÅ
wandb:                         activations/l2_norm/model.transformer_input.0 ‚ñÅ‚ñà
wandb:                         activations/l2_norm/model.transformer_input.1 ‚ñÅ‚ñà
wandb:                        activations/l2_norm/model.transformer_output.0 ‚ñÅ‚ñà
wandb:                                    activations/l2_norm/model_output.0 ‚ñà‚ñÅ
wandb:                                             activations/max/_output.0 ‚ñÅ‚ñà
wandb:                              activations/max/model.embedding_output.0 ‚ñà‚ñÅ
wandb:                              activations/max/model.embedding_output.1 ‚ñà‚ñÅ
wandb:                                 activations/max/model.lm_head_input.0 ‚ñà‚ñÅ
wandb:                                activations/max/model.lm_head_output.0 ‚ñÅ‚ñà
wandb:                                activations/max/model.lm_head_output.1 ‚ñÅ‚ñà
wandb:                                activations/max/model.out_norm_input.0 ‚ñÅ‚ñà
wandb:                               activations/max/model.out_norm_output.0 ‚ñà‚ñÅ
wandb:                               activations/max/model.out_norm_output.1 ‚ñà‚ñÅ
wandb:        activations/max/model.transformer.layers.0.attention.k_input.0 ‚ñà‚ñÅ
wandb:       activations/max/model.transformer.layers.0.attention.k_output.0 ‚ñà‚ñÅ
wandb:       activations/max/model.transformer.layers.0.attention.k_output.1 ‚ñÅ‚ñà
wandb:       activations/max/model.transformer.layers.0.attention.pe_input.0 ‚ñÅ‚ñà
wandb:       activations/max/model.transformer.layers.0.attention.pe_input.1 ‚ñà‚ñÅ
wandb:      activations/max/model.transformer.layers.0.attention.pe_output.0 ‚ñÅ‚ñà
wandb:      activations/max/model.transformer.layers.0.attention.pe_output.1 ‚ñÅ‚ñà
wandb:        activations/max/model.transformer.layers.0.attention.q_input.0 ‚ñà‚ñÅ
wandb:       activations/max/model.transformer.layers.0.attention.q_output.0 ‚ñÅ‚ñà
wandb:       activations/max/model.transformer.layers.0.attention.q_output.1 ‚ñà‚ñÅ
wandb:          activations/max/model.transformer.layers.0.attention_input.0 ‚ñà‚ñÅ
wandb:          activations/max/model.transformer.layers.0.attention_input.1 ‚ñà‚ñÅ
wandb:          activations/max/model.transformer.layers.0.attention_input.2 ‚ñà‚ñÅ
wandb:         activations/max/model.transformer.layers.0.attention_output.0 ‚ñÅ‚ñà
wandb:          activations/max/model.transformer.layers.0.attn_post_input.0 ‚ñÅ‚ñà
wandb:         activations/max/model.transformer.layers.0.attn_post_output.0 ‚ñÅ‚ñà
wandb:         activations/max/model.transformer.layers.0.attn_post_output.1 ‚ñÅ‚ñÅ
wandb:           activations/max/model.transformer.layers.0.attn_pre_input.0 ‚ñà‚ñÅ
wandb:          activations/max/model.transformer.layers.0.attn_pre_output.0 ‚ñà‚ñÅ
wandb:          activations/max/model.transformer.layers.0.attn_pre_output.1 ‚ñà‚ñÅ
wandb:                activations/max/model.transformer.layers.0.ffn_input.0 ‚ñÅ‚ñà
wandb:                activations/max/model.transformer.layers.0.ffn_input.1 ‚ñÅ‚ñà
wandb:               activations/max/model.transformer.layers.0.ffn_output.0 ‚ñÅ‚ñà
wandb:           activations/max/model.transformer.layers.0.ffn_post_input.0 ‚ñÅ‚ñà
wandb:          activations/max/model.transformer.layers.0.ffn_post_output.0 ‚ñÅ‚ñà
wandb:          activations/max/model.transformer.layers.0.ffn_post_output.1 ‚ñÅ‚ñà
wandb:            activations/max/model.transformer.layers.0.ffn_pre_input.0 ‚ñÅ‚ñà
wandb:           activations/max/model.transformer.layers.0.ffn_pre_output.0 ‚ñÅ‚ñà
wandb:           activations/max/model.transformer.layers.0.ffn_pre_output.1 ‚ñà‚ñÅ
wandb:                    activations/max/model.transformer.layers.0_input.0 ‚ñÅ‚ñà
wandb:                    activations/max/model.transformer.layers.0_input.2 ‚ñà‚ñÅ
wandb:                    activations/max/model.transformer.layers.0_input.3 ‚ñà‚ñÅ
wandb:                    activations/max/model.transformer.layers.0_input.4 ‚ñà‚ñÅ
wandb:                    activations/max/model.transformer.layers.0_input.5 ‚ñÅ‚ñà
wandb:                   activations/max/model.transformer.layers.0_output.0 ‚ñÅ‚ñà
wandb:                   activations/max/model.transformer.layers.0_output.4 ‚ñà‚ñÅ
wandb:        activations/max/model.transformer.layers.1.attention.k_input.0 ‚ñÅ‚ñà
wandb:       activations/max/model.transformer.layers.1.attention.k_output.0 ‚ñà‚ñÅ
wandb:       activations/max/model.transformer.layers.1.attention.k_output.1 ‚ñà‚ñÅ
wandb:       activations/max/model.transformer.layers.1.attention.pe_input.0 ‚ñÅ‚ñà
wandb:       activations/max/model.transformer.layers.1.attention.pe_input.1 ‚ñà‚ñÅ
wandb:      activations/max/model.transformer.layers.1.attention.pe_output.0 ‚ñÅ‚ñà
wandb:      activations/max/model.transformer.layers.1.attention.pe_output.1 ‚ñà‚ñÅ
wandb:        activations/max/model.transformer.layers.1.attention.q_input.0 ‚ñÅ‚ñà
wandb:       activations/max/model.transformer.layers.1.attention.q_output.0 ‚ñÅ‚ñÅ
wandb:       activations/max/model.transformer.layers.1.attention.q_output.1 ‚ñÅ‚ñà
wandb:          activations/max/model.transformer.layers.1.attention_input.0 ‚ñÅ‚ñà
wandb:          activations/max/model.transformer.layers.1.attention_input.1 ‚ñÅ‚ñà
wandb:          activations/max/model.transformer.layers.1.attention_input.2 ‚ñÅ‚ñà
wandb:         activations/max/model.transformer.layers.1.attention_output.0 ‚ñÅ‚ñà
wandb:          activations/max/model.transformer.layers.1.attn_post_input.0 ‚ñÅ‚ñà
wandb:         activations/max/model.transformer.layers.1.attn_post_output.0 ‚ñÅ‚ñà
wandb:         activations/max/model.transformer.layers.1.attn_post_output.1 ‚ñÅ‚ñà
wandb:           activations/max/model.transformer.layers.1.attn_pre_input.0 ‚ñÅ‚ñà
wandb:          activations/max/model.transformer.layers.1.attn_pre_output.0 ‚ñÅ‚ñà
wandb:          activations/max/model.transformer.layers.1.attn_pre_output.1 ‚ñà‚ñÅ
wandb:                activations/max/model.transformer.layers.1.ffn_input.0 ‚ñÅ‚ñà
wandb:                activations/max/model.transformer.layers.1.ffn_input.1 ‚ñÅ‚ñà
wandb:               activations/max/model.transformer.layers.1.ffn_output.0 ‚ñÅ‚ñà
wandb:           activations/max/model.transformer.layers.1.ffn_post_input.0 ‚ñÅ‚ñà
wandb:          activations/max/model.transformer.layers.1.ffn_post_output.0 ‚ñÅ‚ñà
wandb:          activations/max/model.transformer.layers.1.ffn_post_output.1 ‚ñÅ‚ñà
wandb:            activations/max/model.transformer.layers.1.ffn_pre_input.0 ‚ñÅ‚ñà
wandb:           activations/max/model.transformer.layers.1.ffn_pre_output.0 ‚ñà‚ñÅ
wandb:           activations/max/model.transformer.layers.1.ffn_pre_output.1 ‚ñà‚ñÅ
wandb:                    activations/max/model.transformer.layers.1_input.0 ‚ñÅ‚ñà
wandb:                    activations/max/model.transformer.layers.1_input.2 ‚ñà‚ñÅ
wandb:                    activations/max/model.transformer.layers.1_input.3 ‚ñà‚ñÅ
wandb:                    activations/max/model.transformer.layers.1_input.4 ‚ñà‚ñÅ
wandb:                    activations/max/model.transformer.layers.1_input.5 ‚ñÅ‚ñà
wandb:                   activations/max/model.transformer.layers.1_output.0 ‚ñÅ‚ñà
wandb:                   activations/max/model.transformer.layers.1_output.4 ‚ñà‚ñÅ
wandb:                             activations/max/model.transformer_input.0 ‚ñà‚ñÅ
wandb:                             activations/max/model.transformer_input.1 ‚ñà‚ñÅ
wandb:                            activations/max/model.transformer_output.0 ‚ñÅ‚ñà
wandb:                                        activations/max/model_output.0 ‚ñÅ‚ñà
wandb:                                                   l2_norm/grad/global ‚ñà‚ñÑ‚ñÇ‚ñÅ
wandb:                                   l2_norm/grad/model.embedding.weight ‚ñà‚ñÖ‚ñÅ‚ñÅ
wandb:                                       l2_norm/grad/model.lm_head.bias ‚ñà‚ñÜ‚ñÇ‚ñÅ
wandb:                                     l2_norm/grad/model.lm_head.weight ‚ñÅ‚ñÖ‚ñÜ‚ñà
wandb:                                    l2_norm/grad/model.out_norm.weight ‚ñÅ‚ñÅ‚ñÖ‚ñà
wandb:            l2_norm/grad/model.transformer.layers.0.attention.k.weight ‚ñà‚ñÜ‚ñÇ‚ñÅ
wandb:                   l2_norm/grad/model.transformer.layers.0.attention.o ‚ñà‚ñÖ‚ñÇ‚ñÅ
wandb:            l2_norm/grad/model.transformer.layers.0.attention.q.weight ‚ñà‚ñÜ‚ñÅ‚ñÅ
wandb:               l2_norm/grad/model.transformer.layers.0.attention.sel_o ‚ñà‚ñÖ‚ñÅ‚ñÖ
wandb:               l2_norm/grad/model.transformer.layers.0.attention.sel_v ‚ñà‚ñÑ‚ñÅ‚ñÉ
wandb:                   l2_norm/grad/model.transformer.layers.0.attention.v ‚ñà‚ñÖ‚ñÅ‚ñÅ
wandb:              l2_norm/grad/model.transformer.layers.0.attn_post.weight ‚ñà‚ñÑ‚ñÅ‚ñÅ
wandb:               l2_norm/grad/model.transformer.layers.0.attn_pre.weight ‚ñà‚ñÉ‚ñÅ‚ñÅ
wandb:                l2_norm/grad/model.transformer.layers.0.ffn.expert_sel ‚ñà‚ñÖ‚ñÇ‚ñÅ
wandb:                      l2_norm/grad/model.transformer.layers.0.ffn.keys ‚ñà‚ñÖ‚ñÇ‚ñÅ
wandb:                    l2_norm/grad/model.transformer.layers.0.ffn.values ‚ñà‚ñÑ‚ñÇ‚ñÅ
wandb:               l2_norm/grad/model.transformer.layers.0.ffn_post.weight ‚ñà‚ñÉ‚ñÅ‚ñÇ
wandb:                l2_norm/grad/model.transformer.layers.0.ffn_pre.weight ‚ñà‚ñÖ‚ñÇ‚ñÅ
wandb:            l2_norm/grad/model.transformer.layers.1.attention.k.weight ‚ñà‚ñÑ‚ñÉ‚ñÅ
wandb:                   l2_norm/grad/model.transformer.layers.1.attention.o ‚ñà‚ñÑ‚ñÇ‚ñÅ
wandb:            l2_norm/grad/model.transformer.layers.1.attention.q.weight ‚ñà‚ñÑ‚ñÇ‚ñÅ
wandb:               l2_norm/grad/model.transformer.layers.1.attention.sel_o ‚ñÜ‚ñá‚ñÅ‚ñà
wandb:               l2_norm/grad/model.transformer.layers.1.attention.sel_v ‚ñà‚ñÜ‚ñÜ‚ñÅ
wandb:                   l2_norm/grad/model.transformer.layers.1.attention.v ‚ñà‚ñÑ‚ñÉ‚ñÅ
wandb:              l2_norm/grad/model.transformer.layers.1.attn_post.weight ‚ñà‚ñÇ‚ñÅ‚ñÜ
wandb:               l2_norm/grad/model.transformer.layers.1.attn_pre.weight ‚ñà‚ñÉ‚ñÉ‚ñÅ
wandb:                l2_norm/grad/model.transformer.layers.1.ffn.expert_sel ‚ñà‚ñÖ‚ñÇ‚ñÅ
wandb:                      l2_norm/grad/model.transformer.layers.1.ffn.keys ‚ñà‚ñÜ‚ñÇ‚ñÅ
wandb:                    l2_norm/grad/model.transformer.layers.1.ffn.values ‚ñà‚ñÖ‚ñÇ‚ñÅ
wandb:               l2_norm/grad/model.transformer.layers.1.ffn_post.weight ‚ñà‚ñÉ‚ñÅ‚ñÇ
wandb:                l2_norm/grad/model.transformer.layers.1.ffn_pre.weight ‚ñà‚ñÖ‚ñÇ‚ñÅ
wandb:                                 l2_norm/grad/model.transformer.router ‚ñÅ‚ñÉ‚ñÖ‚ñà
wandb:                                    l2_norm/grad/model.transformer.tau ‚ñÇ‚ñà‚ñÖ‚ñÅ
wandb:                                 l2_norm/moment/model.embedding.weight ‚ñà‚ñà‚ñÉ‚ñÅ
wandb:                                     l2_norm/moment/model.lm_head.bias ‚ñÅ‚ñÜ‚ñà‚ñá
wandb:                                   l2_norm/moment/model.lm_head.weight ‚ñÅ‚ñÖ‚ñá‚ñà
wandb:                                  l2_norm/moment/model.out_norm.weight ‚ñÅ‚ñÇ‚ñÑ‚ñà
wandb:          l2_norm/moment/model.transformer.layers.0.attention.k.weight ‚ñà‚ñà‚ñÑ‚ñÅ
wandb:                 l2_norm/moment/model.transformer.layers.0.attention.o ‚ñà‚ñà‚ñÑ‚ñÅ
wandb:          l2_norm/moment/model.transformer.layers.0.attention.q.weight ‚ñà‚ñà‚ñÑ‚ñÅ
wandb:             l2_norm/moment/model.transformer.layers.0.attention.sel_o ‚ñà‚ñà‚ñÉ‚ñÅ
wandb:             l2_norm/moment/model.transformer.layers.0.attention.sel_v ‚ñÇ‚ñá‚ñÅ‚ñà
wandb:                 l2_norm/moment/model.transformer.layers.0.attention.v ‚ñà‚ñá‚ñÉ‚ñÅ
wandb:            l2_norm/moment/model.transformer.layers.0.attn_post.weight ‚ñà‚ñà‚ñÑ‚ñÅ
wandb:             l2_norm/moment/model.transformer.layers.0.attn_pre.weight ‚ñà‚ñá‚ñÉ‚ñÅ
wandb:              l2_norm/moment/model.transformer.layers.0.ffn.expert_sel ‚ñà‚ñà‚ñÑ‚ñÅ
wandb:                    l2_norm/moment/model.transformer.layers.0.ffn.keys ‚ñà‚ñà‚ñÑ‚ñÅ
wandb:                  l2_norm/moment/model.transformer.layers.0.ffn.values ‚ñà‚ñà‚ñÑ‚ñÅ
wandb:             l2_norm/moment/model.transformer.layers.0.ffn_post.weight ‚ñà‚ñá‚ñÇ‚ñÅ
wandb:              l2_norm/moment/model.transformer.layers.0.ffn_pre.weight ‚ñà‚ñà‚ñÑ‚ñÅ
wandb:          l2_norm/moment/model.transformer.layers.1.attention.k.weight ‚ñÉ‚ñà‚ñÑ‚ñÅ
wandb:                 l2_norm/moment/model.transformer.layers.1.attention.o ‚ñÅ‚ñà‚ñÖ‚ñÇ
wandb:          l2_norm/moment/model.transformer.layers.1.attention.q.weight ‚ñÇ‚ñà‚ñÉ‚ñÅ
wandb:             l2_norm/moment/model.transformer.layers.1.attention.sel_o ‚ñÅ‚ñà‚ñà‚ñá
wandb:             l2_norm/moment/model.transformer.layers.1.attention.sel_v ‚ñÇ‚ñà‚ñÖ‚ñÅ
wandb:                 l2_norm/moment/model.transformer.layers.1.attention.v ‚ñÉ‚ñà‚ñÑ‚ñÅ
wandb:            l2_norm/moment/model.transformer.layers.1.attn_post.weight ‚ñÅ‚ñÜ‚ñÜ‚ñà
wandb:             l2_norm/moment/model.transformer.layers.1.attn_pre.weight ‚ñÑ‚ñà‚ñÑ‚ñÅ
wandb:              l2_norm/moment/model.transformer.layers.1.ffn.expert_sel ‚ñÅ‚ñà‚ñÖ‚ñÅ
wandb:                    l2_norm/moment/model.transformer.layers.1.ffn.keys ‚ñÅ‚ñà‚ñÖ‚ñÅ
wandb:                  l2_norm/moment/model.transformer.layers.1.ffn.values ‚ñÇ‚ñà‚ñÖ‚ñÅ
wandb:             l2_norm/moment/model.transformer.layers.1.ffn_post.weight ‚ñÅ‚ñà‚ñÖ‚ñÅ
wandb:              l2_norm/moment/model.transformer.layers.1.ffn_pre.weight ‚ñÅ‚ñà‚ñÖ‚ñÅ
wandb:                               l2_norm/moment/model.transformer.router ‚ñÅ‚ñÇ‚ñÖ‚ñà
wandb:                                  l2_norm/moment/model.transformer.tau ‚ñÅ‚ñÜ‚ñà‚ñÖ
wandb:                                  l2_norm/param/model.embedding.weight ‚ñà‚ñá‚ñÑ‚ñÅ
wandb:                                      l2_norm/param/model.lm_head.bias ‚ñà‚ñÖ‚ñÅ‚ñÅ
wandb:                                    l2_norm/param/model.lm_head.weight ‚ñà‚ñÜ‚ñÑ‚ñÅ
wandb:                                   l2_norm/param/model.out_norm.weight ‚ñÅ‚ñÅ‚ñÉ‚ñà
wandb:           l2_norm/param/model.transformer.layers.0.attention.k.weight ‚ñÅ‚ñÅ‚ñÉ‚ñà
wandb:                  l2_norm/param/model.transformer.layers.0.attention.o ‚ñÅ‚ñÅ‚ñÉ‚ñà
wandb:           l2_norm/param/model.transformer.layers.0.attention.q.weight ‚ñÅ‚ñÅ‚ñÉ‚ñà
wandb:              l2_norm/param/model.transformer.layers.0.attention.sel_o ‚ñá‚ñà‚ñÑ‚ñÅ
wandb:              l2_norm/param/model.transformer.layers.0.attention.sel_v ‚ñÅ‚ñÖ‚ñà‚ñá
wandb:                  l2_norm/param/model.transformer.layers.0.attention.v ‚ñÅ‚ñÇ‚ñÖ‚ñà
wandb:             l2_norm/param/model.transformer.layers.0.attn_post.weight ‚ñà‚ñÜ‚ñÅ‚ñÖ
wandb:              l2_norm/param/model.transformer.layers.0.attn_pre.weight ‚ñà‚ñá‚ñÖ‚ñÅ
wandb:               l2_norm/param/model.transformer.layers.0.ffn.expert_sel ‚ñÅ‚ñÇ‚ñÑ‚ñà
wandb:                     l2_norm/param/model.transformer.layers.0.ffn.keys ‚ñÅ‚ñÇ‚ñÑ‚ñà
wandb:                   l2_norm/param/model.transformer.layers.0.ffn.values ‚ñÅ‚ñÇ‚ñÑ‚ñà
wandb:              l2_norm/param/model.transformer.layers.0.ffn_post.weight ‚ñÅ‚ñÅ‚ñÉ‚ñà
wandb:               l2_norm/param/model.transformer.layers.0.ffn_pre.weight ‚ñà‚ñÖ‚ñÇ‚ñÅ
wandb:           l2_norm/param/model.transformer.layers.1.attention.k.weight ‚ñÅ‚ñÇ‚ñÖ‚ñà
wandb:                  l2_norm/param/model.transformer.layers.1.attention.o ‚ñÅ‚ñÇ‚ñÑ‚ñà
wandb:           l2_norm/param/model.transformer.layers.1.attention.q.weight ‚ñÅ‚ñÇ‚ñÑ‚ñà
wandb:              l2_norm/param/model.transformer.layers.1.attention.sel_o ‚ñÅ‚ñÇ‚ñÅ‚ñà
wandb:              l2_norm/param/model.transformer.layers.1.attention.sel_v ‚ñá‚ñà‚ñÜ‚ñÅ
wandb:                  l2_norm/param/model.transformer.layers.1.attention.v ‚ñÅ‚ñÅ‚ñÑ‚ñà
wandb:             l2_norm/param/model.transformer.layers.1.attn_post.weight ‚ñà‚ñà‚ñÜ‚ñÅ
wandb:              l2_norm/param/model.transformer.layers.1.attn_pre.weight ‚ñà‚ñà‚ñÖ‚ñÅ
wandb:               l2_norm/param/model.transformer.layers.1.ffn.expert_sel ‚ñà‚ñá‚ñÑ‚ñÅ
wandb:                     l2_norm/param/model.transformer.layers.1.ffn.keys ‚ñÅ‚ñÅ‚ñÉ‚ñà
wandb:                   l2_norm/param/model.transformer.layers.1.ffn.values ‚ñÅ‚ñÇ‚ñÑ‚ñà
wandb:              l2_norm/param/model.transformer.layers.1.ffn_post.weight ‚ñà‚ñÉ‚ñÅ‚ñá
wandb:               l2_norm/param/model.transformer.layers.1.ffn_pre.weight ‚ñà‚ñá‚ñÑ‚ñÅ
wandb:                                l2_norm/param/model.transformer.router ‚ñÅ‚ñÇ‚ñÉ‚ñà
wandb:                                   l2_norm/param/model.transformer.tau ‚ñÅ‚ñÇ‚ñÖ‚ñà
wandb:                                 l2_norm/update/model.embedding.weight ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:                                     l2_norm/update/model.lm_head.bias ‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:                                   l2_norm/update/model.lm_head.weight ‚ñÅ‚ñÉ‚ñÖ‚ñà
wandb:                                  l2_norm/update/model.out_norm.weight ‚ñÅ‚ñÇ‚ñÖ‚ñà
wandb:          l2_norm/update/model.transformer.layers.0.attention.k.weight ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:                 l2_norm/update/model.transformer.layers.0.attention.o ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:          l2_norm/update/model.transformer.layers.0.attention.q.weight ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:             l2_norm/update/model.transformer.layers.0.attention.sel_o ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:             l2_norm/update/model.transformer.layers.0.attention.sel_v ‚ñÅ‚ñÉ‚ñÖ‚ñà
wandb:                 l2_norm/update/model.transformer.layers.0.attention.v ‚ñÅ‚ñÖ‚ñÜ‚ñà
wandb:            l2_norm/update/model.transformer.layers.0.attn_post.weight ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:             l2_norm/update/model.transformer.layers.0.attn_pre.weight ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:              l2_norm/update/model.transformer.layers.0.ffn.expert_sel ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:                    l2_norm/update/model.transformer.layers.0.ffn.keys ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:                  l2_norm/update/model.transformer.layers.0.ffn.values ‚ñÅ‚ñÖ‚ñá‚ñà
wandb:             l2_norm/update/model.transformer.layers.0.ffn_post.weight ‚ñÅ‚ñÉ‚ñÖ‚ñà
wandb:              l2_norm/update/model.transformer.layers.0.ffn_pre.weight ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:          l2_norm/update/model.transformer.layers.1.attention.k.weight ‚ñÅ‚ñÖ‚ñÜ‚ñà
wandb:                 l2_norm/update/model.transformer.layers.1.attention.o ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:          l2_norm/update/model.transformer.layers.1.attention.q.weight ‚ñÅ‚ñÖ‚ñÜ‚ñà
wandb:             l2_norm/update/model.transformer.layers.1.attention.sel_o ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:             l2_norm/update/model.transformer.layers.1.attention.sel_v ‚ñÅ‚ñÜ‚ñà‚ñá
wandb:                 l2_norm/update/model.transformer.layers.1.attention.v ‚ñÅ‚ñÖ‚ñá‚ñà
wandb:            l2_norm/update/model.transformer.layers.1.attn_post.weight ‚ñÅ‚ñÉ‚ñÖ‚ñà
wandb:             l2_norm/update/model.transformer.layers.1.attn_pre.weight ‚ñÅ‚ñÜ‚ñà‚ñà
wandb:              l2_norm/update/model.transformer.layers.1.ffn.expert_sel ‚ñÅ‚ñÑ‚ñá‚ñà
wandb:                    l2_norm/update/model.transformer.layers.1.ffn.keys ‚ñÅ‚ñÖ‚ñá‚ñà
wandb:                  l2_norm/update/model.transformer.layers.1.ffn.values ‚ñÅ‚ñÜ‚ñà‚ñà
wandb:             l2_norm/update/model.transformer.layers.1.ffn_post.weight ‚ñÅ‚ñÖ‚ñà‚ñà
wandb:              l2_norm/update/model.transformer.layers.1.ffn_pre.weight ‚ñÅ‚ñÖ‚ñá‚ñà
wandb:                               l2_norm/update/model.transformer.router ‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:                                  l2_norm/update/model.transformer.tau ‚ñÅ‚ñÑ‚ñá‚ñà
wandb:                                                      loss/train/total ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                              lr-DecoupledAdamW/group0 ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                                                  memory/alloc_retries ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             memory/current_active_mem ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                          memory/current_allocated_mem ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                           memory/current_inactive_mem ‚ñÅ‚ñà‚ñÑ‚ñÅ‚ñÑ‚ñà‚ñÅ‚ñÇ‚ñÖ‚ñà‚ñÑ‚ñà‚ñÖ‚ñà‚ñÖ‚ñà‚ñÖ‚ñà‚ñÖ‚ñà‚ñÖ‚ñà‚ñÖ‚ñà‚ñÖ‚ñà‚ñÖ‚ñà‚ñÖ‚ñà‚ñÖ‚ñà‚ñÖ‚ñà‚ñÖ‚ñà‚ñÖ‚ñà‚ñÖ‚ñà
wandb:                                           memory/current_reserved_mem ‚ñÅ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                memory/peak_active_mem ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                             memory/peak_allocated_mem ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                              memory/peak_inactive_mem ‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                              memory/peak_reserved_mem ‚ñÅ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                  metrics/exit_entropy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà
wandb:                                               metrics/shannon_entropy ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ
wandb:                                    metrics/train/LanguageCrossEntropy ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                      metrics/train/LanguagePerplexity ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                           metrics/train/TokenAccuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                                            throughput/batches_per_sec ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                     throughput/device/batches_per_sec ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                     throughput/device/samples_per_sec ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                      throughput/device/tokens_per_sec ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                            throughput/samples_per_sec ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                             throughput/tokens_per_sec ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                            time/batch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:                                                   time/batch_in_epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:                                                            time/epoch ‚ñÅ‚ñÅ
wandb:                                               time/remaining_estimate ‚ñÅ‚ñÉ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                           time/sample ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:                                                  time/sample_in_epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:                                                            time/token ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:                                                   time/token_in_epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:                                                            time/total ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                                                            time/train ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                                                              time/val ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                  trainer/device_train_microbatch_size ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                                         activations/average/_output.0 -0.01257
wandb:                          activations/average/model.embedding_output.0 0.00039
wandb:                          activations/average/model.embedding_output.1 7e-05
wandb:                             activations/average/model.lm_head_input.0 -0.07057
wandb:                            activations/average/model.lm_head_output.0 -0.01239
wandb:                            activations/average/model.lm_head_output.1 -0.01276
wandb:                            activations/average/model.out_norm_input.0 -0.02313
wandb:                           activations/average/model.out_norm_output.0 -0.0721
wandb:                           activations/average/model.out_norm_output.1 -0.06904
wandb:    activations/average/model.transformer.layers.0.attention.k_input.0 0.0047
wandb:   activations/average/model.transformer.layers.0.attention.k_output.0 -0.00494
wandb:   activations/average/model.transformer.layers.0.attention.k_output.1 -0.0036
wandb:   activations/average/model.transformer.layers.0.attention.pe_input.0 0.00027
wandb:   activations/average/model.transformer.layers.0.attention.pe_input.1 -0.00183
wandb:  activations/average/model.transformer.layers.0.attention.pe_output.0 -0.00024
wandb:  activations/average/model.transformer.layers.0.attention.pe_output.1 -0.00048
wandb:    activations/average/model.transformer.layers.0.attention.q_input.0 0.0047
wandb:   activations/average/model.transformer.layers.0.attention.q_output.0 0.00024
wandb:   activations/average/model.transformer.layers.0.attention.q_output.1 0.00031
wandb:      activations/average/model.transformer.layers.0.attention_input.0 0.0047
wandb:      activations/average/model.transformer.layers.0.attention_input.1 0.0047
wandb:      activations/average/model.transformer.layers.0.attention_input.2 0.0047
wandb:     activations/average/model.transformer.layers.0.attention_output.0 -0.0003
wandb:      activations/average/model.transformer.layers.0.attn_post_input.0 -0.0003
wandb:     activations/average/model.transformer.layers.0.attn_post_output.0 -0.02905
wandb:     activations/average/model.transformer.layers.0.attn_post_output.1 -0.03418
wandb:       activations/average/model.transformer.layers.0.attn_pre_input.0 0.00023
wandb:      activations/average/model.transformer.layers.0.attn_pre_output.0 0.00807
wandb:      activations/average/model.transformer.layers.0.attn_pre_output.1 0.00133
wandb:            activations/average/model.transformer.layers.0.ffn_input.0 -0.032
wandb:            activations/average/model.transformer.layers.0.ffn_input.1 -0.032
wandb:           activations/average/model.transformer.layers.0.ffn_output.0 -0.00175
wandb:       activations/average/model.transformer.layers.0.ffn_post_input.0 -0.00175
wandb:      activations/average/model.transformer.layers.0.ffn_post_output.0 -0.03662
wandb:      activations/average/model.transformer.layers.0.ffn_post_output.1 -0.03198
wandb:        activations/average/model.transformer.layers.0.ffn_pre_input.0 -0.00768
wandb:       activations/average/model.transformer.layers.0.ffn_pre_output.0 -0.02847
wandb:       activations/average/model.transformer.layers.0.ffn_pre_output.1 -0.03554
wandb:                activations/average/model.transformer.layers.0_input.0 -0.02313
wandb:                activations/average/model.transformer.layers.0_input.2 0.00023
wandb:                activations/average/model.transformer.layers.0_input.3 0.00016
wandb:                activations/average/model.transformer.layers.0_input.4 1.45849
wandb:                activations/average/model.transformer.layers.0_input.5 1.00033
wandb:               activations/average/model.transformer.layers.0_output.0 -0.02313
wandb:               activations/average/model.transformer.layers.0_output.4 0.48047
wandb:    activations/average/model.transformer.layers.1.attention.k_input.0 -0.04456
wandb:   activations/average/model.transformer.layers.1.attention.k_output.0 0.01532
wandb:   activations/average/model.transformer.layers.1.attention.k_output.1 0.01221
wandb:   activations/average/model.transformer.layers.1.attention.pe_input.0 0.01219
wandb:   activations/average/model.transformer.layers.1.attention.pe_input.1 0.01115
wandb:  activations/average/model.transformer.layers.1.attention.pe_output.0 0.0087
wandb:  activations/average/model.transformer.layers.1.attention.pe_output.1 0.00235
wandb:    activations/average/model.transformer.layers.1.attention.q_input.0 -0.03842
wandb:   activations/average/model.transformer.layers.1.attention.q_output.0 0.01392
wandb:   activations/average/model.transformer.layers.1.attention.q_output.1 0.01349
wandb:      activations/average/model.transformer.layers.1.attention_input.0 -0.03842
wandb:      activations/average/model.transformer.layers.1.attention_input.1 -0.04456
wandb:      activations/average/model.transformer.layers.1.attention_input.2 -0.04456
wandb:     activations/average/model.transformer.layers.1.attention_output.0 -0.00021
wandb:      activations/average/model.transformer.layers.1.attn_post_input.0 -0.00021
wandb:     activations/average/model.transformer.layers.1.attn_post_output.0 -0.00476
wandb:     activations/average/model.transformer.layers.1.attn_post_output.1 -0.01007
wandb:       activations/average/model.transformer.layers.1.attn_pre_input.0 -0.01076
wandb:      activations/average/model.transformer.layers.1.attn_pre_output.0 -0.04354
wandb:      activations/average/model.transformer.layers.1.attn_pre_output.1 -0.04558
wandb:            activations/average/model.transformer.layers.1.ffn_input.0 -0.02801
wandb:            activations/average/model.transformer.layers.1.ffn_input.1 -0.02801
wandb:           activations/average/model.transformer.layers.1.ffn_output.0 -0.00427
wandb:       activations/average/model.transformer.layers.1.ffn_post_input.0 -0.00427
wandb:      activations/average/model.transformer.layers.1.ffn_post_output.0 -0.08496
wandb:      activations/average/model.transformer.layers.1.ffn_post_output.1 -0.07959
wandb:        activations/average/model.transformer.layers.1.ffn_pre_input.0 -0.00995
wandb:       activations/average/model.transformer.layers.1.ffn_pre_output.0 -0.03099
wandb:       activations/average/model.transformer.layers.1.ffn_pre_output.1 -0.03732
wandb:                activations/average/model.transformer.layers.1_input.0 -0.01076
wandb:                activations/average/model.transformer.layers.1_input.2 0.00023
wandb:                activations/average/model.transformer.layers.1_input.3 0.00016
wandb:                activations/average/model.transformer.layers.1_input.4 0.97795
wandb:                activations/average/model.transformer.layers.1_input.5 1.00033
wandb:               activations/average/model.transformer.layers.1_output.0 -0.02313
wandb:               activations/average/model.transformer.layers.1_output.4 0.47656
wandb:                         activations/average/model.transformer_input.0 0.00023
wandb:                         activations/average/model.transformer_input.1 0.00023
wandb:                        activations/average/model.transformer_output.0 -0.02313
wandb:                                    activations/average/model_output.0 -0.01257
wandb:                                        activations/kurtosis/_output.0 3.08899
wandb:                         activations/kurtosis/model.embedding_output.0 2.97924
wandb:                         activations/kurtosis/model.embedding_output.1 2.98151
wandb:                            activations/kurtosis/model.lm_head_input.0 2.85458
wandb:                           activations/kurtosis/model.lm_head_output.0 3.10314
wandb:                           activations/kurtosis/model.lm_head_output.1 3.07485
wandb:                           activations/kurtosis/model.out_norm_input.0 2.85413
wandb:                          activations/kurtosis/model.out_norm_output.0 2.84273
wandb:                          activations/kurtosis/model.out_norm_output.1 2.86642
wandb:   activations/kurtosis/model.transformer.layers.0.attention.k_input.0 2.98039
wandb:  activations/kurtosis/model.transformer.layers.0.attention.k_output.0 2.94003
wandb:  activations/kurtosis/model.transformer.layers.0.attention.k_output.1 2.93466
wandb:  activations/kurtosis/model.transformer.layers.0.attention.pe_input.0 2.87539
wandb:  activations/kurtosis/model.transformer.layers.0.attention.pe_input.1 2.82036
wandb: activations/kurtosis/model.transformer.layers.0.attention.pe_output.0 2.86009
wandb: activations/kurtosis/model.transformer.layers.0.attention.pe_output.1 2.83246
wandb:   activations/kurtosis/model.transformer.layers.0.attention.q_input.0 2.98039
wandb:  activations/kurtosis/model.transformer.layers.0.attention.q_output.0 2.99306
wandb:  activations/kurtosis/model.transformer.layers.0.attention.q_output.1 2.98103
wandb:     activations/kurtosis/model.transformer.layers.0.attention_input.0 2.98039
wandb:     activations/kurtosis/model.transformer.layers.0.attention_input.1 2.98039
wandb:     activations/kurtosis/model.transformer.layers.0.attention_input.2 2.98039
wandb:    activations/kurtosis/model.transformer.layers.0.attention_output.0 nan
wandb:     activations/kurtosis/model.transformer.layers.0.attn_post_input.0 nan
wandb:    activations/kurtosis/model.transformer.layers.0.attn_post_output.0 nan
wandb:    activations/kurtosis/model.transformer.layers.0.attn_post_output.1 nan
wandb:      activations/kurtosis/model.transformer.layers.0.attn_pre_input.0 2.98037
wandb:     activations/kurtosis/model.transformer.layers.0.attn_pre_output.0 2.97925
wandb:     activations/kurtosis/model.transformer.layers.0.attn_pre_output.1 2.98153
wandb:           activations/kurtosis/model.transformer.layers.0.ffn_input.0 3.05183
wandb:           activations/kurtosis/model.transformer.layers.0.ffn_input.1 3.05183
wandb:          activations/kurtosis/model.transformer.layers.0.ffn_output.0 2.92312
wandb:      activations/kurtosis/model.transformer.layers.0.ffn_post_input.0 2.92312
wandb:     activations/kurtosis/model.transformer.layers.0.ffn_post_output.0 2.93478
wandb:     activations/kurtosis/model.transformer.layers.0.ffn_post_output.1 2.91257
wandb:       activations/kurtosis/model.transformer.layers.0.ffn_pre_input.0 3.05169
wandb:      activations/kurtosis/model.transformer.layers.0.ffn_pre_output.0 3.06586
wandb:      activations/kurtosis/model.transformer.layers.0.ffn_pre_output.1 3.03779
wandb:               activations/kurtosis/model.transformer.layers.0_input.0 2.85413
wandb:               activations/kurtosis/model.transformer.layers.0_input.2 2.98037
wandb:               activations/kurtosis/model.transformer.layers.0_input.3 3.06143
wandb:               activations/kurtosis/model.transformer.layers.0_input.4 2.83184
wandb:               activations/kurtosis/model.transformer.layers.0_input.5 nan
wandb:              activations/kurtosis/model.transformer.layers.0_output.0 2.85413
wandb:              activations/kurtosis/model.transformer.layers.0_output.4 2.63732
wandb:   activations/kurtosis/model.transformer.layers.1.attention.k_input.0 2.8863
wandb:  activations/kurtosis/model.transformer.layers.1.attention.k_output.0 2.98614
wandb:  activations/kurtosis/model.transformer.layers.1.attention.k_output.1 2.9615
wandb:  activations/kurtosis/model.transformer.layers.1.attention.pe_input.0 nan
wandb:  activations/kurtosis/model.transformer.layers.1.attention.pe_input.1 2.84613
wandb: activations/kurtosis/model.transformer.layers.1.attention.pe_output.0 nan
wandb: activations/kurtosis/model.transformer.layers.1.attention.pe_output.1 2.84497
wandb:   activations/kurtosis/model.transformer.layers.1.attention.q_input.0 nan
wandb:  activations/kurtosis/model.transformer.layers.1.attention.q_output.0 nan
wandb:  activations/kurtosis/model.transformer.layers.1.attention.q_output.1 nan
wandb:     activations/kurtosis/model.transformer.layers.1.attention_input.0 nan
wandb:     activations/kurtosis/model.transformer.layers.1.attention_input.1 2.8863
wandb:     activations/kurtosis/model.transformer.layers.1.attention_input.2 2.8863
wandb:    activations/kurtosis/model.transformer.layers.1.attention_output.0 nan
wandb:     activations/kurtosis/model.transformer.layers.1.attn_post_input.0 nan
wandb:    activations/kurtosis/model.transformer.layers.1.attn_post_output.0 nan
wandb:    activations/kurtosis/model.transformer.layers.1.attn_post_output.1 nan
wandb:      activations/kurtosis/model.transformer.layers.1.attn_pre_input.0 2.88615
wandb:     activations/kurtosis/model.transformer.layers.1.attn_pre_output.0 2.89575
wandb:     activations/kurtosis/model.transformer.layers.1.attn_pre_output.1 2.87685
wandb:           activations/kurtosis/model.transformer.layers.1.ffn_input.0 nan
wandb:           activations/kurtosis/model.transformer.layers.1.ffn_input.1 nan
wandb:          activations/kurtosis/model.transformer.layers.1.ffn_output.0 nan
wandb:      activations/kurtosis/model.transformer.layers.1.ffn_post_input.0 nan
wandb:     activations/kurtosis/model.transformer.layers.1.ffn_post_output.0 nan
wandb:     activations/kurtosis/model.transformer.layers.1.ffn_post_output.1 nan
wandb:       activations/kurtosis/model.transformer.layers.1.ffn_pre_input.0 2.88467
wandb:      activations/kurtosis/model.transformer.layers.1.ffn_pre_output.0 2.88421
wandb:      activations/kurtosis/model.transformer.layers.1.ffn_pre_output.1 2.88522
wandb:               activations/kurtosis/model.transformer.layers.1_input.0 2.88615
wandb:               activations/kurtosis/model.transformer.layers.1_input.2 2.98037
wandb:               activations/kurtosis/model.transformer.layers.1_input.3 3.06143
wandb:               activations/kurtosis/model.transformer.layers.1_input.4 3.0109
wandb:               activations/kurtosis/model.transformer.layers.1_input.5 nan
wandb:              activations/kurtosis/model.transformer.layers.1_output.0 2.85413
wandb:              activations/kurtosis/model.transformer.layers.1_output.4 2.95298
wandb:                        activations/kurtosis/model.transformer_input.0 2.98037
wandb:                        activations/kurtosis/model.transformer_input.1 2.98037
wandb:                       activations/kurtosis/model.transformer_output.0 2.85413
wandb:                                   activations/kurtosis/model_output.0 3.08899
wandb:                                         activations/l2_norm/_output.0 128.23232
wandb:                          activations/l2_norm/model.embedding_output.0 0.99204
wandb:                          activations/l2_norm/model.embedding_output.1 0.99351
wandb:                             activations/l2_norm/model.lm_head_input.0 20.29751
wandb:                            activations/l2_norm/model.lm_head_output.0 128.22861
wandb:                            activations/l2_norm/model.lm_head_output.1 128.23602
wandb:                            activations/l2_norm/model.out_norm_input.0 6.51133
wandb:                           activations/l2_norm/model.out_norm_output.0 20.29781
wandb:                           activations/l2_norm/model.out_norm_output.1 20.29722
wandb:    activations/l2_norm/model.transformer.layers.0.attention.k_input.0 20.25526
wandb:   activations/l2_norm/model.transformer.layers.0.attention.k_output.0 6.44591
wandb:   activations/l2_norm/model.transformer.layers.0.attention.k_output.1 6.43783
wandb:   activations/l2_norm/model.transformer.layers.0.attention.pe_input.0 0.74977
wandb:   activations/l2_norm/model.transformer.layers.0.attention.pe_input.1 0.74477
wandb:  activations/l2_norm/model.transformer.layers.0.attention.pe_output.0 0.74979
wandb:  activations/l2_norm/model.transformer.layers.0.attention.pe_output.1 0.7448
wandb:    activations/l2_norm/model.transformer.layers.0.attention.q_input.0 20.25526
wandb:   activations/l2_norm/model.transformer.layers.0.attention.q_output.0 6.46707
wandb:   activations/l2_norm/model.transformer.layers.0.attention.q_output.1 6.43464
wandb:      activations/l2_norm/model.transformer.layers.0.attention_input.0 20.25526
wandb:      activations/l2_norm/model.transformer.layers.0.attention_input.1 20.25526
wandb:      activations/l2_norm/model.transformer.layers.0.attention_input.2 20.25526
wandb:     activations/l2_norm/model.transformer.layers.0.attention_output.0 0.19126
wandb:      activations/l2_norm/model.transformer.layers.0.attn_post_input.0 0.19126
wandb:     activations/l2_norm/model.transformer.layers.0.attn_post_output.0 19.16676
wandb:     activations/l2_norm/model.transformer.layers.0.attn_post_output.1 19.01989
wandb:       activations/l2_norm/model.transformer.layers.0.attn_pre_input.0 0.99278
wandb:      activations/l2_norm/model.transformer.layers.0.attn_pre_output.0 20.2552
wandb:      activations/l2_norm/model.transformer.layers.0.attn_pre_output.1 20.25531
wandb:            activations/l2_norm/model.transformer.layers.0.ffn_input.0 20.29607
wandb:            activations/l2_norm/model.transformer.layers.0.ffn_input.1 20.29607
wandb:           activations/l2_norm/model.transformer.layers.0.ffn_output.0 1.0204
wandb:       activations/l2_norm/model.transformer.layers.0.ffn_post_input.0 1.0204
wandb:      activations/l2_norm/model.transformer.layers.0.ffn_post_output.0 20.25947
wandb:      activations/l2_norm/model.transformer.layers.0.ffn_post_output.1 20.25924
wandb:        activations/l2_norm/model.transformer.layers.0.ffn_pre_input.0 4.88322
wandb:       activations/l2_norm/model.transformer.layers.0.ffn_pre_output.0 20.29608
wandb:       activations/l2_norm/model.transformer.layers.0.ffn_pre_output.1 20.29606
wandb:                activations/l2_norm/model.transformer.layers.0_input.0 6.51133
wandb:                activations/l2_norm/model.transformer.layers.0_input.2 0.99278
wandb:                activations/l2_norm/model.transformer.layers.0_input.3 0.36133
wandb:                activations/l2_norm/model.transformer.layers.0_input.4 46.68598
wandb:                activations/l2_norm/model.transformer.layers.0_input.5 1.00033
wandb:               activations/l2_norm/model.transformer.layers.0_output.0 6.51133
wandb:               activations/l2_norm/model.transformer.layers.0_output.4 15.39077
wandb:    activations/l2_norm/model.transformer.layers.1.attention.k_input.0 20.29636
wandb:   activations/l2_norm/model.transformer.layers.1.attention.k_output.0 6.39627
wandb:   activations/l2_norm/model.transformer.layers.1.attention.k_output.1 6.4734
wandb:   activations/l2_norm/model.transformer.layers.1.attention.pe_input.0 0.66005
wandb:   activations/l2_norm/model.transformer.layers.1.attention.pe_input.1 0.74254
wandb:  activations/l2_norm/model.transformer.layers.1.attention.pe_output.0 0.66007
wandb:  activations/l2_norm/model.transformer.layers.1.attention.pe_output.1 0.74256
wandb:    activations/l2_norm/model.transformer.layers.1.attention.q_input.0 17.90796
wandb:   activations/l2_norm/model.transformer.layers.1.attention.q_output.0 5.809
wandb:   activations/l2_norm/model.transformer.layers.1.attention.q_output.1 5.74712
wandb:      activations/l2_norm/model.transformer.layers.1.attention_input.0 17.90796
wandb:      activations/l2_norm/model.transformer.layers.1.attention_input.1 20.29636
wandb:      activations/l2_norm/model.transformer.layers.1.attention_input.2 20.29636
wandb:     activations/l2_norm/model.transformer.layers.1.attention_output.0 0.48801
wandb:      activations/l2_norm/model.transformer.layers.1.attn_post_input.0 0.48801
wandb:     activations/l2_norm/model.transformer.layers.1.attn_post_output.0 17.83298
wandb:     activations/l2_norm/model.transformer.layers.1.attn_post_output.1 17.70121
wandb:       activations/l2_norm/model.transformer.layers.1.attn_pre_input.0 4.87089
wandb:      activations/l2_norm/model.transformer.layers.1.attn_pre_output.0 20.29637
wandb:      activations/l2_norm/model.transformer.layers.1.attn_pre_output.1 20.29635
wandb:            activations/l2_norm/model.transformer.layers.1.ffn_input.0 17.9081
wandb:            activations/l2_norm/model.transformer.layers.1.ffn_input.1 17.9081
wandb:           activations/l2_norm/model.transformer.layers.1.ffn_output.0 0.92792
wandb:       activations/l2_norm/model.transformer.layers.1.ffn_post_input.0 0.92792
wandb:      activations/l2_norm/model.transformer.layers.1.ffn_post_output.0 17.94693
wandb:      activations/l2_norm/model.transformer.layers.1.ffn_post_output.1 17.80745
wandb:        activations/l2_norm/model.transformer.layers.1.ffn_pre_input.0 6.07671
wandb:       activations/l2_norm/model.transformer.layers.1.ffn_pre_output.0 20.29661
wandb:       activations/l2_norm/model.transformer.layers.1.ffn_pre_output.1 20.29627
wandb:                activations/l2_norm/model.transformer.layers.1_input.0 4.87089
wandb:                activations/l2_norm/model.transformer.layers.1_input.2 0.99278
wandb:                activations/l2_norm/model.transformer.layers.1_input.3 0.36133
wandb:                activations/l2_norm/model.transformer.layers.1_input.4 31.30088
wandb:                activations/l2_norm/model.transformer.layers.1_input.5 1.00033
wandb:               activations/l2_norm/model.transformer.layers.1_output.0 6.51133
wandb:               activations/l2_norm/model.transformer.layers.1_output.4 15.29092
wandb:                         activations/l2_norm/model.transformer_input.0 0.99278
wandb:                         activations/l2_norm/model.transformer_input.1 0.99278
wandb:                        activations/l2_norm/model.transformer_output.0 6.51133
wandb:                                    activations/l2_norm/model_output.0 128.23232
wandb:                                             activations/max/_output.0 4.09375
wandb:                              activations/max/model.embedding_output.0 0.14494
wandb:                              activations/max/model.embedding_output.1 0.14422
wandb:                                 activations/max/model.lm_head_input.0 2.85237
wandb:                                activations/max/model.lm_head_output.0 4.25
wandb:                                activations/max/model.lm_head_output.1 3.95312
wandb:                                activations/max/model.out_norm_input.0 0.91426
wandb:                               activations/max/model.out_norm_output.0 2.83305
wandb:                               activations/max/model.out_norm_output.1 2.87169
wandb:        activations/max/model.transformer.layers.0.attention.k_input.0 2.94961
wandb:       activations/max/model.transformer.layers.0.attention.k_output.0 1.00781
wandb:       activations/max/model.transformer.layers.0.attention.k_output.1 1.00781
wandb:       activations/max/model.transformer.layers.0.attention.pe_input.0 0.25271
wandb:       activations/max/model.transformer.layers.0.attention.pe_input.1 0.24866
wandb:      activations/max/model.transformer.layers.0.attention.pe_output.0 0.25399
wandb:      activations/max/model.transformer.layers.0.attention.pe_output.1 0.25021
wandb:        activations/max/model.transformer.layers.0.attention.q_input.0 2.94961
wandb:       activations/max/model.transformer.layers.0.attention.q_output.0 1.03125
wandb:       activations/max/model.transformer.layers.0.attention.q_output.1 1.01562
wandb:          activations/max/model.transformer.layers.0.attention_input.0 2.94961
wandb:          activations/max/model.transformer.layers.0.attention_input.1 2.94961
wandb:          activations/max/model.transformer.layers.0.attention_input.2 2.94961
wandb:         activations/max/model.transformer.layers.0.attention_output.0 0.02808
wandb:          activations/max/model.transformer.layers.0.attn_post_input.0 0.02808
wandb:         activations/max/model.transformer.layers.0.attn_post_output.0 2.82812
wandb:         activations/max/model.transformer.layers.0.attn_post_output.1 2.76562
wandb:           activations/max/model.transformer.layers.0.attn_pre_input.0 0.14458
wandb:          activations/max/model.transformer.layers.0.attn_pre_output.0 2.95886
wandb:          activations/max/model.transformer.layers.0.attn_pre_output.1 2.94035
wandb:                activations/max/model.transformer.layers.0.ffn_input.0 2.9707
wandb:                activations/max/model.transformer.layers.0.ffn_input.1 2.9707
wandb:               activations/max/model.transformer.layers.0.ffn_output.0 0.14941
wandb:           activations/max/model.transformer.layers.0.ffn_post_input.0 0.14941
wandb:          activations/max/model.transformer.layers.0.ffn_post_output.0 2.96875
wandb:          activations/max/model.transformer.layers.0.ffn_post_output.1 2.95312
wandb:            activations/max/model.transformer.layers.0.ffn_pre_input.0 0.71465
wandb:           activations/max/model.transformer.layers.0.ffn_pre_output.0 3.00543
wandb:           activations/max/model.transformer.layers.0.ffn_pre_output.1 2.93597
wandb:                    activations/max/model.transformer.layers.0_input.0 0.91426
wandb:                    activations/max/model.transformer.layers.0_input.2 0.14458
wandb:                    activations/max/model.transformer.layers.0_input.3 0.06635
wandb:                    activations/max/model.transformer.layers.0_input.4 1.55859
wandb:                    activations/max/model.transformer.layers.0_input.5 1.00033
wandb:                   activations/max/model.transformer.layers.0_output.0 0.91426
wandb:                   activations/max/model.transformer.layers.0_output.4 0.53125
wandb:        activations/max/model.transformer.layers.1.attention.k_input.0 2.90726
wandb:       activations/max/model.transformer.layers.1.attention.k_output.0 1.05469
wandb:       activations/max/model.transformer.layers.1.attention.k_output.1 1.07031
wandb:       activations/max/model.transformer.layers.1.attention.pe_input.0 0.23101
wandb:       activations/max/model.transformer.layers.1.attention.pe_input.1 0.26251
wandb:      activations/max/model.transformer.layers.1.attention.pe_output.0 0.22633
wandb:      activations/max/model.transformer.layers.1.attention.pe_output.1 0.25089
wandb:        activations/max/model.transformer.layers.1.attention.q_input.0 2.57045
wandb:       activations/max/model.transformer.layers.1.attention.q_output.0 0.89453
wandb:       activations/max/model.transformer.layers.1.attention.q_output.1 0.89844
wandb:          activations/max/model.transformer.layers.1.attention_input.0 2.57045
wandb:          activations/max/model.transformer.layers.1.attention_input.1 2.90726
wandb:          activations/max/model.transformer.layers.1.attention_input.2 2.90726
wandb:         activations/max/model.transformer.layers.1.attention_output.0 0.0708
wandb:          activations/max/model.transformer.layers.1.attn_post_input.0 0.0708
wandb:         activations/max/model.transformer.layers.1.attn_post_output.0 2.59375
wandb:         activations/max/model.transformer.layers.1.attn_post_output.1 2.57812
wandb:           activations/max/model.transformer.layers.1.attn_pre_input.0 0.6976
wandb:          activations/max/model.transformer.layers.1.attn_pre_output.0 2.93436
wandb:          activations/max/model.transformer.layers.1.attn_pre_output.1 2.88016
wandb:                activations/max/model.transformer.layers.1.ffn_input.0 2.56277
wandb:                activations/max/model.transformer.layers.1.ffn_input.1 2.56277
wandb:               activations/max/model.transformer.layers.1.ffn_output.0 0.12598
wandb:           activations/max/model.transformer.layers.1.ffn_post_input.0 0.12598
wandb:          activations/max/model.transformer.layers.1.ffn_post_output.0 2.4375
wandb:          activations/max/model.transformer.layers.1.ffn_post_output.1 2.40625
wandb:            activations/max/model.transformer.layers.1.ffn_pre_input.0 0.86847
wandb:           activations/max/model.transformer.layers.1.ffn_pre_output.0 2.89627
wandb:           activations/max/model.transformer.layers.1.ffn_pre_output.1 2.90289
wandb:                    activations/max/model.transformer.layers.1_input.0 0.6976
wandb:                    activations/max/model.transformer.layers.1_input.2 0.14458
wandb:                    activations/max/model.transformer.layers.1_input.3 0.06635
wandb:                    activations/max/model.transformer.layers.1_input.4 1.0332
wandb:                    activations/max/model.transformer.layers.1_input.5 1.00033
wandb:                   activations/max/model.transformer.layers.1_output.0 0.91426
wandb:                   activations/max/model.transformer.layers.1_output.4 0.52344
wandb:                             activations/max/model.transformer_input.0 0.14458
wandb:                             activations/max/model.transformer_input.1 0.14458
wandb:                            activations/max/model.transformer_output.0 0.91426
wandb:                                        activations/max/model_output.0 4.09375
wandb:                                                   l2_norm/grad/global 0.0014
wandb:                                   l2_norm/grad/model.embedding.weight 0.00015
wandb:                                       l2_norm/grad/model.lm_head.bias 4e-05
wandb:                                     l2_norm/grad/model.lm_head.weight 0.00078
wandb:                                    l2_norm/grad/model.out_norm.weight 4e-05
wandb:            l2_norm/grad/model.transformer.layers.0.attention.k.weight 4e-05
wandb:                   l2_norm/grad/model.transformer.layers.0.attention.o 0.00038
wandb:            l2_norm/grad/model.transformer.layers.0.attention.q.weight 4e-05
wandb:               l2_norm/grad/model.transformer.layers.0.attention.sel_o 9e-05
wandb:               l2_norm/grad/model.transformer.layers.0.attention.sel_v 9e-05
wandb:                   l2_norm/grad/model.transformer.layers.0.attention.v 0.00038
wandb:              l2_norm/grad/model.transformer.layers.0.attn_post.weight 1e-05
wandb:               l2_norm/grad/model.transformer.layers.0.attn_pre.weight 1e-05
wandb:                l2_norm/grad/model.transformer.layers.0.ffn.expert_sel 5e-05
wandb:                      l2_norm/grad/model.transformer.layers.0.ffn.keys 0.00027
wandb:                    l2_norm/grad/model.transformer.layers.0.ffn.values 0.00067
wandb:               l2_norm/grad/model.transformer.layers.0.ffn_post.weight 1e-05
wandb:                l2_norm/grad/model.transformer.layers.0.ffn_pre.weight 1e-05
wandb:            l2_norm/grad/model.transformer.layers.1.attention.k.weight 1e-05
wandb:                   l2_norm/grad/model.transformer.layers.1.attention.o 0.00032
wandb:            l2_norm/grad/model.transformer.layers.1.attention.q.weight 1e-05
wandb:               l2_norm/grad/model.transformer.layers.1.attention.sel_o 7e-05
wandb:               l2_norm/grad/model.transformer.layers.1.attention.sel_v 3e-05
wandb:                   l2_norm/grad/model.transformer.layers.1.attention.v 0.00027
wandb:              l2_norm/grad/model.transformer.layers.1.attn_post.weight 1e-05
wandb:               l2_norm/grad/model.transformer.layers.1.attn_pre.weight 1e-05
wandb:                l2_norm/grad/model.transformer.layers.1.ffn.expert_sel 4e-05
wandb:                      l2_norm/grad/model.transformer.layers.1.ffn.keys 0.00022
wandb:                    l2_norm/grad/model.transformer.layers.1.ffn.values 0.00051
wandb:               l2_norm/grad/model.transformer.layers.1.ffn_post.weight 1e-05
wandb:                l2_norm/grad/model.transformer.layers.1.ffn_pre.weight 0.0
wandb:                                 l2_norm/grad/model.transformer.router 0.00011
wandb:                                    l2_norm/grad/model.transformer.tau 1e-05
wandb:                                 l2_norm/moment/model.embedding.weight 0.0001
wandb:                                     l2_norm/moment/model.lm_head.bias 4e-05
wandb:                                   l2_norm/moment/model.lm_head.weight 0.00063
wandb:                                  l2_norm/moment/model.out_norm.weight 3e-05
wandb:          l2_norm/moment/model.transformer.layers.0.attention.k.weight 3e-05
wandb:                 l2_norm/moment/model.transformer.layers.0.attention.o 0.00025
wandb:          l2_norm/moment/model.transformer.layers.0.attention.q.weight 3e-05
wandb:             l2_norm/moment/model.transformer.layers.0.attention.sel_o 5e-05
wandb:             l2_norm/moment/model.transformer.layers.0.attention.sel_v 7e-05
wandb:                 l2_norm/moment/model.transformer.layers.0.attention.v 0.00021
wandb:            l2_norm/moment/model.transformer.layers.0.attn_post.weight 1e-05
wandb:             l2_norm/moment/model.transformer.layers.0.attn_pre.weight 0.0
wandb:              l2_norm/moment/model.transformer.layers.0.ffn.expert_sel 3e-05
wandb:                    l2_norm/moment/model.transformer.layers.0.ffn.keys 0.00018
wandb:                  l2_norm/moment/model.transformer.layers.0.ffn.values 0.0004
wandb:             l2_norm/moment/model.transformer.layers.0.ffn_post.weight 1e-05
wandb:              l2_norm/moment/model.transformer.layers.0.ffn_pre.weight 0.0
wandb:          l2_norm/moment/model.transformer.layers.1.attention.k.weight 0.0
wandb:                 l2_norm/moment/model.transformer.layers.1.attention.o 0.00017
wandb:          l2_norm/moment/model.transformer.layers.1.attention.q.weight 0.0
wandb:             l2_norm/moment/model.transformer.layers.1.attention.sel_o 3e-05
wandb:             l2_norm/moment/model.transformer.layers.1.attention.sel_v 1e-05
wandb:                 l2_norm/moment/model.transformer.layers.1.attention.v 0.00013
wandb:            l2_norm/moment/model.transformer.layers.1.attn_post.weight 1e-05
wandb:             l2_norm/moment/model.transformer.layers.1.attn_pre.weight 0.0
wandb:              l2_norm/moment/model.transformer.layers.1.ffn.expert_sel 2e-05
wandb:                    l2_norm/moment/model.transformer.layers.1.ffn.keys 0.00013
wandb:                  l2_norm/moment/model.transformer.layers.1.ffn.values 0.00032
wandb:             l2_norm/moment/model.transformer.layers.1.ffn_post.weight 0.0
wandb:              l2_norm/moment/model.transformer.layers.1.ffn_pre.weight 0.0
wandb:                               l2_norm/moment/model.transformer.router 8e-05
wandb:                                  l2_norm/moment/model.transformer.tau 1e-05
wandb:                                  l2_norm/param/model.embedding.weight 221.70644
wandb:                                      l2_norm/param/model.lm_head.bias 6.31625
wandb:                                    l2_norm/param/model.lm_head.weight 127.95715
wandb:                                   l2_norm/param/model.out_norm.weight 20.30485
wandb:           l2_norm/param/model.transformer.layers.0.attention.k.weight 6.47594
wandb:                  l2_norm/param/model.transformer.layers.0.attention.o 22.68863
wandb:           l2_norm/param/model.transformer.layers.0.attention.q.weight 6.48541
wandb:              l2_norm/param/model.transformer.layers.0.attention.sel_o 2.24174
wandb:              l2_norm/param/model.transformer.layers.0.attention.sel_v 2.25992
wandb:                  l2_norm/param/model.transformer.layers.0.attention.v 20.48014
wandb:             l2_norm/param/model.transformer.layers.0.attn_post.weight 20.29767
wandb:              l2_norm/param/model.transformer.layers.0.attn_pre.weight 20.29748
wandb:               l2_norm/param/model.transformer.layers.0.ffn.expert_sel 4.75071
wandb:                     l2_norm/param/model.transformer.layers.0.ffn.keys 15.00094
wandb:                   l2_norm/param/model.transformer.layers.0.ffn.values 7.18316
wandb:              l2_norm/param/model.transformer.layers.0.ffn_post.weight 20.30284
wandb:               l2_norm/param/model.transformer.layers.0.ffn_pre.weight 20.29759
wandb:           l2_norm/param/model.transformer.layers.1.attention.k.weight 6.48556
wandb:                  l2_norm/param/model.transformer.layers.1.attention.o 22.69142
wandb:           l2_norm/param/model.transformer.layers.1.attention.q.weight 6.49083
wandb:              l2_norm/param/model.transformer.layers.1.attention.sel_o 2.24069
wandb:              l2_norm/param/model.transformer.layers.1.attention.sel_v 2.25418
wandb:                  l2_norm/param/model.transformer.layers.1.attention.v 20.49268
wandb:             l2_norm/param/model.transformer.layers.1.attn_post.weight 20.29143
wandb:              l2_norm/param/model.transformer.layers.1.attn_pre.weight 20.29735
wandb:               l2_norm/param/model.transformer.layers.1.ffn.expert_sel 4.75241
wandb:                     l2_norm/param/model.transformer.layers.1.ffn.keys 15.0052
wandb:                   l2_norm/param/model.transformer.layers.1.ffn.values 7.18292
wandb:              l2_norm/param/model.transformer.layers.1.ffn_post.weight 20.29776
wandb:               l2_norm/param/model.transformer.layers.1.ffn_pre.weight 20.29704
wandb:                                l2_norm/param/model.transformer.router 0.36248
wandb:                                   l2_norm/param/model.transformer.tau 1.00089
wandb:                                 l2_norm/update/model.embedding.weight 0.0138
wandb:                                     l2_norm/update/model.lm_head.bias 0.00484
wandb:                                   l2_norm/update/model.lm_head.weight 0.07289
wandb:                                  l2_norm/update/model.out_norm.weight 0.00087
wandb:          l2_norm/update/model.transformer.layers.0.attention.k.weight 0.00728
wandb:                 l2_norm/update/model.transformer.layers.0.attention.o 0.0226
wandb:          l2_norm/update/model.transformer.layers.0.attention.q.weight 0.00741
wandb:             l2_norm/update/model.transformer.layers.0.attention.sel_o 0.00163
wandb:             l2_norm/update/model.transformer.layers.0.attention.sel_v 0.00173
wandb:                 l2_norm/update/model.transformer.layers.0.attention.v 0.01793
wandb:            l2_norm/update/model.transformer.layers.0.attn_post.weight 0.00046
wandb:             l2_norm/update/model.transformer.layers.0.attn_pre.weight 0.00039
wandb:              l2_norm/update/model.transformer.layers.0.ffn.expert_sel 0.0054
wandb:                    l2_norm/update/model.transformer.layers.0.ffn.keys 0.01718
wandb:                  l2_norm/update/model.transformer.layers.0.ffn.values 0.01569
wandb:             l2_norm/update/model.transformer.layers.0.ffn_post.weight 0.0006
wandb:              l2_norm/update/model.transformer.layers.0.ffn_pre.weight 0.00041
wandb:          l2_norm/update/model.transformer.layers.1.attention.k.weight 0.00354
wandb:                 l2_norm/update/model.transformer.layers.1.attention.o 0.01962
wandb:          l2_norm/update/model.transformer.layers.1.attention.q.weight 0.00362
wandb:             l2_norm/update/model.transformer.layers.1.attention.sel_o 0.00136
wandb:             l2_norm/update/model.transformer.layers.1.attention.sel_v 0.00093
wandb:                 l2_norm/update/model.transformer.layers.1.attention.v 0.01607
wandb:            l2_norm/update/model.transformer.layers.1.attn_post.weight 0.0006
wandb:             l2_norm/update/model.transformer.layers.1.attn_pre.weight 0.00028
wandb:              l2_norm/update/model.transformer.layers.1.ffn.expert_sel 0.00485
wandb:                    l2_norm/update/model.transformer.layers.1.ffn.keys 0.01513
wandb:                  l2_norm/update/model.transformer.layers.1.ffn.values 0.01561
wandb:             l2_norm/update/model.transformer.layers.1.ffn_post.weight 0.00039
wandb:              l2_norm/update/model.transformer.layers.1.ffn_pre.weight 0.00037
wandb:                               l2_norm/update/model.transformer.router 0.00095
wandb:                                  l2_norm/update/model.transformer.tau 4e-05
wandb:                                                      loss/train/total 0.00456
wandb:                                              lr-DecoupledAdamW/group0 5e-05
wandb:                                                  memory/alloc_retries 0
wandb:                                             memory/current_active_mem 1.1979
wandb:                                          memory/current_allocated_mem 1.1979
wandb:                                           memory/current_inactive_mem 0.74826
wandb:                                           memory/current_reserved_mem 3.8609
wandb:                                                memory/peak_active_mem 2.8091
wandb:                                             memory/peak_allocated_mem 2.8091
wandb:                                              memory/peak_inactive_mem 0.86756
wandb:                                              memory/peak_reserved_mem 3.8609
wandb:                                                  metrics/exit_entropy 0.66797
wandb:                                               metrics/shannon_entropy 10.56794
wandb:                                    metrics/train/LanguageCrossEntropy 9.3493
wandb:                                      metrics/train/LanguagePerplexity 11490.77051
wandb:                                           metrics/train/TokenAccuracy 0.16961
wandb:                                            throughput/batches_per_sec 0.42011
wandb:                                     throughput/device/batches_per_sec 0.42011
wandb:                                     throughput/device/samples_per_sec 0.84022
wandb:                                      throughput/device/tokens_per_sec 860.38556
wandb:                                            throughput/samples_per_sec 0.84022
wandb:                                             throughput/tokens_per_sec 860.38556
wandb:                                                            time/batch 40
wandb:                                                   time/batch_in_epoch 40
wandb:                                                            time/epoch 0
wandb:                                               time/remaining_estimate 0
wandb:                                                           time/sample 80
wandb:                                                  time/sample_in_epoch 80
wandb:                                                            time/token 81920
wandb:                                                   time/token_in_epoch 81920
wandb:                                                            time/total 0.03436
wandb:                                                            time/train 0.03436
wandb:                                                              time/val 0
wandb:                                  trainer/device_train_microbatch_size 10
wandb: 
wandb: üöÄ View run 87cbuqjw_29jul25_c33chxzl_scale_add=True_prot_emb=True_d_model=412_n_layers=16_n_heads=4_n_experts_ffn=180_n_experts_attn=10_ff_expert_size=10_dropout=0.0 at: https://wandb.ai/camlsys/dyna/runs/mnxj6fh8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/camlsys/dyna
wandb: Synced 5 W&B file(s), 0 media file(s), 320 artifact file(s) and 280 other file(s)
wandb: Find logs at: ./wandb/run-20250729_165630-mnxj6fh8/logs

tensor([[[[0.5412, 0.3965, 0.4201,  ..., 0.4263, 0.3657, 0.5675],
          [0.4882, 0.5574, 0.6118,  ..., 0.3937, 0.5804, 0.4111],
          [0.5167, 0.4593, 0.3433,  ..., 0.3720, 0.4835, 0.5243],
          [0.5955, 0.5903, 0.5765,  ..., 0.3789, 0.5414, 0.4545]],

         [[0.4845, 0.5470, 0.6645,  ..., 0.4671, 0.3549, 0.5675],
          [0.4876, 0.6584, 0.5722,  ..., 0.4852, 0.6261, 0.3711],
          [0.5431, 0.4031, 0.3923,  ..., 0.4865, 0.5499, 0.6758],
          [0.6926, 0.6654, 0.6496,  ..., 0.4292, 0.4230, 0.4306]],

         [[0.4111, 0.5660, 0.3854,  ..., 0.4901, 0.5250, 0.4144],
          [0.4972, 0.5936, 0.5655,  ..., 0.5063, 0.6558, 0.3639],
          [0.5946, 0.4996, 0.4244,  ..., 0.6105, 0.5665, 0.5641],
          [0.7122, 0.5443, 0.5361,  ..., 0.4952, 0.5675, 0.5400]],

         ...,

         [[0.4140, 0.5679, 0.5574,  ..., 0.2705, 0.4530, 0.5708],
          [0.4102, 0.4446, 0.4896,  ..., 0.5383, 0.5917, 0.4168],
          [0.6206, 0.4869, 0.6478,  ..., 0.4671, 0.4511, 0.5679],
          [0.4788, 0.5014, 0.5443,  ..., 0.4182, 0.4996, 0.3495]],

         [[0.5679, 0.5334, 0.4239,  ..., 0.4472, 0.3803, 0.6016],
          [0.4752, 0.5542, 0.6174,  ..., 0.5174, 0.6307, 0.4808],
          [0.4782, 0.3730, 0.5101,  ..., 0.4460, 0.4810, 0.6280],
          [0.5950, 0.6951, 0.5041,  ..., 0.4947, 0.5022, 0.4583]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]],


        [[[0.5487, 0.6243, 0.5988,  ..., 0.3630, 0.4722, 0.5271],
          [0.4830, 0.6118, 0.5605,  ..., 0.4644, 0.5426, 0.3594],
          [0.5545, 0.3174, 0.3057,  ..., 0.5978, 0.5402, 0.7162],
          [0.6137, 0.6081, 0.5257,  ..., 0.4613, 0.4644, 0.4168]],

         [[0.5306, 0.5559, 0.4579,  ..., 0.3259, 0.4788, 0.5856],
          [0.4591, 0.5409, 0.5105,  ..., 0.4050, 0.5363, 0.3739],
          [0.5390, 0.4220, 0.3390,  ..., 0.4969, 0.6558, 0.6935],
          [0.5832, 0.5889, 0.4992,  ..., 0.4036, 0.4458, 0.4977]],

         [[0.6021, 0.5170, 0.4748,  ..., 0.3900, 0.5523, 0.5397],
          [0.5134, 0.3208, 0.6053,  ..., 0.4721, 0.5366, 0.3956],
          [0.6289, 0.3603, 0.4846,  ..., 0.5027, 0.6885, 0.6689],
          [0.5336, 0.4961, 0.4741,  ..., 0.3961, 0.5964, 0.3965]],

         ...,

         [[0.5870, 0.5808, 0.4378,  ..., 0.4625, 0.3844, 0.6628],
          [0.5879, 0.6234, 0.6943,  ..., 0.5344, 0.5390, 0.4017],
          [0.5067, 0.6109, 0.3684,  ..., 0.3191, 0.4083, 0.5879],
          [0.5095, 0.3812, 0.5941,  ..., 0.3576, 0.5007, 0.5689]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],

         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],
       device='cuda:0', requires_grad=True)
tensor([-0.0100, -0.0100, -0.0080,  0.0020, -0.0120,  0.0260,  0.0020,  0.0240,
        -0.0120,  0.0060], device='cuda:0')
selected experts tensor([2076, 1778, 1889, 1814, 1725,  900, 1606, 1126, 1769, 1701],
       device='cuda:0')
total tokens tensor(16384, device='cuda:0')
attn_o for batch tensor([[1797., 1567., 1748., 1616., 1559., 1587., 1651., 1495., 1716., 1648.],
        [2076., 1778., 1889., 1814., 1725.,  900., 1606., 1126., 1769., 1701.],
        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])
